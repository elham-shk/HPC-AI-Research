1)
=== Experiment 161 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006358099099657259
rmse: 0.07973768932980976
mae: 0.039407357876825436
r2: 0.7133161100098739
pearson: 0.8461642050965059

=== Experiment 8 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007635137841996091
rmse: 0.08737927581524174
mae: 0.038927451032472055
r2: 0.65573499518555
pearson: 0.8193746034874974

=== Experiment 148 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060582565704577485
rmse: 0.07783480307971331
mae: 0.030492130599745662
r2: 0.7268358776806905
pearson: 0.8538684734564691

=== Experiment 267 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00650125092278374
rmse: 0.08063033500354405
mae: 0.03255872225415906
r2: 0.7068614573110366
pearson: 0.8419686679368102

=== Experiment 273 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006373905140173475
rmse: 0.07983674054076528
mae: 0.03595789300493828
r2: 0.7126034225997682
pearson: 0.8444146559571832

=== Experiment 87 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007184516873421027
rmse: 0.08476152944243648
mae: 0.039115470975341925
r2: 0.6760532963251442
pearson: 0.8241467524412353

=== Experiment 442 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00741814437314508
rmse: 0.08612865012958859
mae: 0.035640595132079574
r2: 0.6655191351899129
pearson: 0.8348419689549746

=== Experiment 3 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006947999834444289
rmse: 0.08335466294361875
mae: 0.03604791271657584
r2: 0.6867177455134954
pearson: 0.8293163752524175

=== Experiment 323 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006768364863708314
rmse: 0.08227007270999774
mae: 0.03574713115250883
r2: 0.6948174072805837
pearson: 0.8408621186401709

=== Experiment 0 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0075221460123254275
rmse: 0.08673030619296479
mae: 0.03787313848300255
r2: 0.6608297470538929
pearson: 0.8412615529104032

=== Experiment 11 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0066501984327825185
rmse: 0.08154874881187643
mae: 0.03730696384799561
r2: 0.7001454796420044
pearson: 0.8377406465004674

=== Experiment 188 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005988388051188632
rmse: 0.07738467581626632
mae: 0.03116276891389675
r2: 0.7299862184630479
pearson: 0.8548849673508091

=== Experiment 115 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007519508651030455
rmse: 0.08671510047869664
mae: 0.04083161518699712
r2: 0.6609486645138918
pearson: 0.823790237957289

=== Experiment 158 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007000598102412492
rmse: 0.08366957692263355
mae: 0.040044901474070985
r2: 0.6843461127610773
pearson: 0.8282939201238736

=== Experiment 68 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006683780922483104
rmse: 0.08175439390322152
mae: 0.0372904144752622
r2: 0.6986312599621889
pearson: 0.8364162233141607

=== Experiment 251 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006663358232162725
rmse: 0.08162939563761773
mae: 0.04157525614809918
r2: 0.6995521100800213
pearson: 0.8396437345706154

=== Experiment 138 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0063054806272098
rmse: 0.07940705653284096
mae: 0.03796680797566127
r2: 0.715688653773369
pearson: 0.8475845542119828

=== Experiment 225 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006873850248787461
rmse: 0.08290868620830667
mae: 0.039880491282328824
r2: 0.6900611176950334
pearson: 0.831604578395726

=== Experiment 155 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0066587788966343394
rmse: 0.08160134126737341
mae: 0.03580536048043011
r2: 0.6997585903034765
pearson: 0.837582834983912

=== Experiment 176 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006648626735555651
rmse: 0.08153911169221584
mae: 0.03706199514780196
r2: 0.7002163467782068
pearson: 0.8448399855465007

=== Experiment 46 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006799207822976362
rmse: 0.08245730909371445
mae: 0.032990240856295504
r2: 0.6934267118222119
pearson: 0.8458314734103876

=== Experiment 256 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006852575303876578
rmse: 0.0827802833039159
mae: 0.03704221426232305
r2: 0.6910203955972458
pearson: 0.8330908735366903

=== Experiment 305 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006745338142352646
rmse: 0.08213000756333001
mae: 0.04214772424783562
r2: 0.6958556720116756
pearson: 0.8374949402392582

=== Experiment 13 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007490408698238051
rmse: 0.08654714725649859
mae: 0.040021735846563704
r2: 0.6622607685774324
pearson: 0.8190114825999366

=== Experiment 24 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006701509467519465
rmse: 0.08186274773985701
mae: 0.04300834985630307
r2: 0.6978318876694287
pearson: 0.8415205715228792

=== Experiment 137 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006632495543453175
rmse: 0.08144013472148223
mae: 0.03668575315076654
r2: 0.7009436951302268
pearson: 0.8375388235326129

=== Experiment 192 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006234408477447752
rmse: 0.07895827048161422
mae: 0.034607780732773236
r2: 0.7188932657248945
pearson: 0.8482147486400472

=== Experiment 222 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007737168625179935
rmse: 0.08796117680647489
mae: 0.04132971440384343
r2: 0.6511344720789725
pearson: 0.8087711375448525

=== Experiment 224 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006536223658528084
rmse: 0.08084691495986773
mae: 0.03686251436519663
r2: 0.7052845520490022
pearson: 0.8402103370130832

=== Experiment 199 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006535123739606233
rmse: 0.08084011219441889
mae: 0.03601688982186026
r2: 0.705334146909077
pearson: 0.8401387388888792

=== Experiment 499 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006097308742418694
rmse: 0.07808526584714107
mae: 0.03331026113988483
r2: 0.7250750324351469
pearson: 0.8524233657603904

=== Experiment 194 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006492097317272326
rmse: 0.08057355221952378
mae: 0.03638275327868569
r2: 0.7072741893547363
pearson: 0.8416187608794717

=== Experiment 250 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006418842229489764
rmse: 0.08011767738451836
mae: 0.03854661605048445
r2: 0.7105772290208222
pearson: 0.8439406740881911

=== Experiment 289 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006636143517574749
rmse: 0.08146252830335399
mae: 0.037610014011335964
r2: 0.7007792095826813
pearson: 0.8383243720097128

=== Experiment 101 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006962024004258853
rmse: 0.08343874402373787
mae: 0.0386905613465046
r2: 0.6860854018690661
pearson: 0.8305085921528783

=== Experiment 389 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006562224144049829
rmse: 0.08100755609231665
mae: 0.035854200838308696
r2: 0.7041122016005156
pearson: 0.8396168261531136

=== Experiment 436 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0059831115405139445
rmse: 0.07735057556679165
mae: 0.029590559962691866
r2: 0.7302241340069996
pearson: 0.8556593789850714

=== Experiment 60 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007803467061906029
rmse: 0.0883372348554449
mae: 0.038008744060052654
r2: 0.6481451047471665
pearson: 0.8316373869203983

=== Experiment 232 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006553330580983225
rmse: 0.08095264406418869
mae: 0.03266112864326216
r2: 0.7045132084448325
pearson: 0.8398293321246045

=== Experiment 343 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006447777076848407
rmse: 0.08029805151339856
mae: 0.03532142593140357
r2: 0.7092725694886213
pearson: 0.8463217201456471

=== Experiment 393 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006476409479764886
rmse: 0.08047614230170881
mae: 0.03672778670076123
r2: 0.7079815470432016
pearson: 0.8420476495875332

=== Experiment 129 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006750142942131187
rmse: 0.08215925353927692
mae: 0.039812663266246176
r2: 0.6956390257044154
pearson: 0.8362368215168301

=== Experiment 16 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007000431012862659
rmse: 0.08366857840828096
mae: 0.037537234809044424
r2: 0.6843536467553375
pearson: 0.8275176760705617

=== Experiment 142 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006534319578119047
rmse: 0.0808351382637467
mae: 0.03145254422779892
r2: 0.7053704061965959
pearson: 0.8572740871363886

=== Experiment 293 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00680840579406191
rmse: 0.0825130643841441
mae: 0.040897536226213574
r2: 0.6930119793543013
pearson: 0.8343931569983104

=== Experiment 75 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007237513554363509
rmse: 0.08507357729849797
mae: 0.03592056194597356
r2: 0.6736636993070708
pearson: 0.8297784382833304

=== Experiment 365 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0076143143629961545
rmse: 0.08726003875197486
mae: 0.040598314321803976
r2: 0.6566739166885434
pearson: 0.8109116000887986

=== Experiment 243 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007601259646269469
rmse: 0.08718520313831625
mae: 0.04779050982435894
r2: 0.6572625481199292
pearson: 0.8169363322083993

=== Experiment 235 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006815117875030403
rmse: 0.08255372720253401
mae: 0.04459503484213215
r2: 0.6927093345776446
pearson: 0.8384219101175228

=== Experiment 65 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067403310431566445
rmse: 0.0820995191408369
mae: 0.038112887876640425
r2: 0.6960814399106305
pearson: 0.8361261263280236

=== Experiment 336 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0070223845287743
rmse: 0.08379966902544604
mae: 0.036405077410294105
r2: 0.6833637723853629
pearson: 0.8284216777209089

=== Experiment 308 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006775619840821141
rmse: 0.08231415334449563
mae: 0.03640525991331829
r2: 0.6944902835557261
pearson: 0.8345921575117777

=== Experiment 233 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0073777859547701176
rmse: 0.08589403911081442
mae: 0.03591313485873405
r2: 0.6673388785113414
pearson: 0.8343343055670104

=== Experiment 408 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007375743332110051
rmse: 0.08588214792440889
mae: 0.04639634652615225
r2: 0.6674309794680569
pearson: 0.8257324290493215

=== Experiment 423 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00693551748850488
rmse: 0.08327975437346631
mae: 0.04422434199957114
r2: 0.6872805689980029
pearson: 0.8351412717098505

=== Experiment 14 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007645302025810223
rmse: 0.08743741776728212
mae: 0.03883189820434376
r2: 0.6552766966109704
pearson: 0.8097615517762258

=== Experiment 448 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0069991812724423015
rmse: 0.08366110967733037
mae: 0.04055796712492039
r2: 0.6844099970008387
pearson: 0.8285943980666665

=== Experiment 264 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00655945519088129
rmse: 0.08099046357986407
mae: 0.036579074161480644
r2: 0.7042370524801755
pearson: 0.8393195146477771

=== Experiment 352 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068309904734037835
rmse: 0.08264980625145944
mae: 0.042764956620113086
r2: 0.6919936461030537
pearson: 0.8359784281656814

=== Experiment 145 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00686558366952492
rmse: 0.08285881769326015
mae: 0.03815327408906931
r2: 0.6904338541155817
pearson: 0.8409167457124432

=== Experiment 413 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00744964710982363
rmse: 0.08631133824604754
mae: 0.042670364784908384
r2: 0.6640986906584918
pearson: 0.8254052416488886

=== Experiment 493 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006927323314134714
rmse: 0.08323054315655229
mae: 0.04172046110557781
r2: 0.6876500407138217
pearson: 0.8323356449220738

=== Experiment 35 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006606207184787615
rmse: 0.08127857764988027
mae: 0.029378057143146376
r2: 0.7021290256520655
pearson: 0.8686865660423885

=== Experiment 340 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006858086510518642
rmse: 0.08281356477364468
mae: 0.042249566480619276
r2: 0.6907718977154227
pearson: 0.8349573651221426

=== Experiment 359 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007558803983417141
rmse: 0.08694138245632595
mae: 0.03796039981285415
r2: 0.6591768552717745
pearson: 0.8124867112516779

=== Experiment 144 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006957241024679146
rmse: 0.08341007747676024
mae: 0.043203980965341574
r2: 0.6863010643131646
pearson: 0.8384709907151121

=== Experiment 427 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00710524685210647
rmse: 0.08429262632108736
mae: 0.037025558508036134
r2: 0.679627546696813
pearson: 0.8248133339099701

=== Experiment 37 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006699867149042182
rmse: 0.08185271619831673
mae: 0.03297791966715712
r2: 0.6979059390867299
pearson: 0.8460387913826185

=== Experiment 230 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062464703649733905
rmse: 0.0790346149795986
mae: 0.03410875508394833
r2: 0.718349400525206
pearson: 0.8489455085845458

=== Experiment 27 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.008088551647305554
rmse: 0.08993637555130601
mae: 0.04085256115314707
r2: 0.6352907662668235
pearson: 0.813502322763393

=== Experiment 88 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006759402068037511
rmse: 0.0822155828784149
mae: 0.03661932593216491
r2: 0.6952215357925522
pearson: 0.834782721336797

=== Experiment 313 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006957983630871118
rmse: 0.0834145288955774
mae: 0.034475822017979454
r2: 0.6862675805268146
pearson: 0.8292902359861983

=== Experiment 40 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061861543735677255
rmse: 0.07865210978459335
mae: 0.03329902289057817
r2: 0.7210690220305896
pearson: 0.8492718964452273

=== Experiment 473 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006271778529764421
rmse: 0.07919456123853721
mae: 0.035933888470667835
r2: 0.7172082665137538
pearson: 0.8472527240137471

=== Experiment 271 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007168590466534579
rmse: 0.08466752899745321
mae: 0.03840464722556237
r2: 0.6767714110019064
pearson: 0.8245080158065703

=== Experiment 211 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00704113499115167
rmse: 0.08391147115354175
mae: 0.03601308621033588
r2: 0.6825183222894771
pearson: 0.8263395283001652

=== Experiment 131 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006228134822044178
rmse: 0.07891853281735652
mae: 0.029589818039489433
r2: 0.7191761420857947
pearson: 0.852534535629539

=== Experiment 55 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00760713684435136
rmse: 0.08721890187540406
mae: 0.048291343000564445
r2: 0.6569975478451695
pearson: 0.8199742780188035

=== Experiment 328 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007099370744206577
rmse: 0.08425776370285754
mae: 0.04587628717653525
r2: 0.6798924978157463
pearson: 0.837515844609279

=== Experiment 400 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071587893648747125
rmse: 0.08460962926803729
mae: 0.03900426854426393
r2: 0.6772133383619549
pearson: 0.8239864134815

=== Experiment 154 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006684419875023362
rmse: 0.08175830156640586
mae: 0.039334331760510766
r2: 0.6986024498733732
pearson: 0.8369380376079862

=== Experiment 474 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006856721722597944
rmse: 0.08280532424064255
mae: 0.04391188757119591
r2: 0.6908334354020779
pearson: 0.8390744446705781

=== Experiment 464 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007075506978965086
rmse: 0.08411603282944985
mae: 0.033470814819142165
r2: 0.6809685044872389
pearson: 0.85557980245029

=== Experiment 301 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006852564817547447
rmse: 0.08278021996556573
mae: 0.03193752874077123
r2: 0.6910208684212108
pearson: 0.8534201612092364

=== Experiment 70 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006015437174915836
rmse: 0.077559249447863
mae: 0.03347988616862025
r2: 0.7287665853794181
pearson: 0.8583076800774753

=== Experiment 351 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00668956787740334
rmse: 0.08178977856311472
mae: 0.039067950162853665
r2: 0.6983703287118691
pearson: 0.8408882799820467

=== Experiment 295 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007394837833686166
rmse: 0.08599324295365401
mae: 0.045373465655160795
r2: 0.6665700167961228
pearson: 0.8241420971401977

=== Experiment 205 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007036550909631089
rmse: 0.08388415171908868
mae: 0.03923728009708202
r2: 0.6827250165076315
pearson: 0.8427051014226974

=== Experiment 290 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006212069110008714
rmse: 0.07881668040464984
mae: 0.032794056021158646
r2: 0.7199005379704131
pearson: 0.8523438080096835

=== Experiment 51 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006712542786873883
rmse: 0.08193010915941637
mae: 0.039657711992865885
r2: 0.6973344001558733
pearson: 0.8378170731434379

=== Experiment 421 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006073712273916625
rmse: 0.07793402513611512
mae: 0.031385603043355866
r2: 0.7261389868143051
pearson: 0.8523062810731791

=== Experiment 72 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006207625720262553
rmse: 0.07878848723171776
mae: 0.03214222159722121
r2: 0.7201008884583822
pearson: 0.8495981863403079

=== Experiment 475 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006864029550770498
rmse: 0.08284943904922047
mae: 0.037962605480142485
r2: 0.6905039286461987
pearson: 0.8385792521307259

=== Experiment 61 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0073880562118908335
rmse: 0.08595380277736893
mae: 0.04387611295706809
r2: 0.6668757971380543
pearson: 0.8236109865824554

=== Experiment 348 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006733532250910871
rmse: 0.08205810289612397
mae: 0.03333250807672375
r2: 0.6963879944606151
pearson: 0.8358100374677162

=== Experiment 287 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007344775653828419
rmse: 0.08570166657555978
mae: 0.04729219711652005
r2: 0.6688272984518581
pearson: 0.8263948542448494

=== Experiment 12 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0061560892636107175
rmse: 0.07846074982824672
mae: 0.03378216477804825
r2: 0.7224246446058842
pearson: 0.8499974376163999

=== Experiment 190 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067279607396148675
rmse: 0.08202414729587176
mae: 0.03877614851131636
r2: 0.6966392114527422
pearson: 0.836725307348001

=== Experiment 210 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007110246064368136
rmse: 0.08432227501893041
mae: 0.036604617689978546
r2: 0.6794021344162562
pearson: 0.8243832172622558

=== Experiment 292 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007935257059756761
rmse: 0.08908005983247183
mae: 0.047042299223833
r2: 0.6422027517492908
pearson: 0.815768389368057

=== Experiment 426 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006921029093194314
rmse: 0.08319272259755846
mae: 0.03823681226329324
r2: 0.6879338443657258
pearson: 0.8312993578537393

=== Experiment 180 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007113235778197614
rmse: 0.08434000105642407
mae: 0.04367678699697967
r2: 0.6792673295355585
pearson: 0.8378786564183476

=== Experiment 90 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064308301472733986
rmse: 0.08019245692253978
mae: 0.040613980361959706
r2: 0.7100366990842443
pearson: 0.8483473332168221

=== Experiment 67 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007214973058693107
rmse: 0.08494099751411628
mae: 0.03581744812509125
r2: 0.6746800403360182
pearson: 0.8216196428396962

=== Experiment 22 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007646936328585212
rmse: 0.08744676282507667
mae: 0.048227232790504775
r2: 0.6552030066181578
pearson: 0.8241595944038036

=== Experiment 296 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067966404578196134
rmse: 0.0824417397791896
mae: 0.04253760694111143
r2: 0.6935424731871458
pearson: 0.8391226469386438

=== Experiment 20 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006175975816776599
rmse: 0.07858737695569562
mae: 0.02895928969754792
r2: 0.721527968676376
pearson: 0.8672224971184074

=== Experiment 45 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007366129086272469
rmse: 0.08582615618954673
mae: 0.03670162841704393
r2: 0.6678644815813202
pearson: 0.8182840832273498

=== Experiment 358 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007113095785375983
rmse: 0.08433917112099207
mae: 0.03833461122901176
r2: 0.6792736417502704
pearson: 0.8246884046310012

=== Experiment 5 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007072077480664183
rmse: 0.08409564483767387
mae: 0.034273786235484195
r2: 0.6811231390561889
pearson: 0.827823089800882

=== Experiment 459 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005413911865825693
rmse: 0.07357928965290228
mae: 0.027588856012303833
r2: 0.7558890968147575
pearson: 0.8707643710724892

=== Experiment 275 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006960630548850661
rmse: 0.0834303934357897
mae: 0.04171290500747478
r2: 0.6861482321601147
pearson: 0.8318644815530127

=== Experiment 110 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006472967890584578
rmse: 0.08045475679277501
mae: 0.03788616345656503
r2: 0.7081367267845824
pearson: 0.8486828290196166

=== Experiment 207 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006408462579761178
rmse: 0.08005287365086389
mae: 0.04092910199555163
r2: 0.7110452428586511
pearson: 0.8469749788828613

=== Experiment 306 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008212682052869057
rmse: 0.09062384924990252
mae: 0.04397644757057322
r2: 0.6296937809139347
pearson: 0.7986394347533845

=== Experiment 299 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0058190629865943954
rmse: 0.07628278302863888
mae: 0.03136794545799462
r2: 0.7376210110999417
pearson: 0.8592905815283021

=== Experiment 311 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006701924625577259
rmse: 0.08186528339642671
mae: 0.04504463594543537
r2: 0.6978131683753279
pearson: 0.8428651655573631

=== Experiment 384 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005957667745585584
rmse: 0.07718592971251681
mae: 0.029602383006867846
r2: 0.7313713835216489
pearson: 0.8568471373735891

=== Experiment 252 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006987069239684961
rmse: 0.08358869085997796
mae: 0.03736546930040422
r2: 0.6849561232269539
pearson: 0.8277468561973126

=== Experiment 446 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005878185526716708
rmse: 0.0766693258527601
mae: 0.029998976698633294
r2: 0.7349552017876465
pearson: 0.858058997086055

=== Experiment 457 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006787265972927905
rmse: 0.08238486495059578
mae: 0.03410933837558449
r2: 0.6939651645848868
pearson: 0.8335944114373637

=== Experiment 33 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005753786304727579
rmse: 0.07585371648592823
mae: 0.030526928166587267
r2: 0.7405643079548518
pearson: 0.863627763774022

=== Experiment 420 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007397135725219379
rmse: 0.08600660280013028
mae: 0.03971389578730235
r2: 0.6664664058782696
pearson: 0.8165422505549343

=== Experiment 429 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005440867478910482
rmse: 0.07376223613008544
mae: 0.02700285414245469
r2: 0.7546736800848369
pearson: 0.8696178280562438

=== Experiment 444 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007159359621759187
rmse: 0.08461299912991613
mae: 0.03614285417399839
r2: 0.6771876257300233
pearson: 0.8274643752716271

=== Experiment 331 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007921519446963038
rmse: 0.0890029181935235
mae: 0.04632886244396128
r2: 0.6428221746637739
pearson: 0.8157326529872762

=== Experiment 105 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007546713699662286
rmse: 0.086871823393217
mae: 0.04829710218621024
r2: 0.6597220008449401
pearson: 0.8216392498255349

=== Experiment 342 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007525088311984231
rmse: 0.08674726688480872
mae: 0.051116398500850826
r2: 0.6606970800572827
pearson: 0.8267266716846208

=== Experiment 396 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00694110418944611
rmse: 0.08331328939278601
mae: 0.04233908258204007
r2: 0.6870286671114579
pearson: 0.8330894858061586

=== Experiment 18 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005974536489995504
rmse: 0.07729512591357558
mae: 0.0315770573315463
r2: 0.7306107792607077
pearson: 0.8588162727011465

=== Experiment 467 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059661348923804305
rmse: 0.07724075926853924
mae: 0.030466084685813116
r2: 0.7309896035993455
pearson: 0.8562066612798013

=== Experiment 113 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007400089474084155
rmse: 0.08602377272640485
mae: 0.042610442611643796
r2: 0.6663332226420011
pearson: 0.8217001468071761

=== Experiment 261 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007478279863480872
rmse: 0.08647704818898984
mae: 0.03373845750137185
r2: 0.6628076523983253
pearson: 0.8216441903759432

=== Experiment 57 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0057417568809093955
rmse: 0.07577438142874804
mae: 0.028144451458086712
r2: 0.7411067093802594
pearson: 0.8691469994134163

=== Experiment 470 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007192167479583039
rmse: 0.08480664761434117
mae: 0.03810939363535275
r2: 0.6757083338605883
pearson: 0.8237578108207793

=== Experiment 112 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006951734317069866
rmse: 0.0833770610963823
mae: 0.03824981025608414
r2: 0.6865493593355798
pearson: 0.8289603260600079

=== Experiment 282 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007659586702843821
rmse: 0.08751906479644204
mae: 0.05141095482244788
r2: 0.6546326068106925
pearson: 0.822347052191456

=== Experiment 85 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006800187371328496
rmse: 0.08246324861008385
mae: 0.042358153883690734
r2: 0.6933825444181463
pearson: 0.8379545383734381

=== Experiment 213 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00759110317052035
rmse: 0.08712693711201117
mae: 0.04782091593302154
r2: 0.6577204991412501
pearson: 0.8273491040479757

=== Experiment 435 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006436201428691428
rmse: 0.08022593987415434
mae: 0.034947170947067285
r2: 0.7097945103691872
pearson: 0.8427172822824043

=== Experiment 354 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006721093910475511
rmse: 0.08198227802687305
mae: 0.03220020237819341
r2: 0.6969488337563132
pearson: 0.8563937755577029

=== Experiment 465 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007251799083572345
rmse: 0.08515749575681723
mae: 0.03795643038456821
r2: 0.6730195710825866
pearson: 0.8297117712243243

=== Experiment 403 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0067098796342876825
rmse: 0.08191385495926609
mae: 0.046157465632881034
r2: 0.697454480533842
pearson: 0.8454084241196146

=== Experiment 245 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0066794870204353145
rmse: 0.08172812869774589
mae: 0.03105303313448631
r2: 0.6988248701156328
pearson: 0.8583136806771011

=== Experiment 171 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006602803865272026
rmse: 0.08125763881181895
mae: 0.039771356020031304
r2: 0.702282479831108
pearson: 0.8404217958624605

=== Experiment 79 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00698211283852258
rmse: 0.08355903804210876
mae: 0.042748719923896186
r2: 0.6851796051738868
pearson: 0.8321950035275538

=== Experiment 330 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071132441306070455
rmse: 0.0843400505727086
mae: 0.038009219845236884
r2: 0.6792669529290933
pearson: 0.8329817506523366

=== Experiment 162 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006193252399947868
rmse: 0.07869721977266966
mae: 0.035041015209380744
r2: 0.7207489751451892
pearson: 0.8542986114230278

=== Experiment 81 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006379848347801976
rmse: 0.07987395287452585
mae: 0.03587641200366621
r2: 0.7123354459836048
pearson: 0.8451846195129374

=== Experiment 241 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006323709412763776
rmse: 0.07952175433655734
mae: 0.03379129154513236
r2: 0.7148667258558423
pearson: 0.8472213878149704

=== Experiment 288 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005755083662368946
rmse: 0.07586226771174816
mae: 0.0282122597977521
r2: 0.7405058106698132
pearson: 0.8643881321819356

=== Experiment 498 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006821136268173425
rmse: 0.08259017052999361
mae: 0.04136115197334804
r2: 0.6924379678797231
pearson: 0.8368070454572598

=== Experiment 195 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006614538011987989
rmse: 0.08132981010667607
mae: 0.034325409203895804
r2: 0.7017533923202781
pearson: 0.8377738370732746

=== Experiment 394 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.008041308183300488
rmse: 0.0896733415419571
mae: 0.043625181361290725
r2: 0.6374209532653732
pearson: 0.8086500419819459

=== Experiment 405 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068534826652772276
rmse: 0.08278576366306727
mae: 0.03906281504059152
r2: 0.6909794830710507
pearson: 0.8328525280822219

=== Experiment 174 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007462265555412791
rmse: 0.08638440574208282
mae: 0.03996160933436686
r2: 0.6635297305006791
pearson: 0.8156419415982603

=== Experiment 422 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006806913321787135
rmse: 0.08250402003409976
mae: 0.03507944202899803
r2: 0.6930792742722836
pearson: 0.8328719136492955

=== Experiment 78 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005880983651415982
rmse: 0.07668757168809026
mae: 0.029550519965697096
r2: 0.7348290355765053
pearson: 0.8576086110177914

=== Experiment 488 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.008705950765843047
rmse: 0.09330568453123876
mae: 0.054742320931060184
r2: 0.607452511750101
pearson: 0.8053291583520746

=== Experiment 132 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006372402845425978
rmse: 0.07982733144372282
mae: 0.03227826900367761
r2: 0.7126711604087225
pearson: 0.8570969520671767

=== Experiment 276 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007938890560449614
rmse: 0.08910045207769494
mae: 0.04776413380162722
r2: 0.6420389188022761
pearson: 0.8083360729085557

=== Experiment 254 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007178264590972921
rmse: 0.08472463981022829
mae: 0.03542757764467787
r2: 0.6763352089888893
pearson: 0.8369264940807452

=== Experiment 170 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.010124778338807636
rmse: 0.10062195753814192
mae: 0.05299517178948483
r2: 0.5434782009588984
pearson: 0.7900902435563846

=== Experiment 47 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007996492830162731
rmse: 0.08942311127534498
mae: 0.054708919886618225
r2: 0.6394416578136095
pearson: 0.8205305880447196

=== Experiment 355 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071271591989928145
rmse: 0.08442250410283277
mae: 0.03177600406370929
r2: 0.6786395286200692
pearson: 0.843633739628178

=== Experiment 390 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006798732941897067
rmse: 0.08245442948621419
mae: 0.0409472918734412
r2: 0.6934481240010661
pearson: 0.8356476334587627

=== Experiment 484 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007917603298543928
rmse: 0.0889809153613511
mae: 0.04307894648334869
r2: 0.642998752072363
pearson: 0.8050367293443363

=== Experiment 458 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006586591234791205
rmse: 0.08115781684342677
mae: 0.035222258574669256
r2: 0.7030135002037639
pearson: 0.8487544882092728

=== Experiment 495 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006617338474924686
rmse: 0.08134702499123546
mae: 0.03673586364553916
r2: 0.7016271206790403
pearson: 0.8461359241443044

=== Experiment 97 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0072510173120830055
rmse: 0.08515290548233223
mae: 0.04639387387894378
r2: 0.6730548208149569
pearson: 0.8309120556331883

=== Experiment 77 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007292840717642698
rmse: 0.08539813064489583
mae: 0.04376115189089922
r2: 0.6711690218661589
pearson: 0.8256291240220098

=== Experiment 83 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00723015098156521
rmse: 0.08503029449299356
mae: 0.04121525110468185
r2: 0.6739956744740299
pearson: 0.8233879402091163

=== Experiment 91 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.009002758740004644
rmse: 0.09488286852748838
mae: 0.04938711218670232
r2: 0.5940695708303341
pearson: 0.8064683994007591

=== Experiment 181 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006436338367119066
rmse: 0.08022679332441915
mae: 0.030765451684376718
r2: 0.7097883358757562
pearson: 0.8487231892746773

=== Experiment 397 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006697450614391178
rmse: 0.08183795338589045
mae: 0.03759721533958859
r2: 0.6980148995705453
pearson: 0.838944489546678

=== Experiment 291 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006391544332803154
rmse: 0.07994713461283746
mae: 0.033308357258507
r2: 0.7118080791676985
pearson: 0.8464777081447475

=== Experiment 80 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006053762184315314
rmse: 0.07780592640869534
mae: 0.03272381760015765
r2: 0.7270385275737221
pearson: 0.8535138000042278

=== Experiment 286 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008067861021953934
rmse: 0.08982127265828477
mae: 0.05343557278915515
r2: 0.6362236974572872
pearson: 0.819413945531333

=== Experiment 462 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007489849153562995
rmse: 0.0865439145957877
mae: 0.04171073167205207
r2: 0.6622859982005563
pearson: 0.8215375949941952

=== Experiment 157 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006287469196152985
rmse: 0.079293563396741
mae: 0.030849811981816275
r2: 0.7165007812722838
pearson: 0.8499606594608632

=== Experiment 206 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008073489809306373
rmse: 0.08985260045934326
mae: 0.04930731024882936
r2: 0.6359698979129846
pearson: 0.8210048693355477

=== Experiment 376 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005996245895121028
rmse: 0.07743543048967332
mae: 0.027700572073752074
r2: 0.7296319117386376
pearson: 0.8720669935431186

=== Experiment 63 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0073037858756366154
rmse: 0.08546218974281326
mae: 0.044801138614533866
r2: 0.6706755095095455
pearson: 0.8326995150816419

=== Experiment 106 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005814713711440874
rmse: 0.07625427011938987
mae: 0.027967809097288365
r2: 0.7378171180023516
pearson: 0.8603081304598621

=== Experiment 479 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00616955701171688
rmse: 0.07854652768720512
mae: 0.031750424244732005
r2: 0.7218173897713864
pearson: 0.8551243155559103

=== Experiment 185 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007751514783348515
rmse: 0.08804268727923129
mae: 0.047492405725032515
r2: 0.6504876101213783
pearson: 0.8142480471420404

=== Experiment 416 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00820535354081381
rmse: 0.0905834065423343
mae: 0.03477527973053067
r2: 0.6300242202969809
pearson: 0.8360699286978355

=== Experiment 220 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007207157534742203
rmse: 0.08489497944367619
mae: 0.03768369042377882
r2: 0.6750324388710893
pearson: 0.8219346874282444

=== Experiment 424 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005821338089457564
rmse: 0.07629769386723012
mae: 0.029781598613995094
r2: 0.7375184277131909
pearson: 0.872565767692491

=== Experiment 100 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0058779878642394225
rmse: 0.0766680367835216
mae: 0.02930759198882497
r2: 0.7349641143017495
pearson: 0.8674675687812066

=== Experiment 478 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007659993757686876
rmse: 0.08752139028652868
mae: 0.049330941696720546
r2: 0.6546142528869777
pearson: 0.8167336629149831

=== Experiment 492 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0056062427846695125
rmse: 0.0748748474767696
mae: 0.028809898101226354
r2: 0.7472169803354006
pearson: 0.8648644606950208

=== Experiment 375 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006225452084176319
rmse: 0.07890153410534119
mae: 0.03807947160413245
r2: 0.7192971055554933
pearson: 0.8491931823085225

=== Experiment 249 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006265823220565406
rmse: 0.07915695307782764
mae: 0.034702609667176094
r2: 0.7174767887844054
pearson: 0.8489299939954258

=== Experiment 130 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007048752403024619
rmse: 0.08395684845814913
mae: 0.0444142021210096
r2: 0.682174856540805
pearson: 0.8363071068165989

=== Experiment 226 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006626311856566062
rmse: 0.0814021612524266
mae: 0.035832798029487495
r2: 0.701222514850318
pearson: 0.8397209276675841

=== Experiment 322 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006908167175785892
rmse: 0.08311538471177217
mae: 0.03036344153009585
r2: 0.6885137825607087
pearson: 0.8444108380117843

=== Experiment 430 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068864343123286875
rmse: 0.08298454261083017
mae: 0.03694364227218274
r2: 0.6894937078086294
pearson: 0.8305222823080424

=== Experiment 69 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006637613978621405
rmse: 0.08147155318650434
mae: 0.03552628912150367
r2: 0.700712907141287
pearson: 0.8473026368711356

=== Experiment 209 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006732717087078226
rmse: 0.08205313575408453
mae: 0.03613507970785942
r2: 0.696424749839048
pearson: 0.8347209441015713

=== Experiment 177 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007213551541266263
rmse: 0.08493262942630625
mae: 0.03789334263443426
r2: 0.674744135931137
pearson: 0.822400668691874

=== Experiment 34 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007156258863006407
rmse: 0.08459467396359187
mae: 0.0461125519816369
r2: 0.6773274375774379
pearson: 0.8371019237990168

=== Experiment 99 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006627024910916707
rmse: 0.08140654095904522
mae: 0.03435653006345534
r2: 0.7011903635434866
pearson: 0.8388958850161233

=== Experiment 193 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.008152028512169356
rmse: 0.0902885846171561
mae: 0.03898937814401303
r2: 0.6324286223684195
pearson: 0.798389008685762

=== Experiment 163 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007061778136527062
rmse: 0.08403438663146809
mae: 0.045660383943422665
r2: 0.6815875319502436
pearson: 0.8352771370101334

=== Experiment 466 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006536397307301059
rmse: 0.08084798888841366
mae: 0.03323354374394151
r2: 0.7052767223022576
pearson: 0.8446077532392661

=== Experiment 214 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061689229693904675
rmse: 0.07854249148957822
mae: 0.02916750610670354
r2: 0.7218459784608212
pearson: 0.858412051046276

=== Experiment 26 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006667618015312294
rmse: 0.08165548368182197
mae: 0.03815021004534896
r2: 0.6993600383326812
pearson: 0.8394794717986445

=== Experiment 314 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007106971317281162
rmse: 0.08430285473980796
mae: 0.042241511304663075
r2: 0.6795497913210813
pearson: 0.838442294058736

=== Experiment 25 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00645688534035574
rmse: 0.08035474684395279
mae: 0.03416966675386189
r2: 0.7088618818959289
pearson: 0.842992955680609

=== Experiment 487 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006238529160647344
rmse: 0.07898436022813216
mae: 0.032090664648294506
r2: 0.7187074659330763
pearson: 0.8508801534294028

=== Experiment 304 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067750881603622646
rmse: 0.08231092370009138
mae: 0.037604446553651576
r2: 0.6945142567936062
pearson: 0.8336815586753726

=== Experiment 228 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007151684984072619
rmse: 0.08456763555919379
mae: 0.048920059879877624
r2: 0.6775336717654443
pearson: 0.8369454953188157

=== Experiment 215 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006644788891925454
rmse: 0.08151557453594653
mae: 0.03761703925911859
r2: 0.7003893934583282
pearson: 0.8372635440900156

=== Experiment 266 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007353046765772921
rmse: 0.08574990825518661
mae: 0.04827154580939929
r2: 0.6684543576546753
pearson: 0.8274938991641271

=== Experiment 298 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0072231419629123685
rmse: 0.08498906966729526
mae: 0.04407996614744696
r2: 0.6743117080401816
pearson: 0.8349078510280097

=== Experiment 310 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007657329812954123
rmse: 0.08750617014219125
mae: 0.047227973696956374
r2: 0.6547343689824858
pearson: 0.8140391477795426

=== Experiment 42 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006565131158824949
rmse: 0.08102549696746666
mae: 0.03626594334326672
r2: 0.7039811255837805
pearson: 0.8394153593562835

=== Experiment 140 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007947561994120665
rmse: 0.08914909979422488
mae: 0.04196988262708587
r2: 0.6416479276746381
pearson: 0.8054872596982855

=== Experiment 263 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0070771933950801525
rmse: 0.08412605657630787
mae: 0.043750345679564165
r2: 0.6808924647268589
pearson: 0.8306485848514906

=== Experiment 165 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007194175787670533
rmse: 0.08481848729888156
mae: 0.04684735054156217
r2: 0.6756177801328471
pearson: 0.8372526242186038

=== Experiment 32 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006858153634220653
rmse: 0.08281397004262417
mae: 0.03917591304631421
r2: 0.6907688711372424
pearson: 0.8339186747462641

=== Experiment 285 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006060248419883121
rmse: 0.07784759739313167
mae: 0.0328643810760432
r2: 0.7267460660667804
pearson: 0.8552624134919228

=== Experiment 294 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00784346081547622
rmse: 0.08856331529180815
mae: 0.045956671696719235
r2: 0.6463418040012836
pearson: 0.8146452075275824

=== Experiment 490 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00639688562292927
rmse: 0.07998053277472757
mae: 0.03489876233908415
r2: 0.7115672427467934
pearson: 0.8474280383701429

=== Experiment 1 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005863150236686716
rmse: 0.07657121023391701
mae: 0.028293795858686445
r2: 0.7356331363975619
pearson: 0.8649849917432646

=== Experiment 229 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007098341311043358
rmse: 0.0842516546486973
mae: 0.0394811508765024
r2: 0.6799389145039876
pearson: 0.8322263865021541

=== Experiment 21 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00726471337212053
rmse: 0.08523328793447153
mae: 0.04293291114248147
r2: 0.672437271495962
pearson: 0.8245494029959932

=== Experiment 255 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006699112009380248
rmse: 0.08184810327295464
mae: 0.036417230815183235
r2: 0.6979399880017241
pearson: 0.8356668894341258

=== Experiment 196 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006731837860269847
rmse: 0.08204777791183529
mae: 0.035655764134319444
r2: 0.6964643937888605
pearson: 0.8385065810525348

=== Experiment 197 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005648647160066098
rmse: 0.07515748239574087
mae: 0.029336538119550958
r2: 0.7453049857123615
pearson: 0.8634055800166085

=== Experiment 50 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005708959548887224
rmse: 0.0755576571161866
mae: 0.027178108187660543
r2: 0.7425855266459294
pearson: 0.8653439013474084

=== Experiment 44 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006217712115081023
rmse: 0.07885247057055995
mae: 0.0317696428184923
r2: 0.7196460973553793
pearson: 0.8564179552338657

=== Experiment 186 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006844944940658739
rmse: 0.08273418241971536
mae: 0.037285595657554016
r2: 0.6913644453163585
pearson: 0.8334664049250153

=== Experiment 337 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005938384251213724
rmse: 0.0770609126030423
mae: 0.030456213333118272
r2: 0.732240867795561
pearson: 0.856117801489921

=== Experiment 471 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006372355258309451
rmse: 0.07982703338036214
mae: 0.034022927636529324
r2: 0.7126733060908619
pearson: 0.8468756718428618

=== Experiment 480 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006728155212178523
rmse: 0.08202533274652729
mae: 0.03456974065145386
r2: 0.6966304427704408
pearson: 0.8485417424147493

=== Experiment 327 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0072010253406645875
rmse: 0.0848588554051054
mae: 0.039962770977073125
r2: 0.675308936803064
pearson: 0.8227276185668356

=== Experiment 332 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006866314216452149
rmse: 0.08286322595972323
mae: 0.03639131780015385
r2: 0.6904009140761713
pearson: 0.8347615566965414

=== Experiment 279 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006546335860225884
rmse: 0.08090942998332076
mae: 0.037158277791001205
r2: 0.7048285973251688
pearson: 0.8400188196836792

=== Experiment 168 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006817811019725517
rmse: 0.08257003705779425
mae: 0.04469254719735765
r2: 0.6925879018686856
pearson: 0.839092006204657

=== Experiment 434 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006256721297322716
rmse: 0.07909943929840917
mae: 0.03203885774124412
r2: 0.7178871904973554
pearson: 0.8477937478093908

=== Experiment 326 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006953491894120719
rmse: 0.08338760036192863
mae: 0.04212391125462504
r2: 0.6864701109599256
pearson: 0.8337233651372328

=== Experiment 119 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00612304658990601
rmse: 0.07824989833799154
mae: 0.02893355633412296
r2: 0.7239145242200364
pearson: 0.8677574836657818

=== Experiment 258 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007003781712343713
rmse: 0.0836885996557698
mae: 0.03801803957798303
r2: 0.6842025651904924
pearson: 0.8395620415898094

=== Experiment 395 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00795396533001675
rmse: 0.08918500619508164
mae: 0.053013955139160525
r2: 0.6413592040774074
pearson: 0.8213549582769499

=== Experiment 414 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005329188733092478
rmse: 0.07300129268096886
mae: 0.02800765191330375
r2: 0.7597092255801957
pearson: 0.8717282290364105

=== Experiment 318 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006502424548027255
rmse: 0.08063761248962704
mae: 0.03805732737759926
r2: 0.7068085390653585
pearson: 0.8417418737144454

=== Experiment 223 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007580885548925806
rmse: 0.08706828095768175
mae: 0.04583723984491291
r2: 0.658181207201829
pearson: 0.8209517197247952

=== Experiment 86 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006015131570948536
rmse: 0.07755727929052525
mae: 0.03696954383256617
r2: 0.7287803649277971
pearson: 0.8554645325977219

=== Experiment 76 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006735067566765276
rmse: 0.08206745741623336
mae: 0.0376294258382111
r2: 0.6963187677444842
pearson: 0.8347667837037412

=== Experiment 28 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007907124074583733
rmse: 0.08892201119286346
mae: 0.04950328051749159
r2: 0.643471255668469
pearson: 0.8173826787263623

=== Experiment 496 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006108608074750681
rmse: 0.0781575848830469
mae: 0.029642368590898718
r2: 0.7245655505135138
pearson: 0.8587505759858272

=== Experiment 460 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006049628816160619
rmse: 0.07777935983383136
mae: 0.02951937092755243
r2: 0.7272248993245829
pearson: 0.8572145027060357

=== Experiment 31 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007001682577599425
rmse: 0.08367605737365633
mae: 0.03797769554447435
r2: 0.6842972142522127
pearson: 0.8349920430500855

=== Experiment 353 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007666719432161917
rmse: 0.08755980488878397
mae: 0.04577638447350769
r2: 0.654310995185094
pearson: 0.8220802608284893

=== Experiment 344 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0059513631828825775
rmse: 0.07714507879886168
mae: 0.028982164738159396
r2: 0.7316556534790771
pearson: 0.8578404152568267

=== Experiment 167 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005460725884508909
rmse: 0.07389672445047148
mae: 0.030623799200366748
r2: 0.7537782733167566
pearson: 0.8690616201006374

=== Experiment 399 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005602767619924076
rmse: 0.07485163738973301
mae: 0.028349921541612602
r2: 0.7473736739842354
pearson: 0.8677575481869613

=== Experiment 153 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006025179505356519
rmse: 0.07762202976833651
mae: 0.03196459731394551
r2: 0.7283273079877752
pearson: 0.8582691859347091

=== Experiment 497 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006445577411625387
rmse: 0.0802843534670697
mae: 0.03667197381436888
r2: 0.7093717514253806
pearson: 0.8426070891637226

=== Experiment 386 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006704776468053914
rmse: 0.08188269944288545
mae: 0.03255559922466603
r2: 0.6976845800532474
pearson: 0.8461345127615173

=== Experiment 127 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006300025941658984
rmse: 0.0793727027488606
mae: 0.0350032200728297
r2: 0.7159346031440647
pearson: 0.8504427478174499

=== Experiment 9 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006980570679653147
rmse: 0.083549809572812
mae: 0.04730520209073064
r2: 0.6852491404385535
pearson: 0.8381303090334121

=== Experiment 189 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006212646544437681
rmse: 0.07882034346815346
mae: 0.029675372065892484
r2: 0.7198745017061594
pearson: 0.8539949243312897

=== Experiment 147 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007820205592864225
rmse: 0.08843192632112129
mae: 0.03973764364455882
r2: 0.6473903717534523
pearson: 0.8109879071512501

=== Experiment 482 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005850429373298498
rmse: 0.07648809955345012
mae: 0.02912987697722567
r2: 0.7362067145288564
pearson: 0.8625999712842147

=== Experiment 349 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00686800547398628
rmse: 0.08287343044659295
mae: 0.04020437014157907
r2: 0.6903246560183367
pearson: 0.83314153375399

=== Experiment 73 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007731986228580767
rmse: 0.0879317134404918
mae: 0.03575628022460062
r2: 0.6513681440606818
pearson: 0.8083765119091866

=== Experiment 283 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.010247735654532764
rmse: 0.1012311002337363
mae: 0.050997920632350494
r2: 0.5379341097105056
pearson: 0.7402443174732705

=== Experiment 363 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007101836216614545
rmse: 0.08427239296836506
mae: 0.03540191558376627
r2: 0.6797813307500085
pearson: 0.8276207802469285

=== Experiment 476 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007232111271594908
rmse: 0.08504182072130692
mae: 0.033774892998359334
r2: 0.6739072858593805
pearson: 0.8402317369981667

=== Experiment 49 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007409044594350532
rmse: 0.0860758072535514
mae: 0.03582978444748528
r2: 0.6659294402106399
pearson: 0.822166905056664

=== Experiment 441 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007974637464438678
rmse: 0.08930082566493257
mae: 0.0530400027102632
r2: 0.6404271066348248
pearson: 0.8276518245669198

=== Experiment 334 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006283812830328144
rmse: 0.07927050416345378
mae: 0.033411489696353866
r2: 0.7166656451980362
pearson: 0.8474143343502353

=== Experiment 36 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005631576968221363
rmse: 0.07504383364555253
mae: 0.029459349254255054
r2: 0.7460746731494745
pearson: 0.8642471631658957

=== Experiment 333 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006635206199558435
rmse: 0.08145677503779802
mae: 0.03623411557670424
r2: 0.7008214728394917
pearson: 0.8376902151712926

=== Experiment 7 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006453512488600093
rmse: 0.08033375684355919
mae: 0.03253805596498108
r2: 0.7090139622970855
pearson: 0.8496878003224871

=== Experiment 269 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005560656865875331
rmse: 0.07456981202789324
mae: 0.028949266432591582
r2: 0.749272429349933
pearson: 0.865717499661589

=== Experiment 379 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006806259608597839
rmse: 0.08250005823390574
mae: 0.034531273110802174
r2: 0.6931087499122678
pearson: 0.8350952816019207

=== Experiment 433 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005399867455868288
rmse: 0.07348379042937488
mae: 0.028645283128462904
r2: 0.7565223530783194
pearson: 0.8697994265593884

=== Experiment 468 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0074687686058291
rmse: 0.08642203773245051
mae: 0.037698846922131594
r2: 0.6632365108196191
pearson: 0.8241484110399045

=== Experiment 175 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006252219820815274
rmse: 0.07907097963738197
mae: 0.032717266597170586
r2: 0.7180901600918247
pearson: 0.8504306889978921

=== Experiment 23 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0053844569262232215
rmse: 0.07337885885064731
mae: 0.028472425659444776
r2: 0.7572172070773227
pearson: 0.8702758787817749

=== Experiment 208 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006161096997129499
rmse: 0.07849265568911208
mae: 0.029323492412177053
r2: 0.7221988481055944
pearson: 0.858012054034119

=== Experiment 123 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006580246343358078
rmse: 0.08111871758945699
mae: 0.03528223449974649
r2: 0.7032995885658833
pearson: 0.8461368181515561

=== Experiment 156 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006988604316304524
rmse: 0.08359787267810422
mae: 0.03981590438421632
r2: 0.6848869072978166
pearson: 0.8305461981738451

=== Experiment 309 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00670399319756303
rmse: 0.08187791642172527
mae: 0.03624441745376348
r2: 0.6977198973749376
pearson: 0.8469068861276543

=== Experiment 277 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005768388106080194
rmse: 0.0759499052407585
mae: 0.03126109071452494
r2: 0.7399059191586065
pearson: 0.867609376315242

=== Experiment 149 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.008076391147622996
rmse: 0.08986874399713726
mae: 0.04546305455672733
r2: 0.6358390778452648
pearson: 0.7982816661554764

=== Experiment 284 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007600140406511324
rmse: 0.08717878415366508
mae: 0.03843144163724232
r2: 0.6573130141480088
pearson: 0.8112246369989233

=== Experiment 121 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006309141099788872
rmse: 0.07943010197518868
mae: 0.03251765297438165
r2: 0.715523604675876
pearson: 0.8478584655733289

=== Experiment 312 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006525248932671824
rmse: 0.08077901294687763
mae: 0.030977059892994208
r2: 0.7057793976075148
pearson: 0.8558253295295896

=== Experiment 274 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007652105063494575
rmse: 0.08747631144198167
mae: 0.04018181252586576
r2: 0.6549699506360287
pearson: 0.8440492655489046

=== Experiment 350 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00617969966479751
rmse: 0.07861106579100369
mae: 0.031079467893674953
r2: 0.7213600620081041
pearson: 0.8525730845862676

=== Experiment 346 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006450420772199153
rmse: 0.08031451159161185
mae: 0.035002020966737483
r2: 0.7091533664288354
pearson: 0.8432117586424677

=== Experiment 357 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007047562524801231
rmse: 0.08394976191033082
mae: 0.04187770341348864
r2: 0.6822285076261921
pearson: 0.8326911027591607

=== Experiment 259 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006580910564258771
rmse: 0.0811228116146055
mae: 0.042647107696156546
r2: 0.7032696391378168
pearson: 0.8447919086359795

=== Experiment 361 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007308869939207383
rmse: 0.08549192908811558
mae: 0.04020179049302899
r2: 0.6704462713208071
pearson: 0.8223708397337521

=== Experiment 82 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005953056434225673
rmse: 0.07715605247954092
mae: 0.038399449324135033
r2: 0.7315793055212738
pearson: 0.8637442067862442

=== Experiment 372 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006720323036122425
rmse: 0.08197757642259512
mae: 0.03515644489756686
r2: 0.6969835921416061
pearson: 0.8348908446131071

=== Experiment 257 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0075067089052526055
rmse: 0.08664126560278657
mae: 0.04255209035485189
r2: 0.6615257994174161
pearson: 0.8317993912504145

=== Experiment 182 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006189794007225747
rmse: 0.07867524392860659
mae: 0.03425530320999084
r2: 0.7209049125508746
pearson: 0.8517238489854028

=== Experiment 93 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005711346279599199
rmse: 0.07557344956794812
mae: 0.033644820025595414
r2: 0.7424779100086067
pearson: 0.8624105872124028

=== Experiment 203 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0060782844900843245
rmse: 0.07796335350717236
mae: 0.03079333834982518
r2: 0.7259328275996897
pearson: 0.8550144645892332

=== Experiment 367 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00673286704497583
rmse: 0.08205404953429069
mae: 0.03704435667232382
r2: 0.6964179883034465
pearson: 0.84635103921747

=== Experiment 179 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00788045406682162
rmse: 0.08877192161275782
mae: 0.046037829481709446
r2: 0.6446737945800947
pearson: 0.8093321987808847

=== Experiment 184 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0070794682672283525
rmse: 0.08413957610558989
mae: 0.03853881236050754
r2: 0.6807898917429446
pearson: 0.8259852060156075

=== Experiment 366 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005656155379598137
rmse: 0.07520741572210907
mae: 0.027581217208756514
r2: 0.7449664433982819
pearson: 0.866071533353864

=== Experiment 265 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0055616505417039035
rmse: 0.0745764744520945
mae: 0.029701114864635687
r2: 0.7492276249441728
pearson: 0.8687565543650403

=== Experiment 445 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007094930387373895
rmse: 0.08423140974347927
mae: 0.0361754888990547
r2: 0.6800927115509814
pearson: 0.8443276594552732

=== Experiment 437 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0069500426923325455
rmse: 0.08336691605386723
mae: 0.03604939936804288
r2: 0.6866256339504445
pearson: 0.8300577576959032

=== Experiment 253 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007411958744111478
rmse: 0.08609273339900109
mae: 0.04057543792190241
r2: 0.6657980424805343
pearson: 0.8240375998233244

=== Experiment 341 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007837748385518557
rmse: 0.08853105887494261
mae: 0.038393875360586006
r2: 0.6465993749538419
pearson: 0.8168916334550326

=== Experiment 417 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0074447968728717055
rmse: 0.08628323633749319
mae: 0.04489724749091556
r2: 0.6643173857079014
pearson: 0.8283480195600723

=== Experiment 388 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006947239367654926
rmse: 0.08335010118563099
mae: 0.0393687561845546
r2: 0.686752034626321
pearson: 0.8307033160637824

=== Experiment 477 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007049892445195704
rmse: 0.08396363763675145
mae: 0.03980417929763717
r2: 0.6821234525410818
pearson: 0.8324221260203576

=== Experiment 128 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00688457936588098
rmse: 0.08297336540047644
mae: 0.03601220700090236
r2: 0.6895773465275321
pearson: 0.8304579893340424

=== Experiment 133 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007879802882027071
rmse: 0.08876825379620279
mae: 0.04099069507002321
r2: 0.6447031562158726
pearson: 0.8078840504558441

=== Experiment 200 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007103730327557696
rmse: 0.08428363024667183
mae: 0.04310525613683763
r2: 0.6796959261212436
pearson: 0.8291884858683763

=== Experiment 160 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006233820466960169
rmse: 0.07895454684158582
mae: 0.03343769067546138
r2: 0.7189197788589765
pearson: 0.8517828570362881

=== Experiment 169 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006371285032051535
rmse: 0.0798203296914485
mae: 0.0387860764374739
r2: 0.7127215621217886
pearson: 0.8458262152543493

=== Experiment 453 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006825063279103265
rmse: 0.0826139411909592
mae: 0.04129015722581296
r2: 0.6922609006853063
pearson: 0.8369228172814209

=== Experiment 118 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006703817068705617
rmse: 0.08187684085689687
mae: 0.03014736838079652
r2: 0.6977278389475982
pearson: 0.8483943238622151

=== Experiment 38 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00564611033406332
rmse: 0.07514060376429857
mae: 0.031118217714691383
r2: 0.7454193700802842
pearson: 0.8651582813571487

=== Experiment 402 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006379550215478581
rmse: 0.07987208658523064
mae: 0.032552428672449
r2: 0.7123488886388486
pearson: 0.8543228667959315

=== Experiment 391 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006815684932029443
rmse: 0.08255716160351834
mae: 0.03275366806668547
r2: 0.6926837662271388
pearson: 0.8469928822378551

=== Experiment 278 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0069765906166749355
rmse: 0.08352598767254976
mae: 0.03992919041206147
r2: 0.6854285997264811
pearson: 0.8368840257593864

=== Experiment 316 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006323604121238536
rmse: 0.07952109230410845
mae: 0.03330978943735082
r2: 0.7148714734043732
pearson: 0.845920064297875

=== Experiment 368 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006841084282377789
rmse: 0.08271084742872478
mae: 0.03403431416199041
r2: 0.691538520699041
pearson: 0.8317014096304407

=== Experiment 159 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007021012209217594
rmse: 0.08379148052885564
mae: 0.04393092109470341
r2: 0.6834256496701707
pearson: 0.8341325026354239

=== Experiment 48 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006264324915269272
rmse: 0.07914748836993674
mae: 0.030415415586125375
r2: 0.7175443467107534
pearson: 0.8590250352925337

=== Experiment 236 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005786007668642742
rmse: 0.07606581143090989
mae: 0.031080778900912997
r2: 0.7391114608376239
pearson: 0.8637458986636929

=== Experiment 124 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005960310887377223
rmse: 0.07720304972847396
mae: 0.030514838425123123
r2: 0.7312522054215995
pearson: 0.8570348802600867

=== Experiment 338 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006179747816820899
rmse: 0.07861137205787022
mae: 0.03546755593428803
r2: 0.7213578908545626
pearson: 0.853808047672053

=== Experiment 71 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0073930681795513625
rmse: 0.08598295284270809
mae: 0.03888784536485837
r2: 0.6666498097221767
pearson: 0.8213640685312151

=== Experiment 141 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006174819861410607
rmse: 0.07858002202475262
mae: 0.03684701594945489
r2: 0.7215800901950538
pearson: 0.8499021555139897

=== Experiment 381 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006583222016974247
rmse: 0.08113705698985049
mae: 0.03327010497593053
r2: 0.7031654167522242
pearson: 0.8440641301978781

=== Experiment 280 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005825206613923088
rmse: 0.07632304117318103
mae: 0.0319347838575771
r2: 0.7373439976477082
pearson: 0.8587577485665817

=== Experiment 54 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006177150677245057
rmse: 0.0785948514677969
mae: 0.03263717915625644
r2: 0.721474994734949
pearson: 0.8498461449307202

=== Experiment 238 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006536667777259408
rmse: 0.0808496615778904
mae: 0.03652688726958661
r2: 0.7052645269308784
pearson: 0.8438070959128745

=== Experiment 419 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007324390390464005
rmse: 0.08558265239208239
mae: 0.03243281058035397
r2: 0.669746461004719
pearson: 0.8370735555301689

=== Experiment 319 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007273355349631957
rmse: 0.08528396889000861
mae: 0.03330785653860409
r2: 0.6720476085335969
pearson: 0.835620846988325

=== Experiment 107 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005622277736406963
rmse: 0.07498184937974632
mae: 0.03132440083778328
r2: 0.7464939714190809
pearson: 0.8656328104467127

=== Experiment 198 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0058572365357576865
rmse: 0.07653258479731158
mae: 0.030965716607779283
r2: 0.735899782569633
pearson: 0.8583817352511387

=== Experiment 362 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006682393730823931
rmse: 0.08174590956631365
mae: 0.04340483235959884
r2: 0.6986938078235513
pearson: 0.8427595832470766

=== Experiment 102 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0058626937457195065
rmse: 0.07656822934951223
mae: 0.032924696020158004
r2: 0.7356537193743562
pearson: 0.8606809288833125

=== Experiment 383 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006877785049407119
rmse: 0.08293241253820559
mae: 0.040502907728830875
r2: 0.6898836992669313
pearson: 0.8354612337790492

=== Experiment 178 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006322919212924441
rmse: 0.07951678573058925
mae: 0.029842432159572368
r2: 0.7149023556188046
pearson: 0.8482867296284607

=== Experiment 455 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006322842063524536
rmse: 0.07951630061518541
mae: 0.03674161985217214
r2: 0.7149058342512901
pearson: 0.8459247933602112

=== Experiment 463 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006828141605244768
rmse: 0.08263256988164393
mae: 0.0377192766550016
r2: 0.6921221003144609
pearson: 0.8348354240545145

=== Experiment 369 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006537795136930018
rmse: 0.08085663322776937
mae: 0.03586544504198683
r2: 0.7052136947795196
pearson: 0.8446833058967883

=== Experiment 41 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007461710079180909
rmse: 0.08638119054042326
mae: 0.04515532376426441
r2: 0.6635547766794908
pearson: 0.8318470949263997

=== Experiment 146 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006908396960694818
rmse: 0.08311676702504024
mae: 0.03524282510547954
r2: 0.6885034216603014
pearson: 0.8343093963953032

=== Experiment 401 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005457872999562342
rmse: 0.0738774187391678
mae: 0.027882935559739138
r2: 0.7539069086433466
pearson: 0.8684824969526441

=== Experiment 19 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005717525630513302
rmse: 0.07561432159659506
mae: 0.03305746772115136
r2: 0.7421992857956302
pearson: 0.8620625781965954

=== Experiment 143 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005635930453343016
rmse: 0.07507283432336237
mae: 0.03299234322527151
r2: 0.7458783764214546
pearson: 0.8644157248925473

=== Experiment 202 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006559822495940064
rmse: 0.08099273113026911
mae: 0.033978614501574404
r2: 0.7042204908567388
pearson: 0.8469445380396263

=== Experiment 187 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007372529886227279
rmse: 0.08586343742377939
mae: 0.03647530589178621
r2: 0.6675758723285654
pearson: 0.8189453357963902

=== Experiment 15 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00785727877406693
rmse: 0.0886412927143266
mae: 0.039793094866356696
r2: 0.6457187583301772
pearson: 0.8268769819616748

=== Experiment 485 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062012932281044755
rmse: 0.07874829031861248
mae: 0.034761792492492276
r2: 0.7203864177426469
pearson: 0.8545014820903168

=== Experiment 454 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007559170266213018
rmse: 0.08694348892362797
mae: 0.039077568300452475
r2: 0.6591603397417218
pearson: 0.8141299956383458

=== Experiment 29 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005271680650265602
rmse: 0.07260634028971301
mae: 0.028058429184633738
r2: 0.7623022397236167
pearson: 0.8731928694330686

=== Experiment 439 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007733104615986015
rmse: 0.08793807261923595
mae: 0.03984226744881877
r2: 0.651317716464819
pearson: 0.8313313236393052

=== Experiment 432 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005876441565052336
rmse: 0.0766579517405229
mae: 0.03022457658865345
r2: 0.7350338362515169
pearson: 0.8604528205038922

=== Experiment 151 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005930045351550566
rmse: 0.0770067876979073
mae: 0.03154031456652367
r2: 0.7326168651111424
pearson: 0.8570283116394768

=== Experiment 382 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006506789328973747
rmse: 0.08066467212462806
mae: 0.030850311890118942
r2: 0.7066117330135683
pearson: 0.8485345915440482

=== Experiment 335 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00585869681246487
rmse: 0.07654212443135394
mae: 0.029896996933810493
r2: 0.7358339393356228
pearson: 0.8605798924075686

=== Experiment 302 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065907339973388665
rmse: 0.08118333571207127
mae: 0.03850258822858583
r2: 0.7028267048638588
pearson: 0.8392745901001554

=== Experiment 481 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006357081772887565
rmse: 0.07973130986562033
mae: 0.037287686316177196
r2: 0.7133619808261596
pearson: 0.8472368350803574

=== Experiment 58 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006555739365627072
rmse: 0.08096752043645077
mae: 0.03205545351131777
r2: 0.7044045974054285
pearson: 0.8450602265757956

=== Experiment 248 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006025200020243597
rmse: 0.07762216191426001
mae: 0.03298866820880213
r2: 0.7283263829805462
pearson: 0.853612906986073

=== Experiment 218 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00644991379670488
rmse: 0.08031135534097827
mae: 0.03937374992342563
r2: 0.7091762257307352
pearson: 0.8443327762898575

=== Experiment 411 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007239340791094469
rmse: 0.08508431577614331
mae: 0.035497980028283665
r2: 0.6735813100071001
pearson: 0.8218685652620977

=== Experiment 244 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006127680528958579
rmse: 0.07827950261057219
mae: 0.037168641964907045
r2: 0.7237055819477087
pearson: 0.8525393119583807

=== Experiment 443 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007504174045705273
rmse: 0.08662663589050006
mae: 0.03578055175290427
r2: 0.6616400951187282
pearson: 0.8402929207639397

=== Experiment 315 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00689951851460178
rmse: 0.08306334037709885
mae: 0.03532596949394137
r2: 0.6889037468869336
pearson: 0.8313126843974337

=== Experiment 89 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007135469058201029
rmse: 0.08447170566645987
mae: 0.03537588163822348
r2: 0.6782648407258194
pearson: 0.8245425499588334

=== Experiment 17 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00733888127040518
rmse: 0.0856672707071095
mae: 0.0436011639783858
r2: 0.6690930736060954
pearson: 0.8213688838665067

=== Experiment 469 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005456113102180515
rmse: 0.07386550684981803
mae: 0.026476561054699928
r2: 0.7539862616417032
pearson: 0.8727905272244453

=== Experiment 325 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067419646616030246
rmse: 0.08210946755157425
mae: 0.04112410586191053
r2: 0.6960077807738937
pearson: 0.8379197693182368

=== Experiment 297 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005667438461395197
rmse: 0.07528239144312032
mae: 0.028943724642515822
r2: 0.7444576942061166
pearson: 0.8633645161654753

=== Experiment 374 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00656120748600411
rmse: 0.08100128076767743
mae: 0.03599372887285365
r2: 0.7041580422643317
pearson: 0.8431185251094925

=== Experiment 136 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067500925022861615
rmse: 0.08215894657483239
mae: 0.036595148716634364
r2: 0.6956413000148272
pearson: 0.8362619065098196

=== Experiment 472 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005323835486943296
rmse: 0.0729646180483616
mae: 0.028834583967587323
r2: 0.7599506010929185
pearson: 0.8718699143307026

=== Experiment 30 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006959344934076379
rmse: 0.08342268836519462
mae: 0.036827551838264964
r2: 0.6862061999644447
pearson: 0.832899962749242

=== Experiment 125 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006644712412513497
rmse: 0.08151510542539644
mae: 0.035058364127707874
r2: 0.7003928418813522
pearson: 0.8570098279560489

=== Experiment 59 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007208998181717237
rmse: 0.08490581948086501
mae: 0.03160714036044348
r2: 0.6749494449091158
pearson: 0.8547605384948388

=== Experiment 150 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006318958162247787
rmse: 0.0794918748190517
mae: 0.03173010779895711
r2: 0.7150809576504231
pearson: 0.8502198902195037

=== Experiment 260 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0056843393555414765
rmse: 0.07539455786422171
mae: 0.028387901056713852
r2: 0.7436956403277131
pearson: 0.8651472583771277

=== Experiment 64 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006374787619133836
rmse: 0.07984226712170588
mae: 0.03609796839382581
r2: 0.7125636320118542
pearson: 0.8446249882247987

=== Experiment 231 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006284839951677396
rmse: 0.07927698248342577
mae: 0.03366526482688914
r2: 0.7166193327484691
pearson: 0.8482495983122615

=== Experiment 412 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006270023956460394
rmse: 0.0791834828512891
mae: 0.034267925311149604
r2: 0.7172873794517856
pearson: 0.8569516495305582

=== Experiment 242 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006695644193088443
rmse: 0.08182691606732129
mae: 0.034995923773939466
r2: 0.6980963503120188
pearson: 0.8361291979831541

=== Experiment 418 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0081960179662036
rmse: 0.0905318616079643
mae: 0.04175332853540335
r2: 0.6304451572472547
pearson: 0.798257986729497

=== Experiment 204 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006017976366262705
rmse: 0.07757561708592917
mae: 0.03143850044496595
r2: 0.7286520943591712
pearson: 0.8591129754999116

=== Experiment 491 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006330846463681622
rmse: 0.07956661651522969
mae: 0.03946489259543659
r2: 0.714544919371213
pearson: 0.8503370044555117

=== Experiment 415 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006328320063917618
rmse: 0.07955073892753994
mae: 0.03215924362940376
r2: 0.7146588336246185
pearson: 0.8455787094710779

=== Experiment 307 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005995639351069007
rmse: 0.07743151394018463
mae: 0.029704330281134072
r2: 0.7296592605429828
pearson: 0.8543542758948119

=== Experiment 392 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006892744494875252
rmse: 0.08302255413365245
mae: 0.037201167450256235
r2: 0.6892091844549286
pearson: 0.8335874535006726

=== Experiment 324 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007500534191319556
rmse: 0.08660562447854964
mae: 0.0384465855423333
r2: 0.6618042145509577
pearson: 0.8135226914682918

=== Experiment 300 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007705755262446824
rmse: 0.08778243139972157
mae: 0.05281720949256339
r2: 0.6525508867785301
pearson: 0.8261385749323694

=== Experiment 173 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007049502762766201
rmse: 0.08396131706188392
mae: 0.03140741934916428
r2: 0.6821410231503157
pearson: 0.8345078737682962

=== Experiment 486 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00649107099118255
rmse: 0.08056718309077555
mae: 0.03313716812111052
r2: 0.7073204659464036
pearson: 0.8420151247713288

=== Experiment 494 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.008506760798027483
rmse: 0.09223210286027031
mae: 0.055881680866199414
r2: 0.6164338997286954
pearson: 0.8039907438953051

=== Experiment 201 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006264291513835326
rmse: 0.07914727736211351
mae: 0.031684585847697073
r2: 0.7175458527667089
pearson: 0.8492395555847817

=== Experiment 94 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007654517486316956
rmse: 0.08749009936168181
mae: 0.04941116852145032
r2: 0.654861175552756
pearson: 0.8200442969796657

=== Experiment 303 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007438069058777499
rmse: 0.08624424072816399
mae: 0.0438624500432452
r2: 0.6646207398842716
pearson: 0.83704947633257

=== Experiment 122 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007006895609301526
rmse: 0.08370720165733368
mae: 0.033266835687874335
r2: 0.6840621609471957
pearson: 0.8405041400058756

=== Experiment 191 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007157771472719312
rmse: 0.08460361382777518
mae: 0.035145305214992414
r2: 0.6772592346712334
pearson: 0.82485162655335

=== Experiment 126 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00847826034215714
rmse: 0.0920774692427911
mae: 0.040001656131194656
r2: 0.6177189727398793
pearson: 0.7870648151534301

=== Experiment 370 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006770559311048766
rmse: 0.08228340847977049
mae: 0.0333757677520473
r2: 0.6947184606158529
pearson: 0.8357783140396207

=== Experiment 428 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0076133841215435235
rmse: 0.0872547083058761
mae: 0.040240902305910584
r2: 0.6567158608662117
pearson: 0.8299730890849232

=== Experiment 371 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006634149635079975
rmse: 0.08145028934927104
mae: 0.032633547051590286
r2: 0.7008691128667959
pearson: 0.8393679417068354

=== Experiment 4 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006331058039911945
rmse: 0.07956794605814546
mae: 0.03321545369407409
r2: 0.7145353794921097
pearson: 0.8453842203488577

=== Experiment 398 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006644078857144835
rmse: 0.0815112192102709
mae: 0.030383290538285767
r2: 0.7004214086140761
pearson: 0.8606227245698131

=== Experiment 183 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006333424059756339
rmse: 0.07958281259013367
mae: 0.03122155642918212
r2: 0.7144286966986908
pearson: 0.8460152176995565

=== Experiment 66 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006436886040715716
rmse: 0.08023020653541729
mae: 0.037081429963153646
r2: 0.7097636415143327
pearson: 0.8438614370412828

=== Experiment 52 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006743420234743443
rmse: 0.082118330686537
mae: 0.039342502830033634
r2: 0.6959421496216389
pearson: 0.8355026046069609

=== Experiment 410 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007340000539905923
rmse: 0.08567380311335504
mae: 0.04542068320296016
r2: 0.6690426062369351
pearson: 0.8334424659032721

=== Experiment 53 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006054383250286492
rmse: 0.07780991742886309
mae: 0.030088807151705255
r2: 0.7270105239824338
pearson: 0.8539767735007993

=== Experiment 135 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005480117611471276
rmse: 0.07402781647104875
mae: 0.03041442558972649
r2: 0.7529039088829751
pearson: 0.8680383806157844

=== Experiment 449 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006741243500944208
rmse: 0.08210507597550963
mae: 0.03792082999854309
r2: 0.6960402975905926
pearson: 0.8386434435812724

=== Experiment 114 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005859704051906085
rmse: 0.07654870378984928
mae: 0.03302601895795424
r2: 0.7357885233525385
pearson: 0.8585626572691828

=== Experiment 270 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006174436774066741
rmse: 0.07857758442499198
mae: 0.04133595025033523
r2: 0.7215973634347791
pearson: 0.8547117202719909

=== Experiment 234 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.008404657043485548
rmse: 0.09167691663382636
mae: 0.04162211659993301
r2: 0.6210377130816922
pearson: 0.7912708602203733

=== Experiment 329 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006949302295217905
rmse: 0.08336247534243393
mae: 0.04100840106217162
r2: 0.6866590181304699
pearson: 0.8321974342447693

=== Experiment 239 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.008618939573038317
rmse: 0.0928382441294444
mae: 0.04036221281205069
r2: 0.6113758081371115
pearson: 0.7867484677363167

=== Experiment 96 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007827813830008945
rmse: 0.08847493334277763
mae: 0.040251964309267536
r2: 0.647047319689238
pearson: 0.8405084376761445

=== Experiment 166 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007703921285478139
rmse: 0.08777198462765975
mae: 0.040226927757108843
r2: 0.6526335799928533
pearson: 0.8081972530381125

=== Experiment 98 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006624401382237719
rmse: 0.08139042561774523
mae: 0.035108994084957004
r2: 0.7013086572969189
pearson: 0.838576753797207

=== Experiment 152 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006271290345237767
rmse: 0.07919147899387766
mae: 0.030466744835949612
r2: 0.7172302785391942
pearson: 0.852041283494665

=== Experiment 451 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007082700328964958
rmse: 0.08415878046267637
mae: 0.034594316213933586
r2: 0.68064415950178
pearson: 0.8278566862425488

=== Experiment 109 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005799830198843822
rmse: 0.07615661625127407
mae: 0.03672494546921254
r2: 0.7384882090346179
pearson: 0.8618341514997938

=== Experiment 447 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006142151336357897
rmse: 0.0783718784791962
mae: 0.032956214763053306
r2: 0.723053099611163
pearson: 0.851574165356614

=== Experiment 360 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006756214484246027
rmse: 0.08219619507158483
mae: 0.03382242341431207
r2: 0.6953652625427473
pearson: 0.836940926508269

=== Experiment 227 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.013075164876198916
rmse: 0.11434668721130016
mae: 0.045583290608016525
r2: 0.41044657055234735
pearson: 0.6440207865276748

=== Experiment 103 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006886835013761794
rmse: 0.08298695688939192
mae: 0.033346097812323984
r2: 0.6894756403573729
pearson: 0.8465453851730976

=== Experiment 111 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064545782742712115
rmse: 0.08034039005550826
mae: 0.033858301907671556
r2: 0.7089659064902629
pearson: 0.8478318190331048

=== Experiment 164 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.009115571778338036
rmse: 0.09547550355111009
mae: 0.03923454135398106
r2: 0.5889828805846968
pearson: 0.8373507466624173

=== Experiment 377 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007294906232131812
rmse: 0.08541022322961
mae: 0.035954986465997776
r2: 0.6710758887269471
pearson: 0.8259360957904596

=== Experiment 364 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006955520665477861
rmse: 0.08339976418118855
mae: 0.031562885920946644
r2: 0.6863786345523627
pearson: 0.8427219490212206

=== Experiment 237 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005901887559711444
rmse: 0.07682374346327732
mae: 0.029755343565283615
r2: 0.7338864875519765
pearson: 0.8574036482557721

=== Experiment 6 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006647083142540482
rmse: 0.08152964578937211
mae: 0.03683718293475211
r2: 0.7002859467078736
pearson: 0.8379765185666711

=== Experiment 2 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007057722592360435
rmse: 0.084010252900229
mae: 0.03615710033759999
r2: 0.6817703946517354
pearson: 0.8267964737013432

=== Experiment 489 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005974339348836941
rmse: 0.07729385065344423
mae: 0.03450572594516618
r2: 0.7306196682687788
pearson: 0.8557667058784323

=== Experiment 385 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006295403972502705
rmse: 0.0793435817977907
mae: 0.03090487689154313
r2: 0.7161430056990371
pearson: 0.8498977864733218

=== Experiment 456 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0063717250789337475
rmse: 0.07982308612759687
mae: 0.03533784746932102
r2: 0.7127017206015492
pearson: 0.8444440135822178

=== Experiment 406 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006049373002889417
rmse: 0.07777771533601008
mae: 0.032112023974893364
r2: 0.7272364338323893
pearson: 0.8612514832317518

=== Experiment 104 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006138827957058198
rmse: 0.0783506729840797
mae: 0.029676541675498613
r2: 0.7232029493210552
pearson: 0.8601283317152889

=== Experiment 317 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006116699504452679
rmse: 0.07820933131316671
mae: 0.03439056626956484
r2: 0.7242007115095613
pearson: 0.8534722805314912

=== Experiment 39 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006504852830232494
rmse: 0.08065266784324307
mae: 0.039650980075667405
r2: 0.7066990488894931
pearson: 0.843682314365997

=== Experiment 92 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00652902124597429
rmse: 0.08080235916094461
mae: 0.03787578759140299
r2: 0.7056093056610286
pearson: 0.8523386123167714

=== Experiment 216 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006544981617954809
rmse: 0.08090106067261918
mae: 0.036151385356437055
r2: 0.704889659513124
pearson: 0.8404635437676823

=== Experiment 407 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00633694663011271
rmse: 0.07960494099057364
mae: 0.038209369665960335
r2: 0.7142698655517239
pearson: 0.8475293490770127

=== Experiment 247 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005788497235090451
rmse: 0.07608217422688741
mae: 0.03186969809252819
r2: 0.7389992073822387
pearson: 0.859842623782248

=== Experiment 117 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065760147391431155
rmse: 0.08109263060934153
mae: 0.03402627439888608
r2: 0.7034903897374644
pearson: 0.8481961411576102

=== Experiment 120 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068566699031609975
rmse: 0.0828050113408663
mae: 0.035848835099876714
r2: 0.6908357719176823
pearson: 0.8435318615982832

=== Experiment 438 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006372928665176315
rmse: 0.07983062485773436
mae: 0.03958927709212532
r2: 0.7126474514276104
pearson: 0.8501800772271987

=== Experiment 431 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006408829732992243
rmse: 0.08005516681009567
mae: 0.03538039567245552
r2: 0.7110286880810588
pearson: 0.8453719857542067

=== Experiment 320 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.011328686922845325
rmse: 0.10643630453395743
mae: 0.04889514453192173
r2: 0.4891944927853291
pearson: 0.7159017097113687

=== Experiment 281 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007280724863756734
rmse: 0.08532716369220726
mae: 0.03682592289360171
r2: 0.6717153203852819
pearson: 0.8214587179787975

=== Experiment 134 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006314990594878821
rmse: 0.07946691509602484
mae: 0.03224073729808711
r2: 0.7152598535168297
pearson: 0.8542174271758328

=== Experiment 483 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007010700794872354
rmse: 0.08372992771328752
mae: 0.041073270964303805
r2: 0.6838905868045971
pearson: 0.8313495134613854

=== Experiment 461 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006721072431321441
rmse: 0.08198214702800508
mae: 0.039074476107795623
r2: 0.6969498022419163
pearson: 0.8429905658372318

=== Experiment 272 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005743571838170773
rmse: 0.07578635654371288
mae: 0.03588123891396272
r2: 0.7410248737561675
pearson: 0.8676943021311252

=== Experiment 108 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006723204378816846
rmse: 0.08199514850780408
mae: 0.03378823472289814
r2: 0.6968536736676905
pearson: 0.8375840623600922

=== Experiment 321 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007232435990678561
rmse: 0.08504372987280462
mae: 0.034817935093092306
r2: 0.6738926444188184
pearson: 0.822912179996112

=== Experiment 450 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005955272859251525
rmse: 0.07717041440378253
mae: 0.03000151956041293
r2: 0.7314793678923814
pearson: 0.8569254134676749

=== Experiment 246 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00699460091435534
rmse: 0.08363373072125468
mae: 0.03366807550547114
r2: 0.68461652333101
pearson: 0.8365475170311547

=== Experiment 356 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006148546424193088
rmse: 0.0784126674982626
mae: 0.030993356503382995
r2: 0.7227647479153676
pearson: 0.8505089285464523

=== Experiment 74 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006263256482678669
rmse: 0.07914073845168915
mae: 0.03040534162158422
r2: 0.7175925218660106
pearson: 0.8506577503058543

=== Experiment 116 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006557110648506895
rmse: 0.08097598809836713
mae: 0.03284920711333268
r2: 0.704342766863925
pearson: 0.8413296831271014

=== Experiment 345 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005871711405631742
rmse: 0.0766270931566097
mae: 0.03125463870678782
r2: 0.7352471170578891
pearson: 0.8576773815493928

=== Experiment 95 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006320490056789661
rmse: 0.07950150977679393
mae: 0.033179037467146295
r2: 0.7150118851997682
pearson: 0.8462822925731596

=== Experiment 84 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005812951856546345
rmse: 0.07624271674426578
mae: 0.03025317231596796
r2: 0.7378965592640944
pearson: 0.8625471063355098

=== Experiment 378 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006399567259642217
rmse: 0.0799972953270435
mae: 0.029929889264629136
r2: 0.7114463289276851
pearson: 0.8495985763142652

=== Experiment 62 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00686411014106655
rmse: 0.08284992541376576
mae: 0.03150066745589732
r2: 0.6905002948652201
pearson: 0.8354487440640497

=== Experiment 56 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00714407054478
rmse: 0.08452260375059444
mae: 0.03287349755976062
r2: 0.6778770034817774
pearson: 0.8284487636539686

=== Experiment 268 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006473427547353417
rmse: 0.08045761335854686
mae: 0.02931606619995941
r2: 0.7081160010631857
pearson: 0.8441810852823107

=== Experiment 425 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006098228584547899
rmse: 0.07809115561027317
mae: 0.031060690255901705
r2: 0.7250335571583943
pearson: 0.8521610650499891

=== Experiment 347 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0058975434171612585
rmse: 0.07679546482157171
mae: 0.02918047118647879
r2: 0.734082363027561
pearson: 0.8572765778127092

=== Experiment 440 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059368414364086635
rmse: 0.07705090159374298
mae: 0.03187747235657212
r2: 0.7323104326360764
pearson: 0.8591383087524306

=== Experiment 404 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006328745810899361
rmse: 0.07955341482865057
mae: 0.03685121822761187
r2: 0.7146396368805972
pearson: 0.8506275214548096

=== Experiment 217 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0052660634171535145
rmse: 0.07256764717939748
mae: 0.02854107406160213
r2: 0.7625555182923073
pearson: 0.8736278916350317

=== Experiment 43 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0058795392570928
rmse: 0.0766781537146846
mae: 0.02900011244046062
r2: 0.734894162680811
pearson: 0.8576267106590603

=== Experiment 1050 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0075697764685268305
rmse: 0.08700446234835792
mae: 0.03735861331894241
r2: 0.6586821107475385
pearson: 0.8257151337354426

=== Experiment 1033 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007135246555267872
rmse: 0.08447038862979069
mae: 0.03621177976503648
r2: 0.6782748732851445
pearson: 0.829801399548566

=== Experiment 1017 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007308308505498508
rmse: 0.08548864547703694
mae: 0.04258353580777824
r2: 0.6704715861196345
pearson: 0.8225568265884312

=== Experiment 1000 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006687757169156131
rmse: 0.08177870853196528
mae: 0.04021728338649504
r2: 0.698451972749782
pearson: 0.8371989533229096

=== Experiment 1063 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007154559024293002
rmse: 0.08458462640629798
mae: 0.03851470239853528
r2: 0.677404082556869
pearson: 0.8276823958765422

=== Experiment 1071 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007776847499682634
rmse: 0.08818643603005302
mae: 0.040290726248091345
r2: 0.6493453690916542
pearson: 0.8060391412595931

=== Experiment 1069 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0067398131034859055
rmse: 0.08209636473977337
mae: 0.04012725555610059
r2: 0.6961047935824214
pearson: 0.838353034894365

=== Experiment 1083 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006374113424225763
rmse: 0.07983804496745749
mae: 0.03628615434020015
r2: 0.7125940311635233
pearson: 0.844264257569378

=== Experiment 1006 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006272669635268933
rmse: 0.0792001870911233
mae: 0.033436582021703395
r2: 0.7171680869587609
pearson: 0.8477292466920656

=== Experiment 1030 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00810004016917525
rmse: 0.09000022316180804
mae: 0.04074163505216272
r2: 0.6347727538722041
pearson: 0.8020898392124399

=== Experiment 1024 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006730609755565709
rmse: 0.08204029348780822
mae: 0.04228935835778991
r2: 0.6965197684894399
pearson: 0.8407584356901259

=== Experiment 1032 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006942696666495995
rmse: 0.08332284600573839
mae: 0.03622673217206851
r2: 0.6869568630221814
pearson: 0.8293481737325784

=== Experiment 1018 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006249878549067703
rmse: 0.07905617337733786
mae: 0.0339981116509727
r2: 0.7181957270044501
pearson: 0.8475321104833807

=== Experiment 1026 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006598935495774943
rmse: 0.08123383221155421
mae: 0.041202967482219695
r2: 0.7024569029091318
pearson: 0.8425393018965895

=== Experiment 1056 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007146062067346303
rmse: 0.0845343839354514
mae: 0.04379172473722466
r2: 0.6777872066057989
pearson: 0.8297002765237006

=== Experiment 1013 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007158430447999907
rmse: 0.08460750822474272
mae: 0.04096728189972595
r2: 0.6772295217658687
pearson: 0.8246735393031235

=== Experiment 1107 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007896487954511429
rmse: 0.08886218517745008
mae: 0.03856010104953713
r2: 0.6439508336412147
pearson: 0.808476450721988

=== Experiment 1080 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00658222461975901
rmse: 0.08113091038413787
mae: 0.04036117646795284
r2: 0.7032103889536713
pearson: 0.8414540365610504

=== Experiment 1076 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.008057212560436563
rmse: 0.08976197725338142
mae: 0.04540801009043793
r2: 0.6367038319003593
pearson: 0.806720458908287

=== Experiment 1055 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006754896415227589
rmse: 0.08218817685791302
mae: 0.041126707597704246
r2: 0.695424693694662
pearson: 0.8363945585825532

=== Experiment 1046 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006859009623271078
rmse: 0.08281913802540496
mae: 0.04120013492651078
r2: 0.6907302749677083
pearson: 0.8348451948489852

=== Experiment 1072 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00727495903663563
rmse: 0.08529337041432722
mae: 0.046470471155741794
r2: 0.6719752989924378
pearson: 0.8251090264439372

=== Experiment 1114 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006479250385298795
rmse: 0.08049379097358253
mae: 0.036935854268840156
r2: 0.7078534518630557
pearson: 0.8416993809303062

=== Experiment 1002 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065179502247824294
rmse: 0.0807338233009092
mae: 0.038419274325829186
r2: 0.7061084931338406
pearson: 0.8414152250476998

=== Experiment 1054 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064986451337777605
rmse: 0.08061417452146838
mae: 0.041390595148281376
r2: 0.7069789511904206
pearson: 0.8439716518145196

=== Experiment 1091 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007397512042716476
rmse: 0.08600879049676537
mae: 0.04024057177298652
r2: 0.6664494378879555
pearson: 0.8222344623366908

=== Experiment 1016 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007225208128273095
rmse: 0.08500122427514262
mae: 0.034945367513401195
r2: 0.6742185455534553
pearson: 0.8423692363675357

=== Experiment 1011 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007490894798813385
rmse: 0.08654995551017565
mae: 0.046328187405679185
r2: 0.66223885051644
pearson: 0.8257310619153184

=== Experiment 1090 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006644591410842982
rmse: 0.08151436321804263
mae: 0.0375105441234548
r2: 0.7003982977934189
pearson: 0.8423583351222776

=== Experiment 1096 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007239716397831708
rmse: 0.085086523009415
mae: 0.040662790420834025
r2: 0.6735643740646351
pearson: 0.8225869203342238

=== Experiment 1110 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007852799047333912
rmse: 0.08861602026345977
mae: 0.04183134839333589
r2: 0.6459207472368905
pearson: 0.8037643151467687

=== Experiment 1065 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006792377852810813
rmse: 0.08241588349833309
mae: 0.04261609986778276
r2: 0.6937346721708768
pearson: 0.8382111275995809

=== Experiment 1133 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007522126968242049
rmse: 0.08673019640380188
mae: 0.03939527297093803
r2: 0.6608306057432305
pearson: 0.8175732154765956

=== Experiment 1126 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006960665530053661
rmse: 0.0834306030785686
mae: 0.03316008541821711
r2: 0.6861466548730677
pearson: 0.8480129241433356

=== Experiment 1183 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007119278804396186
rmse: 0.08437581883689299
mae: 0.03754348926578034
r2: 0.6789948521440041
pearson: 0.8350189022530601

=== Experiment 1115 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064192941930060945
rmse: 0.08012049795780163
mae: 0.03514464643757884
r2: 0.7105568501847952
pearson: 0.843313857722506

=== Experiment 1179 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006605920858606083
rmse: 0.08127681624304733
mae: 0.03784837653873298
r2: 0.7021419359735688
pearson: 0.8385768489167329

=== Experiment 1045 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006517171184043966
rmse: 0.08072899840852707
mae: 0.034534486699739186
r2: 0.7061436197378558
pearson: 0.8403346648253898

=== Experiment 1121 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006900171783766399
rmse: 0.08306727263950826
mae: 0.03832810689935111
r2: 0.6888742912678267
pearson: 0.8341867311222759

=== Experiment 1140 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006746071279701161
rmse: 0.08213447071541376
mae: 0.03747088547995888
r2: 0.6958226151713094
pearson: 0.834439067002537

=== Experiment 1166 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006867870475679362
rmse: 0.08287261595774181
mae: 0.039520219439964956
r2: 0.6903307430325782
pearson: 0.8370365580459737

=== Experiment 1044 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006422714359877693
rmse: 0.08014183900983114
mae: 0.03291724680376389
r2: 0.7104026363658853
pearson: 0.8453196276712461

=== Experiment 1180 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006949970059842372
rmse: 0.08336648043333947
mae: 0.03995959679369156
r2: 0.6866289089174584
pearson: 0.8306584116406286

=== Experiment 1120 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068928780211366175
rmse: 0.0830233582863077
mae: 0.038720720730891243
r2: 0.689203163814573
pearson: 0.8422435673704811

=== Experiment 1167 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007459967144245416
rmse: 0.08637110132587991
mae: 0.04588872671004108
r2: 0.6636333648486079
pearson: 0.825616719581275

=== Experiment 1197 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006879121352242982
rmse: 0.0829404687245194
mae: 0.0388086772816991
r2: 0.6898234459602139
pearson: 0.830973963967806

=== Experiment 1068 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008002338755769792
rmse: 0.08945579218681031
mae: 0.05186029593013522
r2: 0.6391780676009757
pearson: 0.821932509751156

=== Experiment 1206 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006921320370674155
rmse: 0.08319447319788831
mae: 0.043188614798139416
r2: 0.6879207107923608
pearson: 0.8340045799243322

=== Experiment 1138 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007460908302865592
rmse: 0.08637654949617744
mae: 0.043742216491434616
r2: 0.6635909284206598
pearson: 0.8191372339804859

=== Experiment 1178 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007770256011068068
rmse: 0.08814905564478878
mae: 0.04558050269392275
r2: 0.6496425764121487
pearson: 0.8125469895893973

=== Experiment 1087 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007059268051592383
rmse: 0.08401945043614831
mae: 0.037172145770758204
r2: 0.6817007105751756
pearson: 0.8262715396122208

=== Experiment 1042 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006461625349395134
rmse: 0.08038423570200275
mae: 0.03741998260074551
r2: 0.7086481569745802
pearson: 0.842722799810658

=== Experiment 1157 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006923336510055315
rmse: 0.08320658934276369
mae: 0.04073635495556927
r2: 0.6878298039550352
pearson: 0.8328799175922568

=== Experiment 1182 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0076465303504773124
rmse: 0.08744444150703527
mae: 0.04172299503447794
r2: 0.6552213119923584
pearson: 0.8107643056351198

=== Experiment 1145 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00704403428410925
rmse: 0.08392874527901183
mae: 0.04328875801033992
r2: 0.6823875944460961
pearson: 0.8336224285236322

=== Experiment 1059 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007899435893319588
rmse: 0.0888787707685001
mae: 0.049213956006797385
r2: 0.643817912377842
pearson: 0.8213853212427198

=== Experiment 1102 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005425483213949144
rmse: 0.0736578795102679
mae: 0.027733679729419407
r2: 0.7553673498208108
pearson: 0.8718633345760677

=== Experiment 1001 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006833868847560028
rmse: 0.08266721749012741
mae: 0.04121185214226326
r2: 0.6918638614792179
pearson: 0.8352721056786949

=== Experiment 1219 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0066901452378537105
rmse: 0.08179330802610756
mae: 0.039975322218381425
r2: 0.6983442957832777
pearson: 0.8370008715708614

=== Experiment 1101 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0076942368740563595
rmse: 0.08771679926933244
mae: 0.04367120813376554
r2: 0.6530702458414258
pearson: 0.8213638532671003

=== Experiment 1196 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00608278199785532
rmse: 0.07799219190313425
mae: 0.03255410549286133
r2: 0.7257300369538
pearson: 0.8519062939159131

=== Experiment 1211 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0069596307859900635
rmse: 0.08342440162200783
mae: 0.036753962392325305
r2: 0.6861933110274698
pearson: 0.8287624155826598

=== Experiment 1060 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007127271185635561
rmse: 0.08442316735135896
mae: 0.04469594690904967
r2: 0.6786344791916368
pearson: 0.8339907297420412

=== Experiment 1234 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005856603716794881
rmse: 0.07652845037497415
mae: 0.02993309745988711
r2: 0.7359283160981408
pearson: 0.8585089503426614

=== Experiment 1049 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007352524562505634
rmse: 0.08574686328085497
mae: 0.040230034368279155
r2: 0.6684779035701582
pearson: 0.8224499398733585

=== Experiment 1162 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00777025753125221
rmse: 0.08814906426759282
mae: 0.0392647713309272
r2: 0.649642507867715
pearson: 0.8065178078433313

=== Experiment 1204 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006816156715018202
rmse: 0.08256001886517592
mae: 0.04225682820125355
r2: 0.6926624937398231
pearson: 0.8359573249427843

=== Experiment 1195 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0066401996791161
rmse: 0.08148742037343003
mae: 0.035287837771763975
r2: 0.700596319044035
pearson: 0.8390918835267467

=== Experiment 1144 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007107737280213822
rmse: 0.08430739754146027
mae: 0.03842105945318432
r2: 0.6795152543895737
pearson: 0.8256648691269038

=== Experiment 1023 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.009257349756515989
rmse: 0.09621512228603146
mae: 0.04876966482814712
r2: 0.5825901739499133
pearson: 0.7991295264540899

=== Experiment 1201 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008530087463816092
rmse: 0.09235847261521864
mae: 0.05316144122480047
r2: 0.6153821106351378
pearson: 0.8062383563448511

=== Experiment 1243 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006723920874972094
rmse: 0.08199951752889827
mae: 0.038302152870627576
r2: 0.6968213671714045
pearson: 0.8360549748811099

=== Experiment 1149 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006644690785924446
rmse: 0.08151497277141449
mae: 0.032107266025764906
r2: 0.7003938170147339
pearson: 0.8444964401871082

=== Experiment 1194 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0067369392740136026
rmse: 0.08207886009207001
mae: 0.03429419939708508
r2: 0.6962343732884584
pearson: 0.8430473789068473

=== Experiment 1189 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006599945417052466
rmse: 0.08124004811084534
mae: 0.040928864634867786
r2: 0.7024113660032285
pearson: 0.843217195378138

=== Experiment 1041 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006739806633381741
rmse: 0.08209632533421786
mae: 0.03231377767474641
r2: 0.6961050853165704
pearson: 0.8387322543438067

=== Experiment 1254 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006569263374559898
rmse: 0.08105099243414542
mae: 0.035433855600150846
r2: 0.7037948057950187
pearson: 0.8396960145240895

=== Experiment 1247 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007087614170139366
rmse: 0.08418796927197714
mae: 0.036924685467052125
r2: 0.6804225965659729
pearson: 0.8377888971086764

=== Experiment 1084 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005674789435374166
rmse: 0.07533119828712515
mae: 0.028828050402755687
r2: 0.744126242024816
pearson: 0.8669070855572334

=== Experiment 1259 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006717510943459724
rmse: 0.08196042303123943
mae: 0.03793222878876733
r2: 0.6971103881620706
pearson: 0.8355300240849066

=== Experiment 1233 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006576906117397103
rmse: 0.08109812647279284
mae: 0.035943102546439745
r2: 0.7034501978843177
pearson: 0.842575084632694

=== Experiment 1192 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006399514303894929
rmse: 0.07999696434174818
mae: 0.034213449663844216
r2: 0.7114487166790228
pearson: 0.8437277183282949

=== Experiment 1172 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006598676013397417
rmse: 0.08123223506341197
mae: 0.033501944793642296
r2: 0.7024686028553275
pearson: 0.8488237835175132

=== Experiment 1097 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00695245317926979
rmse: 0.08338137189606436
mae: 0.031433188786603616
r2: 0.6865169461553805
pearson: 0.8332811414260471

=== Experiment 1112 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0056059777768127225
rmse: 0.07487307778375832
mae: 0.029060343626887628
r2: 0.7472289294230243
pearson: 0.8691807819623744

=== Experiment 1086 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006330211984291377
rmse: 0.07956262931987214
mae: 0.03506227191912219
r2: 0.7145735277676635
pearson: 0.845473944200731

=== Experiment 1131 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00805293032735363
rmse: 0.08973812081469965
mae: 0.04680863602216011
r2: 0.6368969159053139
pearson: 0.8233899841591975

=== Experiment 1232 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00710983323292966
rmse: 0.08431982704518351
mae: 0.04578778515210074
r2: 0.6794207488041158
pearson: 0.8396179733457321

=== Experiment 1019 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005688399057410264
rmse: 0.07542147610203784
mae: 0.03396105746901142
r2: 0.743512590157261
pearson: 0.8643727865291804

=== Experiment 1203 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007382701475247733
rmse: 0.08592264820900095
mae: 0.0491819436799156
r2: 0.6671172398564917
pearson: 0.8274879012625249

=== Experiment 1229 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007342964376929546
rmse: 0.08569109858631493
mae: 0.04031249291322971
r2: 0.6689089681300244
pearson: 0.8202412556538278

=== Experiment 1143 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006567640991202447
rmse: 0.08104098340470979
mae: 0.03832085463847443
r2: 0.7038679583465408
pearson: 0.8402058657449121

=== Experiment 1266 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005722547438852004
rmse: 0.07564752103573523
mae: 0.029020833830610042
r2: 0.7419728546679052
pearson: 0.861461098484006

=== Experiment 1228 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007082924628955351
rmse: 0.08416011305217781
mae: 0.0458192092052896
r2: 0.6806340459139352
pearson: 0.8320487469593622

=== Experiment 1122 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005745865614194034
rmse: 0.07580148820566807
mae: 0.030251628886915637
r2: 0.7409214484048467
pearson: 0.8617973423568241

=== Experiment 1040 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006901490694544626
rmse: 0.08307521107132154
mae: 0.03331150633788928
r2: 0.6888148221613333
pearson: 0.832837937971131

=== Experiment 1269 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006975432061648923
rmse: 0.08351905208782559
mae: 0.0385288599420813
r2: 0.6854808384626851
pearson: 0.8297942277495692

=== Experiment 1028 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006779755981796734
rmse: 0.08233927362927569
mae: 0.0359231307074657
r2: 0.6943037867795913
pearson: 0.8335519373127948

=== Experiment 1113 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005581556321371662
rmse: 0.07470981409006223
mae: 0.02894350237025656
r2: 0.7483300821001573
pearson: 0.8653122558672359

=== Experiment 1132 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.008327869596481578
rmse: 0.09125716189144596
mae: 0.041416135846561716
r2: 0.6245000252703612
pearson: 0.796027936752192

=== Experiment 1152 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007788728168662275
rmse: 0.08825377141325053
mae: 0.041044817028078766
r2: 0.6488096749564443
pearson: 0.8131588042193995

=== Experiment 1239 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006845412452114557
rmse: 0.08273700775417586
mae: 0.031443012561468524
r2: 0.6913433654305832
pearson: 0.8323448770563218

=== Experiment 1297 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007330443289052563
rmse: 0.0856180079717612
mae: 0.03934780690117938
r2: 0.6694735384714453
pearson: 0.8277510433152651

=== Experiment 1185 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005982418696666674
rmse: 0.077346096841836
mae: 0.03483851108785977
r2: 0.7302553740311291
pearson: 0.8548938663178388

=== Experiment 1267 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007566003746941462
rmse: 0.08698277845034304
mae: 0.043663556831688224
r2: 0.6588522211033212
pearson: 0.8211205285443578

=== Experiment 1181 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007483793882007725
rmse: 0.08650892371315069
mae: 0.04669980513481854
r2: 0.6625590277298545
pearson: 0.8281441037335296

=== Experiment 1281 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067593658182531
rmse: 0.08221536242243964
mae: 0.03655497924536767
r2: 0.6952231702793765
pearson: 0.8339638769804165

=== Experiment 1241 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005980643916848083
rmse: 0.07733462301484428
mae: 0.03432923939410074
r2: 0.7303353980720091
pearson: 0.855306918525602

=== Experiment 1199 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007391681073170702
rmse: 0.08597488629344445
mae: 0.0460836271115167
r2: 0.6667123537383683
pearson: 0.8244117760926162

=== Experiment 1109 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005360025289858812
rmse: 0.07321219358726258
mae: 0.02754988489584077
r2: 0.7583188188078083
pearson: 0.8714311981843141

=== Experiment 1285 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007521134881986593
rmse: 0.08672447683316742
mae: 0.03858217301753737
r2: 0.6608753384758385
pearson: 0.82616120461355

=== Experiment 1240 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007219904649899471
rmse: 0.08497002206601732
mae: 0.036639980425540945
r2: 0.6744576770590827
pearson: 0.8214124865125808

=== Experiment 1293 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006303784355865856
rmse: 0.07939637495418701
mae: 0.03886206896883774
r2: 0.7157651379016815
pearson: 0.8471753104914247

=== Experiment 1165 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0059366358809019686
rmse: 0.07704956768796285
mae: 0.031680954823966725
r2: 0.7323197010434014
pearson: 0.8563652053299831

=== Experiment 1202 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006216612517123963
rmse: 0.07884549776064556
mae: 0.03275935802689776
r2: 0.7196956777432929
pearson: 0.8493027086214504

=== Experiment 1300 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006558733314729536
rmse: 0.08098600690693138
mae: 0.03450461471758136
r2: 0.704269601558136
pearson: 0.8392107479555975

=== Experiment 1289 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006890450860770328
rmse: 0.08300873966499146
mae: 0.043591759120188284
r2: 0.6893126034072141
pearson: 0.8344710431336129

=== Experiment 1170 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006797179634446069
rmse: 0.08244500976072518
mae: 0.03989405905755748
r2: 0.6935181619503688
pearson: 0.8343163478462858

=== Experiment 1316 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007200641660091615
rmse: 0.08485659467649886
mae: 0.044310306061495136
r2: 0.6753262367912287
pearson: 0.8266766331765564

=== Experiment 1284 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007357910505064663
rmse: 0.08577826359320095
mae: 0.04172980216022242
r2: 0.668235053790706
pearson: 0.8199820990946303

=== Experiment 1062 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.009180056017176103
rmse: 0.09581260886321853
mae: 0.05172399840450142
r2: 0.5860753146371724
pearson: 0.8008375974744091

=== Experiment 1283 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007246830322261351
rmse: 0.08512831680622701
mae: 0.04491225412086476
r2: 0.6732436103431801
pearson: 0.8309388829264742

=== Experiment 1223 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006385615096231934
rmse: 0.07991004377568525
mae: 0.03515887557559756
r2: 0.7120754258350381
pearson: 0.8550722896309613

=== Experiment 1251 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059738166458342574
rmse: 0.0772904693078924
mae: 0.0328234448781607
r2: 0.7306432367171107
pearson: 0.855054209188353

=== Experiment 1324 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007059125987511592
rmse: 0.08401860500812658
mae: 0.04086916506380403
r2: 0.6817071161820512
pearson: 0.8288451753112509

=== Experiment 1154 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00613365184400051
rmse: 0.07831763431054663
mae: 0.032128383896138125
r2: 0.7234363379806449
pearson: 0.8508119514027099

=== Experiment 1260 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007507025551661613
rmse: 0.08664309292529677
mae: 0.035861175993007866
r2: 0.6615115219702008
pearson: 0.8301839149238328

=== Experiment 1218 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0052934413490654
rmse: 0.07275603994903379
mae: 0.028227560966853908
r2: 0.7613210593923497
pearson: 0.8731500288080625

=== Experiment 1237 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0069219384255757585
rmse: 0.08319818763396086
mae: 0.03727993775171572
r2: 0.6878928429688744
pearson: 0.8318882878494721

=== Experiment 1302 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006520527122351939
rmse: 0.08074978094305854
mae: 0.04021007978785398
r2: 0.7059923019566108
pearson: 0.8448040615982927

=== Experiment 1298 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006755620299356082
rmse: 0.08219258056148428
mae: 0.03802289340439739
r2: 0.695392054077914
pearson: 0.8346124060820497

=== Experiment 1215 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0063673312513735595
rmse: 0.07979555909556345
mae: 0.0368946547355339
r2: 0.7128998363523666
pearson: 0.8535545171666536

=== Experiment 1338 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006018410366543577
rmse: 0.07757841430799921
mae: 0.03637520866730431
r2: 0.728632525477525
pearson: 0.855631935638951

=== Experiment 1191 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0056487918778350054
rmse: 0.07515844515312305
mae: 0.03045743839873294
r2: 0.7452984604518567
pearson: 0.8633079999280509

=== Experiment 1303 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0071876367563107136
rmse: 0.08477993132994809
mae: 0.03578746650789751
r2: 0.6759126221788136
pearson: 0.8282774686599513

=== Experiment 1103 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006231761752819273
rmse: 0.07894150842756473
mae: 0.028054183224752855
r2: 0.7190126053734801
pearson: 0.8609518478242343

=== Experiment 1392 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007679235159727459
rmse: 0.08763124533936202
mae: 0.037879117619007086
r2: 0.6537466665377564
pearson: 0.8214575398200044

=== Experiment 1186 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006577906245818975
rmse: 0.0811042924007045
mae: 0.03003059801092358
r2: 0.7034051025339656
pearson: 0.8563486320396401

=== Experiment 1244 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006259791511692195
rmse: 0.07911884422621576
mae: 0.032421451234978
r2: 0.7177487558827403
pearson: 0.8529010351071924

=== Experiment 1292 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005953997488930857
rmse: 0.07716215062406476
mae: 0.02931958634828697
r2: 0.7315368737788069
pearson: 0.8587313951333241

=== Experiment 1358 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00682469848328629
rmse: 0.0826117333269693
mae: 0.03975760632615409
r2: 0.6922773491681349
pearson: 0.8367180078054952

=== Experiment 1265 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.008631922716794341
rmse: 0.09290814128371282
mae: 0.038636377559447516
r2: 0.6107904038994711
pearson: 0.7962878441942349

=== Experiment 1141 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007539032495437736
rmse: 0.08682760215183727
mae: 0.04661157756078121
r2: 0.6600683429626686
pearson: 0.8320506549858679

=== Experiment 1004 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007054858927119439
rmse: 0.08399320762489929
mae: 0.04283843620778387
r2: 0.6818995160570565
pearson: 0.838061952566585

=== Experiment 1313 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007091097146802288
rmse: 0.08420865244618446
mae: 0.036016743906181044
r2: 0.680265550681217
pearson: 0.8248636436801331

=== Experiment 1253 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006903608586562194
rmse: 0.08308795692855972
mae: 0.041608990733779494
r2: 0.6887193273424168
pearson: 0.8337831220131199

=== Experiment 1093 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006404900139761859
rmse: 0.0800306200136039
mae: 0.03147387712171475
r2: 0.7112058717102707
pearson: 0.856317761673224

=== Experiment 1221 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006323923442441391
rmse: 0.07952310005552721
mae: 0.03508857972091981
r2: 0.7148570753518804
pearson: 0.8494063656901341

=== Experiment 1225 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006583237672084126
rmse: 0.08113715346303521
mae: 0.035071725186951876
r2: 0.7031647108702099
pearson: 0.8479703953509806

=== Experiment 1108 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00646982912190986
rmse: 0.08043524800676541
mae: 0.04281021186554122
r2: 0.7082782524826476
pearson: 0.8473964984845818

=== Experiment 1325 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005926224919198915
rmse: 0.0769819778857293
mae: 0.026591800338446898
r2: 0.7327891267243776
pearson: 0.871467631956659

=== Experiment 1047 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006847358085767984
rmse: 0.08274876485947077
mae: 0.0378688354486935
r2: 0.691255637665489
pearson: 0.831529389798532

=== Experiment 1250 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007229394689499606
rmse: 0.08502584718483908
mae: 0.03905987797338614
r2: 0.6740297753503984
pearson: 0.8216976987391843

=== Experiment 1146 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006549496791523528
rmse: 0.08092896138913144
mae: 0.03291811291829964
r2: 0.7046860723241903
pearson: 0.8500805780640582

=== Experiment 1010 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006719443917622424
rmse: 0.08197221430230138
mae: 0.03135060254746535
r2: 0.6970232312078415
pearson: 0.8365464636664348

=== Experiment 1395 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007152432733831455
rmse: 0.08457205645975185
mae: 0.04486065597013561
r2: 0.6774999560579281
pearson: 0.8287215750457088

=== Experiment 1216 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007050415739738393
rmse: 0.08396675377635122
mae: 0.03676410701548552
r2: 0.6820998574204715
pearson: 0.8336707829121695

=== Experiment 1322 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0073395572163968865
rmse: 0.08567121579852177
mae: 0.04996868264973132
r2: 0.6690625954988361
pearson: 0.830062737354494

=== Experiment 1413 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006643542191203106
rmse: 0.08150792716787188
mae: 0.029591093258574036
r2: 0.7004456066451841
pearson: 0.8447955314324472

=== Experiment 1249 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007399101963692253
rmse: 0.08601803278204084
mae: 0.04265842031651493
r2: 0.6663777490509355
pearson: 0.8285020792378517

=== Experiment 1404 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006840767600881662
rmse: 0.08270893301742964
mae: 0.03969510769517127
r2: 0.6915527997283191
pearson: 0.8329435733390604

=== Experiment 1390 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007872687791425019
rmse: 0.08872816797063388
mae: 0.041628568882708536
r2: 0.6450239725195281
pearson: 0.8034506636890874

=== Experiment 1027 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007421657720115665
rmse: 0.08614904364016855
mae: 0.03426950707598635
r2: 0.6653607199213514
pearson: 0.8568994864917165

=== Experiment 1169 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006280404592061253
rmse: 0.07924900372914005
mae: 0.038240784880584675
r2: 0.71681932116141
pearson: 0.8487893348140921

=== Experiment 1226 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006987044467096613
rmse: 0.08358854267838753
mae: 0.033472666370302266
r2: 0.6849572402120592
pearson: 0.8353436902775078

=== Experiment 1273 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006101681315965758
rmse: 0.07811325954001509
mae: 0.034426091017394446
r2: 0.7248778750184264
pearson: 0.8541359956651302

=== Experiment 1245 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006264690392845802
rmse: 0.07914979717501366
mae: 0.03242042288442653
r2: 0.7175278674876867
pearson: 0.8475460656071794

=== Experiment 1287 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0071856215580510165
rmse: 0.08476804561891832
mae: 0.03836300683386216
r2: 0.6760034865813868
pearson: 0.82765859252854

=== Experiment 1252 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005732276513834966
rmse: 0.07571179903974655
mae: 0.027547628753129376
r2: 0.7415341749590167
pearson: 0.8621044665732266

=== Experiment 1171 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006976400780952408
rmse: 0.08352485127764316
mae: 0.03984969685005855
r2: 0.685437159335657
pearson: 0.8296136515434367

=== Experiment 1155 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006833837503141963
rmse: 0.08266702790799947
mae: 0.029736284297636716
r2: 0.6918652747852327
pearson: 0.8521650087886345

=== Experiment 1348 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067840590926872705
rmse: 0.08236539985143805
mae: 0.0356442827193105
r2: 0.694109761403481
pearson: 0.8331788051379927

=== Experiment 1374 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007340558363087658
rmse: 0.08567705855763057
mae: 0.044836986771209356
r2: 0.6690174542351875
pearson: 0.8243028709460671

=== Experiment 1276 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005656371393276697
rmse: 0.07520885182793777
mae: 0.027874282258608794
r2: 0.7449567034365905
pearson: 0.8636153880773841

=== Experiment 1294 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005858687415373541
rmse: 0.07654206304623322
mae: 0.02945817496503418
r2: 0.735834363046335
pearson: 0.859953676743974

=== Experiment 1309 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00700273003516442
rmse: 0.08368231614364184
mae: 0.03804580607137594
r2: 0.6842499848516284
pearson: 0.8277800297740877

=== Experiment 1043 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006753501974884349
rmse: 0.08217969320266624
mae: 0.03539811624044893
r2: 0.6954875683959986
pearson: 0.8417454429708838

=== Experiment 1296 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00743872575671253
rmse: 0.0862480478429079
mae: 0.04087067934202469
r2: 0.6645911296634162
pearson: 0.8165670942348552

=== Experiment 1200 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006588514377491893
rmse: 0.08116966414549152
mae: 0.03099213841478087
r2: 0.7029267865458295
pearson: 0.8510784503525722

=== Experiment 1190 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0057436791626206745
rmse: 0.07578706461277329
mae: 0.026969587544355102
r2: 0.7410200345439586
pearson: 0.8740591391943697

=== Experiment 1382 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007604028848735151
rmse: 0.08720108284152871
mae: 0.049438971441037254
r2: 0.6571376859995708
pearson: 0.8224183258750721

=== Experiment 1168 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007683070330817402
rmse: 0.08765312504878194
mae: 0.050053879686091034
r2: 0.6535737403612989
pearson: 0.8290563521700849

=== Experiment 1304 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005514983209207284
rmse: 0.07426293294239922
mae: 0.02955830806439168
r2: 0.7513318344265818
pearson: 0.8676503862982013

=== Experiment 1314 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007818554346125073
rmse: 0.0884225895692106
mae: 0.05434802931145782
r2: 0.6474648257421514
pearson: 0.8244795635425324

=== Experiment 1198 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006720108591268271
rmse: 0.08197626846391748
mae: 0.03394742391392705
r2: 0.6969932613656956
pearson: 0.8356748178988006

=== Experiment 1075 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005868335309044161
rmse: 0.07660506059683107
mae: 0.0302411403805872
r2: 0.7353993437670895
pearson: 0.8577102055835483

=== Experiment 1264 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007491163657050205
rmse: 0.08655150869309099
mae: 0.046929928293010265
r2: 0.6622267278168754
pearson: 0.8273567095292527

=== Experiment 1329 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006901035326535618
rmse: 0.08307247032883769
mae: 0.03504033647530471
r2: 0.6888353545044367
pearson: 0.831122468686359

=== Experiment 1376 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007148489546168245
rmse: 0.08454874065394614
mae: 0.045330244097161736
r2: 0.6776777526541327
pearson: 0.8416752351258269

=== Experiment 1310 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005453981193857324
rmse: 0.0738510744258831
mae: 0.028205138894817836
r2: 0.7540823884496711
pearson: 0.8686696064713313

=== Experiment 1085 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006425998033371172
rmse: 0.08016232302878436
mae: 0.04121671024681497
r2: 0.7102545769733196
pearson: 0.8506205334654656

=== Experiment 1359 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006986094434931567
rmse: 0.0835828596958226
mae: 0.041148427974073844
r2: 0.6850000767442401
pearson: 0.8305888701553449

=== Experiment 1270 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007586598210418773
rmse: 0.08710108042050209
mae: 0.045451870768563274
r2: 0.6579236258094457
pearson: 0.8226114884404934

=== Experiment 1130 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007488985399850864
rmse: 0.0865389241893546
mae: 0.03643094755394235
r2: 0.6623249444752692
pearson: 0.8179615961011041

=== Experiment 1459 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007579352080136371
rmse: 0.0870594743846778
mae: 0.03743354279123965
r2: 0.6582503506346135
pearson: 0.8309760511655672

=== Experiment 1077 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007832159056699121
rmse: 0.08849948619454873
mae: 0.04911241241893127
r2: 0.6468513953302586
pearson: 0.8145432115658953

=== Experiment 1421 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007339884244774939
rmse: 0.08567312440185043
mae: 0.033178144521283606
r2: 0.669047849933206
pearson: 0.8363961943575026

=== Experiment 1280 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006497337661993816
rmse: 0.08060606467254071
mae: 0.03306788601402873
r2: 0.7070379045176964
pearson: 0.8409214203279161

=== Experiment 1398 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006568575773544265
rmse: 0.08104675054278404
mae: 0.03686056248186381
r2: 0.7038258094221774
pearson: 0.8402423044312182

=== Experiment 1473 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006923943373238655
rmse: 0.08321023598836057
mae: 0.04217421497384967
r2: 0.6878024407611991
pearson: 0.8317370316517769

=== Experiment 1361 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007278952358972793
rmse: 0.08531677653880738
mae: 0.03540523543235419
r2: 0.6717952418458508
pearson: 0.8220497676165548

=== Experiment 1153 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007305604711371618
rmse: 0.08547283025249379
mae: 0.03230727182679091
r2: 0.670593499006785
pearson: 0.8450975358769273

=== Experiment 1386 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007103212341365988
rmse: 0.08428055731523129
mae: 0.03448869021238261
r2: 0.6797192818906447
pearson: 0.8256663348678291

=== Experiment 1357 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00843531479300442
rmse: 0.09184396982385082
mae: 0.04525493417195182
r2: 0.6196553686493937
pearson: 0.789763764085235

=== Experiment 1408 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007340464405995019
rmse: 0.08567651023469046
mae: 0.04628379830205489
r2: 0.6690216907191409
pearson: 0.822863168341837

=== Experiment 1431 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00663211249304139
rmse: 0.08143778295755226
mae: 0.04210757566210611
r2: 0.7009609667047008
pearson: 0.8411345723976756

=== Experiment 1208 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006162882719852315
rmse: 0.07850402995931047
mae: 0.03178151516568158
r2: 0.7221183306539793
pearson: 0.8534422711973797

=== Experiment 1354 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005982628016527976
rmse: 0.077347449967843
mae: 0.030176936677190853
r2: 0.7302459358907119
pearson: 0.8586655672825616

=== Experiment 1389 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007251127726946111
rmse: 0.08515355381278054
mae: 0.04170831665760197
r2: 0.6730498422573767
pearson: 0.8234854499300063

=== Experiment 1291 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007566323534756059
rmse: 0.08698461665579758
mae: 0.04820225335225489
r2: 0.6588378020115102
pearson: 0.8223320616736494

=== Experiment 1224 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006704448124508944
rmse: 0.08188069445546334
mae: 0.03834347607525962
r2: 0.6976993849191748
pearson: 0.8451909093794264

=== Experiment 1451 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006967855912412829
rmse: 0.08347368395136774
mae: 0.037968876512869135
r2: 0.6858224436972223
pearson: 0.8291444009947205

=== Experiment 1331 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008286088641378012
rmse: 0.09102795527406958
mae: 0.04931853281363995
r2: 0.6263839101467764
pearson: 0.8238721587166379

=== Experiment 1420 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00645621536547547
rmse: 0.08035057787891428
mae: 0.031911493872754736
r2: 0.708892090768402
pearson: 0.8424557313975684

=== Experiment 1048 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006702832225790707
rmse: 0.08187082646334229
mae: 0.039058006481855465
r2: 0.6977722450811689
pearson: 0.840231355783532

=== Experiment 1308 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006759675993256303
rmse: 0.0822172487575223
mae: 0.03692876332057361
r2: 0.6952091846250004
pearson: 0.833943818779048

=== Experiment 1461 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006807153961014742
rmse: 0.08250547836971035
mae: 0.04058107253195934
r2: 0.6930684239554242
pearson: 0.8352800506276625

=== Experiment 1487 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006857042948783893
rmse: 0.08280726386485604
mae: 0.041088941576648656
r2: 0.6908189514547358
pearson: 0.8334617379006116

=== Experiment 1399 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0054615537099873416
rmse: 0.07390232547076811
mae: 0.030511555817366524
r2: 0.7537409470302878
pearson: 0.8686870966021504

=== Experiment 1070 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006662997883894048
rmse: 0.08162718838655444
mae: 0.034973828358338954
r2: 0.6995683580248546
pearson: 0.8459545675275657

=== Experiment 1355 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00822204981596137
rmse: 0.09067551938622337
mae: 0.05429092467953403
r2: 0.6292713925991702
pearson: 0.8151190314340829

=== Experiment 1340 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008271901437901989
rmse: 0.09094999416108826
mae: 0.05611282293030838
r2: 0.6270236049072444
pearson: 0.8146316589404521

=== Experiment 1347 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007482214893464588
rmse: 0.0864997970718116
mae: 0.04022807088372882
r2: 0.6626302236282987
pearson: 0.8145865919476917

=== Experiment 1443 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007605051783827244
rmse: 0.08720694802495524
mae: 0.04237372943815406
r2: 0.6570915623064967
pearson: 0.8141757320984969

=== Experiment 1088 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006471892503719618
rmse: 0.08044807333752386
mae: 0.036413807201654325
r2: 0.7081852155050095
pearson: 0.8442259415019299

=== Experiment 1007 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005807583045649596
rmse: 0.07620749993045038
mae: 0.029113483300202702
r2: 0.7381386365844345
pearson: 0.8599525570049509

=== Experiment 1430 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006890947911900003
rmse: 0.0830117335796573
mae: 0.03269124849656498
r2: 0.6892901915905458
pearson: 0.8373481295947083

=== Experiment 1346 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005587213542187511
rmse: 0.07474766579758535
mae: 0.026378181552155888
r2: 0.748075000503504
pearson: 0.8675914026926694

=== Experiment 1128 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007071109613656965
rmse: 0.08408989007994341
mae: 0.03630369727155124
r2: 0.6811667797535536
pearson: 0.8260545273865076

=== Experiment 1479 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005905210897800774
rmse: 0.07684537004791359
mae: 0.0312658690832939
r2: 0.7337366397002416
pearson: 0.8577846231526863

=== Experiment 1454 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006211058035711574
rmse: 0.07881026605532793
mae: 0.03884688662075527
r2: 0.7199461268654636
pearson: 0.8513671485796156

=== Experiment 1279 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005698410368337419
rmse: 0.07548781602574961
mae: 0.02897037958020419
r2: 0.7430611845538705
pearson: 0.8674114325856898

=== Experiment 1135 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006690995841839543
rmse: 0.0817985075770918
mae: 0.03692722215526015
r2: 0.6983059424239358
pearson: 0.8360864426796412

=== Experiment 1414 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0072176210225861305
rmse: 0.08495658316214307
mae: 0.03642104300875668
r2: 0.6745606448095398
pearson: 0.8340612353492612

=== Experiment 1317 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007999401366378178
rmse: 0.08943937257370592
mae: 0.040511206171831265
r2: 0.6393105131957995
pearson: 0.8051682701768829

=== Experiment 1344 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007364814287629241
rmse: 0.08581849618601599
mae: 0.03725363859784172
r2: 0.6679237652734562
pearson: 0.8175287599663025

=== Experiment 1105 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007832530244315475
rmse: 0.08850158328705467
mae: 0.051590069841707456
r2: 0.6468346586440095
pearson: 0.819182147558003

=== Experiment 1082 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00794103536483066
rmse: 0.0891124871431084
mae: 0.03922671621302654
r2: 0.6419422105167354
pearson: 0.8055202559604457

=== Experiment 1008 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0060072070500167135
rmse: 0.07750617427029097
mae: 0.03235051664152787
r2: 0.7291376780887644
pearson: 0.8552816520290544

=== Experiment 1176 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005749192787255034
rmse: 0.07582343165048014
mae: 0.03573658708305205
r2: 0.740771427635927
pearson: 0.8624362007579313

=== Experiment 1406 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005710053202717072
rmse: 0.0755648939833642
mae: 0.02918098988579961
r2: 0.7425362142760954
pearson: 0.8620517365007289

=== Experiment 1238 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007284338181289112
rmse: 0.08534833437911435
mae: 0.040144774601439656
r2: 0.6715523974880897
pearson: 0.8229566289810877

=== Experiment 1457 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006889812245121385
rmse: 0.0830048928986803
mae: 0.03766739939977176
r2: 0.6893413983057486
pearson: 0.830894668273212

=== Experiment 1447 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0070537942572724685
rmse: 0.08398686955276086
mae: 0.03876062334297805
r2: 0.6819475215518593
pearson: 0.8283069348105899

=== Experiment 1469 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006449994808891147
rmse: 0.08031185970260649
mae: 0.04311876112225019
r2: 0.7091725729269127
pearson: 0.8478296119591932

=== Experiment 1274 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007957951161666692
rmse: 0.08920734925815636
mae: 0.03774594685938525
r2: 0.6411794846826102
pearson: 0.8277785561974904

=== Experiment 1057 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007021585165209919
rmse: 0.08379489939853092
mae: 0.03883276019007902
r2: 0.6833998153366541
pearson: 0.8350099184741218

=== Experiment 1453 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006445315306528078
rmse: 0.08028272109568832
mae: 0.034027041349833344
r2: 0.7093835696288571
pearson: 0.8501700246881347

=== Experiment 1364 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.008162216940825088
rmse: 0.09034498846546546
mae: 0.042652534155739955
r2: 0.631969230604603
pearson: 0.8001881144882317

=== Experiment 1388 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007341704190610508
rmse: 0.08568374519481807
mae: 0.04534841526044798
r2: 0.6689657893764995
pearson: 0.8258479424315308

=== Experiment 1117 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006252516435675911
rmse: 0.07907285523917744
mae: 0.03912523420641488
r2: 0.7180767858583088
pearson: 0.8515243300281622

=== Experiment 1483 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006464567753359961
rmse: 0.08040253573961434
mae: 0.0374822140134744
r2: 0.7085154852748572
pearson: 0.8429485445657072

=== Experiment 1409 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006410480790316752
rmse: 0.08006547814330937
mae: 0.03488059756160641
r2: 0.7109542426329827
pearson: 0.8454632668441981

=== Experiment 1038 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006372400845927477
rmse: 0.07982731891982517
mae: 0.03554821895144299
r2: 0.7126712505652297
pearson: 0.8458725320701799

=== Experiment 1328 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0075153456786603064
rmse: 0.08669109342175992
mae: 0.04385948021262162
r2: 0.6611363711057975
pearson: 0.8343877132925656

=== Experiment 1490 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006607199011067401
rmse: 0.08128467882121083
mae: 0.03836532938720102
r2: 0.7020843046416461
pearson: 0.8398948480715291

=== Experiment 1350 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006902149779509322
rmse: 0.08307917777343082
mae: 0.036494391319333894
r2: 0.6887851043103606
pearson: 0.8396980077555479

=== Experiment 1134 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0063757052119167755
rmse: 0.07984801319955792
mae: 0.03533942178435681
r2: 0.712522258157135
pearson: 0.8447415651493404

=== Experiment 1371 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006089390703650142
rmse: 0.0780345481415132
mae: 0.035104534365890006
r2: 0.7254320533182255
pearson: 0.8536630662280302

=== Experiment 1290 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0057516293381645245
rmse: 0.07583949721724507
mae: 0.029374525838833373
r2: 0.7406615646278261
pearson: 0.8654667822895505

=== Experiment 1212 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007655254659748866
rmse: 0.08749431215655602
mae: 0.051494128049840306
r2: 0.6548279367271662
pearson: 0.8272707845296857

=== Experiment 1484 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006979547639986887
rmse: 0.08354368701456076
mae: 0.03969086858513603
r2: 0.6852952688468308
pearson: 0.8301884751522364

=== Experiment 1282 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005640977150717266
rmse: 0.07510643880997997
mae: 0.030679064722361402
r2: 0.7456508230580711
pearson: 0.8636176131071652

=== Experiment 1268 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006438152828847528
rmse: 0.08023810085518929
mae: 0.030142646068337352
r2: 0.7097065225950254
pearson: 0.8448796555148882

=== Experiment 1036 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008181119634263939
rmse: 0.09044954192401385
mae: 0.03758790520752824
r2: 0.6311169164771566
pearson: 0.796052218745664

=== Experiment 1327 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006602236996977818
rmse: 0.08125415064461518
mae: 0.03699224181655086
r2: 0.7023080396729959
pearson: 0.8402718721439045

=== Experiment 1383 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00832458593364924
rmse: 0.09123916885663327
mae: 0.038437972461404485
r2: 0.6246480841822206
pearson: 0.7970706139367522

=== Experiment 1480 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007475403605398924
rmse: 0.08646041640773496
mae: 0.03844082247862677
r2: 0.6629373416093023
pearson: 0.8142174399340889

=== Experiment 1495 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007610697880003347
rmse: 0.08723931384418006
mae: 0.044765421133121874
r2: 0.6568369823149531
pearson: 0.8235228528788484

=== Experiment 1380 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006491089840739846
rmse: 0.08056730007105765
mae: 0.03995704416095024
r2: 0.7073196160281616
pearson: 0.8441406551582692

=== Experiment 1438 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007652056242854017
rmse: 0.08747603239090132
mae: 0.04423001156650033
r2: 0.6549721519372238
pearson: 0.8279224717584952

=== Experiment 1029 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00769118853056624
rmse: 0.08769942149504888
mae: 0.04194385536750944
r2: 0.6532076943077167
pearson: 0.809409257380736

=== Experiment 1306 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065851628714020845
rmse: 0.08114901645369514
mae: 0.031107105908968202
r2: 0.7030779044803084
pearson: 0.8439586911207151

=== Experiment 1301 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006269633206093748
rmse: 0.07918101543990042
mae: 0.03487944522385468
r2: 0.7173049982138353
pearson: 0.8474236225863258

=== Experiment 1307 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006166155326733896
rmse: 0.07852487075273601
mae: 0.028547023022875474
r2: 0.7219707702500733
pearson: 0.8498415798197905

=== Experiment 1349 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007035779714572876
rmse: 0.08387955480671602
mae: 0.03545715333390582
r2: 0.6827597893533772
pearson: 0.827462239593838

=== Experiment 1468 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007068841377250531
rmse: 0.08407640202369825
mae: 0.035880863601532795
r2: 0.6812690535347925
pearson: 0.8369195429481475

=== Experiment 1052 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0077171956434493065
rmse: 0.08784757050396616
mae: 0.04785152374022491
r2: 0.652035045034425
pearson: 0.8187100233073145

=== Experiment 1377 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006010591316018921
rmse: 0.07752800343114043
mae: 0.029434122955720406
r2: 0.7289850830242544
pearson: 0.8572161020643727

=== Experiment 1275 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00672406021659309
rmse: 0.08200036717352606
mae: 0.030516635070816672
r2: 0.6968150843190428
pearson: 0.8656381442134156

=== Experiment 1217 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007268386702918459
rmse: 0.08525483389766506
mae: 0.03408812338225518
r2: 0.6722716426270399
pearson: 0.8227937102825721

=== Experiment 1441 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005436025283521037
rmse: 0.07372940582644781
mae: 0.028541682063026732
r2: 0.7548920125437337
pearson: 0.8689535858641967

=== Experiment 1494 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007191111618795335
rmse: 0.08480042227958146
mae: 0.04188979605746337
r2: 0.675755942158786
pearson: 0.8278702547915471

=== Experiment 1230 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006830686782929765
rmse: 0.08264796901878331
mae: 0.04310572978936999
r2: 0.6920073393728627
pearson: 0.8379607768085775

=== Experiment 1315 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006118316859773178
rmse: 0.07821967054247403
mae: 0.037852060313183576
r2: 0.7241277856700126
pearson: 0.8584042832586234

=== Experiment 1025 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005554593854920973
rmse: 0.07452914768680086
mae: 0.02878708729645546
r2: 0.7495458078453299
pearson: 0.8668147742405887

=== Experiment 1433 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006886438694256572
rmse: 0.08298456901289886
mae: 0.031270176168818525
r2: 0.6894935102294297
pearson: 0.8409745885496427

=== Experiment 1123 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005908136239379006
rmse: 0.07686440163937404
mae: 0.030137143669946378
r2: 0.7336047373360204
pearson: 0.8584959021098981

=== Experiment 1475 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005417563906760337
rmse: 0.07360410251310953
mae: 0.02695893863756666
r2: 0.7557244278964005
pearson: 0.8709301936727513

=== Experiment 1129 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006584894760378176
rmse: 0.08114736446969906
mae: 0.029450555528498477
r2: 0.7030899934883718
pearson: 0.8525959522470389

=== Experiment 1401 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005912948543991468
rmse: 0.07689569912544829
mae: 0.02995795056663211
r2: 0.7333877526391017
pearson: 0.8570900914986408

=== Experiment 1416 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007599088789840546
rmse: 0.08717275256546937
mae: 0.049604132700380936
r2: 0.6573604310808425
pearson: 0.8227196738571907

=== Experiment 1498 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00649001091584606
rmse: 0.08056060399380122
mae: 0.031319013584604476
r2: 0.7073682642767513
pearson: 0.8462913398074983

=== Experiment 1391 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006714916772570067
rmse: 0.08194459574962872
mae: 0.04453369532730442
r2: 0.6972273581857034
pearson: 0.8428765475435294

=== Experiment 1173 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007132225010130504
rmse: 0.08445250150309642
mae: 0.03941318785595474
r2: 0.6784111134255046
pearson: 0.8245142562129922

=== Experiment 1436 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006592379537102055
rmse: 0.08119346979346341
mae: 0.03240542579153473
r2: 0.7027525082001942
pearson: 0.8397800343100672

=== Experiment 1039 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006979098285532798
rmse: 0.08354099763309508
mae: 0.038430034359031535
r2: 0.6853155300413897
pearson: 0.8283499953900302

=== Experiment 1428 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00667268779954476
rmse: 0.08168652152922636
mae: 0.030261563303411083
r2: 0.6991314439930221
pearson: 0.8625598724576043

=== Experiment 1342 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006710144714924515
rmse: 0.08191547298846852
mae: 0.039831875821988466
r2: 0.6974425281645982
pearson: 0.8369190426141425

=== Experiment 1474 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006759706631230593
rmse: 0.08221743508058733
mae: 0.042120368730783925
r2: 0.6952078031722245
pearson: 0.8375822344286553

=== Experiment 1255 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005445516564751778
rmse: 0.07379374339841947
mae: 0.03140932079822967
r2: 0.754464054850472
pearson: 0.8690794858195868

=== Experiment 1299 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007250527341489162
rmse: 0.08515002842917413
mae: 0.04847878863651988
r2: 0.6730769133733807
pearson: 0.8333972530756181

=== Experiment 1425 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005432047681520169
rmse: 0.07370242656466726
mae: 0.031480516903794976
r2: 0.7550713608672764
pearson: 0.8702012416991218

=== Experiment 1335 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006473416518555565
rmse: 0.08045754482057954
mae: 0.04032503274249977
r2: 0.7081164983468268
pearson: 0.8450907167541538

=== Experiment 1366 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006903004762145764
rmse: 0.08308432320327201
mae: 0.038786516761200665
r2: 0.6887465535195901
pearson: 0.8324662984400105

=== Experiment 1427 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0077330081983616565
rmse: 0.08793752440432728
mae: 0.041070268701136024
r2: 0.6513220638930648
pearson: 0.81477396163135

=== Experiment 1450 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005643962889367679
rmse: 0.075126312896133
mae: 0.031096328509982934
r2: 0.7455161974164479
pearson: 0.865936106446617

=== Experiment 1458 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006071365126498251
rmse: 0.0779189651272285
mae: 0.027744272605756543
r2: 0.7262448186583408
pearson: 0.8766468660386216

=== Experiment 1156 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007033299500076019
rmse: 0.08386476912313072
mae: 0.0366574736111671
r2: 0.6828716211334147
pearson: 0.8310534263208396

=== Experiment 1448 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00708203933747214
rmse: 0.0841548533209591
mae: 0.03977675417469574
r2: 0.6806739633172667
pearson: 0.8320886100036484

=== Experiment 1078 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0072359307618445915
rmse: 0.08506427429799535
mae: 0.038548343733106546
r2: 0.6737350667251084
pearson: 0.8209300460957907

=== Experiment 1411 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005746516202262211
rmse: 0.07580577947796732
mae: 0.03053379553627228
r2: 0.7408921136751982
pearson: 0.8687743810676147

=== Experiment 1439 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006198123838861585
rmse: 0.07872816420355289
mae: 0.03184777162691229
r2: 0.7205293241086541
pearson: 0.851853748582798

=== Experiment 1021 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006869500276495106
rmse: 0.08288244854307278
mae: 0.035761942044661224
r2: 0.6902572560311264
pearson: 0.8314835792503945

=== Experiment 1352 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007763590220472655
rmse: 0.08811123776495626
mae: 0.045079698969938575
r2: 0.6499431339762572
pearson: 0.8207343466840412

=== Experiment 1410 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007051366492613753
rmse: 0.08397241506955574
mae: 0.03605305968041272
r2: 0.6820569883917799
pearson: 0.8352964649766347

=== Experiment 1343 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0064622829928538215
rmse: 0.08038832622249216
mae: 0.029786369605185704
r2: 0.7086185041204791
pearson: 0.847194736595082

=== Experiment 1362 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068315511799732845
rmse: 0.08265319824406847
mae: 0.034944972949238734
r2: 0.6919683640906207
pearson: 0.848242747545692

=== Experiment 1127 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.008242022735477245
rmse: 0.09078558660645006
mae: 0.041533262659143984
r2: 0.6283708224489535
pearson: 0.7931880265763356

=== Experiment 1360 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007798771432561133
rmse: 0.08831065299589361
mae: 0.04582111098163804
r2: 0.6483568286076232
pearson: 0.8102613210125728

=== Experiment 1009 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00606868001647432
rmse: 0.07790173307747601
mae: 0.03975277140383574
r2: 0.7263658890875127
pearson: 0.8582194627698636

=== Experiment 1437 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006360206096669011
rmse: 0.07975090028751407
mae: 0.038719494828338266
r2: 0.7132211064419743
pearson: 0.8472616745070668

=== Experiment 1263 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.008034953640389418
rmse: 0.08963790292275593
mae: 0.041451061619869674
r2: 0.6377074768083852
pearson: 0.8002794964075691

=== Experiment 1353 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.009168352329615034
rmse: 0.09575151345861346
mae: 0.04491638760891997
r2: 0.5866030287581141
pearson: 0.7768110576253823

=== Experiment 1351 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007012748960501516
rmse: 0.08374215760596043
mae: 0.038403315187996494
r2: 0.6837982359178449
pearson: 0.8276705760164991

=== Experiment 1460 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007659702896276832
rmse: 0.08751972861176406
mae: 0.04416107395968371
r2: 0.6546273676999388
pearson: 0.8182977923550634

=== Experiment 1429 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006733457505336524
rmse: 0.08205764745187694
mae: 0.04186370937997719
r2: 0.6963913647056643
pearson: 0.8383204395556222

=== Experiment 1385 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005804684505915499
rmse: 0.0761884801391621
mae: 0.030242439353677552
r2: 0.7382693304652999
pearson: 0.8658202062398758

=== Experiment 1485 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007298020366066676
rmse: 0.08542845173633123
mae: 0.03889207661462569
r2: 0.6709354737984593
pearson: 0.8199272941826006

=== Experiment 1257 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006170720379925161
rmse: 0.0785539329373467
mae: 0.03297898127930124
r2: 0.7217649340109127
pearson: 0.8569836075073659

=== Experiment 1470 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006379113809675991
rmse: 0.07986935463415233
mae: 0.03907644393690472
r2: 0.7123685659844106
pearson: 0.8495888340605228

=== Experiment 1491 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0076394834960126605
rmse: 0.08740413889520714
mae: 0.050245066288640665
r2: 0.6555390515586115
pearson: 0.8262030066203272

=== Experiment 1210 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007047007474904385
rmse: 0.08394645599966913
mae: 0.0419850271057521
r2: 0.6822535345817449
pearson: 0.8281389173959208

=== Experiment 1003 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00607179632777168
rmse: 0.07792173206347303
mae: 0.03411154118129523
r2: 0.7262253759827082
pearson: 0.8542073231923938

=== Experiment 1242 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006913546064200723
rmse: 0.08314773637448421
mae: 0.03250827521987488
r2: 0.6882712508495139
pearson: 0.8350823111181852

=== Experiment 1419 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005700440887969594
rmse: 0.07550126414815578
mae: 0.02772183413396502
r2: 0.7429696293173558
pearson: 0.863914981627425

=== Experiment 1092 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006464265325728513
rmse: 0.08040065500808133
mae: 0.03614277556014066
r2: 0.7085291216036507
pearson: 0.8424041944115371

=== Experiment 1424 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005979798883328911
rmse: 0.07732915933416651
mae: 0.031571534258968435
r2: 0.7303735002614609
pearson: 0.856138954226406

=== Experiment 1188 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007486034024152556
rmse: 0.08652187020720574
mae: 0.04490379884762048
r2: 0.6624580207065063
pearson: 0.8166339158957525

=== Experiment 1472 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005989228816969756
rmse: 0.07739010800463943
mae: 0.02846204733305412
r2: 0.7299483087040262
pearson: 0.8591528581176622

=== Experiment 1445 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00578347487417452
rmse: 0.07604916090381617
mae: 0.030637207281641207
r2: 0.7392256634254278
pearson: 0.8611878151138902

=== Experiment 1061 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006382658302542548
rmse: 0.07989154086974758
mae: 0.036756565213586986
r2: 0.7122087463611082
pearson: 0.8455222964522153

=== Experiment 1449 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.008204037011228658
rmse: 0.09057613930406097
mae: 0.047894274002325744
r2: 0.6300835820365266
pearson: 0.8158651068406563

=== Experiment 1262 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007752122768307305
rmse: 0.08804613999663645
mae: 0.03848506328744754
r2: 0.650460196347181
pearson: 0.8096549451967645

=== Experiment 1423 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006472530360142778
rmse: 0.08045203763822753
mae: 0.035821944031530426
r2: 0.7081564548396312
pearson: 0.8416347310465836

=== Experiment 1104 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006228284509001037
rmse: 0.07891948117544259
mae: 0.03479673062289349
r2: 0.7191693927667923
pearson: 0.8484544547209433

=== Experiment 1258 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00583408750801187
rmse: 0.07638119865524415
mae: 0.028031605987581518
r2: 0.7369435620420255
pearson: 0.8624305142418449

=== Experiment 1462 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007182528450419326
rmse: 0.08474979911728008
mae: 0.03784460152625146
r2: 0.6761429534431189
pearson: 0.822371670629909

=== Experiment 1489 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006920863047755958
rmse: 0.08319172463506186
mae: 0.037535498764196415
r2: 0.6879413312814593
pearson: 0.8366672670807888

=== Experiment 1493 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00635750308543356
rmse: 0.07973395189900949
mae: 0.032286901523403695
r2: 0.7133429840288945
pearson: 0.850109023099576

=== Experiment 1465 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006785855316387524
rmse: 0.08237630312406308
mae: 0.033912698403123914
r2: 0.6940287704674151
pearson: 0.8379008725987195

=== Experiment 1209 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00686059742084894
rmse: 0.08282872340467973
mae: 0.03673895346865585
r2: 0.6906586818737656
pearson: 0.8365565694528269

=== Experiment 1207 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006857248556373297
rmse: 0.08280850533836061
mae: 0.03703879583964975
r2: 0.6908096806990245
pearson: 0.8369721231075747

=== Experiment 1434 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0066676474397956344
rmse: 0.08165566385619331
mae: 0.035678543881954176
r2: 0.6993587115956781
pearson: 0.8367483870448008

=== Experiment 1478 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007414930901017661
rmse: 0.08610999303807695
mae: 0.04542048331527319
r2: 0.6656640292337808
pearson: 0.8230348747770423

=== Experiment 1464 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005256614653545373
rmse: 0.07250251480842146
mae: 0.026222318848013338
r2: 0.7629815588846796
pearson: 0.8748124418394229

=== Experiment 1405 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00724292523556218
rmse: 0.08510537724234692
mae: 0.0375646210385865
r2: 0.6734196889836856
pearson: 0.8224592612937791

=== Experiment 1079 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005749849472554373
rmse: 0.07582776188543595
mae: 0.030285925583776418
r2: 0.7407418179848093
pearson: 0.8609300334248321

=== Experiment 1271 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006692358540033433
rmse: 0.08180683675606479
mae: 0.03603197756683758
r2: 0.6982444989621427
pearson: 0.8363393085838569

=== Experiment 1320 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006180465362854188
rmse: 0.07861593580727884
mae: 0.03850080541573791
r2: 0.7213255370197379
pearson: 0.8516578711911325

=== Experiment 1393 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0072015663945221705
rmse: 0.08486204330866758
mae: 0.03926953546891189
r2: 0.6752845409227617
pearson: 0.8234664163365475

=== Experiment 1476 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0079497407358171
rmse: 0.08916131860743817
mae: 0.04242316903879312
r2: 0.6415496891704835
pearson: 0.8081438917030878

=== Experiment 1418 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007094367891767314
rmse: 0.08422807068767107
mae: 0.03972416176396621
r2: 0.680118074230309
pearson: 0.8342649783392697

=== Experiment 1333 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006439426703632974
rmse: 0.0802460385541428
mae: 0.04108510595570585
r2: 0.709649084141625
pearson: 0.8459582518349439

=== Experiment 1339 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0053507631252464076
rmse: 0.07314891062241739
mae: 0.02848605819609931
r2: 0.7587364457335912
pearson: 0.8711777437601684

=== Experiment 1150 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006793165973191978
rmse: 0.08242066472185224
mae: 0.0315275261507544
r2: 0.6936991361697683
pearson: 0.8416048419003435

=== Experiment 1368 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006027108113846213
rmse: 0.07763445184868772
mae: 0.03362313630943268
r2: 0.7282403478798203
pearson: 0.8534902018126552

=== Experiment 1367 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005970172921102947
rmse: 0.07726689408215492
mae: 0.032228479582046145
r2: 0.7308075306615196
pearson: 0.855091133489299

=== Experiment 1220 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00580603550530919
rmse: 0.07619734578913619
mae: 0.0287040477118687
r2: 0.7382084144972587
pearson: 0.8659021600839852

=== Experiment 1497 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00694143793756898
rmse: 0.08331529233921574
mae: 0.03748591793260376
r2: 0.6870136185554916
pearson: 0.8450639501468968

=== Experiment 1396 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006390989667698651
rmse: 0.07994366558832945
mae: 0.04057176390803415
r2: 0.7118330887731337
pearson: 0.8470991521365222

=== Experiment 1119 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007144414649382986
rmse: 0.08452463930347758
mae: 0.03930750849074628
r2: 0.6778614879566809
pearson: 0.8262167589807365

=== Experiment 1035 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006523645011987503
rmse: 0.08076908450631035
mae: 0.03695557167745585
r2: 0.7058517176852314
pearson: 0.84092692949617

=== Experiment 1012 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00630838023881566
rmse: 0.07942531233061448
mae: 0.03268669079646846
r2: 0.7155579115622777
pearson: 0.8493707444963656

=== Experiment 1341 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00757097004983121
rmse: 0.08701132138883542
mae: 0.038199572295119874
r2: 0.6586282926918594
pearson: 0.823738289503307

=== Experiment 1356 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005969979367657155
rmse: 0.07726564157280488
mae: 0.03197255995115955
r2: 0.730816257901201
pearson: 0.8552000804704184

=== Experiment 1446 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005914778129026083
rmse: 0.07690759474217149
mae: 0.03047039639764629
r2: 0.7333052574551528
pearson: 0.861742214593158

=== Experiment 1116 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.008165108982874952
rmse: 0.09036099259567124
mae: 0.0373202816265052
r2: 0.6318388297014595
pearson: 0.8115301796630109

=== Experiment 1214 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00644144554403928
rmse: 0.08025861663422365
mae: 0.03266752791665085
r2: 0.7095580555162642
pearson: 0.8425787153342356

=== Experiment 1073 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00626192054972507
rmse: 0.07913229776598851
mae: 0.03158614419421527
r2: 0.7176527584948993
pearson: 0.8491111998285562

=== Experiment 1486 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006026752930566197
rmse: 0.07763216427851408
mae: 0.03980101965132784
r2: 0.7282563629375882
pearson: 0.8593588652824501

=== Experiment 1373 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006377622690554401
rmse: 0.07986001934982487
mae: 0.03798959630624792
r2: 0.7124357998893114
pearson: 0.8448496451482175

=== Experiment 1492 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006479218904159533
rmse: 0.08049359542323559
mae: 0.040620721008941194
r2: 0.7078548713337691
pearson: 0.8485751972174304

=== Experiment 1397 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00654218442830477
rmse: 0.08088377110585764
mae: 0.040695549699665705
r2: 0.7050157835633069
pearson: 0.841518110433986

=== Experiment 1125 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062208607680932886
rmse: 0.07887243351192665
mae: 0.031541749154186595
r2: 0.7195041259768844
pearson: 0.8539867891027567

=== Experiment 1272 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006437599039536032
rmse: 0.080234649868595
mae: 0.03633443180552801
r2: 0.7097314927113394
pearson: 0.8437531421500732

=== Experiment 1321 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0057134205268166375
rmse: 0.07558717170801298
mae: 0.02881162688010282
r2: 0.7423843831145187
pearson: 0.8628112617954192

=== Experiment 1139 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006978556119541634
rmse: 0.08353775266034892
mae: 0.040984280701337146
r2: 0.6853399760673058
pearson: 0.8377614187447447

=== Experiment 1151 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007093243358967565
rmse: 0.08422139490039075
mae: 0.04101851357510932
r2: 0.6801687789193049
pearson: 0.8259133341141025

=== Experiment 1074 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059786523969482105
rmse: 0.07732174595124072
mae: 0.028632721756992403
r2: 0.7304251948277594
pearson: 0.8557788229261464

=== Experiment 1174 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00679545761804412
rmse: 0.08243456567511058
mae: 0.03735442940025934
r2: 0.6935958069120153
pearson: 0.8328362799458441

=== Experiment 1337 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006303577393813145
rmse: 0.07939507159649864
mae: 0.028992289909921025
r2: 0.7157744697295472
pearson: 0.8490786178930502

=== Experiment 1014 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007095509599288453
rmse: 0.08423484789140687
mae: 0.04101978744683888
r2: 0.6800665951406847
pearson: 0.8268522250634442

=== Experiment 1051 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006402880412709165
rmse: 0.08001800055430756
mae: 0.030384984349465988
r2: 0.7112969403141267
pearson: 0.8486567830649356

=== Experiment 1363 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005557433572143449
rmse: 0.07454819630375673
mae: 0.03024066448114135
r2: 0.7494177662456236
pearson: 0.8659886701820582

=== Experiment 1326 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007359155227852668
rmse: 0.0857855187537656
mae: 0.03672769550891964
r2: 0.6681789297880394
pearson: 0.8198331521784679

=== Experiment 1159 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0074313326235675116
rmse: 0.08620517747541334
mae: 0.03550899399246019
r2: 0.6649244827829582
pearson: 0.8297836959778615

=== Experiment 1222 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.009574525245101363
rmse: 0.09784950303962388
mae: 0.04293376233958632
r2: 0.5682888707692066
pearson: 0.7698946303263263

=== Experiment 1184 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006265519019142583
rmse: 0.07915503154659584
mae: 0.03593986083701647
r2: 0.7174905050926711
pearson: 0.8488541775395197

=== Experiment 1426 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005591900364536586
rmse: 0.07477901018692736
mae: 0.030891631075429003
r2: 0.7478636737466124
pearson: 0.8717106706471056

=== Experiment 1187 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005686753187849222
rmse: 0.07541056416609826
mae: 0.032174086520271115
r2: 0.7435868016913658
pearson: 0.8627543243384336

=== Experiment 1288 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060622287201664155
rmse: 0.07786031543839529
mae: 0.03361562571646363
r2: 0.7266567751986033
pearson: 0.8563361537431831

=== Experiment 1440 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006432579170972427
rmse: 0.08020336134459968
mae: 0.03508358972538741
r2: 0.7099578363754593
pearson: 0.8430920875931874

=== Experiment 1246 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007021839614761289
rmse: 0.08379641767260274
mae: 0.043763782525038646
r2: 0.6833883423183746
pearson: 0.8441426302203744

=== Experiment 1022 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006469537077529499
rmse: 0.08043343258576933
mae: 0.03370069779704779
r2: 0.7082914206352195
pearson: 0.8478638530317817

=== Experiment 1319 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005692836374419335
rmse: 0.07545088716787454
mae: 0.03154078843960852
r2: 0.7433125134863345
pearson: 0.8626002701042632

=== Experiment 1403 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006857417878184767
rmse: 0.08280952770173712
mae: 0.03658017975524395
r2: 0.6908020460530695
pearson: 0.8337545204956214

=== Experiment 1369 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007971397767954287
rmse: 0.0892826845919985
mae: 0.03977343078106374
r2: 0.6405731831234067
pearson: 0.8022082244852996

=== Experiment 1481 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00634568932882573
rmse: 0.07965983510418366
mae: 0.03252707377742923
r2: 0.7138756611147097
pearson: 0.8473009023142739

=== Experiment 1136 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006387685680030274
rmse: 0.07992299844244005
mae: 0.03442484610030393
r2: 0.7119820641229047
pearson: 0.8475550735588662

=== Experiment 1231 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006361213402337343
rmse: 0.07975721536223129
mae: 0.03921021621206201
r2: 0.7131756874727384
pearson: 0.8479504063098409

=== Experiment 1261 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0066954965849296295
rmse: 0.08182601410877613
mae: 0.04695142772353527
r2: 0.6981030058989325
pearson: 0.8466171157232103

=== Experiment 1118 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005624346046335706
rmse: 0.07499564018218463
mae: 0.031880264460871985
r2: 0.7464007122347267
pearson: 0.8644062251609426

=== Experiment 1318 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006845384968003477
rmse: 0.08273684166079509
mae: 0.03653171465004338
r2: 0.691344604677055
pearson: 0.8335898172323968

=== Experiment 1375 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007172942210804138
rmse: 0.08469322411388137
mae: 0.03526596976922642
r2: 0.676575192768141
pearson: 0.8245530617356702

=== Experiment 1334 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006252776036681357
rmse: 0.07907449675262788
mae: 0.03521597996963224
r2: 0.7180650805632321
pearson: 0.847697226736351

=== Experiment 1142 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006227805860711156
rmse: 0.07891644860680919
mae: 0.034102551712886696
r2: 0.7191909748075138
pearson: 0.8489831059479267

=== Experiment 1248 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005918278181408377
rmse: 0.07693034629720821
mae: 0.028609107305930463
r2: 0.733147441633726
pearson: 0.8625112031360797

=== Experiment 1482 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.008265132153010205
rmse: 0.09091277222156524
mae: 0.04236179602226736
r2: 0.627328828983625
pearson: 0.7964992534719031

=== Experiment 1094 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00658587597528388
rmse: 0.08115341012726379
mae: 0.03850329603782129
r2: 0.7030457509401393
pearson: 0.8440775322240696

=== Experiment 1137 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006359075704379658
rmse: 0.07974381295360573
mae: 0.036066656692866066
r2: 0.7132720753327775
pearson: 0.8470255145651676

=== Experiment 1066 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006255803348755949
rmse: 0.07909363658826132
mae: 0.03682212261984061
r2: 0.7179285803942108
pearson: 0.8491234532985826

=== Experiment 1417 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006651948335202949
rmse: 0.08155947728622928
mae: 0.037888411207659706
r2: 0.700066577312058
pearson: 0.8389636990361069

=== Experiment 1037 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0057236573524369625
rmse: 0.07565485676701109
mae: 0.028944045422964765
r2: 0.7419228091528696
pearson: 0.8614966087258964

=== Experiment 1256 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007160250908366715
rmse: 0.08461826580807902
mae: 0.037501888673882704
r2: 0.6771474380091751
pearson: 0.8268653439547468

=== Experiment 1496 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007365859628715426
rmse: 0.08582458638825721
mae: 0.03839838078074099
r2: 0.6678766313039699
pearson: 0.8225470819630928

=== Experiment 1452 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005873949659099117
rmse: 0.07664169660895509
mae: 0.02947239817787232
r2: 0.7351461951941758
pearson: 0.8580643727929483

=== Experiment 1064 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006905610356279825
rmse: 0.08310000214368123
mae: 0.036934126892425735
r2: 0.688629068426895
pearson: 0.8303709685442727

=== Experiment 1330 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005699240202079363
rmse: 0.07549331230035786
mae: 0.030639920451388303
r2: 0.743023767715684
pearson: 0.8642316942108332

=== Experiment 1175 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005956564083423691
rmse: 0.07717878000735494
mae: 0.03369166524129515
r2: 0.7314211471627696
pearson: 0.8559598094087105

=== Experiment 1467 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005710806351735915
rmse: 0.07556987727749673
mae: 0.0321172953758069
r2: 0.7425022551183219
pearson: 0.8626921856057277

=== Experiment 1370 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007428680030362873
rmse: 0.08618979075483867
mae: 0.041590372317330246
r2: 0.6650440870430598
pearson: 0.8223946966852291

=== Experiment 1455 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005754324912863082
rmse: 0.07585726671099534
mae: 0.02928496050494431
r2: 0.7405400223510814
pearson: 0.8629620461404995

=== Experiment 1100 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006210974153872189
rmse: 0.07880973387768918
mae: 0.03341201274181893
r2: 0.719949909060682
pearson: 0.8505001665568925

=== Experiment 1332 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006498485298093374
rmse: 0.08061318315321243
mae: 0.03239156896479454
r2: 0.7069861581110807
pearson: 0.8417269845987333

=== Experiment 1193 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0058911608547791245
rmse: 0.07675389797775176
mae: 0.03147267646307091
r2: 0.734370149956191
pearson: 0.8575264795292828

=== Experiment 1381 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006161526416221398
rmse: 0.07849539105082157
mae: 0.030091463781430343
r2: 0.7221794857877426
pearson: 0.850783757043349

=== Experiment 1499 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005815083514260542
rmse: 0.07625669488156789
mae: 0.02863315208330815
r2: 0.7378004437559755
pearson: 0.8661614666431771

=== Experiment 1415 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006178878892089629
rmse: 0.07860584515218719
mae: 0.03712907548379477
r2: 0.7213970702882527
pearson: 0.8531910815810049

=== Experiment 1227 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064681401913035605
rmse: 0.08042474862443501
mae: 0.03366625448378731
r2: 0.7083544056203305
pearson: 0.8473985260888075

=== Experiment 1163 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007101022939060121
rmse: 0.08426756753971316
mae: 0.034950339128069975
r2: 0.6798180010769865
pearson: 0.8338080291236517

=== Experiment 1236 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006251816106543385
rmse: 0.07906842673623514
mae: 0.036152815428817285
r2: 0.718108363390656
pearson: 0.8512696862003227

=== Experiment 1463 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005710233584876087
rmse: 0.07556608753188224
mae: 0.035134256341535476
r2: 0.7425280809239376
pearson: 0.8631928485948275

=== Experiment 1034 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007297455361140141
rmse: 0.08542514478267005
mae: 0.03699751929510746
r2: 0.6709609496219191
pearson: 0.8286378291425008

=== Experiment 1336 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007015108584649973
rmse: 0.08375624504865278
mae: 0.039426999129605865
r2: 0.6836918415035409
pearson: 0.827296309722173

=== Experiment 1053 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006167860905219876
rmse: 0.07853573011833452
mae: 0.03303499285051657
r2: 0.721893866466823
pearson: 0.8496610642304148

=== Experiment 1311 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006693548455726785
rmse: 0.08181410914828069
mae: 0.041512856097985426
r2: 0.6981908461872515
pearson: 0.8399287739595758

=== Experiment 1278 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005849265029452699
rmse: 0.0764804879002004
mae: 0.029051084969638357
r2: 0.7362592142803964
pearson: 0.8587180000255638

=== Experiment 1442 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005704971758260603
rmse: 0.07553126344938633
mae: 0.03147317882821292
r2: 0.7427653343701226
pearson: 0.861948768012613

=== Experiment 1444 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0067916375314345245
rmse: 0.08241139200034497
mae: 0.039908139608801546
r2: 0.6937680529358933
pearson: 0.8425356467000686

=== Experiment 1095 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005981261904496958
rmse: 0.07733861845479888
mae: 0.03163152385197394
r2: 0.7303075332809186
pearson: 0.8547990300048516

=== Experiment 1015 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006787543593958
rmse: 0.08238654983647513
mae: 0.03912943715279831
r2: 0.69395264677483
pearson: 0.8406555920053523

=== Experiment 1148 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007460945247732473
rmse: 0.08637676335527092
mae: 0.04391241429758024
r2: 0.6635892625928728
pearson: 0.8260742028035458

=== Experiment 1422 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006199906122858228
rmse: 0.07873948261741519
mae: 0.03197909703078699
r2: 0.7204489617076886
pearson: 0.8488905841306646

=== Experiment 1099 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00645649339172913
rmse: 0.08035230794276621
mae: 0.03688042340441179
r2: 0.708879554686993
pearson: 0.8429616387406004

=== Experiment 1400 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007086035058123695
rmse: 0.08417859025977861
mae: 0.03584513430441896
r2: 0.6804937980317391
pearson: 0.8277038598871336

=== Experiment 1124 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00625459003852742
rmse: 0.07908596612881087
mae: 0.03417847381481713
r2: 0.7179832880183965
pearson: 0.8477329323134546

=== Experiment 1345 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064249065399199
rmse: 0.08015551471932483
mae: 0.03481391191320776
r2: 0.7103037919326347
pearson: 0.8431454554053013

=== Experiment 1058 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006272066754716376
rmse: 0.07919638094456322
mae: 0.03176407424289338
r2: 0.7171952705775276
pearson: 0.8478889723211754

=== Experiment 1402 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005968957689666015
rmse: 0.07725902982607286
mae: 0.03464469338602621
r2: 0.7308623249121466
pearson: 0.8554577096483261

=== Experiment 1312 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008935002425362762
rmse: 0.09452514176325133
mae: 0.060320219611367354
r2: 0.5971246732357021
pearson: 0.8026220891882166

=== Experiment 1089 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006690318377793855
rmse: 0.08179436641843896
mae: 0.03856101652733907
r2: 0.6983364889795813
pearson: 0.8373269886535095

=== Experiment 1098 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006080337879141463
rmse: 0.07797652133265155
mae: 0.030788421560641144
r2: 0.7258402411908691
pearson: 0.8539844406965754

=== Experiment 1235 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006652903471718466
rmse: 0.08156533253606256
mae: 0.03577010226621631
r2: 0.700023510626961
pearson: 0.8366749940254802

=== Experiment 1323 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006478316809180686
rmse: 0.0804879917079603
mae: 0.034827565053133086
r2: 0.7078955463993231
pearson: 0.8459242562197655

=== Experiment 1177 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006556265440859717
rmse: 0.08097076905192217
mae: 0.03950563869674091
r2: 0.704380876904732
pearson: 0.8441363807398305

=== Experiment 1387 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005994105561731956
rmse: 0.07742160913938663
mae: 0.032567140737086574
r2: 0.7297284184291183
pearson: 0.8566991238529997

=== Experiment 1471 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006377953024106402
rmse: 0.07986208752660053
mae: 0.03632288543165781
r2: 0.7124209052948425
pearson: 0.8454277428543355

=== Experiment 1466 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006012520202019227
rmse: 0.07754044236409299
mae: 0.030421821291951576
r2: 0.7288981104034682
pearson: 0.863166270487895

=== Experiment 1106 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006244574528572314
rmse: 0.07902262036007357
mae: 0.03099230311077129
r2: 0.7184348829541093
pearson: 0.8478328496469536

=== Experiment 1161 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.011875864056873152
rmse: 0.10897643808123457
mae: 0.04685271861984323
r2: 0.4645225166430883
pearson: 0.6823337489202673

=== Experiment 1286 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006404251549826196
rmse: 0.08002656777487209
mae: 0.030942542498825318
r2: 0.7112351163450032
pearson: 0.8565066923102266

=== Experiment 1305 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005525284179236368
rmse: 0.07433225530842158
mae: 0.03091301237899367
r2: 0.7508673682217739
pearson: 0.8668439530745737

=== Experiment 1412 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065977152319143945
rmse: 0.08122632105367321
mae: 0.03664556529392933
r2: 0.7025119240695247
pearson: 0.8404730824141925

=== Experiment 1456 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00725652290226914
rmse: 0.08518522701894467
mae: 0.034391719756418894
r2: 0.6728065761766597
pearson: 0.8204126001401333

=== Experiment 1365 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006671018371679883
rmse: 0.08167630238741151
mae: 0.03331630340735036
r2: 0.699206717760678
pearson: 0.8468984028742433

=== Experiment 1378 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006279009537044689
rmse: 0.07924020152072235
mae: 0.029153305696431795
r2: 0.7168822235780961
pearson: 0.8476711354663184

=== Experiment 1081 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006750385120958288
rmse: 0.0821607273638585
mae: 0.033789247832430695
r2: 0.6956281059676924
pearson: 0.8401279323080213

=== Experiment 1205 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007040235649880718
rmse: 0.08390611211276994
mae: 0.04103669724495977
r2: 0.6825588731915538
pearson: 0.8290823298328844

=== Experiment 1379 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0054371713225304435
rmse: 0.07373717734311806
mae: 0.03169098274600059
r2: 0.7548403381492099
pearson: 0.8689587417335437

=== Experiment 1488 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005792841009994467
rmse: 0.07611071547419895
mae: 0.03393278991245717
r2: 0.7388033484836581
pearson: 0.860602862549563

=== Experiment 1394 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005777819543478729
rmse: 0.07601196973818485
mae: 0.03043839103367192
r2: 0.7394806597973996
pearson: 0.8662051736341928

=== Experiment 1435 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006653288796940176
rmse: 0.08156769456678407
mae: 0.03166276765493091
r2: 0.7000061364823087
pearson: 0.8380369060344802

=== Experiment 1164 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0059072160469870955
rmse: 0.07685841558988252
mae: 0.02881529685439018
r2: 0.733646228405964
pearson: 0.8571104021578468

=== Experiment 1147 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005714377709572073
rmse: 0.07559350309102014
mae: 0.03183373065887921
r2: 0.7423412241653643
pearson: 0.8650811617014784

=== Experiment 1160 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005990282493167857
rmse: 0.07739691526907165
mae: 0.03327082461028344
r2: 0.7299007989080123
pearson: 0.8621107321020113

=== Experiment 1477 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005873776583469609
rmse: 0.07664056747878116
mae: 0.032120787047802205
r2: 0.735153999098134
pearson: 0.868058345696932

=== Experiment 1500 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007429680964329473
rmse: 0.08619559712844661
mae: 0.038261465842260874
r2: 0.6649989553710507
pearson: 0.817611347759461

=== Experiment 1516 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0065268992844220615
rmse: 0.08078922752707852
mae: 0.03544034300689257
r2: 0.7057049839734718
pearson: 0.8428742784642438

=== Experiment 1517 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007081465119067132
rmse: 0.08415144157450384
mae: 0.043649479008273546
r2: 0.6806998545724154
pearson: 0.8316938614660697

=== Experiment 1515 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00602788827522004
rmse: 0.07763947626832654
mae: 0.034434782267485906
r2: 0.7282051707468498
pearson: 0.8542963462526635

=== Experiment 1551 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006267991815380788
rmse: 0.07917064996184374
mae: 0.036719554776322695
r2: 0.7173790077986514
pearson: 0.8474737131855461

=== Experiment 1548 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007199563551068291
rmse: 0.08485024190341646
mae: 0.03629593131757401
r2: 0.6753748482525783
pearson: 0.8389322926708904

=== Experiment 1510 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005951263128917229
rmse: 0.07714443031688827
mae: 0.029731717587822473
r2: 0.7316601648683376
pearson: 0.859781920674985

=== Experiment 1566 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006932577815846522
rmse: 0.08326210311928543
mae: 0.04484898957999242
r2: 0.6874131175443192
pearson: 0.8349114169240653

=== Experiment 1531 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007293176505871203
rmse: 0.0854000966385355
mae: 0.044787658078525264
r2: 0.6711538813227272
pearson: 0.8310263279555744

=== Experiment 1558 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00669458600668548
rmse: 0.08182044980740133
mae: 0.04289150814745579
r2: 0.698144063471185
pearson: 0.8398184727310032

=== Experiment 1583 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006142667099754536
rmse: 0.07837516889777359
mae: 0.03324909664913665
r2: 0.7230298440666134
pearson: 0.8527886644678703

=== Experiment 1555 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0072869317157862904
rmse: 0.08536352684716284
mae: 0.036464690851267495
r2: 0.6714354561591136
pearson: 0.8201931333163814

=== Experiment 1588 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007145738270625283
rmse: 0.08453246873613288
mae: 0.047485640303043
r2: 0.6778018064574358
pearson: 0.8329506713586622

=== Experiment 1525 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.008001511271626611
rmse: 0.08945116696626496
mae: 0.037855228654598644
r2: 0.639215378496788
pearson: 0.8231025070153982

=== Experiment 1556 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006685840823601577
rmse: 0.08176699103918143
mae: 0.043515451926910566
r2: 0.6985383799273888
pearson: 0.8419831766364809

=== Experiment 1554 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007361954281055345
rmse: 0.08580183145513472
mae: 0.041210138830696526
r2: 0.6680527217110881
pearson: 0.8211327009232504

=== Experiment 1606 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006734275816395651
rmse: 0.08206263349658997
mae: 0.04011115108412387
r2: 0.6963544674201746
pearson: 0.8362295566485599

=== Experiment 1563 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006138380981595202
rmse: 0.07834782052868607
mae: 0.031690747797320666
r2: 0.7232231032479528
pearson: 0.851481970261871

=== Experiment 1616 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006582058995773927
rmse: 0.08112988965710434
mae: 0.03815282325916949
r2: 0.7032178568662619
pearson: 0.8390603233760936

=== Experiment 1549 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00694461156775347
rmse: 0.08333433606715464
mae: 0.043335092399128035
r2: 0.6868705209672943
pearson: 0.8351942867257188

=== Experiment 1615 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007281273597851792
rmse: 0.08533037910294194
mae: 0.04375620472262319
r2: 0.671690578206452
pearson: 0.8270473753254387

=== Experiment 1613 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007659384483687022
rmse: 0.08751790950249567
mae: 0.03772212061136327
r2: 0.654641724783469
pearson: 0.8241379755150151

=== Experiment 1574 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007531438921859847
rmse: 0.08678386325728907
mae: 0.049633222542400685
r2: 0.6604107338531082
pearson: 0.8248157169430401

=== Experiment 1528 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006391195019102595
rmse: 0.07994494992870153
mae: 0.03591927465354532
r2: 0.7118238295687118
pearson: 0.8442949782065529

=== Experiment 1532 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005628597602759297
rmse: 0.07502398018473358
mae: 0.028835657026662926
r2: 0.7462090114268402
pearson: 0.864337948121279

=== Experiment 1586 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007740088357782509
rmse: 0.08797777195282061
mae: 0.04882927372219825
r2: 0.651002822620995
pearson: 0.8200011556163125

=== Experiment 1596 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006049326656186516
rmse: 0.07777741739211014
mae: 0.030998679584284265
r2: 0.7272385235848244
pearson: 0.8543769253955203

=== Experiment 1553 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00827046505141647
rmse: 0.09094209724553569
mae: 0.054721170963038517
r2: 0.6270883709416745
pearson: 0.825027719499035

=== Experiment 1640 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006742753884162683
rmse: 0.08211427332786111
mae: 0.03629328894986527
r2: 0.695972195076042
pearson: 0.8370765270571953

=== Experiment 1569 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00558754649359628
rmse: 0.0747498929336777
mae: 0.027954284002096975
r2: 0.7480599878710257
pearson: 0.8660808654698375

=== Experiment 1634 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008118016927441449
rmse: 0.09010003844306311
mae: 0.03860569058374477
r2: 0.6339621897541574
pearson: 0.8140590949863553

=== Experiment 1633 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006578417275171156
rmse: 0.08110744278530273
mae: 0.037829380161989175
r2: 0.7033820604453971
pearson: 0.8413472044932119

=== Experiment 1592 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006772626629330935
rmse: 0.08229596970284107
mae: 0.03517709947129469
r2: 0.6946252461444062
pearson: 0.8363493372934623

=== Experiment 1607 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006896641116070626
rmse: 0.083046018062702
mae: 0.04250473456439814
r2: 0.6890334875202634
pearson: 0.837110125092018

=== Experiment 1600 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0066406710236925175
rmse: 0.08149031245303037
mae: 0.03961784651097421
r2: 0.7005750663245389
pearson: 0.8391851019525101

=== Experiment 1579 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0070541565847432805
rmse: 0.08398902657337612
mae: 0.036610083746346826
r2: 0.6819311843656728
pearson: 0.8261916542370065

=== Experiment 1599 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0070470987358935595
rmse: 0.08394699956456787
mae: 0.035085567371408506
r2: 0.68224941966391
pearson: 0.8384698677493234

=== Experiment 1609 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007597985650569101
rmse: 0.08716642501886321
mae: 0.03851372501163731
r2: 0.6574101711450633
pearson: 0.8156405046026185

=== Experiment 1655 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006820781251185015
rmse: 0.08258802123301548
mae: 0.04304884092753853
r2: 0.6924539754394756
pearson: 0.8370938537385034

=== Experiment 1622 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007450349208284744
rmse: 0.08631540539373457
mae: 0.04134236973356386
r2: 0.6640670333478973
pearson: 0.8219241924190419

=== Experiment 1644 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007696488183794353
rmse: 0.08772963116185063
mae: 0.038898257867028524
r2: 0.6529687352762168
pearson: 0.8102852735666812

=== Experiment 1620 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006015221795652355
rmse: 0.07755786095330605
mae: 0.036404912140757535
r2: 0.7287762967356126
pearson: 0.8560790441158318

=== Experiment 1594 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007752738617141835
rmse: 0.08804963723458396
mae: 0.05244447785702493
r2: 0.6504324279942879
pearson: 0.8209926375126914

=== Experiment 1683 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007764844006972964
rmse: 0.08811835227109596
mae: 0.0343782736573091
r2: 0.6498866012947919
pearson: 0.8366871551455056

=== Experiment 1573 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007735024839051045
rmse: 0.08794898998312059
mae: 0.041361635311203016
r2: 0.6512311344519718
pearson: 0.807198417263618

=== Experiment 1514 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006705138416617489
rmse: 0.08188490957812367
mae: 0.040383814550226334
r2: 0.6976682599518215
pearson: 0.8386824811622321

=== Experiment 1595 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00578916609848105
rmse: 0.07608656976419065
mae: 0.02921059598682189
r2: 0.7389690486263458
pearson: 0.8643230217531632

=== Experiment 1598 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067133158628713396
rmse: 0.0819348269228131
mae: 0.030951138925570533
r2: 0.6972995424994047
pearson: 0.840311608055499

=== Experiment 1650 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006942976627048283
rmse: 0.08332452596353779
mae: 0.04246959303571019
r2: 0.6869442397240987
pearson: 0.8336133898757943

=== Experiment 1652 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006543467443564042
rmse: 0.08089170194503292
mae: 0.031373324468555605
r2: 0.7049579329699647
pearson: 0.840997230537834

=== Experiment 1699 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007419901997820436
rmse: 0.08613885300966362
mae: 0.04260118483183043
r2: 0.665439884666891
pearson: 0.8176885542992373

=== Experiment 1664 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005884131338869857
rmse: 0.0767080917431131
mae: 0.030745990017792182
r2: 0.7346871077346132
pearson: 0.8579315500433837

=== Experiment 1509 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005475240211545545
rmse: 0.07399486611614042
mae: 0.026782613892180614
r2: 0.7531238286989186
pearson: 0.8710841453619839

=== Experiment 1561 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006086841175136744
rmse: 0.07801821053534068
mae: 0.03025678171426479
r2: 0.7255470104367612
pearson: 0.8592527228835598

=== Experiment 1617 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00777814409328438
rmse: 0.08819378715807809
mae: 0.038023884495724525
r2: 0.6492869062568258
pearson: 0.8087053472364505

=== Experiment 1535 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006417179030597346
rmse: 0.08010729698721177
mae: 0.02989074624804356
r2: 0.7106522219268516
pearson: 0.8536086893016747

=== Experiment 1635 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007154810695555946
rmse: 0.08458611408237138
mae: 0.04032510906983024
r2: 0.6773927348103919
pearson: 0.8282580424350586

=== Experiment 1660 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006582488651930996
rmse: 0.08113253756620087
mae: 0.04483812618633228
r2: 0.7031984838592457
pearson: 0.8468100379045819

=== Experiment 1723 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006820364656173066
rmse: 0.08258549906716715
mae: 0.042365663485130454
r2: 0.6924727595252069
pearson: 0.8351014389273327

=== Experiment 1729 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006758026981531279
rmse: 0.08220721976524495
mae: 0.03732159574623663
r2: 0.6952835378378963
pearson: 0.8346891360260352

=== Experiment 1727 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006685518395742997
rmse: 0.08176501938936355
mae: 0.03616636860171335
r2: 0.6985529180576208
pearson: 0.8360007701252913

=== Experiment 1684 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005966745506812627
rmse: 0.07724471183720363
mae: 0.034398930121935054
r2: 0.7309620712633498
pearson: 0.8561036547161313

=== Experiment 1502 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071418520975344586
rmse: 0.08450947933536485
mae: 0.03817259099799218
r2: 0.6779770322916656
pearson: 0.8288933450874183

=== Experiment 1631 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007003502815694505
rmse: 0.08368693336294804
mae: 0.042316791155550225
r2: 0.6842151405176535
pearson: 0.8328348238530725

=== Experiment 1689 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0069945590379072884
rmse: 0.08363348036466789
mae: 0.043312375156078976
r2: 0.6846184115216214
pearson: 0.8443209173781483

=== Experiment 1670 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006919758262457261
rmse: 0.08318508437488815
mae: 0.03624284538984907
r2: 0.6879911455643242
pearson: 0.83007094805406

=== Experiment 1734 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007940019363197662
rmse: 0.0891067862914922
mae: 0.03897403832770673
r2: 0.6419880215831906
pearson: 0.8055820958840042

=== Experiment 1584 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0063896894778346655
rmse: 0.0799355332617145
mae: 0.03938756493379928
r2: 0.7118917137618435
pearson: 0.8471956353601519

=== Experiment 1696 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007483647887852268
rmse: 0.08650807989923408
mae: 0.04779745946699948
r2: 0.6625656105420672
pearson: 0.8247210808307068

=== Experiment 1639 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007215673569390867
rmse: 0.08494512092751924
mae: 0.03591289485643936
r2: 0.6746484546169745
pearson: 0.8227143820199936

=== Experiment 1710 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007189952494384219
rmse: 0.08479358757821383
mae: 0.03263997645153485
r2: 0.6758082065683135
pearson: 0.8396036491137745

=== Experiment 1663 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006603016439775982
rmse: 0.08125894682910911
mae: 0.03746057153340434
r2: 0.7022728949402857
pearson: 0.8496241826157754

=== Experiment 1762 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006663882379606988
rmse: 0.08163260610569154
mae: 0.03583090083071758
r2: 0.6995284765024544
pearson: 0.8390766281229387

=== Experiment 1720 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0071381500135608185
rmse: 0.0844875731309689
mae: 0.03991833189767939
r2: 0.6781439576286252
pearson: 0.8270544947720279

=== Experiment 1575 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0069274652114741835
rmse: 0.08323139558768784
mae: 0.03931030490396401
r2: 0.6876436426252388
pearson: 0.8474004085407799

=== Experiment 1686 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006806821451667883
rmse: 0.08250346327074932
mae: 0.04002226755679629
r2: 0.6930834166555255
pearson: 0.8347092510106443

=== Experiment 1717 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007370018750303912
rmse: 0.08584881333078467
mae: 0.034931606239777535
r2: 0.6676890983421147
pearson: 0.8186664262247996

=== Experiment 1567 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007355886269942019
rmse: 0.08576646355039957
mae: 0.038277332304442954
r2: 0.668326325661452
pearson: 0.8321304213979763

=== Experiment 1576 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007643267594353337
rmse: 0.08742578334995539
mae: 0.04837402961811854
r2: 0.6553684282299395
pearson: 0.8252600682312181

=== Experiment 1724 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007440606363717153
rmse: 0.08625894947028484
mae: 0.041564346928021495
r2: 0.6645063339212962
pearson: 0.830705638452129

=== Experiment 1534 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065444926505575625
rmse: 0.08089803860760508
mae: 0.03462248523863821
r2: 0.7049117068378548
pearson: 0.8412553999313619

=== Experiment 1533 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00774377164325994
rmse: 0.08799870250895714
mae: 0.05101297473849509
r2: 0.6508367448999556
pearson: 0.8225073940351838

=== Experiment 1694 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006566563376712861
rmse: 0.08103433455463716
mae: 0.03959026699328487
r2: 0.7039165475095819
pearson: 0.8408693840690368

=== Experiment 1703 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005864630360235561
rmse: 0.0765808746374417
mae: 0.032111812477816915
r2: 0.7355663982781966
pearson: 0.8619985752784165

=== Experiment 1508 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0070315303800803095
rmse: 0.0838542210033598
mae: 0.037350770608688015
r2: 0.6829513899753721
pearson: 0.82725568547339

=== Experiment 1643 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005883605197556553
rmse: 0.07670466216310813
mae: 0.02954468340203466
r2: 0.7347108312148576
pearson: 0.8584147538256814

=== Experiment 1653 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007740879606600486
rmse: 0.08798226870569141
mae: 0.03715232316392909
r2: 0.6509671455600476
pearson: 0.8189550097649564

=== Experiment 1501 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00482803854476533
rmse: 0.0694840884286851
mae: 0.025882003526230803
r2: 0.7823058669988008
pearson: 0.8852122479028482

=== Experiment 1585 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006850834592161556
rmse: 0.08276976858830497
mae: 0.036337605644696847
r2: 0.6910988835223304
pearson: 0.836263418708369

=== Experiment 1614 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007128010393504142
rmse: 0.08442754522964731
mae: 0.03762284079184815
r2: 0.6786011486341927
pearson: 0.8327343803982605

=== Experiment 1704 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0060221276970235165
rmse: 0.07760236914568727
mae: 0.033334179977654824
r2: 0.7284649126823066
pearson: 0.853608302389723

=== Experiment 1685 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007290262098057428
rmse: 0.085383031675254
mae: 0.0465099181503352
r2: 0.6712852906883213
pearson: 0.828354562522108

=== Experiment 1654 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006237356440745622
rmse: 0.07897693613166835
mae: 0.038735314819041126
r2: 0.7187603433572913
pearson: 0.8570406997878495

=== Experiment 1570 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006080003343737674
rmse: 0.0779743761997342
mae: 0.03757809197544561
r2: 0.7258553252449855
pearson: 0.8574753560436935

=== Experiment 1624 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006061645434139764
rmse: 0.07785656962735878
mae: 0.035427103083518895
r2: 0.7266830753088214
pearson: 0.855744157265808

=== Experiment 1578 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0060990327672996196
rmse: 0.07809630444073279
mae: 0.03007882061114957
r2: 0.724997296912067
pearson: 0.8625452544442205

=== Experiment 1545 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00781123262638701
rmse: 0.08838117800972677
mae: 0.03944623716430823
r2: 0.6477949588626053
pearson: 0.8271744421047627

=== Experiment 1627 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005936525860400628
rmse: 0.0770488537254165
mae: 0.036085443628932304
r2: 0.7323246618193832
pearson: 0.8587721180882965

=== Experiment 1636 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005639925914262918
rmse: 0.07509944017276639
mae: 0.027607296224449813
r2: 0.745698222847113
pearson: 0.8636794187841207

=== Experiment 1776 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006661873134254245
rmse: 0.08162029854303551
mae: 0.03813381322337333
r2: 0.699619072491073
pearson: 0.8372290148364115

=== Experiment 1557 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007625164847328786
rmse: 0.08732218989082206
mae: 0.04815273288756084
r2: 0.6561846731256482
pearson: 0.8185508230723628

=== Experiment 1758 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0070873848621023135
rmse: 0.08418660737969141
mae: 0.03410222543711311
r2: 0.6804329359644372
pearson: 0.8274139109616332

=== Experiment 1651 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007231820460433145
rmse: 0.08504011089146783
mae: 0.045547011723029364
r2: 0.6739203984066686
pearson: 0.8284712233848529

=== Experiment 1792 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007486763180231128
rmse: 0.08652608381425296
mae: 0.04752361671456845
r2: 0.6624251433798494
pearson: 0.823525764922723

=== Experiment 1730 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006551513924858578
rmse: 0.08094142279981603
mae: 0.03271435514079876
r2: 0.7045951206699212
pearson: 0.839533863889403

=== Experiment 1677 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007331692277483802
rmse: 0.08562530161981213
mae: 0.040596378898016455
r2: 0.6694172221328027
pearson: 0.8195655064884966

=== Experiment 1765 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006331012652370981
rmse: 0.0795676608451636
mae: 0.03642884438174248
r2: 0.7145374259963551
pearson: 0.8459372338920628

=== Experiment 1676 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060326936397892056
rmse: 0.07767041676075394
mae: 0.03468651319157571
r2: 0.7279884989734288
pearson: 0.8617624033709073

=== Experiment 1665 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0062621170040001
rmse: 0.07913353905898624
mae: 0.04256178834388443
r2: 0.7176439004581043
pearson: 0.8547265302328696

=== Experiment 1681 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006785744119717621
rmse: 0.08237562819012442
mae: 0.03474560051070586
r2: 0.6940337842763196
pearson: 0.8335011406082015

=== Experiment 1577 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005976806512523459
rmse: 0.07730980864368672
mae: 0.03224318099179503
r2: 0.7305084249440356
pearson: 0.8551148116055582

=== Experiment 1794 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0069515427900014
rmse: 0.08337591252874778
mae: 0.042714113809695226
r2: 0.6865579952067993
pearson: 0.8323903033532246

=== Experiment 1712 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006785372706832104
rmse: 0.08237337377352043
mae: 0.03940033563694996
r2: 0.6940505311198562
pearson: 0.8359746317804867

=== Experiment 1759 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006069464008143129
rmse: 0.07790676484197716
mae: 0.03135071578721585
r2: 0.7263305392482271
pearson: 0.8532488821898911

=== Experiment 1779 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0058623114046838926
rmse: 0.0765657325745917
mae: 0.02927495286611517
r2: 0.7356709589633708
pearson: 0.8594341190455954

=== Experiment 1673 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0063777867295440995
rmse: 0.07986104638397934
mae: 0.03146295833323838
r2: 0.7124284034434649
pearson: 0.8512235708173876

=== Experiment 1815 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006567578168610099
rmse: 0.08104059580611497
mae: 0.040252007233103404
r2: 0.7038707909895794
pearson: 0.8421113139700611

=== Experiment 1537 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006768198238864176
rmse: 0.08226906003391661
mae: 0.04067472710144988
r2: 0.6948249203214687
pearson: 0.8372885297371073

=== Experiment 1804 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065060013901622185
rmse: 0.08065978793774639
mae: 0.03605827519283488
r2: 0.7066472608277816
pearson: 0.8408583168696862

=== Experiment 1560 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007205379648394045
rmse: 0.08488450770543494
mae: 0.047141849268013666
r2: 0.6751126029840202
pearson: 0.8328482021010416

=== Experiment 1713 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062144538314809206
rmse: 0.07883180723211235
mae: 0.03671174145611256
r2: 0.7197930119288276
pearson: 0.8540986495093286

=== Experiment 1754 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007201992430482759
rmse: 0.08486455343948236
mae: 0.03704387019863919
r2: 0.6752653311488116
pearson: 0.8246838580512541

=== Experiment 1709 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007223346840531034
rmse: 0.08499027497620557
mae: 0.039982528441263515
r2: 0.6743024701985294
pearson: 0.8220847558625592

=== Experiment 1764 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006514022162588123
rmse: 0.08070949239456363
mae: 0.03226322398242469
r2: 0.7062856077293094
pearson: 0.8443659985116932

=== Experiment 1760 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007616500772455667
rmse: 0.08727256597840852
mae: 0.04400191581417974
r2: 0.6565753324483261
pearson: 0.8246895311842872

=== Experiment 1833 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006419875979934268
rmse: 0.0801241285751943
mae: 0.03066805027155745
r2: 0.710530617668268
pearson: 0.8546404321369561

=== Experiment 1761 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007152544633633603
rmse: 0.08457271802202886
mae: 0.041760572402303305
r2: 0.6774949105451002
pearson: 0.830782929437166

=== Experiment 1700 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005841902301173894
rmse: 0.07643233805905648
mae: 0.03093912230671718
r2: 0.7365911964579033
pearson: 0.8585519431186827

=== Experiment 1802 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068763474084535545
rmse: 0.08292374453950783
mae: 0.03930026076257401
r2: 0.6899485218647733
pearson: 0.8324245899728886

=== Experiment 1603 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005586011988058419
rmse: 0.07473962796307203
mae: 0.02645769310300827
r2: 0.7481291780503403
pearson: 0.8653294897389975

=== Experiment 1626 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00823064522704398
rmse: 0.09072290354174066
mae: 0.04124526192393275
r2: 0.6288838292965848
pearson: 0.80274008271756

=== Experiment 1801 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00722007476659652
rmse: 0.08497102309962214
mae: 0.047717203162904544
r2: 0.6744500065720869
pearson: 0.8312274020631731

=== Experiment 1582 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006385134008416965
rmse: 0.07990703353533382
mae: 0.03278100880891271
r2: 0.7120971178728721
pearson: 0.8458899912611407

=== Experiment 1662 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0064628987052049456
rmse: 0.08039215574423257
mae: 0.03778179195753794
r2: 0.7085907419215627
pearson: 0.8539037142990538

=== Experiment 1811 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005829248592903646
rmse: 0.07634951599652512
mae: 0.031135341274142064
r2: 0.7371617465944859
pearson: 0.8587552506382264

=== Experiment 1785 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006867838399667686
rmse: 0.0828724224315163
mae: 0.03661194807529306
r2: 0.690332189325827
pearson: 0.8315384602554234

=== Experiment 1845 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00662444613193104
rmse: 0.08139070052488208
mae: 0.039680674710719885
r2: 0.701306639552943
pearson: 0.8384855533554585

=== Experiment 1581 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005852243288183627
rmse: 0.07649995613190655
mae: 0.03336524260987402
r2: 0.7361249259050509
pearson: 0.863020319430196

=== Experiment 1750 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00725413345614321
rmse: 0.08517120086122544
mae: 0.03618485667164775
r2: 0.6729143152508021
pearson: 0.8206693469652436

=== Experiment 1675 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008161771476696954
rmse: 0.09034252308130958
mae: 0.05055451301667328
r2: 0.6319893163860767
pearson: 0.8032244931320738

=== Experiment 1688 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007195049467228167
rmse: 0.08482363743219319
mae: 0.03768935766416165
r2: 0.6755783863061291
pearson: 0.8232109855528562

=== Experiment 1544 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006120016450677189
rmse: 0.07823053400480652
mae: 0.029833345481976838
r2: 0.724051151864198
pearson: 0.8610681538585653

=== Experiment 1832 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006377971062057504
rmse: 0.079862200458399
mae: 0.03253172217758734
r2: 0.7124200919715664
pearson: 0.8477237016913172

=== Experiment 1872 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00638784581510927
rmse: 0.07992400024466537
mae: 0.0362698143937454
r2: 0.7119748437026735
pearson: 0.8440678175202927

=== Experiment 1559 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005621360237593461
rmse: 0.07497573099072433
mae: 0.02868235172935921
r2: 0.7465353410367597
pearson: 0.866827359528913

=== Experiment 1529 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006300720652319915
rmse: 0.07937707888502774
mae: 0.03488834674763411
r2: 0.7159032789461288
pearson: 0.8483034258947095

=== Experiment 1806 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007324374935480235
rmse: 0.08558256209929821
mae: 0.037257552218854385
r2: 0.6697471578631349
pearson: 0.8188607818785313

=== Experiment 1752 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00704539512395095
rmse: 0.08393685200167415
mae: 0.03733607018843318
r2: 0.6823262347765853
pearson: 0.834240884683966

=== Experiment 1864 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.008634452955140173
rmse: 0.09292175716773857
mae: 0.0393970770829439
r2: 0.6106763165661004
pearson: 0.7917378492932392

=== Experiment 1836 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006556686153581814
rmse: 0.08097336693988841
mae: 0.04216695625026756
r2: 0.7043619071532623
pearson: 0.8437007510673525

=== Experiment 1698 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0072019626545794645
rmse: 0.08486437800737989
mae: 0.04150340995933726
r2: 0.6752666737311857
pearson: 0.8305219245929173

=== Experiment 1645 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006027564303290539
rmse: 0.07763738985366869
mae: 0.03125648678544635
r2: 0.72821977849856
pearson: 0.8535206630071869

=== Experiment 1751 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006606145732875545
rmse: 0.0812781996163519
mae: 0.0361252000642408
r2: 0.7021317964917336
pearson: 0.8396865368331599

=== Experiment 1812 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006851458079906724
rmse: 0.08277353490039388
mae: 0.04495032766371892
r2: 0.6910707707343187
pearson: 0.8391062408541141

=== Experiment 1863 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006925678876096009
rmse: 0.08322066375664165
mae: 0.044182652924372116
r2: 0.6877241877012144
pearson: 0.8411385283599669

=== Experiment 1831 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005724628953875425
rmse: 0.0756612777705705
mae: 0.032486444758640605
r2: 0.7418790000716438
pearson: 0.8663570547296799

=== Experiment 1803 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006578633082880094
rmse: 0.08110877315605318
mae: 0.034874705648597244
r2: 0.7033723297707852
pearson: 0.8405717995995229

=== Experiment 1828 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005439163973392887
rmse: 0.07375068795199735
mae: 0.03104554109904315
r2: 0.7547504903988551
pearson: 0.8691135854072902

=== Experiment 1829 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00727973253178961
rmse: 0.08532134862852093
mae: 0.035270796625559986
r2: 0.6717600641969208
pearson: 0.821355388397318

=== Experiment 1693 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007052411442074667
rmse: 0.08397863681957851
mae: 0.04433996110476038
r2: 0.6820098720805046
pearson: 0.8358268422683801

=== Experiment 1879 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005896974574985625
rmse: 0.07679176111397384
mae: 0.028456681856338616
r2: 0.7341080118708936
pearson: 0.8645996220292654

=== Experiment 1854 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005460667775750026
rmse: 0.0738963312739545
mae: 0.02855332358840213
r2: 0.7537808934151191
pearson: 0.8689556920710129

=== Experiment 1834 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007114153501936618
rmse: 0.08434544150063249
mae: 0.04726386830331519
r2: 0.679225949776088
pearson: 0.833907212694176

=== Experiment 1511 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006866463411894183
rmse: 0.08286412620606208
mae: 0.03597049553185279
r2: 0.6903941869193557
pearson: 0.8310851769114781

=== Experiment 1857 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005944936333448885
rmse: 0.07710341324123651
mae: 0.031417868501780875
r2: 0.7319454372913696
pearson: 0.8555933473522699

=== Experiment 1896 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007239806931435144
rmse: 0.08508705501681876
mae: 0.04283110178379654
r2: 0.6735602919443031
pearson: 0.8231834682283382

=== Experiment 1861 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007213329247998666
rmse: 0.08493132077154261
mae: 0.030693580828026767
r2: 0.6747541590367341
pearson: 0.8543719036414686

=== Experiment 1821 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006594286179941833
rmse: 0.08120521030045937
mae: 0.03597054029305679
r2: 0.7026665385137264
pearson: 0.8386070643910342

=== Experiment 1901 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00789449209305889
rmse: 0.08885095437337119
mae: 0.03649510994283786
r2: 0.6440408261556638
pearson: 0.8244808095416002

=== Experiment 1746 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007282931139827032
rmse: 0.08534009104651244
mae: 0.045644001886293
r2: 0.6716158403683286
pearson: 0.826293712627151

=== Experiment 1849 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005880578855022843
rmse: 0.07668493238585299
mae: 0.030709692260169095
r2: 0.7348472876677072
pearson: 0.8581268273517447

=== Experiment 1562 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006387458055692632
rmse: 0.07992157440699371
mae: 0.031094394961086666
r2: 0.7119923276041037
pearson: 0.8565957246178472

=== Experiment 1902 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006874472243564466
rmse: 0.08291243720675726
mae: 0.040830526576584494
r2: 0.6900330722242991
pearson: 0.8323290235355522

=== Experiment 1625 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006016040028612524
rmse: 0.07756313575799088
mae: 0.033269341254403735
r2: 0.7287394029715706
pearson: 0.8539121345532102

=== Experiment 1841 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007035589951902511
rmse: 0.08387842363744392
mae: 0.04144359044506945
r2: 0.6827683456686631
pearson: 0.8335591881803145

=== Experiment 1798 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.01039344674593095
rmse: 0.10194825523730629
mae: 0.04214984905700264
r2: 0.5313640607317172
pearson: 0.8052026804096719

=== Experiment 1813 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00684822937000411
rmse: 0.08275402932790711
mae: 0.03925589463299397
r2: 0.6912163518427636
pearson: 0.8484428422025887

=== Experiment 1850 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006680958609361534
rmse: 0.08173713115446085
mae: 0.035346618379356855
r2: 0.6987585168186455
pearson: 0.8370913579267514

=== Experiment 1839 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006600364075916466
rmse: 0.08124262474782844
mae: 0.0400956573945982
r2: 0.7023924888593167
pearson: 0.84694131680543

=== Experiment 1853 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068593086610165095
rmse: 0.08282094337193044
mae: 0.039610586007591465
r2: 0.6907167914873833
pearson: 0.8341643347165449

=== Experiment 1817 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006384723009324016
rmse: 0.07990446176105571
mae: 0.03184019580756656
r2: 0.7121156496410787
pearson: 0.8474546191773157

=== Experiment 1619 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006665088830478846
rmse: 0.08163999528710696
mae: 0.033749453781365506
r2: 0.699474078163642
pearson: 0.8408635251082167

=== Experiment 1775 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007512482783496371
rmse: 0.08667457979994117
mae: 0.048338497357362
r2: 0.6612654577886828
pearson: 0.8354311359349533

=== Experiment 1769 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006926837255653046
rmse: 0.08322762315273124
mae: 0.039614830424525294
r2: 0.6876719568768294
pearson: 0.8327997430821173

=== Experiment 1539 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005473779362262767
rmse: 0.07398499416951229
mae: 0.029522675351132996
r2: 0.7531896977501099
pearson: 0.8743483322004573

=== Experiment 1682 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006003370332823272
rmse: 0.07748141927470915
mae: 0.03375258571621138
r2: 0.7293106739783484
pearson: 0.8549899903249504

=== Experiment 1830 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005709225673057838
rmse: 0.07555941816251524
mae: 0.028417805668291416
r2: 0.7425735272242064
pearson: 0.8638559911170111

=== Experiment 1905 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00729696403068478
rmse: 0.08542226893898791
mae: 0.04239378466701344
r2: 0.6709831034959002
pearson: 0.828728693849798

=== Experiment 1852 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.008222629067842673
rmse: 0.09067871342185371
mae: 0.048221999382774924
r2: 0.6292452743867905
pearson: 0.8043189170944413

=== Experiment 1899 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0069769435138914105
rmse: 0.08352810014534875
mae: 0.03752073711181303
r2: 0.6854126877463124
pearson: 0.8283835624072265

=== Experiment 1780 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005718700556343976
rmse: 0.07562209039919471
mae: 0.031783868220149586
r2: 0.7421463089070497
pearson: 0.8629108458016105

=== Experiment 1816 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006542028380945709
rmse: 0.0808828064606175
mae: 0.03912170688662117
r2: 0.7050228196700452
pearson: 0.8430168032326941

=== Experiment 1753 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005862680986486109
rmse: 0.07656814603009603
mae: 0.02926005683750774
r2: 0.7356542946825744
pearson: 0.8579404422278345

=== Experiment 1895 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065044309674192925
rmse: 0.08065005249483284
mae: 0.03333682059260396
r2: 0.7067180704980642
pearson: 0.8434318159456545

=== Experiment 1893 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00597129024781218
rmse: 0.07727412405075958
mae: 0.030830828113366515
r2: 0.7307571508919732
pearson: 0.8568064752907584

=== Experiment 1702 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007424196604666387
rmse: 0.08616377779941167
mae: 0.0452451816915891
r2: 0.6652462427344078
pearson: 0.8283065664511615

=== Experiment 1773 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068030270850898125
rmse: 0.082480464869506
mae: 0.039703322956681546
r2: 0.6932545029745022
pearson: 0.8451909052573021

=== Experiment 1716 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006552978786557449
rmse: 0.0809504711941657
mae: 0.037174752799676664
r2: 0.704529070700654
pearson: 0.8407566387555843

=== Experiment 1736 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006235629231884601
rmse: 0.07896600048048907
mae: 0.03252987325729373
r2: 0.7188382224446321
pearson: 0.856145404722391

=== Experiment 1530 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006690923356506296
rmse: 0.08179806450342389
mae: 0.030516032503299945
r2: 0.6983092107557087
pearson: 0.848382349001498

=== Experiment 1771 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008216340319532963
rmse: 0.09064403079923665
mae: 0.04441730555518542
r2: 0.6295288312801857
pearson: 0.8145053204209634

=== Experiment 1911 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007060382677188932
rmse: 0.08402608331458114
mae: 0.03973717917516851
r2: 0.6816504525976113
pearson: 0.8279303805358043

=== Experiment 1963 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006929367028543741
rmse: 0.08324281968160221
mae: 0.03829882554790841
r2: 0.6875578905306262
pearson: 0.8294327541756902

=== Experiment 1612 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007383693386601025
rmse: 0.08592842013327735
mae: 0.044548189859163846
r2: 0.6670725150101455
pearson: 0.8262351039197997

=== Experiment 1610 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006763356793882731
rmse: 0.08223963031217207
mae: 0.036393130134029575
r2: 0.6950432189447991
pearson: 0.8389175479300521

=== Experiment 1593 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006791043975941672
rmse: 0.08240779074785146
mae: 0.030961444830988512
r2: 0.6937948160918221
pearson: 0.8427959531599235

=== Experiment 1782 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006019918135716799
rmse: 0.07758813141013772
mae: 0.03435465543134869
r2: 0.7285645408291244
pearson: 0.8546397686383289

=== Experiment 1960 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007358600479183239
rmse: 0.08578228534600392
mae: 0.03787714294620523
r2: 0.6682039431613804
pearson: 0.8178923838732257

=== Experiment 1966 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006476244369022983
rmse: 0.08047511645858603
mae: 0.037851530939014555
r2: 0.7079889918138831
pearson: 0.8435847881588886

=== Experiment 1822 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006885586493381431
rmse: 0.08297943415920254
mae: 0.039056050442453964
r2: 0.6895319355918076
pearson: 0.8336764616194663

=== Experiment 1978 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006114428063661607
rmse: 0.0781948084188561
mae: 0.02973064919118246
r2: 0.7243031297750933
pearson: 0.8540170188892905

=== Experiment 1944 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006814440620577934
rmse: 0.08254962519950006
mae: 0.037808018091682286
r2: 0.6927398716828239
pearson: 0.8343982484041184

=== Experiment 1521 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005781161031620375
rmse: 0.07603394657401637
mae: 0.03568505216021861
r2: 0.7393299935677944
pearson: 0.8613473554255362

=== Experiment 1919 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0067516915470336916
rmse: 0.0821686774083269
mae: 0.043165789214516255
r2: 0.6955691997909547
pearson: 0.8381880011171972

=== Experiment 1733 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006105849983914791
rmse: 0.07813993846884441
mae: 0.02899403961926716
r2: 0.7246899116153757
pearson: 0.8569303612561224

=== Experiment 1979 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005828582551035675
rmse: 0.07634515407696597
mae: 0.03121505024525859
r2: 0.7371917781291653
pearson: 0.8587988794780288

=== Experiment 1891 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006738310068528054
rmse: 0.08208721013975352
mae: 0.03991366476510006
r2: 0.6961725647671346
pearson: 0.8362522530010182

=== Experiment 1912 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007401785173790553
rmse: 0.08603362815661417
mae: 0.0479134402411597
r2: 0.666256764288574
pearson: 0.8257667378610566

=== Experiment 1840 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007431123194553878
rmse: 0.08620396275435299
mae: 0.03969428987602578
r2: 0.6649339258450062
pearson: 0.8218566324300343

=== Experiment 1719 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006274196507015558
rmse: 0.07920982582366634
mae: 0.03185001494473911
r2: 0.7170992409837331
pearson: 0.8472543555339787

=== Experiment 1952 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0074615176369883605
rmse: 0.08638007662064419
mae: 0.03852771380581946
r2: 0.6635634538132527
pearson: 0.8161687056026748

=== Experiment 1990 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.008392799796845233
rmse: 0.09161222514951393
mae: 0.040139718263967665
r2: 0.6215723511139306
pearson: 0.811811113731313

=== Experiment 1522 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0057628612979138405
rmse: 0.07591351195876687
mae: 0.02919750507313521
r2: 0.7401551205062931
pearson: 0.8603676267991993

=== Experiment 1906 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005829150430458342
rmse: 0.07634887314465316
mae: 0.03163166380311524
r2: 0.73716617269594
pearson: 0.861442202795755

=== Experiment 1856 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006072283259819763
rmse: 0.07792485649534277
mae: 0.02924932246146713
r2: 0.7262034204309753
pearson: 0.8615527526545886

=== Experiment 1909 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007281663009929556
rmse: 0.08533266086282296
mae: 0.04441414192775539
r2: 0.6716730197872591
pearson: 0.8248807187178308

=== Experiment 1568 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065818416987954605
rmse: 0.08112855045417403
mae: 0.03623634316493362
r2: 0.7032276546913813
pearson: 0.8546552403138736

=== Experiment 1565 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0064621450297909135
rmse: 0.08038746811407183
mae: 0.03244976867878812
r2: 0.7086247248142634
pearson: 0.8432916399312569

=== Experiment 1846 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007487478625764409
rmse: 0.08653021799212347
mae: 0.035523150237147666
r2: 0.6623928842556455
pearson: 0.8323610069375575

=== Experiment 1740 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007585451806156504
rmse: 0.0870944992875928
mae: 0.036177785979856256
r2: 0.6579753166730603
pearson: 0.8123533501238638

=== Experiment 1725 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.008082479589034575
rmse: 0.08990261169195572
mae: 0.0407595032704894
r2: 0.6355645527017447
pearson: 0.7973754652724215

=== Experiment 1868 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005999352873796097
rmse: 0.07745548962982608
mae: 0.027233860957520898
r2: 0.7294918194376647
pearson: 0.8565330685290755

=== Experiment 1777 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007680850063977856
rmse: 0.08764045905846145
mae: 0.04115089006923286
r2: 0.6536738512158797
pearson: 0.8168719532958881

=== Experiment 1572 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006344951778648245
rmse: 0.079655205596673
mae: 0.03429085383046816
r2: 0.713908916927592
pearson: 0.8450321490805777

=== Experiment 1976 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00625675398172095
rmse: 0.0790996459013626
mae: 0.03351177630390244
r2: 0.7178857167722246
pearson: 0.8478219649138323

=== Experiment 1941 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00778257373634505
rmse: 0.0882188967078202
mae: 0.04232135086864676
r2: 0.6490871756008552
pearson: 0.8087489184338754

=== Experiment 1871 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006840128268754563
rmse: 0.0827050679750314
mae: 0.03932054019263458
r2: 0.6915816269325383
pearson: 0.8317722374605175

=== Experiment 1957 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007166727106080385
rmse: 0.08465652429718802
mae: 0.037701787535081056
r2: 0.6768554291046582
pearson: 0.8231453869938976

=== Experiment 1705 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067853956190229775
rmse: 0.08237351284862736
mae: 0.037160756202929426
r2: 0.6940494980192542
pearson: 0.8338277005951448

=== Experiment 1808 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067789933751473104
rmse: 0.08233464261868943
mae: 0.03608641226649439
r2: 0.6943381723777651
pearson: 0.8357195202365582

=== Experiment 1737 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006558832030426437
rmse: 0.08098661636607889
mae: 0.03086493294377999
r2: 0.704265150510811
pearson: 0.8570764384844235

=== Experiment 1950 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006759980355326507
rmse: 0.08221909969907544
mae: 0.039020458363511853
r2: 0.6951954610732151
pearson: 0.8401300680206866

=== Experiment 1964 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062196835713026555
rmse: 0.07886497049579524
mae: 0.030305667532481796
r2: 0.71955720526205
pearson: 0.8511945546190859

=== Experiment 1597 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006852935003965091
rmse: 0.08278245589474313
mae: 0.03626223514143301
r2: 0.6910041768785707
pearson: 0.8322126714752421

=== Experiment 1538 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006135647402577388
rmse: 0.07833037343570748
mae: 0.034671319343703516
r2: 0.7233463591227258
pearson: 0.8523122248640886

=== Experiment 1933 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006956627972152892
rmse: 0.083406402464996
mae: 0.034660260349752524
r2: 0.6863287065817484
pearson: 0.8306071920593755

=== Experiment 1637 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006479654987895431
rmse: 0.08049630418780375
mae: 0.03225756764403332
r2: 0.7078352085100525
pearson: 0.8541126587468351

=== Experiment 1921 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006666271706210644
rmse: 0.0816472394279846
mae: 0.03805361223097272
r2: 0.6994207428175185
pearson: 0.8502967767146665

=== Experiment 1888 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005917382017791655
rmse: 0.07692452156361881
mae: 0.032864070920925574
r2: 0.7331878492567723
pearson: 0.8563048960845767

=== Experiment 1996 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0075397341544734525
rmse: 0.08683164258767337
mae: 0.04793196544886473
r2: 0.6600367054655718
pearson: 0.8216406838598372

=== Experiment 1870 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006925882355312805
rmse: 0.08322188627586379
mae: 0.037231760141769585
r2: 0.6877150129128871
pearson: 0.8434958601791203

=== Experiment 1744 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007065600174917819
rmse: 0.08405712447447759
mae: 0.03804068956007691
r2: 0.681415197921412
pearson: 0.825568301030723

=== Experiment 1934 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006399317954297816
rmse: 0.07999573710078442
mae: 0.032544704203164077
r2: 0.7114575699959367
pearson: 0.8442612433042613

=== Experiment 1527 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007006398867962652
rmse: 0.08370423446853004
mae: 0.03518711832044802
r2: 0.6840845587955329
pearson: 0.8280105143319259

=== Experiment 1827 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00698985651100453
rmse: 0.08360536173598276
mae: 0.040603448760333824
r2: 0.6848304463899264
pearson: 0.8307353323754387

=== Experiment 1975 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007419979313543078
rmse: 0.08613930179391448
mae: 0.04952352001107292
r2: 0.6654363985349862
pearson: 0.8261282741076363

=== Experiment 1998 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007425124003811839
rmse: 0.0861691592381627
mae: 0.04290832069038851
r2: 0.665204426715119
pearson: 0.8184974262400022

=== Experiment 1589 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006741824659842579
rmse: 0.08210861501598099
mae: 0.03526510785172426
r2: 0.6960140933916527
pearson: 0.8344855566604722

=== Experiment 1658 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006905991317015761
rmse: 0.08310229429453654
mae: 0.034675668327352166
r2: 0.6886118910749854
pearson: 0.8350189541344456

=== Experiment 1924 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005995114985574058
rmse: 0.0774281278707813
mae: 0.029386397528804856
r2: 0.7296829039523592
pearson: 0.867480701143079

=== Experiment 1885 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0074873989656690475
rmse: 0.08652975768872259
mae: 0.04228900619661655
r2: 0.6623964760942842
pearson: 0.8159292457494789

=== Experiment 1953 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007427864534793107
rmse: 0.08618505981197151
mae: 0.04623075326339845
r2: 0.6650808573793736
pearson: 0.8234674455742902

=== Experiment 1755 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005600962108750195
rmse: 0.07483957581888205
mae: 0.029321536798388954
r2: 0.7474550836883993
pearson: 0.8648741893904589

=== Experiment 1936 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00581600469026156
rmse: 0.07626273461043448
mae: 0.030085650277685626
r2: 0.7377589083355302
pearson: 0.861450137749495

=== Experiment 1898 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00701047813653157
rmse: 0.0837285980805338
mae: 0.04247403985797923
r2: 0.6839006263711838
pearson: 0.8313211988388873

=== Experiment 1862 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006917896961057724
rmse: 0.08317389591126367
mae: 0.044258971824956105
r2: 0.6880750708252079
pearson: 0.8364840091157102

=== Experiment 1945 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006338466993105443
rmse: 0.0796144898439062
mae: 0.03448509853017142
r2: 0.7142013130535443
pearson: 0.8474590831106636

=== Experiment 1881 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0056095577203452585
rmse: 0.07489698071581563
mae: 0.029543885904455686
r2: 0.747067511344796
pearson: 0.8649145780114526

=== Experiment 1913 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007724803056958336
rmse: 0.08789085877927429
mae: 0.04531853616157769
r2: 0.6516920301075815
pearson: 0.8168018076191034

=== Experiment 1993 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006198160068318335
rmse: 0.07872839429531339
mae: 0.035503781746755766
r2: 0.7205276905383949
pearson: 0.8494349952245639

=== Experiment 1787 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0055603170454613824
rmse: 0.07456753345432168
mae: 0.029646155929539827
r2: 0.7492877517028316
pearson: 0.8748263554873645

=== Experiment 1994 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0054492603720632665
rmse: 0.07381910573871284
mae: 0.027417709574710274
r2: 0.7542952482265727
pearson: 0.8735470749514772

=== Experiment 1858 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00631933583563938
rmse: 0.07949425033069613
mae: 0.03223407665904994
r2: 0.7150639285234229
pearson: 0.8465929044687849

=== Experiment 1855 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006761489413078778
rmse: 0.08222827623803614
mae: 0.03367394432891853
r2: 0.6951274183233523
pearson: 0.850191117137513

=== Experiment 1992 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006382461367112271
rmse: 0.07989030834282887
mae: 0.038993552375355435
r2: 0.7122176260929813
pearson: 0.8466543323466496

=== Experiment 1692 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00924523944323449
rmse: 0.09615216816710109
mae: 0.04082710075689439
r2: 0.5831362226456196
pearson: 0.8086757686395942

=== Experiment 1955 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006816001406233893
rmse: 0.08255907827873257
mae: 0.038259186915474586
r2: 0.692669496544551
pearson: 0.8343736253682936

=== Experiment 1925 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006925739314657629
rmse: 0.08322102687817322
mae: 0.03895701309899671
r2: 0.687721462553072
pearson: 0.8441013219409642

=== Experiment 1810 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0062797779610259084
rmse: 0.07924505007270743
mae: 0.02778296237195349
r2: 0.7168475756789923
pearson: 0.8496320096391929

=== Experiment 1630 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067026205245961
rmse: 0.08186953355550586
mae: 0.036868932912022985
r2: 0.6977817905948569
pearson: 0.8445533397083542

=== Experiment 1793 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007165601881068933
rmse: 0.08464987821059716
mae: 0.04534242507505942
r2: 0.6769061650051753
pearson: 0.8350564097877649

=== Experiment 1984 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006510585418460223
rmse: 0.08068819875582936
mae: 0.04212390684692809
r2: 0.7064405690094097
pearson: 0.8453248884073725

=== Experiment 1552 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005486449599580371
rmse: 0.07407057175140726
mae: 0.027212138381456252
r2: 0.7526184023260574
pearson: 0.8694409276642545

=== Experiment 1714 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006981922209640604
rmse: 0.0835578973505234
mae: 0.04342839109959215
r2: 0.6851882005462728
pearson: 0.8335473449731815

=== Experiment 1642 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007150732240525899
rmse: 0.08456200234458677
mae: 0.03918335188561085
r2: 0.6775766305526314
pearson: 0.8324336356219796

=== Experiment 1807 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007249085546964463
rmse: 0.08514156180717185
mae: 0.036213243819386856
r2: 0.6731419232539171
pearson: 0.83345637335192

=== Experiment 1982 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059804876474167405
rmse: 0.07733361266239112
mae: 0.030603310890940292
r2: 0.7303424441918889
pearson: 0.8551131208651345

=== Experiment 1995 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006765695591974088
rmse: 0.08225384849339323
mae: 0.03943316275983191
r2: 0.6949377635682435
pearson: 0.8362476459926833

=== Experiment 1947 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006636759599086677
rmse: 0.08146630959535774
mae: 0.03809328253114411
r2: 0.7007514307384667
pearson: 0.8373886115200407

=== Experiment 1772 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007089529742282903
rmse: 0.08419934526041697
mae: 0.03774338419263169
r2: 0.6803362242611285
pearson: 0.8276153388851186

=== Experiment 1814 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005938876903436249
rmse: 0.0770641090484815
mae: 0.037282775000720594
r2: 0.7322186543236802
pearson: 0.862912497967219

=== Experiment 1835 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006129880926397828
rmse: 0.0782935560975348
mae: 0.03420863756601482
r2: 0.7236063669956421
pearson: 0.854654535028841

=== Experiment 1923 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005843582213337657
rmse: 0.07644332680710368
mae: 0.03447175362518596
r2: 0.736515449957825
pearson: 0.8634043830651773

=== Experiment 1884 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00553862374458869
rmse: 0.07442193053521717
mae: 0.028140839129616332
r2: 0.7502658930912295
pearson: 0.8676226309913716

=== Experiment 1587 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006665186018919026
rmse: 0.08164059051059727
mae: 0.03698215767365825
r2: 0.6994696959796501
pearson: 0.8412565572143055

=== Experiment 1641 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007271113559262154
rmse: 0.08527082478352226
mae: 0.04433350533182605
r2: 0.6721486898746838
pearson: 0.8354001258369812

=== Experiment 1929 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.008294911772155824
rmse: 0.09107640623210725
mae: 0.0537119118328412
r2: 0.6259860790633617
pearson: 0.8067637453809008

=== Experiment 1657 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006297198157972349
rmse: 0.07935488742334872
mae: 0.03751757260666464
r2: 0.7160621066658922
pearson: 0.8478106271250174

=== Experiment 1687 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007717335975174653
rmse: 0.0878483692231942
mae: 0.03835930812430472
r2: 0.6520287175386938
pearson: 0.811510986942426

=== Experiment 1873 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006202631484006064
rmse: 0.07875678690758063
mae: 0.035511171942756714
r2: 0.720326076373056
pearson: 0.851245393551015

=== Experiment 1869 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006711253259993446
rmse: 0.08192223910510166
mae: 0.04382473815837653
r2: 0.6973925443553479
pearson: 0.841188025657197

=== Experiment 1678 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007407576950676921
rmse: 0.08606728153414003
mae: 0.04522773043245318
r2: 0.6659956156179239
pearson: 0.8271938403100874

=== Experiment 1707 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005828345473988362
rmse: 0.07634360139519462
mae: 0.03442663794414575
r2: 0.7372024678288829
pearson: 0.8617174455544702

=== Experiment 1914 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067646321201261395
rmse: 0.08224738366736135
mae: 0.03758949647653074
r2: 0.6949857150457976
pearson: 0.8435967509717133

=== Experiment 1672 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00551829606691683
rmse: 0.07428523451478652
mae: 0.027838579066126135
r2: 0.7511824591305585
pearson: 0.8688836887578897

=== Experiment 1601 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006934346158559458
rmse: 0.08327272157531215
mae: 0.04059664003217996
r2: 0.6873333837497002
pearson: 0.83198986158939

=== Experiment 1851 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00699606888612854
rmse: 0.08364250645532174
mae: 0.038522249892134146
r2: 0.6845503331298601
pearson: 0.8289734544198987

=== Experiment 1795 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005502999998292611
rmse: 0.07418220809798405
mae: 0.030476989124792655
r2: 0.7518721521325099
pearson: 0.867581807001229

=== Experiment 1985 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006010959380489593
rmse: 0.07753037714657135
mae: 0.02825786733434953
r2: 0.7289684871592693
pearson: 0.8545137677369505

=== Experiment 1970 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0073976032597232915
rmse: 0.08600932077236334
mae: 0.04009312630828492
r2: 0.6664453249532658
pearson: 0.8265157327438326

=== Experiment 1877 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007359135812862613
rmse: 0.08578540559362421
mae: 0.03522022194303686
r2: 0.6681798052013956
pearson: 0.8177914810315463

=== Experiment 1726 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006549834989717429
rmse: 0.08093105083784238
mae: 0.0368169674649916
r2: 0.7046708231164798
pearson: 0.8420455497842085

=== Experiment 1546 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006415796513894435
rmse: 0.08009866736653261
mae: 0.032243059831383446
r2: 0.7107145589964936
pearson: 0.8488667348449596

=== Experiment 1671 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006399980407315661
rmse: 0.07999987754562916
mae: 0.034627114833852175
r2: 0.7114277002809297
pearson: 0.8450937120705406

=== Experiment 1536 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006811245968288616
rmse: 0.0825302730414035
mae: 0.036112354482599585
r2: 0.6928839171484753
pearson: 0.8345353178274545

=== Experiment 1796 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007924919112606841
rmse: 0.08902201476380346
mae: 0.04423530654900149
r2: 0.6426688852362021
pearson: 0.8081754343913634

=== Experiment 1889 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0056538317305986924
rmse: 0.07519196586470321
mae: 0.030593971058717792
r2: 0.7450712157089507
pearson: 0.864011746308985

=== Experiment 1997 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0065287885452023605
rmse: 0.08080091921013251
mae: 0.03107379214975312
r2: 0.7056197980364156
pearson: 0.8435887316437726

=== Experiment 1988 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006548370446493014
rmse: 0.08092200223976798
mae: 0.03384484277081722
r2: 0.7047368587258738
pearson: 0.848913091449878

=== Experiment 1987 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006755993680156682
rmse: 0.08219485190787001
mae: 0.04132466255599383
r2: 0.6953752185019524
pearson: 0.837359623178436

=== Experiment 1937 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006813950043006355
rmse: 0.08254665373597136
mae: 0.04397756591206486
r2: 0.6927619916096068
pearson: 0.8400773149422212

=== Experiment 1892 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006197951635123754
rmse: 0.07872707053564075
mae: 0.03264856013082506
r2: 0.7205370886994001
pearson: 0.848963900623317

=== Experiment 1638 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006307886793910869
rmse: 0.07942220592448228
mae: 0.03392179281309273
r2: 0.7155801607758528
pearson: 0.8461921216216013

=== Experiment 1632 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00642251542520252
rmse: 0.08014059785902848
mae: 0.04121157665968426
r2: 0.7104116062428312
pearson: 0.8478411257728905

=== Experiment 1999 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006871531324127611
rmse: 0.08289470021736982
mae: 0.04161195028461218
r2: 0.6901656769873092
pearson: 0.8378913734856971

=== Experiment 1954 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007553129852880279
rmse: 0.08690874439824958
mae: 0.048801705515048394
r2: 0.6594326993203057
pearson: 0.8228575870118303

=== Experiment 1883 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0056986556309315374
rmse: 0.07548944052602018
mae: 0.030377310728970035
r2: 0.74305012577145
pearson: 0.8624078397322421

=== Experiment 1940 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006379216217259799
rmse: 0.07986999572592826
mae: 0.037659274087585036
r2: 0.7123639484715297
pearson: 0.8460118992241872

=== Experiment 1781 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006564802178986294
rmse: 0.08102346684131885
mae: 0.03562997836065737
r2: 0.7039959591399103
pearson: 0.8532147270240061

=== Experiment 1980 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005809969016549912
rmse: 0.07622315275918408
mae: 0.028640559885129313
r2: 0.7380310542066851
pearson: 0.8598931680472665

=== Experiment 1927 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006406391832107449
rmse: 0.0800399389811577
mae: 0.028469122601592248
r2: 0.7111386119589519
pearson: 0.8461379801057595

=== Experiment 1602 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0071932254128034435
rmse: 0.08481288470983311
mae: 0.03722913195403735
r2: 0.6756606321173122
pearson: 0.8241790517636032

=== Experiment 1690 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006215542007327377
rmse: 0.07883870881316726
mae: 0.02810464205450011
r2: 0.7197439465588537
pearson: 0.8539312677696602

=== Experiment 1766 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006073053729696686
rmse: 0.07792980001062935
mae: 0.039059540924720776
r2: 0.7261686802833347
pearson: 0.8555775289299233

=== Experiment 1732 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006165989710826652
rmse: 0.07852381620137072
mae: 0.037528383372970375
r2: 0.7219782377984378
pearson: 0.851374813202435

=== Experiment 1920 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007192188791082427
rmse: 0.08480677326182401
mae: 0.036509203608178345
r2: 0.6757073729344608
pearson: 0.8276888586783364

=== Experiment 1524 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006759286262154222
rmse: 0.0822148785935625
mae: 0.03660089509890498
r2: 0.6952267574288595
pearson: 0.8393534173932654

=== Experiment 1741 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006720051199833763
rmse: 0.08197591841409135
mae: 0.039604855147463355
r2: 0.6969958491202176
pearson: 0.8384841069084175

=== Experiment 1962 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068320818187601945
rmse: 0.08265640821352084
mae: 0.03083054227511247
r2: 0.6919444378212708
pearson: 0.8461381078349849

=== Experiment 1742 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007619857564421589
rmse: 0.08729179551608267
mae: 0.03686843566630871
r2: 0.6564239761760199
pearson: 0.8199548358701031

=== Experiment 1904 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005866500127192123
rmse: 0.07659308145774084
mae: 0.03076633093273056
r2: 0.7354820913090725
pearson: 0.8605873103414738

=== Experiment 1916 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006898278678590156
rmse: 0.08305587684559207
mae: 0.03746887088694689
r2: 0.6889596505470041
pearson: 0.831260421531705

=== Experiment 1721 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006162743823991773
rmse: 0.07850314531273109
mae: 0.03507444781003985
r2: 0.7221245934071985
pearson: 0.8505619070145495

=== Experiment 1837 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006440790792890662
rmse: 0.08025453752212806
mae: 0.03603247303147418
r2: 0.7095875779573773
pearson: 0.8445876512882057

=== Experiment 1695 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0070161356246786
rmse: 0.08376237594934016
mae: 0.034359958221033556
r2: 0.6836455327206851
pearson: 0.8280027143442279

=== Experiment 1897 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0060977439803381935
rmse: 0.07808805273752313
mae: 0.028801036644400868
r2: 0.7250554077489193
pearson: 0.8537797946931498

=== Experiment 1983 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007784417169666464
rmse: 0.08822934415298837
mae: 0.03815643838402809
r2: 0.6490040560037496
pearson: 0.8149618585511814

=== Experiment 1706 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007135971253257013
rmse: 0.08447467817788365
mae: 0.039006289897761365
r2: 0.678242196971778
pearson: 0.8253492707088639

=== Experiment 1679 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006664809623087519
rmse: 0.08163828527772689
mae: 0.036748321619935145
r2: 0.6994866675020284
pearson: 0.8380612792373742

=== Experiment 1504 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006362542190074064
rmse: 0.07976554513117844
mae: 0.03427648043140334
r2: 0.7131157730185338
pearson: 0.8464129247353442

=== Experiment 1890 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006327529853295393
rmse: 0.07954577206423603
mae: 0.02951975467629671
r2: 0.7146944638737763
pearson: 0.8618207118966384

=== Experiment 1843 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006409500793634762
rmse: 0.08005935793918637
mae: 0.03588751703396388
r2: 0.710998430252044
pearson: 0.8434101776135333

=== Experiment 1788 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006008892778717117
rmse: 0.07751704831014347
mae: 0.029854108588222763
r2: 0.7290616693236094
pearson: 0.8623953967703376

=== Experiment 1903 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006020614627893271
rmse: 0.0775926196741241
mae: 0.031999910188558016
r2: 0.7285331363034367
pearson: 0.8535760986916259

=== Experiment 1907 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006174937992196619
rmse: 0.0785807736803133
mae: 0.033870657790217376
r2: 0.7215747637299057
pearson: 0.8495855726804317

=== Experiment 1800 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00671897266239604
rmse: 0.08196933977040465
mae: 0.041006460325845505
r2: 0.697044479898584
pearson: 0.8523947875255277

=== Experiment 1900 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005962036623670936
rmse: 0.07721422552658892
mae: 0.030038874530185007
r2: 0.7311743927316706
pearson: 0.8566473125789741

=== Experiment 1826 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005767457375277114
rmse: 0.07594377772587504
mae: 0.034146212538655775
r2: 0.7399478854008715
pearson: 0.8634043109891032

=== Experiment 1618 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007097415123598932
rmse: 0.0842461579159485
mae: 0.04613083200588302
r2: 0.6799806758882105
pearson: 0.8337391096107152

=== Experiment 1767 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006090595797547842
rmse: 0.07804226930034673
mae: 0.03020154681556576
r2: 0.7253777161647805
pearson: 0.8520095523064196

=== Experiment 1580 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.012275669902487591
rmse: 0.11079562221715979
mae: 0.05107062542790238
r2: 0.4464954470323431
pearson: 0.6795057479449835

=== Experiment 1731 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0055332756648585
rmse: 0.07438599105247237
mae: 0.02862589544479941
r2: 0.7505070356523937
pearson: 0.8663610432480624

=== Experiment 1805 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006127060447024171
rmse: 0.0782755418187838
mae: 0.03165193163233677
r2: 0.7237335411692124
pearson: 0.8510195717667853

=== Experiment 1986 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006200504710387864
rmse: 0.07874328358906468
mae: 0.02997247596902467
r2: 0.7204219716594358
pearson: 0.8492388366153452

=== Experiment 1887 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006519443699882759
rmse: 0.08074307214791099
mae: 0.033834503063418314
r2: 0.7060411529988955
pearson: 0.8416145011064408

=== Experiment 1674 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007423443720023041
rmse: 0.08615940877247848
mae: 0.03662675158931806
r2: 0.6652801899716063
pearson: 0.8174367899734232

=== Experiment 1519 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0061647224924339114
rmse: 0.0785157467800817
mae: 0.03336112386116396
r2: 0.7220353761180209
pearson: 0.850583277712029

=== Experiment 1628 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007298642532551302
rmse: 0.08543209310646264
mae: 0.04940186732770035
r2: 0.6709074205855039
pearson: 0.8311091209045325

=== Experiment 1512 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006939764263401095
rmse: 0.08330524751419383
mae: 0.03593614117195526
r2: 0.6870890837870811
pearson: 0.831564247293477

=== Experiment 1866 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007014877606028508
rmse: 0.08375486616327739
mae: 0.03751873785498029
r2: 0.6837022562279212
pearson: 0.8280241847949382

=== Experiment 1768 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006936728806420527
rmse: 0.08328702663933037
mae: 0.035999231801674365
r2: 0.6872259512063281
pearson: 0.83069192997859

=== Experiment 1591 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00595879115364324
rmse: 0.07719320665475195
mae: 0.02988746167388535
r2: 0.7313207295467787
pearson: 0.8587095688190576

=== Experiment 1844 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006651735260803938
rmse: 0.08155817102414654
mae: 0.03783194285552745
r2: 0.700076184742928
pearson: 0.8395173633542052

=== Experiment 1922 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006738978895292119
rmse: 0.08209128391791737
mae: 0.042934679344857674
r2: 0.6961424076627162
pearson: 0.8407461279582137

=== Experiment 1518 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006413038667027339
rmse: 0.08008145020557095
mae: 0.03417645431282803
r2: 0.7108389090979097
pearson: 0.8471608917552705

=== Experiment 1722 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005948528233167678
rmse: 0.07712670246527903
mae: 0.02998815523367342
r2: 0.7317834801139652
pearson: 0.8571120931790592

=== Experiment 1951 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006228958467264711
rmse: 0.07892375097057103
mae: 0.035676136587902786
r2: 0.7191390042853149
pearson: 0.8527816667252948

=== Experiment 1878 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006864259594296824
rmse: 0.08285082736036391
mae: 0.03634203481918117
r2: 0.6904935560848464
pearson: 0.8322922481552198

=== Experiment 1948 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005452083532976811
rmse: 0.07383822541865975
mae: 0.029528862045510904
r2: 0.7541679531435491
pearson: 0.8685654083889305

=== Experiment 1968 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0058971846707294795
rmse: 0.07679312905937274
mae: 0.03183358672939161
r2: 0.7340985387462735
pearson: 0.8568460275088245

=== Experiment 1708 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005593092468165132
rmse: 0.07478698060601946
mae: 0.029870748716562548
r2: 0.7478099223186863
pearson: 0.866253458869324

=== Experiment 1932 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00692536804366121
rmse: 0.08321879621612661
mae: 0.0448901188669192
r2: 0.6877382029988917
pearson: 0.8422986849271256

=== Experiment 1789 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006970619733308139
rmse: 0.08349023735328663
mae: 0.03917695753662207
r2: 0.6856978242295033
pearson: 0.8332750059983915

=== Experiment 1848 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007605268162178735
rmse: 0.08720818861883747
mae: 0.05048487985280967
r2: 0.6570818059018633
pearson: 0.8280740434760627

=== Experiment 1958 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0070881867490362885
rmse: 0.0841913698014012
mae: 0.04522165462387639
r2: 0.6803967792355218
pearson: 0.838054639646618

=== Experiment 1748 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006865805643341044
rmse: 0.08286015715252441
mae: 0.03322346088972561
r2: 0.6904238454139109
pearson: 0.8311828026395033

=== Experiment 1507 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00694868430641868
rmse: 0.08335876862345484
mae: 0.0373435699142897
r2: 0.6866868829734294
pearson: 0.8297271996396893

=== Experiment 1666 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006903739576136153
rmse: 0.08308874518330478
mae: 0.03650659567756655
r2: 0.68871342108018
pearson: 0.830749709032624

=== Experiment 1928 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00674863113414704
rmse: 0.08215005255109117
mae: 0.03287815893674555
r2: 0.6957071924610229
pearson: 0.8526188369083668

=== Experiment 1783 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007772837571660678
rmse: 0.08816369758387337
mae: 0.04250295471148328
r2: 0.6495261749812595
pearson: 0.8072696502601795

=== Experiment 1823 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006765591757642175
rmse: 0.08225321730876048
mae: 0.037244526220236454
r2: 0.6949424454125686
pearson: 0.8342558372861192

=== Experiment 1667 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005861243861698969
rmse: 0.07655876084223782
mae: 0.030796344627825687
r2: 0.7357190940066995
pearson: 0.8578323804841115

=== Experiment 1971 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006505563575941784
rmse: 0.08065707393615133
mae: 0.034996452344093334
r2: 0.7066670016782841
pearson: 0.8413908553372409

=== Experiment 1778 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006228337924282764
rmse: 0.07891981959104293
mae: 0.03601017351026988
r2: 0.7191669842952491
pearson: 0.8540964794072603

=== Experiment 1867 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0061969132088732235
rmse: 0.07872047515655138
mae: 0.03132014835250025
r2: 0.7205839108819903
pearson: 0.8491298492335089

=== Experiment 1874 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0066169855897071075
rmse: 0.08134485595111167
mae: 0.03721828610601331
r2: 0.7016430321181844
pearson: 0.8441691433573483

=== Experiment 1656 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005975142475466506
rmse: 0.07729904576038767
mae: 0.03448358495916718
r2: 0.7305834556425348
pearson: 0.8601624588204048

=== Experiment 1939 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007634057597462308
rmse: 0.08737309424223402
mae: 0.040116096034704264
r2: 0.6557837029361264
pearson: 0.8113509030042333

=== Experiment 1541 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007121372646134518
rmse: 0.08438822575534172
mae: 0.039679733987328644
r2: 0.678900441741593
pearson: 0.8357081396492835

=== Experiment 1981 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0068549663789815194
rmse: 0.08279472434268695
mae: 0.03842433102421364
r2: 0.6909125830731566
pearson: 0.8316195333630977

=== Experiment 1697 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007137562625071688
rmse: 0.08448409687670035
mae: 0.03436195700495199
r2: 0.6781704427170707
pearson: 0.8244991479332144

=== Experiment 1784 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007069129486020801
rmse: 0.08407811538100031
mae: 0.04484019465528968
r2: 0.6812560628371471
pearson: 0.8406914703888808

=== Experiment 1820 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006971690434849297
rmse: 0.08349664924324388
mae: 0.03639140933936316
r2: 0.6856495467682642
pearson: 0.828088660425215

=== Experiment 1949 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006149557958723125
rmse: 0.07841911730390189
mae: 0.03197834686858076
r2: 0.7227191382686187
pearson: 0.8503542387565278

=== Experiment 1711 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00631418503837208
rmse: 0.07946184643193285
mae: 0.03677473589059387
r2: 0.7152961757051661
pearson: 0.8477232121399488

=== Experiment 1825 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00621543051827079
rmse: 0.07883800173945804
mae: 0.03318531374142048
r2: 0.7197489735513452
pearson: 0.8484556829596596

=== Experiment 1571 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0060687737908440105
rmse: 0.07790233495116825
mae: 0.033744591269644415
r2: 0.7263616608424566
pearson: 0.8530526467976713

=== Experiment 1824 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006906781161263579
rmse: 0.08310704639958984
mae: 0.04061178063836644
r2: 0.6885762773454882
pearson: 0.8318910103998881

=== Experiment 1564 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006247986914743688
rmse: 0.07904420860976273
mae: 0.03490417010854315
r2: 0.718281019963548
pearson: 0.8506787690553662

=== Experiment 1668 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006152759305336021
rmse: 0.07843952642218094
mae: 0.031210963129755364
r2: 0.7225747909588636
pearson: 0.8512551363620312

=== Experiment 1542 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006285571181542224
rmse: 0.0792815942167047
mae: 0.03325914681834779
r2: 0.716586361915703
pearson: 0.850335031418092

=== Experiment 1715 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00603900942809621
rmse: 0.07771106374317759
mae: 0.03289681968265292
r2: 0.7277037228584569
pearson: 0.8550946139479672

=== Experiment 1908 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059965549626489825
rmse: 0.07743742611069264
mae: 0.02959937412110257
r2: 0.7296179760198331
pearson: 0.8580064828575679

=== Experiment 1526 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006787214563608436
rmse: 0.08238455294294214
mae: 0.042729412033477424
r2: 0.6939674826084746
pearson: 0.8383815571262361

=== Experiment 1743 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007871194375300952
rmse: 0.08871975188931128
mae: 0.039106992415448884
r2: 0.6450913099952598
pearson: 0.8106922848184218

=== Experiment 1523 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061906827512927315
rmse: 0.07868089190707443
mae: 0.02935578825304506
r2: 0.7208648394720605
pearson: 0.8547178868371774

=== Experiment 1659 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007530673742409277
rmse: 0.0867794546100013
mae: 0.04112420905901538
r2: 0.6604452354577532
pearson: 0.8128885326914959

=== Experiment 1611 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006929084924510752
rmse: 0.08324112519969172
mae: 0.03824564451046476
r2: 0.6875706104773103
pearson: 0.8299352522737746

=== Experiment 1757 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006151762107358057
rmse: 0.07843316968832802
mae: 0.037094561907989605
r2: 0.7226197541767896
pearson: 0.8524960690165502

=== Experiment 1749 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006634858370079144
rmse: 0.08145463995426623
mae: 0.03623040618367319
r2: 0.7008371563176214
pearson: 0.8509884348104402

=== Experiment 1977 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00656296776323634
rmse: 0.08101214577602756
mae: 0.04347515542286955
r2: 0.7040786721386876
pearson: 0.8473548084357468

=== Experiment 1661 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006432085018710452
rmse: 0.08020028066478603
mae: 0.03659153264088682
r2: 0.7099801174834588
pearson: 0.857715717381372

=== Experiment 1946 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006490284019338529
rmse: 0.08056229899486812
mae: 0.03895934961826795
r2: 0.7073559501604778
pearson: 0.8433133101430179

=== Experiment 1786 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005867533770031551
rmse: 0.07659982878591538
mae: 0.03448466250701003
r2: 0.7354354848083837
pearson: 0.8586326600376146

=== Experiment 1969 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006816789642550761
rmse: 0.0825638519120248
mae: 0.03654879573511963
r2: 0.6926339553159528
pearson: 0.8360759882993988

=== Experiment 1629 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005692497403145957
rmse: 0.07544864083034204
mae: 0.029968137383641638
r2: 0.7433277975518586
pearson: 0.8626716992096065

=== Experiment 1799 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006304135229451686
rmse: 0.07939858455571916
mae: 0.02996501816637722
r2: 0.7157493171661276
pearson: 0.8476277588686668

=== Experiment 1809 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007859263355849234
rmse: 0.08865248646174136
mae: 0.038450699048968924
r2: 0.6456292744110945
pearson: 0.8135070652737582

=== Experiment 1540 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006418363052411252
rmse: 0.08011468687083069
mae: 0.03352227646494975
r2: 0.7105988349043904
pearson: 0.8437975594648596

=== Experiment 1797 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0075734122863814314
rmse: 0.08702535427323138
mae: 0.03773794576706319
r2: 0.6585181733207205
pearson: 0.8261498404984593

=== Experiment 1649 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006706484485253356
rmse: 0.081893128437332
mae: 0.035671324241182044
r2: 0.6976075663094804
pearson: 0.8489247797271644

=== Experiment 1774 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007021660933881463
rmse: 0.08379535150520859
mae: 0.034419822588444365
r2: 0.6833963989606024
pearson: 0.8331422698918503

=== Experiment 1974 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065403838836817635
rmse: 0.08087263989558993
mae: 0.032648366161816954
r2: 0.7050969693278779
pearson: 0.8398603940822725

=== Experiment 1520 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008649697087061502
rmse: 0.09300374770438824
mae: 0.06073407123636431
r2: 0.6099889653672234
pearson: 0.8187379819285819

=== Experiment 1605 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006729002708427433
rmse: 0.08203049864792626
mae: 0.03993312569281868
r2: 0.6965922295375893
pearson: 0.8358515718509807

=== Experiment 1738 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006077164316974586
rmse: 0.07795616920407637
mae: 0.03057274316650094
r2: 0.7259833357121819
pearson: 0.8570077248807314

=== Experiment 1513 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0060844507146977245
rmse: 0.07800288914327293
mae: 0.03325081150420515
r2: 0.7256547952458344
pearson: 0.8549792957837329

=== Experiment 1972 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007257802338022803
rmse: 0.08519273641586354
mae: 0.03819912558127271
r2: 0.6727488869816498
pearson: 0.8232845537799679

=== Experiment 1604 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005557927062442879
rmse: 0.07455150610445693
mae: 0.028922789129743803
r2: 0.7493955149852238
pearson: 0.8660394308800851

=== Experiment 1506 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00611972631676871
rmse: 0.07822867963073843
mae: 0.03566221212840979
r2: 0.7240642338744334
pearson: 0.8525991703294559

=== Experiment 1648 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007892983257408531
rmse: 0.08884246314352462
mae: 0.03771564214337618
r2: 0.6441088588910493
pearson: 0.8126887238338378

=== Experiment 1918 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006866408593542139
rmse: 0.08286379543287972
mae: 0.03922151510264848
r2: 0.6903966586547221
pearson: 0.8359050400819112

=== Experiment 1647 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00671473931788985
rmse: 0.08194351297015433
mae: 0.0394032829325307
r2: 0.6972353595391305
pearson: 0.8361187153803755

=== Experiment 1745 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006725262715787968
rmse: 0.08200769912506976
mae: 0.038600950007741636
r2: 0.6967608641596059
pearson: 0.834722687875702

=== Experiment 1680 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007116837771365908
rmse: 0.08436135235619394
mae: 0.04367876646846551
r2: 0.6791049172489585
pearson: 0.8268678711403569

=== Experiment 1547 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006387729028561181
rmse: 0.07992326963132315
mae: 0.029346844160303816
r2: 0.7119801095567251
pearson: 0.8450833622710011

=== Experiment 1550 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00555216583059622
rmse: 0.07451285681408423
mae: 0.03016015915888005
r2: 0.7496552863934776
pearson: 0.8712029966574811

=== Experiment 1943 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006775405656940239
rmse: 0.08231285231930818
mae: 0.03485381140209821
r2: 0.6944999410126462
pearson: 0.8334797040482765

=== Experiment 1989 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006388575723981874
rmse: 0.07992856638262615
mae: 0.036382300464368036
r2: 0.7119419324328653
pearson: 0.8463310034570426

=== Experiment 1865 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007168308897921312
rmse: 0.08466586619128934
mae: 0.045059397314675474
r2: 0.6767841068067508
pearson: 0.8315559052467048

=== Experiment 1859 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006341926104762008
rmse: 0.0796362110146007
mae: 0.030671070292883015
r2: 0.7140453432314214
pearson: 0.8465719860021746

=== Experiment 1931 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00739727450491527
rmse: 0.0860074095931
mae: 0.045500338005441704
r2: 0.6664601483628635
pearson: 0.8264528739554003

=== Experiment 1790 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006288138850349996
rmse: 0.079297785910768
mae: 0.0312047069427759
r2: 0.7164705868592779
pearson: 0.8479912949990254

=== Experiment 1935 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006247363809988851
rmse: 0.07904026701617885
mae: 0.031377789209311294
r2: 0.7183091154826944
pearson: 0.8548499785066997

=== Experiment 1818 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006298439387340383
rmse: 0.0793627077873505
mae: 0.03713512639646868
r2: 0.7160061401799943
pearson: 0.8477379880764783

=== Experiment 1926 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006264444522280337
rmse: 0.07914824396207623
mae: 0.032961778658575684
r2: 0.7175389536832675
pearson: 0.8515783380782486

=== Experiment 1728 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006069339841769547
rmse: 0.07790596794706775
mae: 0.029804560096657183
r2: 0.7263361378553621
pearson: 0.8526703960970524

=== Experiment 1747 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005943303100616526
rmse: 0.07709282132998199
mae: 0.03182787654535836
r2: 0.7320190790409398
pearson: 0.8562230488640837

=== Experiment 1701 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006370764910186319
rmse: 0.07981707154604407
mae: 0.03225925754131817
r2: 0.7127450141877683
pearson: 0.8585771737868447

=== Experiment 1791 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006685003594172317
rmse: 0.08176187127367082
mae: 0.04049311511541667
r2: 0.6985761302338612
pearson: 0.8376981405496042

=== Experiment 1819 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006710023846084829
rmse: 0.08191473521952462
mae: 0.03961352392738582
r2: 0.697447978087381
pearson: 0.8360794417844637

=== Experiment 1505 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007022510129121379
rmse: 0.08380041843046715
mae: 0.042258749615665066
r2: 0.6833581091209942
pearson: 0.8327396843958037

=== Experiment 1608 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005626506291592561
rmse: 0.07501004127176948
mae: 0.029734698577420286
r2: 0.7463033077268918
pearson: 0.8639421390256774

=== Experiment 1991 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006542145819750062
rmse: 0.08088353243862476
mae: 0.033436670425048796
r2: 0.7050175244060458
pearson: 0.8408517334553944

=== Experiment 1875 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00595938154982675
rmse: 0.0771970307060236
mae: 0.03151463401993674
r2: 0.7312941088427174
pearson: 0.8562153696138851

=== Experiment 1756 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006872817511427111
rmse: 0.08290245781294492
mae: 0.03306992292979624
r2: 0.690107683368069
pearson: 0.8321506146530694

=== Experiment 1961 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00697710035786813
rmse: 0.0835290390096051
mae: 0.03687501307213165
r2: 0.6854056157204345
pearson: 0.842181162521875

=== Experiment 1735 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0062403731075029376
rmse: 0.07899603222632727
mae: 0.0320818015222418
r2: 0.7186243231809406
pearson: 0.8509433497901314

=== Experiment 1669 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006966603813093649
rmse: 0.08346618364998874
mae: 0.0330675932970671
r2: 0.6858789003044332
pearson: 0.8328600852631566

=== Experiment 1838 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061152399725528115
rmse: 0.07819999982450647
mae: 0.031977400494439694
r2: 0.7242665211605372
pearson: 0.8517891730486425

=== Experiment 1763 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005845776758706754
rmse: 0.07645767952734868
mae: 0.032023927872037866
r2: 0.7364164988730257
pearson: 0.8583354927227937

=== Experiment 1847 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007029877855908155
rmse: 0.0838443668704592
mae: 0.03455828876269765
r2: 0.683025901562963
pearson: 0.8264558026013877

=== Experiment 1880 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067011015663580266
rmse: 0.08186025632966236
mae: 0.035536857042302895
r2: 0.6978502797532669
pearson: 0.8355911808951322

=== Experiment 1942 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006335928068767047
rmse: 0.07959854313218959
mae: 0.036110892701094334
r2: 0.7143157920344967
pearson: 0.8464062705171448

=== Experiment 1959 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006750188096443435
rmse: 0.08215952833630093
mae: 0.04087074566536422
r2: 0.69563698971635
pearson: 0.8360530030896732

=== Experiment 1842 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005915784125073745
rmse: 0.07691413475476237
mae: 0.03065507116432595
r2: 0.7332598975361364
pearson: 0.8595224747757599

=== Experiment 1938 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006457682551991072
rmse: 0.08035970726670844
mae: 0.031653947654986296
r2: 0.7088259359741715
pearson: 0.8580931166084811

=== Experiment 1917 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006432884312720341
rmse: 0.08020526362228567
mae: 0.033136014474415804
r2: 0.709944077668349
pearson: 0.8452614212461352

=== Experiment 1770 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006354042256782776
rmse: 0.07971224659224438
mae: 0.030581634450594353
r2: 0.7134990312695313
pearson: 0.8490986815309193

=== Experiment 1973 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006663630755402888
rmse: 0.08163106489200596
mae: 0.036247491894487185
r2: 0.699539822127069
pearson: 0.8390996668256477

=== Experiment 1915 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007215949091750477
rmse: 0.08494674267887191
mae: 0.034729971529936475
r2: 0.6746360314350464
pearson: 0.8217647270145433

=== Experiment 1543 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006077921207422649
rmse: 0.0779610236427322
mae: 0.03492811450564384
r2: 0.7259492078550116
pearson: 0.8524675764946474

=== Experiment 1860 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006119012006101856
rmse: 0.07822411396814831
mae: 0.030331113646433917
r2: 0.7240964418280098
pearson: 0.862152940418446

=== Experiment 1691 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006244536147720409
rmse: 0.07902237751245156
mae: 0.03443538533500991
r2: 0.718436613529829
pearson: 0.8580958292295371

=== Experiment 1646 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005885896025935552
rmse: 0.07671959349433202
mae: 0.0344429684752504
r2: 0.7346075387715192
pearson: 0.859570427865188

=== Experiment 1965 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005597986640952161
rmse: 0.07481969420515003
mae: 0.02909249123619038
r2: 0.7475892462218092
pearson: 0.8712416966240266

=== Experiment 1503 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065241194970067405
rmse: 0.08077202174643607
mae: 0.031509942173989225
r2: 0.7058303233645506
pearson: 0.8447151815119679

=== Experiment 1876 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0058875779715652716
rmse: 0.07673055435460682
mae: 0.034949884243241136
r2: 0.7345317005833559
pearson: 0.8618303121617722

=== Experiment 1956 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006070753014942721
rmse: 0.07791503715549855
mae: 0.02993299348760659
r2: 0.7262724184989688
pearson: 0.8535808871415124

=== Experiment 1623 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005935995295840732
rmse: 0.0770454106085543
mae: 0.02953108621585845
r2: 0.7323485847418696
pearson: 0.8599090430735167

=== Experiment 1739 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005601686119771879
rmse: 0.07484441274919511
mae: 0.028578397411638704
r2: 0.7474224383500934
pearson: 0.8674529596923053

=== Experiment 2012 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006871912890747211
rmse: 0.0828970016993812
mae: 0.0396434449248814
r2: 0.6901484723163718
pearson: 0.8312968276214215

=== Experiment 2021 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006844958869176667
rmse: 0.08273426659599194
mae: 0.0400379293586085
r2: 0.6913638172856155
pearson: 0.8325184088495327

=== Experiment 2057 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006743755599552779
rmse: 0.08212037262185784
mae: 0.037358470833886215
r2: 0.6959270281699912
pearson: 0.8354328338267557

=== Experiment 2005 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006736763792143203
rmse: 0.08207779110175421
mae: 0.03711631026941611
r2: 0.6962422856887592
pearson: 0.8369130448807591

=== Experiment 2029 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007268717285015867
rmse: 0.08525677266361815
mae: 0.04188609171256457
r2: 0.6722567368257679
pearson: 0.8243391572714096

=== Experiment 2000 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005390086301489838
rmse: 0.0734172071212862
mae: 0.0266319015863882
r2: 0.7569633810242283
pearson: 0.8735559618214463

=== Experiment 2071 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006593803524799717
rmse: 0.08120223842234718
mae: 0.036613220214272595
r2: 0.7026883012216516
pearson: 0.8391123588633999

=== Experiment 2025 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006599063466365154
rmse: 0.08123461987579651
mae: 0.036883819704976936
r2: 0.7024511327715433
pearson: 0.8391767215384471

=== Experiment 2036 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007240869651453558
rmse: 0.08509329968601263
mae: 0.04532807742508404
r2: 0.6735123743664118
pearson: 0.8255101852684196

=== Experiment 2009 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0066546155456621075
rmse: 0.08157582696891345
mae: 0.04523081795884721
r2: 0.6999463139663835
pearson: 0.8446973968972495

=== Experiment 2015 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006889698148964126
rmse: 0.08300420561010223
mae: 0.043040672678972665
r2: 0.689346542851262
pearson: 0.8350337331888347

=== Experiment 2026 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006460653008258281
rmse: 0.08037818739097244
mae: 0.03900261470564679
r2: 0.7086919994084808
pearson: 0.8453338782925687

=== Experiment 2080 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006642401980207535
rmse: 0.08150093238857783
mae: 0.03896241370648096
r2: 0.7004970182571291
pearson: 0.8378553577644593

=== Experiment 2022 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071591179000575055
rmse: 0.08461157072207977
mae: 0.03772432088368944
r2: 0.6771985248551622
pearson: 0.8387504057274333

=== Experiment 2001 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007467383460472186
rmse: 0.08641402351743718
mae: 0.04056326184186188
r2: 0.6632989664141133
pearson: 0.8156216028109421

=== Experiment 2099 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006625070832787159
rmse: 0.08139453810168812
mae: 0.04184907496659954
r2: 0.7012784720662926
pearson: 0.84033835550594

=== Experiment 2006 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006324932030151529
rmse: 0.07952944127901018
mae: 0.03326824841528364
r2: 0.7148115985759437
pearson: 0.8511327676669455

=== Experiment 2064 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006505747527963747
rmse: 0.08065821426218006
mae: 0.04125593944198607
r2: 0.7066587073625774
pearson: 0.844064505593796

=== Experiment 2085 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007148258148802229
rmse: 0.08454737221701351
mae: 0.03380878022533496
r2: 0.6776881862595205
pearson: 0.8294096978772124

=== Experiment 2089 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006038877519849661
rmse: 0.07771021502897583
mae: 0.034122823689637226
r2: 0.7277096705432367
pearson: 0.8533546009355212

=== Experiment 2035 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006705355226235401
rmse: 0.08188623343539132
mae: 0.03734329400162384
r2: 0.697658484101575
pearson: 0.8356836668542585

=== Experiment 2101 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006470452079838747
rmse: 0.08043912033232802
mae: 0.04219613697035294
r2: 0.708250163583819
pearson: 0.8456645236288111

=== Experiment 2063 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007610382048570104
rmse: 0.08723750368144485
mae: 0.0415720556896926
r2: 0.6568512230152699
pearson: 0.8131465779116234

=== Experiment 2088 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005718458230266195
rmse: 0.07562048816469115
mae: 0.031532081313762325
r2: 0.7421572352832408
pearson: 0.8636052443678911

=== Experiment 2048 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006822756708734269
rmse: 0.08259998007708155
mae: 0.044856116428513365
r2: 0.6923649029280452
pearson: 0.842037431477025

=== Experiment 2100 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006438914253947746
rmse: 0.08024284550006777
mae: 0.03552132940313965
r2: 0.7096721902723855
pearson: 0.8436970691310469

=== Experiment 2061 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006665717254245186
rmse: 0.08164384394579415
mae: 0.03036847877344677
r2: 0.6994457428126082
pearson: 0.8398222895141283

=== Experiment 2044 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006365586053700424
rmse: 0.0797846229150732
mae: 0.034573574947045556
r2: 0.7129785265473282
pearson: 0.847128565151378

=== Experiment 2049 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005839928884841745
rmse: 0.07641942740456609
mae: 0.030053826309776762
r2: 0.7366801769317529
pearson: 0.8591885107764485

=== Experiment 2028 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005282332534837545
rmse: 0.07267965695321865
mae: 0.029004275494650705
r2: 0.7618219509365207
pearson: 0.8763528156976325

=== Experiment 2110 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006446831739335611
rmse: 0.08029216486890618
mae: 0.03798218979298182
r2: 0.7093151943409985
pearson: 0.8433673074374592

=== Experiment 2008 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006020608522271027
rmse: 0.07759258033002271
mae: 0.028755981580959784
r2: 0.7285334116032566
pearson: 0.8578319890500506

=== Experiment 2076 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006405830816800645
rmse: 0.08003643430838636
mae: 0.03091172575178784
r2: 0.7111639078922141
pearson: 0.8466790916886613

=== Experiment 2139 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064468690057076375
rmse: 0.08029239693587206
mae: 0.038927723107864666
r2: 0.7093135140166859
pearson: 0.8435851977074026

=== Experiment 2137 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007866034400797367
rmse: 0.08869066693174298
mae: 0.0473022248749827
r2: 0.6453239709745477
pearson: 0.8089115567444224

=== Experiment 2084 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006786522977472255
rmse: 0.0823803555313538
mae: 0.03958828878923145
r2: 0.6939986659229647
pearson: 0.834353328861566

=== Experiment 2075 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0065448494561447
rmse: 0.08090024385713988
mae: 0.03561813248428577
r2: 0.7048956186309729
pearson: 0.8423126550477817

=== Experiment 2056 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00620688288255241
rmse: 0.07878377296469376
mae: 0.036410186263774506
r2: 0.7201343826837847
pearson: 0.8526958731983658

=== Experiment 2073 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006886937031642612
rmse: 0.08298757154925436
mae: 0.038280806762993974
r2: 0.6894710404160282
pearson: 0.831783226018923

=== Experiment 2146 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062286530288027725
rmse: 0.07892181592438667
mae: 0.036950883179792754
r2: 0.7191527763711298
pearson: 0.850230339272888

=== Experiment 2098 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007396897337475344
rmse: 0.08600521692011098
mae: 0.043690697014284297
r2: 0.6664771546767256
pearson: 0.8199881594069296

=== Experiment 2093 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007709413379490358
rmse: 0.0878032651983419
mae: 0.05085484643174527
r2: 0.6523859438910977
pearson: 0.8272109164539808

=== Experiment 2042 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064370704496209815
rmse: 0.08023135577578745
mae: 0.037851875576030654
r2: 0.7097553265979588
pearson: 0.8433414374750301

=== Experiment 2108 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006339305553566535
rmse: 0.07961975605065953
mae: 0.03942306002973904
r2: 0.7141635027314325
pearson: 0.8466327553318619

=== Experiment 2176 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005969579262233024
rmse: 0.07726305237455368
mae: 0.032413883523920786
r2: 0.7308342984786751
pearson: 0.8575943400264061

=== Experiment 2143 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007151008852422599
rmse: 0.08456363788545641
mae: 0.03981500033947835
r2: 0.6775641582439553
pearson: 0.8257929642621147

=== Experiment 2111 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007271900624906416
rmse: 0.08527543975205532
mae: 0.04143163525205049
r2: 0.6721132014311981
pearson: 0.828745830009814

=== Experiment 2105 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006883832248797471
rmse: 0.08296886312826922
mae: 0.03745410285027963
r2: 0.6896110337079961
pearson: 0.8309188571656869

=== Experiment 2192 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007409434486413318
rmse: 0.08607807204168387
mae: 0.037556442694498156
r2: 0.6659118601491336
pearson: 0.8165555854815171

=== Experiment 2014 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005978388156092414
rmse: 0.07732003722252347
mae: 0.03431899522613042
r2: 0.730437109331646
pearson: 0.8548059100623515

=== Experiment 2149 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005805001167937057
rmse: 0.07619055825978083
mae: 0.03064754140542351
r2: 0.7382550523141214
pearson: 0.8614990096705839

=== Experiment 2043 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006016328143154217
rmse: 0.07756499302619847
mae: 0.03082303423377212
r2: 0.7287264120136945
pearson: 0.8601578430116401

=== Experiment 2126 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0060520883300788945
rmse: 0.07779516906645871
mae: 0.03289133011160753
r2: 0.7271140009245227
pearson: 0.8530311788395597

=== Experiment 2116 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005905049673059797
rmse: 0.07684432102022762
mae: 0.03259564001545402
r2: 0.7337439092528517
pearson: 0.8569269700923824

=== Experiment 2122 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007077621080659584
rmse: 0.08412859847079104
mae: 0.04557075463442117
r2: 0.6808731805723212
pearson: 0.8330606694979249

=== Experiment 2115 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0056824733188583654
rmse: 0.07538218170667632
mae: 0.028312860486770484
r2: 0.7437797791004492
pearson: 0.8639635210275873

=== Experiment 2212 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006612401156474374
rmse: 0.08131667206959699
mae: 0.03805322040158837
r2: 0.7018497421948848
pearson: 0.8386479085752682

=== Experiment 2145 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006390631916875969
rmse: 0.07994142803875828
mae: 0.03493956314299978
r2: 0.7118492196002701
pearson: 0.8446873646764418

=== Experiment 2217 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006914960330850334
rmse: 0.08315624048049751
mae: 0.03937918940246283
r2: 0.6882074821887494
pearson: 0.8308625019405683

=== Experiment 2154 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006747680877571606
rmse: 0.08214426868364953
mae: 0.037738300227711394
r2: 0.6957500391117692
pearson: 0.8411934314819219

=== Experiment 2230 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0075038938822848825
rmse: 0.0866250188010651
mae: 0.03918000685741812
r2: 0.6616527275640443
pearson: 0.8197550057385786

=== Experiment 2193 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007801541405162698
rmse: 0.0883263347205277
mae: 0.04177362236336019
r2: 0.648231931762177
pearson: 0.810531151462575

=== Experiment 2138 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007638294078979365
rmse: 0.08739733450729127
mae: 0.037283959639131226
r2: 0.6555926818491404
pearson: 0.8103366296546202

=== Experiment 2092 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007050594540921383
rmse: 0.0839678184837583
mae: 0.04385903361948414
r2: 0.6820917953538268
pearson: 0.8302671672912112

=== Experiment 2054 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063021624816497895
rmse: 0.07938616051711904
mae: 0.03844887488369554
r2: 0.7158382674962425
pearson: 0.8474903227994967

=== Experiment 2166 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007953914838009233
rmse: 0.08918472312010187
mae: 0.050302006987048106
r2: 0.6413614807398031
pearson: 0.815223000793446

=== Experiment 2197 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071116767251581505
rmse: 0.08433075788321928
mae: 0.04423405059109153
r2: 0.6793376265509163
pearson: 0.8288886962447842

=== Experiment 2183 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059559431183941565
rmse: 0.07717475700249503
mae: 0.030725262733044763
r2: 0.7314491462026437
pearson: 0.8577980164275509

=== Experiment 2010 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067677658643298345
rmse: 0.0822664321842745
mae: 0.03980416464560142
r2: 0.6948444158989195
pearson: 0.836454065123196

=== Experiment 2107 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007655455094343304
rmse: 0.08749545756405475
mae: 0.036021134837727654
r2: 0.654818899219521
pearson: 0.8097556204107106

=== Experiment 2233 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064755336090198505
rmse: 0.08047070031396428
mae: 0.036263414757705593
r2: 0.708021039669595
pearson: 0.8416640342818221

=== Experiment 2052 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006359089256259385
rmse: 0.07974389792491576
mae: 0.03969601618390314
r2: 0.7132714642844853
pearson: 0.8481237565099479

=== Experiment 2232 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00617090876981296
rmse: 0.07855513203994352
mae: 0.036040928942918445
r2: 0.7217564395937885
pearson: 0.8499532544657786

=== Experiment 2160 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007834861595316446
rmse: 0.08851475354604138
mae: 0.04268084967743722
r2: 0.6467295390534815
pearson: 0.8063361655253117

=== Experiment 2119 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006617460227812159
rmse: 0.08134777334268074
mae: 0.043178480706104624
r2: 0.7016216308949318
pearson: 0.8489960141351082

=== Experiment 2147 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007465375407925448
rmse: 0.08640240394760697
mae: 0.03774712886141601
r2: 0.6633895086196342
pearson: 0.8157883304805906

=== Experiment 2165 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006656379194453776
rmse: 0.0815866361265972
mae: 0.041414874588426145
r2: 0.6998667918186084
pearson: 0.8403055959672941

=== Experiment 2216 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006651828303004924
rmse: 0.08155874142607232
mae: 0.03883474688738893
r2: 0.7000719895110366
pearson: 0.8380634345362427

=== Experiment 2184 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00601870216278494
rmse: 0.07758029493875968
mae: 0.030679538230779856
r2: 0.728619368513431
pearson: 0.8598913196693272

=== Experiment 2260 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006440277064204386
rmse: 0.08025133683749067
mae: 0.03209421102871308
r2: 0.7096107417577322
pearson: 0.8424277242251914

=== Experiment 2104 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005909677111447003
rmse: 0.07687442429993868
mae: 0.03142133046381463
r2: 0.733535260092664
pearson: 0.8588130422514892

=== Experiment 2059 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006249566051205097
rmse: 0.0790541969234088
mae: 0.03540258753168268
r2: 0.7182098173955342
pearson: 0.8490168186000608

=== Experiment 2051 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007289725874781173
rmse: 0.08537989151305578
mae: 0.041404192557090255
r2: 0.6713094687598378
pearson: 0.8269730894076327

=== Experiment 2168 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007792554447523915
rmse: 0.08827544645893282
mae: 0.04344955561862454
r2: 0.6486371497266471
pearson: 0.8089919184975284

=== Experiment 2038 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0060115216771095825
rmse: 0.07753400336052294
mae: 0.03386704570378311
r2: 0.7289431334521597
pearson: 0.8540193104925328

=== Experiment 2153 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006576492706703974
rmse: 0.0810955776026287
mae: 0.031402793535090845
r2: 0.7034688383905149
pearson: 0.8410279724911341

=== Experiment 2130 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005697143370908439
rmse: 0.07547942349348225
mae: 0.02757812410776611
r2: 0.7431183129102951
pearson: 0.8647172548044992

=== Experiment 2106 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007364961548917348
rmse: 0.08581935416278398
mae: 0.03896314631554013
r2: 0.6679171253267902
pearson: 0.8200979918722932

=== Experiment 2251 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007766060639329145
rmse: 0.08812525540007896
mae: 0.038193733862851645
r2: 0.6498317438773324
pearson: 0.8203933291124519

=== Experiment 2194 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006753816387437236
rmse: 0.0821816061380966
mae: 0.03666617022285308
r2: 0.6954733916723725
pearson: 0.8353624429931858

=== Experiment 2186 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006557430847115765
rmse: 0.0809779651949576
mae: 0.039467420161850744
r2: 0.7043283292495812
pearson: 0.841809290446965

=== Experiment 2041 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00533179640756158
rmse: 0.07301915096439823
mae: 0.027367149140557812
r2: 0.7595916466861092
pearson: 0.8740250825268542

=== Experiment 2256 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007903613470793257
rmse: 0.08890226921059584
mae: 0.0389483394433348
r2: 0.6436295472482467
pearson: 0.8099525909114532

=== Experiment 2257 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006498522318847325
rmse: 0.08061341277261076
mae: 0.03568259338921034
r2: 0.706984488861579
pearson: 0.8416396578931491

=== Experiment 2269 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006978538482332899
rmse: 0.08353764709598241
mae: 0.043331017043427705
r2: 0.6853407713212849
pearson: 0.8312688227729648

=== Experiment 2053 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0076517719820641625
rmse: 0.087474407583385
mae: 0.04458586365191
r2: 0.6549849691311258
pearson: 0.8146976781066316

=== Experiment 2037 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007336752629557961
rmse: 0.08565484591987753
mae: 0.03557258135321368
r2: 0.6691890530850102
pearson: 0.8307992682546974

=== Experiment 2263 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006344265976569454
rmse: 0.07965090066389365
mae: 0.039045056483066214
r2: 0.71393983944148
pearson: 0.8472101833664851

=== Experiment 2033 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005677821594858282
rmse: 0.0753513211221826
mae: 0.028583983815105855
r2: 0.7439895232882325
pearson: 0.863724392575342

=== Experiment 2066 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006855978221600927
rmse: 0.08280083466729624
mae: 0.03986638973831786
r2: 0.6908669595347935
pearson: 0.8396489985171243

=== Experiment 2081 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006505703645237689
rmse: 0.08065794223284951
mae: 0.03528775917424726
r2: 0.706660686015381
pearson: 0.8474008031050974

=== Experiment 2182 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006880680423300911
rmse: 0.08294986692756602
mae: 0.044128748499794375
r2: 0.6897531481324117
pearson: 0.8370761509185177

=== Experiment 2231 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006844382181259775
rmse: 0.0827307813408031
mae: 0.04630520557477725
r2: 0.6913898198899684
pearson: 0.8401947398657185

=== Experiment 2121 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062273681763017185
rmse: 0.07891367547074284
mae: 0.032473913567828694
r2: 0.7192107098048957
pearson: 0.8498581928606964

=== Experiment 2087 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006504846325584406
rmse: 0.08065262751816835
mae: 0.029066486699790707
r2: 0.7066993421812124
pearson: 0.8422542341875692

=== Experiment 2284 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006291693854081056
rmse: 0.0793201982730821
mae: 0.029375583176250653
r2: 0.7163102933057214
pearson: 0.851106827043739

=== Experiment 2242 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0074452969510304555
rmse: 0.08628613417595236
mae: 0.03800119622511068
r2: 0.6642948374038247
pearson: 0.8185497119201174

=== Experiment 2148 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007138416227863441
rmse: 0.08448914858053334
mae: 0.04106339731031499
r2: 0.67813195414289
pearson: 0.8246627467018907

=== Experiment 2288 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006834997405810472
rmse: 0.08267404312001725
mae: 0.0376554569241149
r2: 0.691812975284422
pearson: 0.8352447004280091

=== Experiment 2202 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006539709635840133
rmse: 0.08086847120998475
mae: 0.0376082588224669
r2: 0.705127370866298
pearson: 0.8410364451312083

=== Experiment 2199 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0077008200059594375
rmse: 0.0877543161671233
mae: 0.053683399581933675
r2: 0.6527734153213739
pearson: 0.8299578034528029

=== Experiment 2187 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005915058229409109
rmse: 0.0769094157396161
mae: 0.028394261770628884
r2: 0.7332926278521639
pearson: 0.8735637887741146

=== Experiment 2169 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065673150386181475
rmse: 0.08103897234428721
mae: 0.039736100311197065
r2: 0.7038826554051043
pearson: 0.8429171386669346

=== Experiment 2140 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005732799657977953
rmse: 0.07571525379986488
mae: 0.02981280320697853
r2: 0.7415105866198628
pearson: 0.8644075511944374

=== Experiment 2267 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007286797020646058
rmse: 0.08536273789333411
mae: 0.044914085021509294
r2: 0.6714415295037028
pearson: 0.8264332095285549

=== Experiment 2222 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006197626430295163
rmse: 0.07872500511460868
mae: 0.030539207759922592
r2: 0.7205517520419876
pearson: 0.8570542674176979

=== Experiment 2310 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006802099727130522
rmse: 0.08247484299548877
mae: 0.042310727180037394
r2: 0.6932963171367251
pearson: 0.8355315253598339

=== Experiment 2204 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.008132544298316918
rmse: 0.0901806204143491
mae: 0.03342673084100496
r2: 0.6333071569953674
pearson: 0.8269774175538989

=== Experiment 2091 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006950708023135716
rmse: 0.0833709063350982
mae: 0.0464672869329827
r2: 0.686595634477364
pearson: 0.8357644878486113

=== Experiment 2262 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0073491537106480384
rmse: 0.08572720519559726
mae: 0.04512749856995317
r2: 0.6686298937967917
pearson: 0.8225416922473596

=== Experiment 2241 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005996173620500731
rmse: 0.07743496381158017
mae: 0.033644863904547025
r2: 0.7296351705694564
pearson: 0.8588576369238337

=== Experiment 2243 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00751984110544545
rmse: 0.08671701739246715
mae: 0.041659518584724234
r2: 0.660933674290644
pearson: 0.8152572317527363

=== Experiment 2201 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006773429584792795
rmse: 0.08230084801988856
mae: 0.03486148456768529
r2: 0.6945890412360387
pearson: 0.8393252413004617

=== Experiment 2237 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007005014870505468
rmse: 0.08369596687120275
mae: 0.03499363653056238
r2: 0.6841469626317334
pearson: 0.8438432317285538

=== Experiment 2272 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007020167742703192
rmse: 0.08378644128200691
mae: 0.03729843349110008
r2: 0.683463726293627
pearson: 0.8285951426109667

=== Experiment 2311 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007239114541631318
rmse: 0.08508298620541782
mae: 0.043627526113995146
r2: 0.6735915114958135
pearson: 0.8229754949282952

=== Experiment 2329 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006469376232456659
rmse: 0.08043243271502273
mae: 0.03461507641144337
r2: 0.7082986730687606
pearson: 0.8418465216963757

=== Experiment 2334 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006852288838518393
rmse: 0.08277855301053766
mae: 0.03601898624436294
r2: 0.6910333121941632
pearson: 0.8345083489843113

=== Experiment 2159 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006737849954498428
rmse: 0.08208440749922258
mae: 0.03887300947571476
r2: 0.6961933111062187
pearson: 0.84470511379773

=== Experiment 2316 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007406331211385568
rmse: 0.08606004422137818
mae: 0.039329260688215566
r2: 0.666051785454279
pearson: 0.8279722319418064

=== Experiment 2244 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063900408612269895
rmse: 0.0799377311488573
mae: 0.03441815609307974
r2: 0.7118758700393393
pearson: 0.8447987208642724

=== Experiment 2131 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007382987381952211
rmse: 0.08592431193761292
mae: 0.03755728261577678
r2: 0.667104348449024
pearson: 0.8201325333604528

=== Experiment 2261 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005802791437537216
rmse: 0.07617605553937022
mae: 0.028438818909026622
r2: 0.7383546880852737
pearson: 0.8616600643857206

=== Experiment 2346 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00698905187104321
rmse: 0.08360054946615608
mae: 0.03818634178554186
r2: 0.6848667272516333
pearson: 0.8297621391130687

=== Experiment 2114 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007423840336216041
rmse: 0.08616171038353429
mae: 0.04097721749575956
r2: 0.6652623067220282
pearson: 0.8266502157102991

=== Experiment 2338 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007263448623933945
rmse: 0.08522586827914366
mae: 0.04043823576466068
r2: 0.6724942984350368
pearson: 0.8284837310174009

=== Experiment 2226 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007336942860579514
rmse: 0.0856559563636967
mae: 0.03974348566437306
r2: 0.6691804756519755
pearson: 0.8193895056465977

=== Experiment 2120 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005946503377275682
rmse: 0.07711357453312408
mae: 0.03126201145095636
r2: 0.7318747799749279
pearson: 0.8592329658471857

=== Experiment 2248 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007624037259911775
rmse: 0.08731573317513731
mae: 0.048289847518294085
r2: 0.6562355155459921
pearson: 0.8236162130431645

=== Experiment 2156 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007088389575639393
rmse: 0.0841925743497572
mae: 0.039093441542640106
r2: 0.6803876338732586
pearson: 0.8316820900523672

=== Experiment 2177 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007583229582872168
rmse: 0.08708174081213678
mae: 0.037104842079655774
r2: 0.6580755157428758
pearson: 0.8112929087072684

=== Experiment 2332 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006995347568991259
rmse: 0.08363819443885227
mae: 0.04259430799977414
r2: 0.6845828570021042
pearson: 0.8386817820044752

=== Experiment 2069 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008080543504407048
rmse: 0.0898918433697243
mae: 0.050490829039867256
r2: 0.6356518499054639
pearson: 0.8278595925482901

=== Experiment 2171 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006862500267977743
rmse: 0.0828402092463421
mae: 0.04244262767344073
r2: 0.690572883334235
pearson: 0.8362376169322493

=== Experiment 2040 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007331650079186895
rmse: 0.08562505520691298
mae: 0.03374998810507063
r2: 0.6694191248354373
pearson: 0.8199744061730059

=== Experiment 2301 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007383021356173145
rmse: 0.08592450963591905
mae: 0.038008845142379795
r2: 0.6671028165663553
pearson: 0.8173242336345344

=== Experiment 2062 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008190034738458255
rmse: 0.09049881070189959
mae: 0.04138582170859355
r2: 0.6307149383528725
pearson: 0.8065500914723082

=== Experiment 2017 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00867242374625841
rmse: 0.09312584896932972
mae: 0.03975465840439849
r2: 0.6089642303068024
pearson: 0.7875986329004044

=== Experiment 2277 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006267344086572193
rmse: 0.07916655914318996
mae: 0.0327848179103321
r2: 0.7174082136055435
pearson: 0.8485274399498176

=== Experiment 2020 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064196553426301125
rmse: 0.08012275171653875
mae: 0.034691171081431306
r2: 0.7105405661072024
pearson: 0.8431314015497033

=== Experiment 2304 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007105183314541418
rmse: 0.08429224943339345
mae: 0.04255396938997028
r2: 0.6796304115776561
pearson: 0.8270571058196567

=== Experiment 2393 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00801762889944609
rmse: 0.08954121341285302
mae: 0.03819799030347232
r2: 0.6384886417523181
pearson: 0.8082143639105359

=== Experiment 2209 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006571669650253692
rmse: 0.08106583528375991
mae: 0.03840921819594226
r2: 0.7036863078830686
pearson: 0.8496510907217437

=== Experiment 2291 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007310358831133159
rmse: 0.08550063643700648
mae: 0.047881830022126856
r2: 0.6703791378391855
pearson: 0.8360765419596604

=== Experiment 2058 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006808170538170924
rmse: 0.08251163880429793
mae: 0.03777420134044217
r2: 0.6930225869388816
pearson: 0.8357506267267418

=== Experiment 2426 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006251469680024738
rmse: 0.07906623603046206
mae: 0.033679574331951755
r2: 0.7181239836099077
pearson: 0.8491584694898113

=== Experiment 2214 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006593086251804318
rmse: 0.0811978217183461
mae: 0.040158100727089134
r2: 0.7027206427453182
pearson: 0.8437818636110265

=== Experiment 2228 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005390586140789721
rmse: 0.07342061114421292
mae: 0.027144364702856586
r2: 0.7569408434901927
pearson: 0.8732681722492425

=== Experiment 2141 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00800056277767257
rmse: 0.08944586506749526
mae: 0.03596612196437671
r2: 0.6392581456717117
pearson: 0.8243734910415383

=== Experiment 2418 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0056800726746458335
rmse: 0.07536625687033842
mae: 0.028990719220203843
r2: 0.743888023091388
pearson: 0.8651964519696304

=== Experiment 2300 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006533897669667498
rmse: 0.08083252853689224
mae: 0.03971262704844015
r2: 0.70538942986298
pearson: 0.8426437178357647

=== Experiment 2372 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007275627758770463
rmse: 0.0852972904538618
mae: 0.03875910268507141
r2: 0.6719451466057054
pearson: 0.8225679466444007

=== Experiment 2428 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0064424489880817205
rmse: 0.08026486770737071
mae: 0.035371041509737564
r2: 0.70951281066604
pearson: 0.8424676880120147

=== Experiment 2382 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006698561100373934
rmse: 0.08184473776837417
mae: 0.041511122524850404
r2: 0.6979648282463452
pearson: 0.8391936079590401

=== Experiment 2313 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007232084951323736
rmse: 0.0850416659721794
mae: 0.046237917374012207
r2: 0.6739084726288231
pearson: 0.8286393920772244

=== Experiment 2134 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060873896073025706
rmse: 0.07802172522639172
mae: 0.030139866977804318
r2: 0.7255222818717875
pearson: 0.855843980415869

=== Experiment 2383 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0070578073097555495
rmse: 0.08401075710738208
mae: 0.041688417756909464
r2: 0.6817665747816766
pearson: 0.8345818112461606

=== Experiment 2434 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006795382685147411
rmse: 0.08243411117460667
mae: 0.03875894046360547
r2: 0.6935991856033474
pearson: 0.8336690469816394

=== Experiment 2068 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006665615964131051
rmse: 0.08164322362652672
mae: 0.04100797773139308
r2: 0.6994503099392735
pearson: 0.8436364911022418

=== Experiment 2361 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006990822587851041
rmse: 0.08361113913738433
mae: 0.037773781983507566
r2: 0.6847868864101202
pearson: 0.8281934173289266

=== Experiment 2423 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00680183268015173
rmse: 0.08247322401938541
mae: 0.03577580028330859
r2: 0.6933083581674626
pearson: 0.8461362432261986

=== Experiment 2319 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007042463077979227
rmse: 0.08391938439942959
mae: 0.04104325444892955
r2: 0.6824584394389581
pearson: 0.8283885562235207

=== Experiment 2191 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006911004170734506
rmse: 0.08313244956534425
mae: 0.03938230793161226
r2: 0.6883858637071327
pearson: 0.8316829369044318

=== Experiment 2268 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007215367299605976
rmse: 0.08494331815749827
mae: 0.043219501567005734
r2: 0.6746622641867757
pearson: 0.8237421403538978

=== Experiment 2027 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006714305833766514
rmse: 0.08194086790952677
mae: 0.040271885540923924
r2: 0.6972549051474608
pearson: 0.8393140108024932

=== Experiment 2388 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006444522671876086
rmse: 0.08027778442306494
mae: 0.04078115460548013
r2: 0.70941930917645
pearson: 0.8448441235845436

=== Experiment 2355 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007615653186242098
rmse: 0.08726770987164782
mae: 0.044687285864656104
r2: 0.6566135497376449
pearson: 0.8143314575321403

=== Experiment 2432 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00697625340974904
rmse: 0.08352396907324891
mae: 0.0409338406264361
r2: 0.6854438042383537
pearson: 0.8314581693741879

=== Experiment 2458 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00696031662761042
rmse: 0.08342851207836814
mae: 0.03242810936822828
r2: 0.6861623867306699
pearson: 0.8396552131935963

=== Experiment 2396 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006825703943578616
rmse: 0.08261781855979142
mae: 0.035492324982344364
r2: 0.6922320134060909
pearson: 0.8321265938281276

=== Experiment 2278 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007227357109897889
rmse: 0.0850138642216544
mae: 0.04034429460693646
r2: 0.6741216489178324
pearson: 0.8338088988580502

=== Experiment 2294 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006410937438092821
rmse: 0.0800683298070643
mae: 0.03694209433837317
r2: 0.7109336525857457
pearson: 0.8463726175628382

=== Experiment 2072 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005852356262504818
rmse: 0.07650069452302259
mae: 0.030236008343319187
r2: 0.7361198319426321
pearson: 0.8583919596107576

=== Experiment 2390 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007046411357537408
rmse: 0.08394290534367635
mae: 0.03419038531367149
r2: 0.6822804132514505
pearson: 0.8399709067896759

=== Experiment 2309 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0069522594332978365
rmse: 0.08338021008187636
mae: 0.03602517212796334
r2: 0.6865256820759819
pearson: 0.8291967539671464

=== Experiment 2282 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006982664149409227
rmse: 0.0835623369073007
mae: 0.041235830458578704
r2: 0.6851547468086583
pearson: 0.8296844920654767

=== Experiment 2249 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006873478545649719
rmse: 0.08290644453629475
mae: 0.04333287564022845
r2: 0.6900778776259152
pearson: 0.8391898337727437

=== Experiment 2205 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006193283047221843
rmse: 0.07869741448879908
mae: 0.03019742695829297
r2: 0.7207475932730947
pearson: 0.8501099602255664

=== Experiment 2422 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006759489304171282
rmse: 0.08221611340954571
mae: 0.04459383767060904
r2: 0.695217602353676
pearson: 0.8458946622367296

=== Experiment 2290 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063917083309429105
rmse: 0.07994816027240971
mae: 0.031552807791470974
r2: 0.7118006845637579
pearson: 0.8537517629348326

=== Experiment 2032 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00661185139308969
rmse: 0.08131329161391568
mae: 0.035707383160950366
r2: 0.701874530783931
pearson: 0.8395923321376108

=== Experiment 2443 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007368276097469455
rmse: 0.0858386631854752
mae: 0.0462539350670919
r2: 0.66776767379142
pearson: 0.8263366075304709

=== Experiment 2416 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006753845729464247
rmse: 0.08218178465733296
mae: 0.03473048794518939
r2: 0.695472068653289
pearson: 0.834428263371505

=== Experiment 2306 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00757417660347122
rmse: 0.08702974550963148
mae: 0.04619246775069239
r2: 0.6584837105995437
pearson: 0.8209267082166608

=== Experiment 2090 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006419537196469677
rmse: 0.08012201443092702
mae: 0.030335089594152962
r2: 0.7105458932655764
pearson: 0.8503828589910892

=== Experiment 2078 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007808185054359183
rmse: 0.08836393525844796
mae: 0.03720502015377083
r2: 0.6479323725440009
pearson: 0.828724842785069

=== Experiment 2491 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067785120875515316
rmse: 0.08233171981412468
mae: 0.0395192188218652
r2: 0.6943598734236278
pearson: 0.8344149316244168

=== Experiment 2436 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007344791484888078
rmse: 0.08570175893695577
mae: 0.03638515997060001
r2: 0.6688265846363458
pearson: 0.8179784953121231

=== Experiment 2360 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006536031417771642
rmse: 0.08084572603280672
mae: 0.036995489312653736
r2: 0.7052932201000985
pearson: 0.8532922537734067

=== Experiment 2336 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0076280302087311765
rmse: 0.08733859518409474
mae: 0.04111790344524318
r2: 0.6560554752411564
pearson: 0.8186797591884291

=== Experiment 2172 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006543325700501462
rmse: 0.08089082581171651
mae: 0.0390451071413595
r2: 0.7049643241022703
pearson: 0.8404298910826811

=== Experiment 2403 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007229197813602538
rmse: 0.08502468943549596
mae: 0.050265109287353046
r2: 0.6740386523979454
pearson: 0.8390628125128665

=== Experiment 2349 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007059183541297452
rmse: 0.08401894751362607
mae: 0.04380587539739955
r2: 0.6817045211071779
pearson: 0.8340919400151653

=== Experiment 2337 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0072419301240179545
rmse: 0.08509953069211343
mae: 0.047812340488658324
r2: 0.6734645581252328
pearson: 0.8307971114031598

=== Experiment 2352 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007006521777006435
rmse: 0.08370496865184548
mae: 0.04536049109516307
r2: 0.684079016880845
pearson: 0.8376695757325435

=== Experiment 2238 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006567855098968581
rmse: 0.08104230437844534
mae: 0.034933224504012726
r2: 0.7038583043216018
pearson: 0.841238882413532

=== Experiment 2478 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006632248928824244
rmse: 0.08143862062206263
mae: 0.04068621579279903
r2: 0.700954814875302
pearson: 0.840024728825306

=== Experiment 2208 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005983850610315054
rmse: 0.07735535282263958
mae: 0.030769693669347967
r2: 0.7301908096749571
pearson: 0.8548107275942234

=== Experiment 2210 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005951439675432415
rmse: 0.07714557456803608
mae: 0.030847119766391394
r2: 0.731652204463672
pearson: 0.863712490936944

=== Experiment 2307 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006132347189702718
rmse: 0.078309304617668
mae: 0.034970059175687344
r2: 0.7234951642687102
pearson: 0.8522232659787734

=== Experiment 2342 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007473941635196544
rmse: 0.08645196143059188
mae: 0.04692048006310392
r2: 0.6630032612022654
pearson: 0.8223835158830081

=== Experiment 2402 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007091731992183603
rmse: 0.08421242184015136
mae: 0.043064353437227136
r2: 0.6802369257823904
pearson: 0.829973280021348

=== Experiment 2401 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006218008423214664
rmse: 0.07885434942483936
mae: 0.033685199129222715
r2: 0.7196327369520485
pearson: 0.8523272932805589

=== Experiment 2011 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006021527283590849
rmse: 0.07759850052411355
mae: 0.029396465511778276
r2: 0.7284919850597236
pearson: 0.8543542796446426

=== Experiment 2315 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007176105088812187
rmse: 0.08471189461233992
mae: 0.03821781903174617
r2: 0.676432579990847
pearson: 0.8234313366322534

=== Experiment 2437 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00717358962234224
rmse: 0.08469704612524713
mae: 0.04063423757923015
r2: 0.6765460012668356
pearson: 0.8239734190461606

=== Experiment 2162 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00783836062029102
rmse: 0.08853451654745183
mae: 0.03529658941555851
r2: 0.6465717695574167
pearson: 0.8262885737963281

=== Experiment 2493 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0070044138131859415
rmse: 0.08369237607563751
mae: 0.03686343775165753
r2: 0.6841740640417253
pearson: 0.8282489447696888

=== Experiment 2470 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007278775562378358
rmse: 0.08531574041393744
mae: 0.03934709786642259
r2: 0.6718032135264798
pearson: 0.8233332044671149

=== Experiment 2215 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006451583225670317
rmse: 0.08032174814874435
mae: 0.03489340819296396
r2: 0.7091009519134557
pearson: 0.8431611993081279

=== Experiment 2446 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007089069910690282
rmse: 0.08419661460349984
mae: 0.036574525564944074
r2: 0.6803569578652524
pearson: 0.8312562880492276

=== Experiment 2439 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060617854531084345
rmse: 0.07785746883317254
mae: 0.03325310780189652
r2: 0.726676761915151
pearson: 0.852811665521945

=== Experiment 2264 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006491314553607406
rmse: 0.08056869462519177
mae: 0.03316869886693786
r2: 0.7073094838238669
pearson: 0.8475640643650681

=== Experiment 2286 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006275381560499924
rmse: 0.07921730594068398
mae: 0.030739452682508175
r2: 0.7170458074437049
pearson: 0.8534578282497006

=== Experiment 2424 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007355985136514163
rmse: 0.08576703991927297
mae: 0.04768256298663471
r2: 0.6683218678112286
pearson: 0.8261241950374311

=== Experiment 2489 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005915101215153513
rmse: 0.07690969519607728
mae: 0.03149118760173297
r2: 0.7332906896438681
pearson: 0.8566442682962778

=== Experiment 2283 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006899850375395703
rmse: 0.08306533799001665
mae: 0.033052468279161855
r2: 0.6888887834297988
pearson: 0.8551187263949042

=== Experiment 2196 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006852373617764088
rmse: 0.08277906509356148
mae: 0.03417067506916883
r2: 0.6910294895352891
pearson: 0.8424969310780457

=== Experiment 2369 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007590938148440373
rmse: 0.08712599008585425
mae: 0.044436247516089586
r2: 0.6577279399142046
pearson: 0.8232621443758775

=== Experiment 2016 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060094592232343615
rmse: 0.07752070190106873
mae: 0.02910995524020181
r2: 0.7290361285896381
pearson: 0.8595390967149749

=== Experiment 2227 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006619083510567198
rmse: 0.08135775015674412
mae: 0.03302536995844908
r2: 0.7015484377899683
pearson: 0.8550710845597795

=== Experiment 2155 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006585481555686299
rmse: 0.08115098000447252
mae: 0.03759824311998503
r2: 0.7030635351461968
pearson: 0.8463429239278883

=== Experiment 2453 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005640566929633928
rmse: 0.07510370782880116
mae: 0.032878703997460144
r2: 0.7456693197461667
pearson: 0.8649125441304722

=== Experiment 2440 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007103593498407111
rmse: 0.08428281852434166
mae: 0.041730218783544056
r2: 0.6797020956874205
pearson: 0.8269779312267367

=== Experiment 2095 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007039780808975331
rmse: 0.08390340165318287
mae: 0.038176335192412446
r2: 0.6825793817677869
pearson: 0.8298293687139723

=== Experiment 2221 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006720914660633618
rmse: 0.08198118479647398
mae: 0.037728120035541934
r2: 0.6969569160527902
pearson: 0.8367720145232441

=== Experiment 2445 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006971830141785466
rmse: 0.08349748584110461
mae: 0.03588975773865001
r2: 0.6856432474440022
pearson: 0.8291306095994877

=== Experiment 2195 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0062960773016506755
rmse: 0.07934782480730443
mae: 0.03197281873618426
r2: 0.7161126455841099
pearson: 0.8493509865622356

=== Experiment 2452 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007008997894230365
rmse: 0.08371975808750504
mae: 0.04267704150692316
r2: 0.6839673698450406
pearson: 0.8329751998270746

=== Experiment 2391 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005585919838738993
rmse: 0.07473901149158312
mae: 0.027576574335696487
r2: 0.7481333330225952
pearson: 0.8681030321738655

=== Experiment 2279 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0069797426701204415
rmse: 0.08354485424082349
mae: 0.03809375053116417
r2: 0.6852864750239481
pearson: 0.8311429389969377

=== Experiment 2345 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006895479565164619
rmse: 0.08303902435099186
mae: 0.039343105603206474
r2: 0.6890858613393775
pearson: 0.8366573858082126

=== Experiment 2465 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0074208750998390905
rmse: 0.08614450127453922
mae: 0.04288208260810056
r2: 0.6653960079251645
pearson: 0.8206424929931773

=== Experiment 2276 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005965411497565524
rmse: 0.07723607639934543
mae: 0.029559736914011196
r2: 0.7310222211531592
pearson: 0.8635704096976704

=== Experiment 2320 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0058558860225527256
rmse: 0.07652376116313629
mae: 0.031228955877584407
r2: 0.7359606766156381
pearson: 0.8578883364007728

=== Experiment 2459 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005993780032871132
rmse: 0.0774195067981651
mae: 0.029072890143721265
r2: 0.7297430963821766
pearson: 0.8562895854078156

=== Experiment 2136 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007341837667337113
rmse: 0.0856845240830403
mae: 0.040842120571720764
r2: 0.6689597709696444
pearson: 0.8189783844063274

=== Experiment 2412 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005667718169173851
rmse: 0.07528424914398663
mae: 0.028348986383072903
r2: 0.744445082305486
pearson: 0.8648283583297044

=== Experiment 2479 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006863467561778626
rmse: 0.08284604735157029
mae: 0.03739109428350966
r2: 0.6905292684824917
pearson: 0.8314585220629959

=== Experiment 2499 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065669442915050405
rmse: 0.08103668485016549
mae: 0.040661512496380496
r2: 0.7038993722292559
pearson: 0.8417499498973847

=== Experiment 2408 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007266128818042777
rmse: 0.08524159089342935
mae: 0.03803628344865285
r2: 0.672373449662313
pearson: 0.837224772204897

=== Experiment 2240 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007102650894866564
rmse: 0.0842772264307895
mae: 0.041528587454715084
r2: 0.6797445972661922
pearson: 0.8305346738796031

=== Experiment 2125 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006591601525002065
rmse: 0.08118867855188965
mae: 0.03162877625373796
r2: 0.7027875884233534
pearson: 0.8473625022594069

=== Experiment 2490 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005737975974365421
rmse: 0.07574942887154609
mae: 0.035344395429425725
r2: 0.7412771887922229
pearson: 0.8627824784478617

=== Experiment 2398 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065260591888354
rmse: 0.08078402805527464
mae: 0.038178429748111545
r2: 0.7057428635137201
pearson: 0.8421602148760485

=== Experiment 2083 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007375921610460857
rmse: 0.08588318584252017
mae: 0.04635495027291595
r2: 0.6674229409756854
pearson: 0.8278117928476455

=== Experiment 2441 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065430292135543406
rmse: 0.08088899315453457
mae: 0.037118814092423257
r2: 0.7049776925682221
pearson: 0.8424152486204994

=== Experiment 2170 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006136338444026914
rmse: 0.07833478438105843
mae: 0.03026899157966911
r2: 0.7233152003679173
pearson: 0.8518651365404849

=== Experiment 2497 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006277780250783585
rmse: 0.07923244443271699
mae: 0.03491350627969626
r2: 0.7169376515545591
pearson: 0.8468365095656128

=== Experiment 2384 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006679987369507667
rmse: 0.08173118969834996
mae: 0.03348185599350345
r2: 0.6988023095961811
pearson: 0.8443920432285

=== Experiment 2450 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0077728614361720185
rmse: 0.08816383292582065
mae: 0.042899922927759127
r2: 0.6495250989409469
pearson: 0.810552058612799

=== Experiment 2481 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00692791158423111
rmse: 0.08323407706120799
mae: 0.03542320177166531
r2: 0.6876235158740927
pearson: 0.8392741367006464

=== Experiment 2292 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0056632903286773124
rmse: 0.07525483591555637
mae: 0.028201115279265304
r2: 0.7446447316846332
pearson: 0.863940899483328

=== Experiment 2070 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007272602018273355
rmse: 0.08527955216975142
mae: 0.0467155749959885
r2: 0.6720815759129897
pearson: 0.8283510801092758

=== Experiment 2255 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007402527631819986
rmse: 0.08603794297761881
mae: 0.03612480370451908
r2: 0.6662232871828078
pearson: 0.8199416652178844

=== Experiment 2456 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006314856429881834
rmse: 0.07946607093522262
mae: 0.03242171111423552
r2: 0.7152659029574935
pearson: 0.8529119451709597

=== Experiment 2389 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006655928101174888
rmse: 0.0815838715750539
mae: 0.03507926809711156
r2: 0.6998871314160122
pearson: 0.8510398319896336

=== Experiment 2046 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005444665930811858
rmse: 0.07378797958212338
mae: 0.030549452032968313
r2: 0.7545024095604305
pearson: 0.8721647054214171

=== Experiment 2358 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00694451380578887
rmse: 0.08333374950036071
mae: 0.033088328872831904
r2: 0.6868749290112497
pearson: 0.8292588042841542

=== Experiment 2103 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00575611796043388
rmse: 0.0758690843521515
mae: 0.031503909575697576
r2: 0.7404591746252948
pearson: 0.8661496945976879

=== Experiment 2039 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064842801609377165
rmse: 0.08052502816477444
mae: 0.03749882284207514
r2: 0.7076266614932701
pearson: 0.8437088497180999

=== Experiment 2350 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00625756511853906
rmse: 0.07910477304524083
mae: 0.03329322212829341
r2: 0.7178491429701046
pearson: 0.8473307145697521

=== Experiment 2484 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0069243060218180585
rmse: 0.08321241507021689
mae: 0.04125307228720536
r2: 0.6877860890963677
pearson: 0.8347804232946967

=== Experiment 2466 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00668911452936971
rmse: 0.08178700709385146
mae: 0.03795820739120059
r2: 0.6983907699751717
pearson: 0.8377085922640153

=== Experiment 2483 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007713283302359801
rmse: 0.08782529989905984
mae: 0.04607464509477797
r2: 0.6522114507721978
pearson: 0.828854505644836

=== Experiment 2414 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005614995256119519
rmse: 0.07493327202331097
mae: 0.03071253115006318
r2: 0.7468223352499771
pearson: 0.8650690683823415

=== Experiment 2375 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00624517678613278
rmse: 0.0790264309337881
mae: 0.0367706080946332
r2: 0.7184077274257816
pearson: 0.8492382480004491

=== Experiment 2399 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005750470295003947
rmse: 0.07583185541053276
mae: 0.02791766703179092
r2: 0.7407138253738028
pearson: 0.8613420941752141

=== Experiment 2431 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007391466906483873
rmse: 0.08597364076555018
mae: 0.046227649446726936
r2: 0.6667220104200156
pearson: 0.8269777905913948

=== Experiment 2250 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008018228775851602
rmse: 0.08954456307253725
mae: 0.04149526296033682
r2: 0.6384615935892195
pearson: 0.8268212709825318

=== Experiment 2385 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0055649681376716345
rmse: 0.07459871404837777
mae: 0.02819041901174299
r2: 0.7490780360021736
pearson: 0.865550417036427

=== Experiment 2444 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005542897371815903
rmse: 0.07445063714848855
mae: 0.0280445114239632
r2: 0.7500731971205216
pearson: 0.8678983449470747

=== Experiment 2163 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007137570940008097
rmse: 0.08448414608675463
mae: 0.0342635117205818
r2: 0.6781700678002476
pearson: 0.8345339745473903

=== Experiment 2280 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061199558833862095
rmse: 0.07823014689610527
mae: 0.03820028840942569
r2: 0.7240538828166905
pearson: 0.853059193231511

=== Experiment 2347 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005705532835335219
rmse: 0.07553497756228712
mae: 0.028377013270032298
r2: 0.7427400356517773
pearson: 0.8649360429391868

=== Experiment 2376 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068787703371332165
rmse: 0.08293835263093435
mae: 0.03985146632183945
r2: 0.6898392730770198
pearson: 0.8335764095287882

=== Experiment 2112 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0070756380647821054
rmse: 0.08411681202222362
mae: 0.047113849662456214
r2: 0.6809625938854447
pearson: 0.833178210693242

=== Experiment 2281 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006673118813657051
rmse: 0.08168915970713037
mae: 0.03732957602991585
r2: 0.6991120097564025
pearson: 0.8403564184085534

=== Experiment 2368 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006419902807180577
rmse: 0.08012429598555344
mae: 0.03143003695148111
r2: 0.7105294080395395
pearson: 0.8588292351231064

=== Experiment 2224 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005646940555934163
rmse: 0.07514612801691224
mae: 0.028962812760820467
r2: 0.7453819357415365
pearson: 0.8665160081331186

=== Experiment 2475 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006696839331596012
rmse: 0.08183421858609033
mae: 0.0397710596231684
r2: 0.6980424620427332
pearson: 0.8385331877602283

=== Experiment 2245 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006450925854078456
rmse: 0.08031765592992898
mae: 0.03852804732448189
r2: 0.7091305925092013
pearson: 0.8480241130895668

=== Experiment 2031 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005863107768360702
rmse: 0.07657093292079378
mae: 0.030277378754687192
r2: 0.73563505127569
pearson: 0.8581037881822352

=== Experiment 2438 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006025030010943001
rmse: 0.07762106679853738
mae: 0.031021455478937907
r2: 0.7283340486250833
pearson: 0.8570783775505928

=== Experiment 2404 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007698181266279295
rmse: 0.08773928006474235
mae: 0.042888517096010374
r2: 0.6528923949321535
pearson: 0.8182106327560337

=== Experiment 2482 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007563637952835373
rmse: 0.0869691781773024
mae: 0.04386166561150946
r2: 0.6589588937182991
pearson: 0.8299594105989793

=== Experiment 2411 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0066981686215129505
rmse: 0.08184234002955286
mae: 0.03617634445757455
r2: 0.6979825249454439
pearson: 0.8422796236748346

=== Experiment 2370 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007333901516441776
rmse: 0.08563820126813604
mae: 0.0351103260751866
r2: 0.6693176085205479
pearson: 0.82816186238001

=== Experiment 2299 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008238608356372914
rmse: 0.09076678002646625
mae: 0.046042565811605075
r2: 0.6285247752999847
pearson: 0.8262302422392941

=== Experiment 2425 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005630514729554285
rmse: 0.07503675585707503
mae: 0.029613244062781426
r2: 0.7461225690234462
pearson: 0.8647179150787836

=== Experiment 2271 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005702505811082437
rmse: 0.0755149376685331
mae: 0.028871561232313686
r2: 0.7428765228430398
pearson: 0.8633558248068266

=== Experiment 2335 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005894597368193375
rmse: 0.07677628128656255
mae: 0.03453893346582098
r2: 0.7342151990788676
pearson: 0.857191519754524

=== Experiment 2454 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.008023723397612594
rmse: 0.08957523875275239
mae: 0.04851495199251591
r2: 0.6382138435123844
pearson: 0.8177875906038254

=== Experiment 2469 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006206542451752845
rmse: 0.0787816123962492
mae: 0.03242553858637578
r2: 0.7201497325587018
pearson: 0.8488372561045363

=== Experiment 2487 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006738352769214586
rmse: 0.08208747023276199
mae: 0.04193429230939457
r2: 0.6961706394119729
pearson: 0.8406811050287911

=== Experiment 2405 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006944111480251381
rmse: 0.0833313355242275
mae: 0.042481578505429204
r2: 0.6868930696926581
pearson: 0.8355956610769409

=== Experiment 2158 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005982784529099196
rmse: 0.0773484617112661
mae: 0.030636101283971588
r2: 0.730238878807762
pearson: 0.8624690355318613

=== Experiment 2297 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007288566910112552
rmse: 0.08537310413773504
mae: 0.03624969632318684
r2: 0.6713617259666467
pearson: 0.8206303732274763

=== Experiment 2353 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0059891228099743935
rmse: 0.07738942311436617
mae: 0.03293174094374196
r2: 0.7299530885127901
pearson: 0.8544064456088093

=== Experiment 2448 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0058224341660591756
rmse: 0.07630487642385102
mae: 0.027115789374982345
r2: 0.7374690061016445
pearson: 0.8598874692156123

=== Experiment 2359 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00691407178564875
rmse: 0.0831508976839622
mae: 0.037054154077249206
r2: 0.6882475463008101
pearson: 0.8303567634318633

=== Experiment 2018 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006677524532033596
rmse: 0.08171612161644479
mae: 0.03526617076117853
r2: 0.6989133578539091
pearson: 0.8360486772020425

=== Experiment 2462 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007507617641322043
rmse: 0.08664650968920816
mae: 0.03760508670211382
r2: 0.6614848249080105
pearson: 0.8175052053694053

=== Experiment 2211 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0066322584652100565
rmse: 0.08143867917157091
mae: 0.03734117683476402
r2: 0.7009543848838629
pearson: 0.8380832503295614

=== Experiment 2413 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006113719185761032
rmse: 0.07819027551915284
mae: 0.03033629733327351
r2: 0.7243350927676291
pearson: 0.8523688395901877

=== Experiment 2328 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0072785674889002975
rmse: 0.08531452097328038
mae: 0.039625476354096016
r2: 0.6718125954680253
pearson: 0.8368178540533499

=== Experiment 2152 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006925052389296905
rmse: 0.08321689966164868
mae: 0.03208476710570754
r2: 0.687752435715248
pearson: 0.845978808433516

=== Experiment 2321 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005647107190764817
rmse: 0.07514723674736695
mae: 0.029726999922557618
r2: 0.745374422250364
pearson: 0.8636997777879316

=== Experiment 2486 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0076587526310366045
rmse: 0.0875142995803349
mae: 0.04784928935024625
r2: 0.654670214741377
pearson: 0.8183593824166034

=== Experiment 2013 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00633332185779518
rmse: 0.0795821704767794
mae: 0.03922920199959174
r2: 0.714433304940137
pearson: 0.8483005626263926

=== Experiment 2455 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007160147993927389
rmse: 0.0846176576958225
mae: 0.04134426959482974
r2: 0.6771520783759482
pearson: 0.8253076222988527

=== Experiment 2198 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006616561337621545
rmse: 0.08134224817166996
mae: 0.03479194288041463
r2: 0.7016621614579948
pearson: 0.84419906408772

=== Experiment 2274 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007273589584805359
rmse: 0.08528534214509173
mae: 0.03646569619633861
r2: 0.672037046972723
pearson: 0.8334148625503089

=== Experiment 2379 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006055526196701224
rmse: 0.07781726156002422
mae: 0.03410856825557741
r2: 0.7269589890316426
pearson: 0.8556332310315754

=== Experiment 2050 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0053945925930142255
rmse: 0.07344789032378143
mae: 0.029049454563643218
r2: 0.7567601943227642
pearson: 0.8704572003915703

=== Experiment 2339 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006390844229275234
rmse: 0.0799427559524641
mae: 0.03434059605230698
r2: 0.7118396465276331
pearson: 0.8441118383459669

=== Experiment 2189 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005900126678513511
rmse: 0.0768122820811458
mae: 0.028780316178171474
r2: 0.7339658849101682
pearson: 0.8571165753505244

=== Experiment 2496 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006265152905984892
rmse: 0.07915271887929619
mae: 0.0303741366993181
r2: 0.7175070129738127
pearson: 0.847782266994372

=== Experiment 2463 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007603634980392053
rmse: 0.08719882442092929
mae: 0.03756565429745145
r2: 0.6571554453498064
pearson: 0.8372846099516362

=== Experiment 2305 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008508767356064212
rmse: 0.09224297998256675
mae: 0.0447030267073662
r2: 0.6163434249099731
pearson: 0.8001661808337943

=== Experiment 2406 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006940601833065164
rmse: 0.0833102744747919
mae: 0.03594427750270365
r2: 0.6870513181395708
pearson: 0.8288958820773885

=== Experiment 2129 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00579916926817802
rmse: 0.07615227684172036
mae: 0.03337705864067137
r2: 0.7385180101074411
pearson: 0.8600882300722198

=== Experiment 2341 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0060765000251426
rmse: 0.07795190841244748
mae: 0.03666140563058646
r2: 0.7260132883385089
pearson: 0.855116522021389

=== Experiment 2265 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006172045723458601
rmse: 0.07856236836716801
mae: 0.030171752195868044
r2: 0.7217051748543182
pearson: 0.8553680697452464

=== Experiment 2397 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006589580842122782
rmse: 0.08117623323438199
mae: 0.03580884008819405
r2: 0.7028787001250097
pearson: 0.8394813510928674

=== Experiment 2213 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0077754926226334484
rmse: 0.08817875380517377
mae: 0.037000130103345547
r2: 0.6494064599014747
pearson: 0.8069172763017508

=== Experiment 2047 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0069304000509111865
rmse: 0.08324902432407953
mae: 0.034310971335163
r2: 0.687511312006746
pearson: 0.833726005683979

=== Experiment 2354 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006659071636419832
rmse: 0.08160313496686161
mae: 0.03616552066768092
r2: 0.6997453907953934
pearson: 0.8373219366853762

=== Experiment 2002 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007162324882565251
rmse: 0.08463051980559526
mae: 0.038197764756803435
r2: 0.677053923425389
pearson: 0.8297758247400171

=== Experiment 2495 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00692818235842353
rmse: 0.08323570362785149
mae: 0.04722174284701785
r2: 0.6876113067849333
pearson: 0.839936814695316

=== Experiment 2417 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006929149137735919
rmse: 0.08324151090493204
mae: 0.03665729461308651
r2: 0.6875677151312498
pearson: 0.8377511496052065

=== Experiment 2392 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006832792241901677
rmse: 0.08266070554926129
mae: 0.03192714908784972
r2: 0.6919124051545027
pearson: 0.8471222170009411

=== Experiment 2225 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00611091334971515
rmse: 0.07817233110068517
mae: 0.032183076966218895
r2: 0.7244616066799952
pearson: 0.8516147024566307

=== Experiment 2318 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006846683707661242
rmse: 0.08274468990612777
mae: 0.03857355867251885
r2: 0.6912860450775056
pearson: 0.8323006417545421

=== Experiment 2123 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006504277233274316
rmse: 0.08064909939530829
mae: 0.036578395591266556
r2: 0.7067250023029976
pearson: 0.8442678476186147

=== Experiment 2253 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00580096273375776
rmse: 0.07616405145314789
mae: 0.0318793631387632
r2: 0.7384371435338153
pearson: 0.8625466039898143

=== Experiment 2457 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007613804660547561
rmse: 0.08725711810819539
mae: 0.03640946442285145
r2: 0.6566968989476131
pearson: 0.8104945252988766

=== Experiment 2380 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006799720021605642
rmse: 0.0824604148765069
mae: 0.039090693530954325
r2: 0.6934036170114535
pearson: 0.8339967992365763

=== Experiment 2254 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006848628793691719
rmse: 0.08275644261138657
mae: 0.043908729164236354
r2: 0.6911983420044892
pearson: 0.8416619890087612

=== Experiment 2449 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00808337054634349
rmse: 0.08990756668013816
mae: 0.03984405544206365
r2: 0.6355243798288261
pearson: 0.7989215274089735

=== Experiment 2259 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007244739028832084
rmse: 0.08511603273668296
mae: 0.03581135317866824
r2: 0.6733379058434572
pearson: 0.8220792795402581

=== Experiment 2207 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007619322522232191
rmse: 0.08728873078600806
mae: 0.03884655648437995
r2: 0.6564481009928498
pearson: 0.8225146531071484

=== Experiment 2174 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005874464914223053
rmse: 0.07664505798956026
mae: 0.03105325119584264
r2: 0.7351229625674187
pearson: 0.8577193391422072

=== Experiment 2003 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00692671105378271
rmse: 0.08322686497629662
mae: 0.0339234547620092
r2: 0.687677647263617
pearson: 0.8417125615928065

=== Experiment 2034 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0075477565741293344
rmse: 0.08687782556054988
mae: 0.03808602864449806
r2: 0.6596749780942249
pearson: 0.8197057667954463

=== Experiment 2239 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006482932664730435
rmse: 0.08051666078974236
mae: 0.03181020082435842
r2: 0.7076874195041781
pearson: 0.8413601447998558

=== Experiment 2494 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065142630123493275
rmse: 0.08071098445905196
mae: 0.03261046432735887
r2: 0.7062747479195828
pearson: 0.8508483628272323

=== Experiment 2323 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006622171343348244
rmse: 0.08137672482564191
mae: 0.03734065612574902
r2: 0.7014092087689303
pearson: 0.8382815397877936

=== Experiment 2474 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006482616870778212
rmse: 0.0805146997186117
mae: 0.03219666235232887
r2: 0.7077016585144924
pearson: 0.8438964009663574

=== Experiment 2308 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006749659772215968
rmse: 0.08215631303932747
mae: 0.03564174394420955
r2: 0.6956608116232332
pearson: 0.8419239068766762

=== Experiment 2128 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071497902372105356
rmse: 0.0845564322639652
mae: 0.044452028761917874
r2: 0.6776191050675138
pearson: 0.8271538985658912

=== Experiment 2357 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007069311153481802
rmse: 0.08407919572332861
mae: 0.03440495957184078
r2: 0.6812478715312886
pearson: 0.8269632863751372

=== Experiment 2023 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00595722627695843
rmse: 0.07718306988555476
mae: 0.027874788031009717
r2: 0.7313912891477421
pearson: 0.8563362481428619

=== Experiment 2135 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006739346602361128
rmse: 0.08209352351045196
mae: 0.04021239546148041
r2: 0.6961258279128149
pearson: 0.8362250238125191

=== Experiment 2188 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006038541373372895
rmse: 0.07770805217847694
mae: 0.031414762906246
r2: 0.7277248272399248
pearson: 0.8549082415261224

=== Experiment 2407 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0066637830728107856
rmse: 0.08163199784894883
mae: 0.03928565867320844
r2: 0.699532954202187
pearson: 0.8388832957475901

=== Experiment 2400 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007231591888953613
rmse: 0.08503876697691244
mae: 0.03587249086244882
r2: 0.6739307045940786
pearson: 0.8312741614582589

=== Experiment 2451 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005450238407202571
rmse: 0.073825729980831
mae: 0.03204583097237424
r2: 0.7542511490526076
pearson: 0.8735705450558557

=== Experiment 2055 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0062730251962685715
rmse: 0.07920243175729247
mae: 0.04007260707651883
r2: 0.7171520548697816
pearson: 0.8506193929392085

=== Experiment 2325 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005655009740433797
rmse: 0.07519979880580663
mae: 0.02760418419443488
r2: 0.7450180997639677
pearson: 0.8640567710201188

=== Experiment 2371 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006650088325451928
rmse: 0.08154807370779477
mae: 0.03841989812548141
r2: 0.7001504443330786
pearson: 0.8382483174169071

=== Experiment 2175 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006627382522187572
rmse: 0.08140873738234473
mae: 0.038905462671454116
r2: 0.7011742390086801
pearson: 0.8378876057216521

=== Experiment 2234 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007022256529116051
rmse: 0.08379890529783818
mae: 0.0434868745599811
r2: 0.6833695438336165
pearson: 0.8321376175388299

=== Experiment 2298 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006999884467817571
rmse: 0.08366531221371
mae: 0.04005788975094414
r2: 0.6843782902308677
pearson: 0.8361419916394477

=== Experiment 2295 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00648184392821961
rmse: 0.08050989956657262
mae: 0.035806624938170405
r2: 0.7077365101542616
pearson: 0.8424965754687597

=== Experiment 2266 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007261896641963545
rmse: 0.0852167626817843
mae: 0.03875631635649677
r2: 0.6725642766190025
pearson: 0.8399617460679507

=== Experiment 2340 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006503792492509151
rmse: 0.08064609409332327
mae: 0.04023128424288621
r2: 0.7067468590507482
pearson: 0.8445418991085829

=== Experiment 2200 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006196198831013907
rmse: 0.07871593759216686
mae: 0.034769067646676324
r2: 0.7206161218652452
pearson: 0.85363814189233

=== Experiment 2289 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006124141933202189
rmse: 0.07825689703279953
mae: 0.03359014031802669
r2: 0.7238651356729093
pearson: 0.8521255175358805

=== Experiment 2476 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065369217553308805
rmse: 0.08085123224373814
mae: 0.03724255883101386
r2: 0.7052530751714201
pearson: 0.8406030559990547

=== Experiment 2203 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006489298094881777
rmse: 0.08055617974359122
mae: 0.034467796289146514
r2: 0.7074004050602942
pearson: 0.8416651283137441

=== Experiment 2074 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00608186028866291
rmse: 0.07798628269550299
mae: 0.029793799037233584
r2: 0.7257715964156105
pearson: 0.8519642277730644

=== Experiment 2477 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007362786810189325
rmse: 0.08580668278280734
mae: 0.04089073266676357
r2: 0.6680151833388611
pearson: 0.8190394974368891

=== Experiment 2366 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008284607968985202
rmse: 0.09101982184659121
mae: 0.054803093445355125
r2: 0.6264506730132743
pearson: 0.8088988178815031

=== Experiment 2124 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00718096922455529
rmse: 0.08474059962352927
mae: 0.03597654927426529
r2: 0.6762132582510605
pearson: 0.8231600259976082

=== Experiment 2132 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007184922336918293
rmse: 0.0847639211983394
mae: 0.04226637416040059
r2: 0.6760350141545105
pearson: 0.8331623775836471

=== Experiment 2492 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008841879173226047
rmse: 0.09403126699787707
mae: 0.041115450107452337
r2: 0.601323559687869
pearson: 0.7844150145252797

=== Experiment 2094 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006742494757365754
rmse: 0.08211269547010228
mae: 0.031102026292253386
r2: 0.6959838789892651
pearson: 0.8422556775408888

=== Experiment 2415 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064109117912565065
rmse: 0.08006816965096995
mae: 0.03599520015581955
r2: 0.710934808990307
pearson: 0.844294724832348

=== Experiment 2485 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006275902097891869
rmse: 0.07922059137555001
mae: 0.03150139950668673
r2: 0.7170223366418083
pearson: 0.8503268262155015

=== Experiment 2373 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006169277765229171
rmse: 0.07854475008063347
mae: 0.03102162133075555
r2: 0.7218299808726116
pearson: 0.8538975881338625

=== Experiment 2067 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006432286682316403
rmse: 0.08020153790493299
mae: 0.038043840875917063
r2: 0.7099710245602253
pearson: 0.8442536433866994

=== Experiment 2312 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006539499243723786
rmse: 0.08086717037045246
mae: 0.033667855277568874
r2: 0.7051368573542234
pearson: 0.8420523162477719

=== Experiment 2258 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006157352410704021
rmse: 0.07846879896305296
mae: 0.032488424086244534
r2: 0.7223676898594007
pearson: 0.851207523540067

=== Experiment 2024 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0063203493415316765
rmse: 0.07950062478705232
mae: 0.03350600173636683
r2: 0.7150182299888173
pearson: 0.8501529436226791

=== Experiment 2473 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006534224006940488
rmse: 0.0808345471128557
mae: 0.04249029890270174
r2: 0.7053747154589729
pearson: 0.8443493059675439

=== Experiment 2133 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006914386929070484
rmse: 0.08315279267150613
mae: 0.03383819491864041
r2: 0.6882333366226292
pearson: 0.8300449878453895

=== Experiment 2173 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0066004979027468975
rmse: 0.08124344836814165
mae: 0.03584251359166856
r2: 0.7023864546664342
pearson: 0.8385345598365447

=== Experiment 2327 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007260180563582374
rmse: 0.08520669318535003
mae: 0.04313599465528735
r2: 0.6726416538379056
pearson: 0.8363085685522536

=== Experiment 2206 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005902717568209079
rmse: 0.0768291453044291
mae: 0.02993553593692944
r2: 0.733849062834133
pearson: 0.8606153566048427

=== Experiment 2461 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006728247807682188
rmse: 0.08202589717694156
mae: 0.0376562916993516
r2: 0.6966262676799351
pearson: 0.8379019708893778

=== Experiment 2118 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006233572686476699
rmse: 0.0789529776922739
mae: 0.037967835017152854
r2: 0.7189309511719195
pearson: 0.850518927516771

=== Experiment 2410 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006229091018971661
rmse: 0.07892459071146116
mae: 0.031205475885354616
r2: 0.719133027587189
pearson: 0.8481562028653841

=== Experiment 2356 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006906030429843804
rmse: 0.08310252962361497
mae: 0.03183311215544455
r2: 0.6886101274947838
pearson: 0.8310555187517958

=== Experiment 2185 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006779205791983891
rmse: 0.08233593256886018
mae: 0.038522216742335594
r2: 0.6943285945960954
pearson: 0.8341793506460686

=== Experiment 2270 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006637960318872039
rmse: 0.08147367868748802
mae: 0.03905830010889467
r2: 0.7006972908118219
pearson: 0.839326823556716

=== Experiment 2378 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005989068823153048
rmse: 0.07738907431383998
mae: 0.028245615547090357
r2: 0.7299555227548031
pearson: 0.864278501700028

=== Experiment 2287 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006268299594780184
rmse: 0.07917259371007232
mae: 0.03646679449363246
r2: 0.7173651301609967
pearson: 0.8486638449669918

=== Experiment 2127 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007194035719691106
rmse: 0.08481766160235206
mae: 0.044711404466652865
r2: 0.675624095736391
pearson: 0.8263955349596979

=== Experiment 2447 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006529174752475922
rmse: 0.08080330904409747
mae: 0.03463219675235773
r2: 0.7056023841204334
pearson: 0.8402119599259743

=== Experiment 2151 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006878027349872415
rmse: 0.08293387335626147
mae: 0.034788695261168866
r2: 0.6898727740455957
pearson: 0.831889074606432

=== Experiment 2113 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007630597628143526
rmse: 0.08735329202808287
mae: 0.04610845345715646
r2: 0.6559397114298613
pearson: 0.8161849650351934

=== Experiment 2246 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005647664885036817
rmse: 0.07515094733292998
mae: 0.029163272065712287
r2: 0.7453492760610994
pearson: 0.8660785589385712

=== Experiment 2180 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006215029836522199
rmse: 0.07883546052711432
mae: 0.034481834918138554
r2: 0.7197670401150356
pearson: 0.8484296504197211

=== Experiment 2065 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006531214503652956
rmse: 0.08081592976420525
mae: 0.03553218553703957
r2: 0.7055104126376242
pearson: 0.8430733669510887

=== Experiment 2480 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005619830051774203
rmse: 0.07496552575533771
mae: 0.02875730828635639
r2: 0.7466043364418637
pearson: 0.8664330030384078

=== Experiment 2362 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005488184540228444
rmse: 0.07408228222880585
mae: 0.03066477778564807
r2: 0.7525401746158413
pearson: 0.868092980503157

=== Experiment 2030 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006968959341112986
rmse: 0.08348029313025312
mae: 0.04173633658753641
r2: 0.6857726905827883
pearson: 0.8298257043520946

=== Experiment 2330 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006727892964058253
rmse: 0.08202373415090447
mae: 0.03682511236234719
r2: 0.6966422674227597
pearson: 0.8348366611101337

=== Experiment 2324 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006530371730899472
rmse: 0.0808107154460315
mae: 0.03601487659854642
r2: 0.7055484128901459
pearson: 0.8402102771179455

=== Experiment 2442 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006481148393010057
rmse: 0.08050557988742182
mae: 0.038920429832149495
r2: 0.7077678715307321
pearson: 0.8452378263286914

=== Experiment 2314 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007552361947846749
rmse: 0.0869043264046546
mae: 0.049275029489602704
r2: 0.6594673238202868
pearson: 0.8220205380528761

=== Experiment 2060 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006964273337272677
rmse: 0.08345222188337874
mae: 0.04215111628571994
r2: 0.6859839804334797
pearson: 0.8321592249019347

=== Experiment 2019 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.009082207363360663
rmse: 0.0953006157554119
mae: 0.04764573777305004
r2: 0.5904872673712247
pearson: 0.7739279542582981

=== Experiment 2488 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006391786980271962
rmse: 0.07994865214793781
mae: 0.034947113510923604
r2: 0.7117971383001273
pearson: 0.8447820114750885

=== Experiment 2164 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006232452595157135
rmse: 0.07894588396589866
mae: 0.0306366227551426
r2: 0.7189814555965288
pearson: 0.8586435102438089

=== Experiment 2004 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006788842752843999
rmse: 0.08239443399189049
mae: 0.031063739604544647
r2: 0.6938940682724639
pearson: 0.8332501374716978

=== Experiment 2343 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006013170306200098
rmse: 0.07754463428374718
mae: 0.029411544364608157
r2: 0.7288687974920856
pearson: 0.8604192601343068

=== Experiment 2386 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00700846731914234
rmse: 0.08371658927083891
mae: 0.03787059338956877
r2: 0.6839912932422356
pearson: 0.827890617507521

=== Experiment 2097 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00654081912183772
rmse: 0.08087533073711489
mae: 0.0327899177160394
r2: 0.7050773446309888
pearson: 0.8398911367016936

=== Experiment 2419 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006501374565067076
rmse: 0.08063110172301428
mae: 0.04037435436708925
r2: 0.7068558823349
pearson: 0.8461940926092406

=== Experiment 2433 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006362959351281597
rmse: 0.07976816000937716
mae: 0.038137619726532276
r2: 0.7130969634032929
pearson: 0.8534874353166988

=== Experiment 2374 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060652708823643675
rmse: 0.07787984901349236
mae: 0.03324074767201021
r2: 0.7265196054440558
pearson: 0.8569189205030535

=== Experiment 2471 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006463248798418975
rmse: 0.08039433312379035
mae: 0.03424345002466256
r2: 0.7085749563726298
pearson: 0.8428240052180911

=== Experiment 2161 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006108263226885704
rmse: 0.07815537874571209
mae: 0.038469018302800265
r2: 0.7245810995519666
pearson: 0.8541877479027765

=== Experiment 2079 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007247495000986332
rmse: 0.08513222069807842
mae: 0.04863496024436511
r2: 0.6732136402720182
pearson: 0.8300383969349926

=== Experiment 2467 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005289745258962042
rmse: 0.07273063494128208
mae: 0.02966614068965281
r2: 0.7614877144683365
pearson: 0.8733201667582002

=== Experiment 2236 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006401634981780289
rmse: 0.08001021798358188
mae: 0.03419806077275049
r2: 0.7113530962465537
pearson: 0.8629395111673434

=== Experiment 2181 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006739801266118441
rmse: 0.08209629264539563
mae: 0.03109914229019638
r2: 0.6961053273241103
pearson: 0.8353429818535393

=== Experiment 2472 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006525628265760654
rmse: 0.08078136088084091
mae: 0.032331611105881335
r2: 0.7057622936454966
pearson: 0.845530745284086

=== Experiment 2218 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006504608743869476
rmse: 0.08065115463444697
mae: 0.036297418595662545
r2: 0.7067100546361709
pearson: 0.8421902195644871

=== Experiment 2395 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006901706688140756
rmse: 0.08307651104939805
mae: 0.031962583005919365
r2: 0.68880508310515
pearson: 0.8327540767484127

=== Experiment 2247 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006319214262599439
rmse: 0.07949348566140145
mae: 0.03411328083347515
r2: 0.7150694101982854
pearson: 0.8489983663223128

=== Experiment 2364 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007140430687677838
rmse: 0.08450106915109322
mae: 0.04189782839477954
r2: 0.6780411230364902
pearson: 0.8316529763072721

=== Experiment 2344 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006369516841066034
rmse: 0.07980925285370133
mae: 0.034430617649698604
r2: 0.7128012890750846
pearson: 0.8443124603941704

=== Experiment 2275 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005748434932393657
rmse: 0.07581843398800622
mae: 0.030339146519866213
r2: 0.7408055989780693
pearson: 0.8625513641438419

=== Experiment 2331 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007065185982145653
rmse: 0.08405466068068833
mae: 0.04663895126663064
r2: 0.6814338736912089
pearson: 0.8573905060285034

=== Experiment 2219 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00586234725581471
rmse: 0.07656596669418281
mae: 0.03009163384245469
r2: 0.7356693424516616
pearson: 0.8581753275693335

=== Experiment 2144 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064103113370057975
rmse: 0.08006441991924876
mae: 0.033635966777254785
r2: 0.7109618832081914
pearson: 0.844556235403267

=== Experiment 2109 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005712687281803017
rmse: 0.07558232122529061
mae: 0.033920850691673866
r2: 0.7424174448094569
pearson: 0.8643796422523095

=== Experiment 2273 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006167223465203368
rmse: 0.07853167173315087
mae: 0.036870911760389785
r2: 0.7219226083566084
pearson: 0.8524784565868745

=== Experiment 2365 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061733871963943225
rmse: 0.07857090553375545
mae: 0.038121395362328954
r2: 0.721644688430074
pearson: 0.8532400815577676

=== Experiment 2322 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007333511204623287
rmse: 0.08563592239605577
mae: 0.037539585597810274
r2: 0.6693352075086545
pearson: 0.8286771347260585

=== Experiment 2435 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006494070551054116
rmse: 0.08058579621157885
mae: 0.0389222744601889
r2: 0.7071852171120021
pearson: 0.8421322080140619

=== Experiment 2178 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005709935922122101
rmse: 0.07556411795370936
mae: 0.03378130214874588
r2: 0.7425415024065034
pearson: 0.8623303641201128

=== Experiment 2326 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006794898857680188
rmse: 0.0824311764909381
mae: 0.03216377563308837
r2: 0.6936210011708956
pearson: 0.8334496729220325

=== Experiment 2363 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.009007819301688893
rmse: 0.09490953219613346
mae: 0.041686947329860974
r2: 0.593841392331315
pearson: 0.7735363341440342

=== Experiment 2102 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006047783348749335
rmse: 0.07776749545118021
mae: 0.032293527085560436
r2: 0.7273081106379055
pearson: 0.8533371571446241

=== Experiment 2302 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006442412737608533
rmse: 0.08026464188924369
mae: 0.03329549999644244
r2: 0.709514445183921
pearson: 0.8447150408977243

=== Experiment 2296 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00734655043768235
rmse: 0.08571202038035476
mae: 0.038066495623529734
r2: 0.6687472742290266
pearson: 0.8182299007185829

=== Experiment 2045 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005882443209882933
rmse: 0.0766970873624477
mae: 0.0312938956599324
r2: 0.7347632247276311
pearson: 0.8581718050267326

=== Experiment 2420 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005459114414797668
rmse: 0.0738858201199504
mae: 0.029153018208735185
r2: 0.7538509337767743
pearson: 0.8685158965184949

=== Experiment 2179 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006880875539151368
rmse: 0.08295104302630177
mae: 0.03764766371853941
r2: 0.6897443504445916
pearson: 0.8314809945908689

=== Experiment 2409 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006930311740841453
rmse: 0.08324849392536451
mae: 0.03392116122960082
r2: 0.6875152938689221
pearson: 0.8322915400792291

=== Experiment 2394 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006471880815357824
rmse: 0.08044800069211058
mae: 0.03361222185702071
r2: 0.7081857425280984
pearson: 0.8430387405667135

=== Experiment 2430 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005670960214292867
rmse: 0.07530577809366866
mae: 0.029451509319526573
r2: 0.7442988999180027
pearson: 0.8630649388569706

=== Experiment 2007 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00611287508730513
rmse: 0.07818487761265046
mae: 0.032858606896012804
r2: 0.7243731527954896
pearson: 0.8515230751547692

=== Experiment 2190 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006643409161587413
rmse: 0.0815071111105492
mae: 0.04100484850987588
r2: 0.7004516048920048
pearson: 0.8490138883990868

=== Experiment 2498 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006506479603098761
rmse: 0.08066275226583061
mae: 0.036480006972529386
r2: 0.7066256984169506
pearson: 0.8486335365577878

=== Experiment 2285 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.02055575231868395
rmse: 0.14337277397987372
mae: 0.06489364034963488
r2: 0.0731501752290179
pearson: 0.2802612758483483

=== Experiment 2293 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005947193511099448
rmse: 0.07711804919147948
mae: 0.028996639124595668
r2: 0.7318436621445656
pearson: 0.859792992916717

=== Experiment 2427 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0069114191258425085
rmse: 0.08313494527479108
mae: 0.03730358811385141
r2: 0.6883671535639487
pearson: 0.8297604061284405

=== Experiment 2235 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0077502229312355975
rmse: 0.08803535046352458
mae: 0.04162720701954261
r2: 0.6505458591645621
pearson: 0.8258639286320354

=== Experiment 2142 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005785843287514283
rmse: 0.07606473090410748
mae: 0.02969935593829883
r2: 0.7391188727103563
pearson: 0.8600291966972856

=== Experiment 2421 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006066223616437084
rmse: 0.07788596546514066
mae: 0.036223699745409677
r2: 0.7264766470840471
pearson: 0.8551574214296251

=== Experiment 2468 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006507985532041968
rmse: 0.08067208644904363
mae: 0.029923317583237255
r2: 0.7065577967437113
pearson: 0.8420785958644168

=== Experiment 2303 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007019411301528736
rmse: 0.08378192705786097
mae: 0.03540781217720959
r2: 0.683497833893248
pearson: 0.8420585715652336

=== Experiment 2460 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005869785432718866
rmse: 0.07661452494611493
mae: 0.028579618823883376
r2: 0.7353339583288452
pearson: 0.8606344089423765

=== Experiment 2117 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006211830046281933
rmse: 0.07881516380926917
mae: 0.028966585844613424
r2: 0.7199113172486338
pearson: 0.855743218813258

=== Experiment 2086 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005487518808621318
rmse: 0.07407778890208129
mae: 0.027901314178146064
r2: 0.7525701921609957
pearson: 0.8691037261350408

=== Experiment 2252 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063290340196782945
rmse: 0.0795552262247949
mae: 0.03011952709475308
r2: 0.7146266416736053
pearson: 0.8681062600455673

=== Experiment 2348 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006283059178287682
rmse: 0.07926575034835463
mae: 0.03675114086566395
r2: 0.7166996270368331
pearson: 0.8479675580107612

=== Experiment 2150 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006138638233858536
rmse: 0.07834946224358234
mae: 0.030917688091555134
r2: 0.7232115038566245
pearson: 0.8540381024468979

=== Experiment 2429 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005877522620413098
rmse: 0.0766650025788371
mae: 0.029651508939477864
r2: 0.7349850919411092
pearson: 0.861063245452285

=== Experiment 2367 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006421613259140918
rmse: 0.0801349690156608
mae: 0.031537832753660414
r2: 0.7104522845134751
pearson: 0.8488179036079412

=== Experiment 2381 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006271626889599061
rmse: 0.07919360384272875
mae: 0.029309756316018527
r2: 0.717215103902068
pearson: 0.878040383119792

=== Experiment 2333 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005960614950947639
rmse: 0.07720501894920846
mae: 0.029174653242603658
r2: 0.7312384953290381
pearson: 0.8561651481744119

=== Experiment 2220 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005781105304270638
rmse: 0.07603358010951897
mae: 0.030781011056933115
r2: 0.7393325062894665
pearson: 0.8600903352698004

=== Experiment 2387 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061346774479747925
rmse: 0.07832418175745466
mae: 0.02786636443353734
r2: 0.7233900939488473
pearson: 0.8663863453156744

=== Experiment 2223 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005947092143607363
rmse: 0.07711739196580343
mae: 0.030253569959938553
r2: 0.7318482327601683
pearson: 0.8564300561053098

=== Experiment 2082 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005498287772936131
rmse: 0.0741504401398679
mae: 0.028735677918585906
r2: 0.7520846242998245
pearson: 0.8682344578280196

=== Experiment 2502 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007175986715074456
rmse: 0.08471119592518132
mae: 0.032246950620709085
r2: 0.6764379174105811
pearson: 0.8444067564444339

=== Experiment 2514 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0069167849484124055
rmse: 0.0831672107769186
mae: 0.04057207999427389
r2: 0.688125210985951
pearson: 0.8310913189134713

=== Experiment 2500 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0070481865300195245
rmse: 0.08395347836760264
mae: 0.03643998683132122
r2: 0.6822003715055451
pearson: 0.8260184411329585

=== Experiment 2542 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006952684769626283
rmse: 0.08338276062608076
mae: 0.03846301645739543
r2: 0.686506503848137
pearson: 0.8296574036967009

=== Experiment 2513 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005746796670445228
rmse: 0.07580762936832432
mae: 0.029348999758141887
r2: 0.7408794674882697
pearson: 0.861236726129199

=== Experiment 2516 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007142420613772048
rmse: 0.08451284289249798
mae: 0.04252479334277504
r2: 0.6779513981447359
pearson: 0.8252787802100707

=== Experiment 2501 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.008228436390270691
rmse: 0.09071072919049153
mae: 0.05403484937526141
r2: 0.6289834247745092
pearson: 0.8120559046213445

=== Experiment 2545 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007418412904886714
rmse: 0.08613020901453051
mae: 0.04410970676242871
r2: 0.6655070272118724
pearson: 0.8208740050218469

=== Experiment 2515 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007094698170695731
rmse: 0.08423003128751486
mae: 0.04771480193821871
r2: 0.6801031820987933
pearson: 0.8372528755741958

=== Experiment 2577 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00861022294208969
rmse: 0.0927912869944678
mae: 0.04026388553489715
r2: 0.6117688371901024
pearson: 0.7975392892234583

=== Experiment 2511 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0057384058048605005
rmse: 0.07575226600479025
mae: 0.029613302007661775
r2: 0.7412578079243826
pearson: 0.8614919179345248

=== Experiment 2531 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007296850801631657
rmse: 0.08542160617567231
mae: 0.04416686123330658
r2: 0.6709882089440697
pearson: 0.8221209781965155

=== Experiment 2541 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007243980122012998
rmse: 0.08511157454784278
mae: 0.035628112793941985
r2: 0.6733721246179092
pearson: 0.8356545041597537

=== Experiment 2572 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007663031351870303
rmse: 0.08753874200529901
mae: 0.038614499612357356
r2: 0.6544772891021945
pearson: 0.8251199242257575

=== Experiment 2566 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006481279470843407
rmse: 0.08050639397490988
mae: 0.03826061312478059
r2: 0.7077619612889181
pearson: 0.8430904955779752

=== Experiment 2571 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007683905350472868
rmse: 0.0876578881246455
mae: 0.038695445904712834
r2: 0.6535360896925544
pearson: 0.8216000046433207

=== Experiment 2546 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007861787740748055
rmse: 0.08866672284881208
mae: 0.047914330738522456
r2: 0.6455154510070731
pearson: 0.822328823152828

=== Experiment 2592 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006585980395244144
rmse: 0.08115405347389706
mae: 0.03565079710344725
r2: 0.7030410426900897
pearson: 0.8390583585634493

=== Experiment 2521 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007843786981704343
rmse: 0.08856515670230784
mae: 0.05035116046286458
r2: 0.6463270973096142
pearson: 0.8152987090860023

=== Experiment 2539 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007289508899793554
rmse: 0.08537862085905086
mae: 0.04191763449672267
r2: 0.6713192520665316
pearson: 0.8225989359572033

=== Experiment 2645 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006925092768918996
rmse: 0.08321714227801262
mae: 0.03968609352735555
r2: 0.6877506150158603
pearson: 0.8331215381962855

=== Experiment 2519 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005990936315002452
rmse: 0.07740113897742366
mae: 0.033658769217680226
r2: 0.7298713183692594
pearson: 0.8543723219208678

=== Experiment 2633 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006740167952172857
rmse: 0.08209852588306843
mae: 0.03751613514894197
r2: 0.6960887936113076
pearson: 0.8345170287235901

=== Experiment 2622 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007327650369696436
rmse: 0.0856016960678726
mae: 0.04416877543719699
r2: 0.6695994699759553
pearson: 0.8217084726265412

=== Experiment 2662 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007924039596070118
rmse: 0.08901707474451245
mae: 0.038527094210455824
r2: 0.6427085422497383
pearson: 0.817912887889588

=== Experiment 2532 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007486158016599538
rmse: 0.08652258674241968
mae: 0.04676270927520296
r2: 0.6624524299416448
pearson: 0.8313372815602716

=== Experiment 2588 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007275821843182896
rmse: 0.08529842814016503
mae: 0.04585481538393127
r2: 0.6719363954249719
pearson: 0.8253762370740432

=== Experiment 2565 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00709212548507766
rmse: 0.08421475811921364
mae: 0.03759066711340759
r2: 0.6802191833609867
pearson: 0.8256253791901661

=== Experiment 2537 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006316220074018081
rmse: 0.07947465051208517
mae: 0.03119514641089938
r2: 0.7152044168435756
pearson: 0.8471804529964454

=== Experiment 2560 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005973687246253487
rmse: 0.07728963220415457
mae: 0.03332943047322051
r2: 0.7306490712872545
pearson: 0.8551073926987104

=== Experiment 2525 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00696278286545841
rmse: 0.0834432913148709
mae: 0.042100524710421174
r2: 0.6860511851515783
pearson: 0.8315604326100631

=== Experiment 2607 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065685063518329395
rmse: 0.08104632225976044
mae: 0.03979214180115401
r2: 0.7038289396165862
pearson: 0.8411399198535201

=== Experiment 2544 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007424409823024002
rmse: 0.08616501507586477
mae: 0.036494372951248674
r2: 0.6652366288125076
pearson: 0.827545864688

=== Experiment 2524 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007482492300151546
rmse: 0.08650140056757201
mae: 0.04374054856882038
r2: 0.66261771548288
pearson: 0.8184683552598646

=== Experiment 2647 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007832877219011656
rmse: 0.0885035435392937
mae: 0.038994020449555376
r2: 0.6468190137076737
pearson: 0.8241642076707714

=== Experiment 2517 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006630289326559736
rmse: 0.08142658857252794
mae: 0.03226915029057224
r2: 0.7010431724789217
pearson: 0.8378120006603066

=== Experiment 2618 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005466504775835356
rmse: 0.07393581524427356
mae: 0.029455744410078075
r2: 0.7535177056503316
pearson: 0.8682824925679286

=== Experiment 2604 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006598702379010184
rmse: 0.08123239734865753
mae: 0.04156095796772701
r2: 0.7024674140414522
pearson: 0.8426776033031449

=== Experiment 2529 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006112162235284079
rmse: 0.07818031872078854
mae: 0.029978601254595567
r2: 0.7244052949793687
pearson: 0.857675392149408

=== Experiment 2632 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006413535165318953
rmse: 0.08008455010374319
mae: 0.03583552587283771
r2: 0.7108165222084666
pearson: 0.8447087802178669

=== Experiment 2599 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00646554419829978
rmse: 0.08040860773760344
mae: 0.03350657987808701
r2: 0.708471457802287
pearson: 0.8418005178949931

=== Experiment 2558 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007629206894592555
rmse: 0.08734533126957934
mae: 0.04973564148711256
r2: 0.6560024189935658
pearson: 0.8320750374278624

=== Experiment 2591 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007115795778844256
rmse: 0.08435517636069677
mae: 0.045198223844108334
r2: 0.6791519002331439
pearson: 0.830665942791269

=== Experiment 2578 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006346785135202342
rmse: 0.07966671284295808
mae: 0.03588572008999202
r2: 0.7138262516874896
pearson: 0.8515839803471268

=== Experiment 2551 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007648096725610954
rmse: 0.08745339745036183
mae: 0.05160886947735748
r2: 0.6551506848269968
pearson: 0.8240344729925453

=== Experiment 2603 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005937726928228945
rmse: 0.07705664752783464
mae: 0.03525215120894435
r2: 0.7322705061996347
pearson: 0.8560431844792193

=== Experiment 2683 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006864510780993468
rmse: 0.08285234324383027
mae: 0.03877176907192374
r2: 0.6904822301872505
pearson: 0.8340371209201999

=== Experiment 2642 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006997453498577715
rmse: 0.08365078301234075
mae: 0.035949128844816526
r2: 0.6844879015639415
pearson: 0.8375452009421596

=== Experiment 2670 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0058652341454186055
rmse: 0.07658481667679701
mae: 0.03293118592559509
r2: 0.7355391738700392
pearson: 0.8634515816725029

=== Experiment 2689 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007065204240108827
rmse: 0.08405476928829694
mae: 0.03774579686037843
r2: 0.6814330504476853
pearson: 0.826676015045837

=== Experiment 2530 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007377455999695454
rmse: 0.08589211837936851
mae: 0.041389947026328255
r2: 0.6673537560404337
pearson: 0.8300088449723043

=== Experiment 2624 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007049574171559552
rmse: 0.08396174230897993
mae: 0.043421658209396134
r2: 0.6821378033592529
pearson: 0.8375988231115998

=== Experiment 2553 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006720571403272584
rmse: 0.08197909125668926
mae: 0.039648809843492346
r2: 0.6969723933761202
pearson: 0.8373144244673149

=== Experiment 2561 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007239808416560796
rmse: 0.08508706374391348
mae: 0.03682758116622295
r2: 0.6735602249806412
pearson: 0.8290738114520503

=== Experiment 2690 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006525289177006397
rmse: 0.08077926204791919
mae: 0.03617314621410325
r2: 0.7057775830081818
pearson: 0.8408192844342222

=== Experiment 2663 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006179009899558165
rmse: 0.07860667846664281
mae: 0.028879786975296647
r2: 0.7213911632191572
pearson: 0.871330620837463

=== Experiment 2643 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00648417334107142
rmse: 0.08052436489082929
mae: 0.03877168354221888
r2: 0.7076314779540265
pearson: 0.8433324411730155

=== Experiment 2650 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00757501944245958
rmse: 0.08703458762158628
mae: 0.03909842956721936
r2: 0.6584457073605203
pearson: 0.8125891630054303

=== Experiment 2680 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007157725137578057
rmse: 0.08460333999067683
mae: 0.03421741093192845
r2: 0.6772613239023588
pearson: 0.8231033117103594

=== Experiment 2713 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007195239188613625
rmse: 0.08482475575333905
mae: 0.033521855748017845
r2: 0.6755698318523617
pearson: 0.8279155278482321

=== Experiment 2608 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00726141993245433
rmse: 0.08521396559516714
mae: 0.047728849790080345
r2: 0.6725857712409573
pearson: 0.8328937112972683

=== Experiment 2686 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006892325863546593
rmse: 0.08302003290499585
mae: 0.03745508323484328
r2: 0.6892280603572841
pearson: 0.8335479159744476

=== Experiment 2573 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005757623352058388
rmse: 0.07587900468547534
mae: 0.02978353832971918
r2: 0.7403912971795192
pearson: 0.8630658155292893

=== Experiment 2568 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006839495980635183
rmse: 0.08270124533908291
mae: 0.03716701894511725
r2: 0.6916101365255499
pearson: 0.8320165125904666

=== Experiment 2581 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006740471835757825
rmse: 0.0821003765871864
mae: 0.03428980763074576
r2: 0.6960750916342108
pearson: 0.8443771811738675

=== Experiment 2702 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006705293122227033
rmse: 0.08188585422542183
mae: 0.03631659828246178
r2: 0.6976612843439782
pearson: 0.836095147981685

=== Experiment 2744 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006835282476892395
rmse: 0.08267576716845387
mae: 0.04081737418933712
r2: 0.6918001215548073
pearson: 0.8337169047065296

=== Experiment 2658 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006783586792887846
rmse: 0.08236253270078481
mae: 0.0423627376337338
r2: 0.6941310571935619
pearson: 0.8378131162537611

=== Experiment 2667 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067046505687296054
rmse: 0.08188193066073617
mae: 0.039646542228791375
r2: 0.6976902567983679
pearson: 0.8361282706755927

=== Experiment 2751 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.008280729548951156
rmse: 0.09099851399309307
mae: 0.03847047960132359
r2: 0.6266255492655863
pearson: 0.8100645998613052

=== Experiment 2697 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006616800960000094
rmse: 0.08134372108528165
mae: 0.04115030584414841
r2: 0.7016513569904118
pearson: 0.840421086641339

=== Experiment 2627 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007411253164129559
rmse: 0.08608863551090562
mae: 0.03993729214898737
r2: 0.6658298567713696
pearson: 0.8223185867789651

=== Experiment 2527 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006700782718250029
rmse: 0.08185830879177769
mae: 0.03984636407591261
r2: 0.6978646564741209
pearson: 0.8371974279827069

=== Experiment 2579 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006149697389736025
rmse: 0.07842000631048192
mae: 0.032593621813922384
r2: 0.7227128513856154
pearson: 0.8560637930804552

=== Experiment 2722 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007466293335457725
rmse: 0.08640771571716108
mae: 0.04090291740039433
r2: 0.6633481196712145
pearson: 0.8263513565962598

=== Experiment 2687 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006201658826900616
rmse: 0.07875061159699406
mae: 0.03349912539082235
r2: 0.7203699330538413
pearson: 0.8493733927576163

=== Experiment 2749 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0071652084382866615
rmse: 0.08464755423688662
mae: 0.04085680266523532
r2: 0.6769239051670604
pearson: 0.823896632815134

=== Experiment 2555 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006163019505695905
rmse: 0.07850490115716283
mae: 0.03268904343992441
r2: 0.7221121630404961
pearson: 0.8508746687227605

=== Experiment 2753 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007317085633945471
rmse: 0.08553996512709992
mae: 0.0395083058623671
r2: 0.6700758292610706
pearson: 0.8193269628033981

=== Experiment 2506 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007796643565174204
rmse: 0.0882986045482838
mae: 0.039842400803192714
r2: 0.6484527732115539
pearson: 0.8293427421222459

=== Experiment 2550 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005430691502031132
rmse: 0.07369322561830993
mae: 0.030119960599657714
r2: 0.7551325104035369
pearson: 0.8690249848619014

=== Experiment 2623 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006692973706351424
rmse: 0.08181059654073806
mae: 0.04479232705665287
r2: 0.6982167613836199
pearson: 0.8429011703983779

=== Experiment 2598 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005853921926987207
rmse: 0.07651092684700145
mae: 0.03258412721593669
r2: 0.7360492368202229
pearson: 0.8587385317891425

=== Experiment 2742 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007214407212667615
rmse: 0.08493766663069816
mae: 0.04118760263988571
r2: 0.6747055540842597
pearson: 0.8234192205179562

=== Experiment 2669 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064061490942867024
rmse: 0.08003842261243473
mae: 0.03247550446008069
r2: 0.7111495569004522
pearson: 0.8567686958488652

=== Experiment 2520 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007440569501117899
rmse: 0.08625873579596387
mae: 0.03540897530940244
r2: 0.664507996039672
pearson: 0.8323574132844223

=== Experiment 2621 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005623976547995458
rmse: 0.07499317667625141
mae: 0.030649852485050776
r2: 0.7464173727522598
pearson: 0.8650716765815702

=== Experiment 2754 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007131248475738187
rmse: 0.08444671974528192
mae: 0.03585788434614745
r2: 0.6784551449314484
pearson: 0.8272857842002033

=== Experiment 2597 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005771628670828524
rmse: 0.0759712358121712
mae: 0.03975729090319103
r2: 0.7397598035203861
pearson: 0.8665746304993868

=== Experiment 2767 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064315406138051485
rmse: 0.08019688655929948
mae: 0.03574591347688212
r2: 0.7100046644610281
pearson: 0.8428577460288376

=== Experiment 2710 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006882354111674033
rmse: 0.0829599548678399
mae: 0.04056526913951671
r2: 0.6896776822603141
pearson: 0.8320162709997139

=== Experiment 2554 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006800444547438555
rmse: 0.0824648079330726
mae: 0.038386542561490676
r2: 0.6933709484605356
pearson: 0.8417752813731846

=== Experiment 2538 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005850325743092604
rmse: 0.0764874221234616
mae: 0.03126095085882698
r2: 0.736211387169229
pearson: 0.8581108170191946

=== Experiment 2676 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006286409337492725
rmse: 0.07928687998334104
mae: 0.03799991385434569
r2: 0.7165485698327954
pearson: 0.8559206081764973

=== Experiment 2675 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064359379460688345
rmse: 0.08022429772873574
mae: 0.03923101214857038
r2: 0.7098063906846723
pearson: 0.8462165794769744

=== Experiment 2777 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007547832505077411
rmse: 0.0868782625578885
mae: 0.038854043823094465
r2: 0.6596715544011962
pearson: 0.8256792279138743

=== Experiment 2798 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006600534864498003
rmse: 0.08124367584309565
mae: 0.03598391590901957
r2: 0.7023847880773451
pearson: 0.8386409454834957

=== Experiment 2762 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006507444239991868
rmse: 0.08066873148867452
mae: 0.039224895744190726
r2: 0.7065822033640095
pearson: 0.841568260431255

=== Experiment 2628 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0070883356967535215
rmse: 0.08419225437505234
mae: 0.03787896236868308
r2: 0.6803900632485087
pearson: 0.8255494544534185

=== Experiment 2631 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007847410449095156
rmse: 0.08858561084676876
mae: 0.043074662251057695
r2: 0.6461637167597725
pearson: 0.8236498478528264

=== Experiment 2731 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005883785758561847
rmse: 0.07670583914254407
mae: 0.027513587611319473
r2: 0.7347026897985998
pearson: 0.8672575163211386

=== Experiment 2629 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0076004453578710825
rmse: 0.08718053313596495
mae: 0.04307823720038387
r2: 0.6572992640254168
pearson: 0.821009789562659

=== Experiment 2716 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005929214846626741
rmse: 0.0770013950953276
mae: 0.03332863043356093
r2: 0.7326543122126246
pearson: 0.8564937517788044

=== Experiment 2794 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007914557276588362
rmse: 0.08896379756163944
mae: 0.039777524266340805
r2: 0.6431360958616825
pearson: 0.8093406886710696

=== Experiment 2801 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00654932474213725
rmse: 0.08092789841666007
mae: 0.03806954212763903
r2: 0.7046938299552927
pearson: 0.8400186251973393

=== Experiment 2528 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068681876736500025
rmse: 0.08287452970394464
mae: 0.03214070281897359
r2: 0.6903164407156914
pearson: 0.8329464044377186

=== Experiment 2763 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0075005392396141676
rmse: 0.08660565362384934
mae: 0.04382272740293872
r2: 0.6618039869255758
pearson: 0.828033425823521

=== Experiment 2766 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007177620091906941
rmse: 0.08472083623233981
mae: 0.04070403691281362
r2: 0.6763642691681085
pearson: 0.830851372643598

=== Experiment 2774 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0057292438889326325
rmse: 0.07569176896421852
mae: 0.03005697931308275
r2: 0.7416709146811027
pearson: 0.8614058559314003

=== Experiment 2714 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006158518035111485
rmse: 0.07847622592296016
mae: 0.028437242308162195
r2: 0.7223151323678987
pearson: 0.8509723405929424

=== Experiment 2723 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006911159964088636
rmse: 0.083133386578971
mae: 0.03766973867598744
r2: 0.6883788390533649
pearson: 0.8298235938551765

=== Experiment 2787 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006740358647556032
rmse: 0.08209968725614021
mae: 0.03825292487525012
r2: 0.6960801952404116
pearson: 0.8346969725141588

=== Experiment 2586 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006500900842524369
rmse: 0.08062816407759989
mae: 0.03572622230814099
r2: 0.7068772422758495
pearson: 0.8418010962139827

=== Experiment 2696 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007062866485165935
rmse: 0.08404086199680448
mae: 0.03494498001583033
r2: 0.6815384587891358
pearson: 0.8396505065764197

=== Experiment 2789 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007544901290647491
rmse: 0.0868613912543858
mae: 0.03778617490184118
r2: 0.659803721569715
pearson: 0.8148382455955557

=== Experiment 2793 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007282121145961878
rmse: 0.08533534523256983
mae: 0.036347494508108404
r2: 0.6716523626352027
pearson: 0.8197439406311859

=== Experiment 2780 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006953432837817802
rmse: 0.08338724625395542
mae: 0.04271596320139856
r2: 0.6864727737826315
pearson: 0.8320774816910871

=== Experiment 2841 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007678183706472573
rmse: 0.08762524582831464
mae: 0.038328822496286996
r2: 0.6537940761022394
pearson: 0.8118605369965054

=== Experiment 2503 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007385496215359793
rmse: 0.08593890978689335
mae: 0.042610717203724585
r2: 0.6669912262548983
pearson: 0.8244172008891882

=== Experiment 2840 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006508975160799472
rmse: 0.08067821986632744
mae: 0.03630870078973272
r2: 0.7065131748186044
pearson: 0.8437107852751752

=== Experiment 2678 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006546446532456748
rmse: 0.08091011390708054
mae: 0.03779400540886693
r2: 0.7048236071629896
pearson: 0.8411811403005621

=== Experiment 2637 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007339512583183939
rmse: 0.0856709553068246
mae: 0.0408235075978047
r2: 0.6690646079907632
pearson: 0.8202275404182527

=== Experiment 2809 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007866796539207197
rmse: 0.08869496343765636
mae: 0.05515764252302955
r2: 0.6452896064890914
pearson: 0.8287436861724709

=== Experiment 2510 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062414028969095285
rmse: 0.07900254993928695
mae: 0.03557593521465938
r2: 0.7185778904298425
pearson: 0.8482975069082755

=== Experiment 2654 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007610824102309495
rmse: 0.0872400372667819
mae: 0.042304158305914906
r2: 0.6568312910067235
pearson: 0.8145229476126632

=== Experiment 2748 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005676669873042001
rmse: 0.07534367838805059
mae: 0.031050073424757768
r2: 0.7440414539180172
pearson: 0.8625884624099146

=== Experiment 2605 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007449321904287263
rmse: 0.08630945431577738
mae: 0.03698199751016743
r2: 0.6641133540329927
pearson: 0.8152775764213812

=== Experiment 2827 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007237917588954109
rmse: 0.08507595188391436
mae: 0.04355630398372939
r2: 0.6736454815652096
pearson: 0.8278920302716097

=== Experiment 2665 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006220016572279596
rmse: 0.07886708167720925
mae: 0.03197980916673097
r2: 0.7195421903945647
pearson: 0.8561343103312345

=== Experiment 2833 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006912364588378991
rmse: 0.0831406313927131
mae: 0.03716089049362029
r2: 0.6883245230743087
pearson: 0.8323038090529015

=== Experiment 2620 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005903566763823377
rmse: 0.07683467162566245
mae: 0.03112778055460416
r2: 0.7338107729776437
pearson: 0.8571511228889934

=== Experiment 2775 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006104967686728516
rmse: 0.078134292642402
mae: 0.035547323983027034
r2: 0.7247296940071761
pearson: 0.8657372257625692

=== Experiment 2580 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007271938021702143
rmse: 0.08527565902238542
mae: 0.04102292141025893
r2: 0.6721115152261383
pearson: 0.833444242599331

=== Experiment 2747 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005915054846687441
rmse: 0.07690939374801652
mae: 0.028360637309912683
r2: 0.733292780377595
pearson: 0.8684102828315831

=== Experiment 2783 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006746998948333442
rmse: 0.08214011777647656
mae: 0.03851514021241237
r2: 0.6957807870009781
pearson: 0.8349471755731204

=== Experiment 2610 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008039217103653913
rmse: 0.08966168135638497
mae: 0.05580173851566724
r2: 0.6375152391262827
pearson: 0.820778501472337

=== Experiment 2657 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0071374649577039435
rmse: 0.08448351885251906
mae: 0.04929423541399428
r2: 0.6781748464956956
pearson: 0.8358439453190624

=== Experiment 2589 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007491267829753323
rmse: 0.08655211048699692
mae: 0.048040356599780426
r2: 0.6622220307155415
pearson: 0.8256230715019182

=== Experiment 2806 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006279500353690786
rmse: 0.07924329847810972
mae: 0.03706760639726532
r2: 0.7168600928715485
pearson: 0.8477188977252823

=== Experiment 2673 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0068783534309581045
rmse: 0.0829358392431047
mae: 0.037430252674771035
r2: 0.6898580711929612
pearson: 0.832218595942976

=== Experiment 2804 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005973169370264755
rmse: 0.07728628190218984
mae: 0.03379117518894342
r2: 0.7306724220876517
pearson: 0.8564260769489977

=== Experiment 2864 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006637767642335728
rmse: 0.08147249623238341
mae: 0.035549213065241274
r2: 0.7007059785120406
pearson: 0.8375883593528404

=== Experiment 2786 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00547819564034921
rmse: 0.07401483392097297
mae: 0.030960386271164046
r2: 0.7529905697149448
pearson: 0.8678124810166967

=== Experiment 2826 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006715052493998445
rmse: 0.08194542387466457
mae: 0.04177738047094205
r2: 0.6972212385662333
pearson: 0.8386764658209367

=== Experiment 2738 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006746050926776503
rmse: 0.08213434681530317
mae: 0.04225175904248665
r2: 0.6958235328757245
pearson: 0.8378270551806891

=== Experiment 2851 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00659596015469023
rmse: 0.08121551671134174
mae: 0.036454344388300905
r2: 0.7025910597290936
pearson: 0.838680185384332

=== Experiment 2778 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006539002881557351
rmse: 0.08086410131546229
mae: 0.03761584398030388
r2: 0.7051592381058421
pearson: 0.8412761930895358

=== Experiment 2739 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006609733838201519
rmse: 0.08130026960718838
mae: 0.036514712796912895
r2: 0.7019700104018314
pearson: 0.8395923396516326

=== Experiment 2671 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061181928814335925
rmse: 0.07821887803742517
mae: 0.03099143488861009
r2: 0.7241333757987776
pearson: 0.8516341226277389

=== Experiment 2596 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006863361919899342
rmse: 0.08284540976963867
mae: 0.03573804249815417
r2: 0.6905340318283322
pearson: 0.8322470929027364

=== Experiment 2575 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005958861076469641
rmse: 0.07719365956132435
mae: 0.0323551823506697
r2: 0.7313175767573117
pearson: 0.8573580777625532

=== Experiment 2825 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0070607292209025705
rmse: 0.08402814540915782
mae: 0.04897314674530094
r2: 0.681634827094089
pearson: 0.8392988299790984

=== Experiment 2664 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007781841366771469
rmse: 0.08821474574452658
mae: 0.0428083717374945
r2: 0.6491201978225873
pearson: 0.8180620633660771

=== Experiment 2784 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005616951214431805
rmse: 0.07494632222085221
mae: 0.02759652775196522
r2: 0.7467341419505592
pearson: 0.8661342245964734

=== Experiment 2505 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007813429107379933
rmse: 0.08839360331709492
mae: 0.044010452784110164
r2: 0.6476959205013797
pearson: 0.8159338310764455

=== Experiment 2729 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007035502505650451
rmse: 0.08387790236796848
mae: 0.038311782318806685
r2: 0.6827722885816805
pearson: 0.8282978383023502

=== Experiment 2761 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065350936757761395
rmse: 0.08083992624796327
mae: 0.034405733390937845
r2: 0.705335502473944
pearson: 0.8434347443853902

=== Experiment 2619 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007100086031561844
rmse: 0.08426200823361525
mae: 0.032877205600116255
r2: 0.6798602458237193
pearson: 0.8301279772618793

=== Experiment 2799 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007214297384887083
rmse: 0.0849370201083549
mae: 0.03793788460557122
r2: 0.6747105061705452
pearson: 0.8218769534704166

=== Experiment 2693 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006625026640116687
rmse: 0.08139426662926995
mae: 0.045520633340652136
r2: 0.7012804646943533
pearson: 0.8467197148640402

=== Experiment 2808 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007766567952794549
rmse: 0.08812813371900342
mae: 0.045650010549579643
r2: 0.6498088693364632
pearson: 0.812209373561699

=== Experiment 2917 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006985988060797919
rmse: 0.08358222335399985
mae: 0.03880427116647739
r2: 0.6850048731071074
pearson: 0.8318718018528912

=== Experiment 2797 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006020460045778985
rmse: 0.07759162355421483
mae: 0.032645740656322204
r2: 0.7285401063429331
pearson: 0.86359246120295

=== Experiment 2736 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005705283655568126
rmse: 0.07553332811129221
mae: 0.029689537631547955
r2: 0.742751271057803
pearson: 0.8756795141492392

=== Experiment 2726 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007378022481252788
rmse: 0.0858954159501704
mae: 0.04493632812902568
r2: 0.6673282136363412
pearson: 0.8362216349519594

=== Experiment 2819 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006227811702925542
rmse: 0.07891648562198865
mae: 0.03226756018138385
r2: 0.7191907113846385
pearson: 0.8501976131226945

=== Experiment 2848 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068493172910660685
rmse: 0.08276060228796107
mae: 0.03926350575342026
r2: 0.6911672979609107
pearson: 0.8333099329381469

=== Experiment 2805 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0056154931374848205
rmse: 0.07493659411452339
mae: 0.029399655744839184
r2: 0.74679988599835
pearson: 0.8644872219974501

=== Experiment 2724 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007193855595057353
rmse: 0.08481659976123396
mae: 0.0470562190605957
r2: 0.6756322174768479
pearson: 0.8356215283074385

=== Experiment 2865 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007418655507756235
rmse: 0.08613161735249278
mae: 0.046612395891683374
r2: 0.6654960883552633
pearson: 0.827758906611599

=== Experiment 2863 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005554328785705041
rmse: 0.07452736937330501
mae: 0.02969950554823903
r2: 0.7495577596996104
pearson: 0.8681773207575212

=== Experiment 2905 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0061461777256615075
rmse: 0.07839756198799493
mae: 0.03025400186904699
r2: 0.7228715514896127
pearson: 0.8515919107506159

=== Experiment 2776 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0070568728634152885
rmse: 0.08400519545489606
mae: 0.03650381047692429
r2: 0.6818087085558776
pearson: 0.826592610140596

=== Experiment 2954 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007836772159215055
rmse: 0.08852554523534467
mae: 0.0370568588369552
r2: 0.6466433925681967
pearson: 0.8275657664073139

=== Experiment 2649 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006999586110201714
rmse: 0.08366352915220415
mae: 0.034538430726517945
r2: 0.6843917430444495
pearson: 0.8436512043638307

=== Experiment 2913 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007059092984476827
rmse: 0.08401840860476248
mae: 0.03975078617621015
r2: 0.681708604274363
pearson: 0.8270477326830323

=== Experiment 2634 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00694795622753574
rmse: 0.08335440136870842
mae: 0.036450092656918745
r2: 0.6867197117298091
pearson: 0.8297608909753992

=== Experiment 2705 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063868306129395355
rmse: 0.07991764894527076
mae: 0.03686242281055197
r2: 0.7120206187216822
pearson: 0.8462983901586845

=== Experiment 2715 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006753091570967283
rmse: 0.08217719617367876
mae: 0.03168544120665971
r2: 0.6955060733279865
pearson: 0.8341337234266681

=== Experiment 2695 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00587717037360067
rmse: 0.07666270523273144
mae: 0.03450042228801528
r2: 0.7350009745948456
pearson: 0.8587092366038649

=== Experiment 2907 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0067427350154389025
rmse: 0.08211415843469932
mae: 0.043176137321393075
r2: 0.6959730458584925
pearson: 0.8389309212569503

=== Experiment 2906 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0061984937618508465
rmse: 0.07873051353732456
mae: 0.031061049629875093
r2: 0.7205126444438837
pearson: 0.8520217766456025

=== Experiment 2535 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007000901016841503
rmse: 0.08367138708567884
mae: 0.03787319732035679
r2: 0.684332454482796
pearson: 0.828157046624994

=== Experiment 2881 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007414395348455488
rmse: 0.08610688328150944
mae: 0.041580360716071546
r2: 0.6656881770630945
pearson: 0.8315919300662221

=== Experiment 2889 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007650752642634726
rmse: 0.08746858088842374
mae: 0.039532842212881045
r2: 0.6550309306973534
pearson: 0.8254254004890865

=== Experiment 2718 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00747357097453258
rmse: 0.08644981766627724
mae: 0.04328823134615118
r2: 0.6630199741284634
pearson: 0.8252787719248977

=== Experiment 2885 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00812204507452317
rmse: 0.09012238941862988
mae: 0.054924751651566316
r2: 0.6337805623751658
pearson: 0.8154798709010859

=== Experiment 2674 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007182970448900916
rmse: 0.08475240674400294
mae: 0.045116121273007494
r2: 0.6761230239261226
pearson: 0.8279221151870212

=== Experiment 2756 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006141880275221921
rmse: 0.078370149133595
mae: 0.03287354079914064
r2: 0.723065321638481
pearson: 0.8524584752377964

=== Experiment 2903 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006799256043545783
rmse: 0.08245760149037676
mae: 0.0375370440653262
r2: 0.6934245375779601
pearson: 0.8329195972967505

=== Experiment 2764 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005887511221868821
rmse: 0.07673011939172791
mae: 0.030642787417518993
r2: 0.7345347102977902
pearson: 0.8570940795946395

=== Experiment 2802 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006056935321472657
rmse: 0.07782631509632623
mae: 0.028811253628118542
r2: 0.7268954522159021
pearson: 0.8528286454626905

=== Experiment 2847 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006058963082578344
rmse: 0.07783934148345774
mae: 0.029656802551196317
r2: 0.7268040213601326
pearson: 0.8567414564109476

=== Experiment 2951 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007297109543437445
rmse: 0.08542312066084594
mae: 0.045354620123828356
r2: 0.6709765423899277
pearson: 0.8263392734937025

=== Experiment 2950 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00656807749557389
rmse: 0.08104367646876523
mae: 0.03917557670707361
r2: 0.7038482765565526
pearson: 0.841102217083798

=== Experiment 2635 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067230893026867305
rmse: 0.08199444677956386
mae: 0.0403800652895675
r2: 0.6968588623997485
pearson: 0.8376170296919179

=== Experiment 2940 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006925097524557045
rmse: 0.08321717085167607
mae: 0.041678620982620575
r2: 0.6877504005862338
pearson: 0.8310732819534248

=== Experiment 2821 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006607942997291739
rmse: 0.08128925511586227
mae: 0.034811575294865726
r2: 0.702050758630244
pearson: 0.8570143148042825

=== Experiment 2979 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006811505200315932
rmse: 0.08253184355335783
mae: 0.042056082992782795
r2: 0.6928722284904605
pearson: 0.835753883990377

=== Experiment 2875 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068037529699442635
rmse: 0.08248486509623607
mae: 0.039666518540005226
r2: 0.6932217731459014
pearson: 0.8370323682178451

=== Experiment 2820 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063629015170348175
rmse: 0.07976779749394375
mae: 0.031307005910089065
r2: 0.7130995711240253
pearson: 0.8472575545849904

=== Experiment 2773 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0070692391715143936
rmse: 0.08407876766172535
mae: 0.04448080451345819
r2: 0.6812511171665174
pearson: 0.8319002966197232

=== Experiment 2958 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006898074039293987
rmse: 0.0830546448989699
mae: 0.03564128746040968
r2: 0.6889688776427997
pearson: 0.830223980496148

=== Experiment 2939 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007548857025947698
rmse: 0.08688415865937645
mae: 0.03738176749034504
r2: 0.659625359206081
pearson: 0.8122282557143702

=== Experiment 2688 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0064180003655496705
rmse: 0.08011242329095826
mae: 0.035488600029493576
r2: 0.7106151882953485
pearson: 0.843852887223514

=== Experiment 2878 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005997324430132711
rmse: 0.07744239426911277
mae: 0.03177701411589033
r2: 0.7295832810696605
pearson: 0.8603522668652551

=== Experiment 2698 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007615555183663769
rmse: 0.08726714836445482
mae: 0.03346839997157914
r2: 0.6566179686307674
pearson: 0.8280597617986758

=== Experiment 2927 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006934288297841014
rmse: 0.08327237415758611
mae: 0.03721751295738352
r2: 0.6873359926640285
pearson: 0.8328875478193473

=== Experiment 2897 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005624243104685297
rmse: 0.07499495386147856
mae: 0.027497090276178002
r2: 0.7464053538284356
pearson: 0.8666669336275591

=== Experiment 2613 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007075187863037172
rmse: 0.08411413592873182
mae: 0.04503885441351847
r2: 0.6809828932839748
pearson: 0.8377464119112644

=== Experiment 2984 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0066996377216032525
rmse: 0.08185131472128748
mae: 0.03290086966023171
r2: 0.6979162838689724
pearson: 0.8422868610445736

=== Experiment 2803 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00593461274672977
rmse: 0.07703643778582814
mae: 0.027563296415037554
r2: 0.7324109232727765
pearson: 0.8561300478058441

=== Experiment 2882 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007055226435664767
rmse: 0.08399539532417695
mae: 0.04377472302782945
r2: 0.6818829452585005
pearson: 0.831251495076609

=== Experiment 2871 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007371378737750688
rmse: 0.0858567337938655
mae: 0.03990459044283947
r2: 0.6676277771067156
pearson: 0.8322658067485114

=== Experiment 2709 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006064967744612815
rmse: 0.07787790280055579
mae: 0.029969398299775928
r2: 0.7265332737918522
pearson: 0.8574258835328433

=== Experiment 2752 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007258780245769651
rmse: 0.08519847560707675
mae: 0.045924874775023015
r2: 0.6727047935517544
pearson: 0.834927630308817

=== Experiment 2866 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.008264798567692673
rmse: 0.09091093755809954
mae: 0.041338655683928305
r2: 0.6273438701987701
pearson: 0.8226926967270967

=== Experiment 2746 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060665537218523415
rmse: 0.07788808459483608
mae: 0.030876381240602255
r2: 0.7264617627761637
pearson: 0.8594584816424634

=== Experiment 2818 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006998842495758739
rmse: 0.08365908495649912
mae: 0.03550694235358785
r2: 0.6844252722923936
pearson: 0.8401220159840894

=== Experiment 2666 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007410325497013209
rmse: 0.08608324748180222
mae: 0.04738616774336289
r2: 0.6658716848733482
pearson: 0.8250009052739935

=== Experiment 2890 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007344800130027727
rmse: 0.08570180937429342
mae: 0.046027623696269185
r2: 0.6688261948308042
pearson: 0.8244848422263007

=== Experiment 2855 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007197738764210036
rmse: 0.08483948823637515
mae: 0.03531330372791404
r2: 0.6754571270888668
pearson: 0.845177990618582

=== Experiment 2611 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007800968770308896
rmse: 0.08832309307485159
mae: 0.038952222221132454
r2: 0.6482577516156989
pearson: 0.815538945035402

=== Experiment 2836 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006793036726729258
rmse: 0.08241988065223863
mae: 0.03223443422134131
r2: 0.6937049638358865
pearson: 0.8431890966522555

=== Experiment 2616 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007559416650110812
rmse: 0.0869449058318589
mae: 0.04721580185662928
r2: 0.659149230400212
pearson: 0.8320625625003986

=== Experiment 2873 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006731150751197195
rmse: 0.0820435905552481
mae: 0.035253928664449
r2: 0.6964953752345265
pearson: 0.8370021894944629

=== Experiment 2727 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007341771784919264
rmse: 0.08568413963458618
mae: 0.0360591770645711
r2: 0.6689627415788681
pearson: 0.8182217243677176

=== Experiment 2964 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007223314417602534
rmse: 0.08499008423105918
mae: 0.04020067841673365
r2: 0.674303932134105
pearson: 0.8282057271156623

=== Experiment 2661 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007059954262256257
rmse: 0.08402353397861968
mae: 0.038138061004115174
r2: 0.6816697696383658
pearson: 0.8376113954652812

=== Experiment 2892 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007197420884849643
rmse: 0.0848376147993898
mae: 0.04299602876614017
r2: 0.6754714601293177
pearson: 0.8283956049665159

=== Experiment 2923 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0063456942330234285
rmse: 0.07965986588630079
mae: 0.040018427880863135
r2: 0.7138754399865939
pearson: 0.8471550467556143

=== Experiment 2659 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005846836905213123
rmse: 0.0764646121105255
mae: 0.02970258731429541
r2: 0.7363686973336598
pearson: 0.8582410842971278

=== Experiment 2956 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006549430908692253
rmse: 0.08092855434698097
mae: 0.033672899458329075
r2: 0.7046890429520551
pearson: 0.8396325524005606

=== Experiment 2706 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006663641856105507
rmse: 0.081631132885104
mae: 0.0349988796980766
r2: 0.6995393216012737
pearson: 0.8399426672343487

=== Experiment 2834 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007944499247409862
rmse: 0.08913192047414811
mae: 0.04814544040934046
r2: 0.6417860255758103
pearson: 0.8168590368859475

=== Experiment 2788 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006281041557377193
rmse: 0.07925302238638722
mae: 0.03878289589714682
r2: 0.716790600675664
pearson: 0.8521087180928707

=== Experiment 2701 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006682866725014663
rmse: 0.08174880259070871
mae: 0.03643494821301263
r2: 0.6986724807236717
pearson: 0.8366463777253392

=== Experiment 2625 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0062385241934725735
rmse: 0.07898432878408586
mae: 0.04087057384214666
r2: 0.7187076899008005
pearson: 0.8531153478381964

=== Experiment 2911 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005996610080021393
rmse: 0.07743778199316786
mae: 0.033330809891140255
r2: 0.7296154908017702
pearson: 0.8587694079238888

=== Experiment 2902 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007593463039754862
rmse: 0.08714047876707393
mae: 0.04264196160098474
r2: 0.657614093676124
pearson: 0.8264595025157452

=== Experiment 2740 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005813307315670183
rmse: 0.07624504781079347
mae: 0.02750901042837849
r2: 0.7378805317686501
pearson: 0.8592985000023962

=== Experiment 2719 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006746098887799555
rmse: 0.08213463878169523
mae: 0.03810522665911247
r2: 0.6958213703343041
pearson: 0.8348631138620524

=== Experiment 2672 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0077840353565673155
rmse: 0.08822718037298549
mae: 0.03905044841917461
r2: 0.6490212717883413
pearson: 0.8064151822353154

=== Experiment 2728 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006622100358186331
rmse: 0.08137628867296869
mae: 0.03622591517773828
r2: 0.701412409458638
pearson: 0.8379515558210637

=== Experiment 2796 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006159822377965333
rmse: 0.07848453591609836
mae: 0.031962628057976124
r2: 0.7222563201227039
pearson: 0.8549998246860617

=== Experiment 2972 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006960450022687034
rmse: 0.08342931153190127
mae: 0.04177714876826582
r2: 0.686156372005377
pearson: 0.8320633511308204

=== Experiment 2812 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006790854256415227
rmse: 0.0824066396379274
mae: 0.034361375885546976
r2: 0.6938033704617678
pearson: 0.8378100988732003

=== Experiment 2811 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007701497469067932
rmse: 0.08775817608102354
mae: 0.04554700674203736
r2: 0.6527428688079862
pearson: 0.8164852137992694

=== Experiment 2896 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.008090594703614921
rmse: 0.08994773317663386
mae: 0.04793081991622765
r2: 0.635198645757052
pearson: 0.8128749970816909

=== Experiment 2998 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005723877376567517
rmse: 0.07565631088394092
mae: 0.029407968472393187
r2: 0.741912888361663
pearson: 0.8617546676076756

=== Experiment 2934 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005604296552339198
rmse: 0.07486184977903763
mae: 0.030366785185625987
r2: 0.7473047350945721
pearson: 0.8645402881588738

=== Experiment 2813 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006856305150014337
rmse: 0.08280280882925613
mae: 0.04353389669215761
r2: 0.6908522184765251
pearson: 0.8377606010397336

=== Experiment 2965 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006302264275086751
rmse: 0.07938680164288489
mae: 0.033890393882577176
r2: 0.7158336776749725
pearson: 0.8486014323292657

=== Experiment 2655 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00690592597196813
rmse: 0.08310190113329617
mae: 0.030090469702317015
r2: 0.6886148374544225
pearson: 0.8484074120100985

=== Experiment 2976 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007446643206432737
rmse: 0.08629393493422778
mae: 0.04027111722144593
r2: 0.6642341353402681
pearson: 0.8165431512957605

=== Experiment 2849 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006531385698040153
rmse: 0.08081698891965818
mae: 0.034410665189651045
r2: 0.7055026935580538
pearson: 0.839979303737071

=== Experiment 2590 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007445075897597772
rmse: 0.08628485323391222
mae: 0.032139806766138104
r2: 0.6643048046058113
pearson: 0.8416388503070412

=== Experiment 2795 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0069598416402339245
rmse: 0.0834256653568548
mae: 0.04481097466703266
r2: 0.6861838037024182
pearson: 0.8347822262354192

=== Experiment 2925 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068055397924737015
rmse: 0.08249569559967175
mae: 0.04057042945727419
r2: 0.6931412061044897
pearson: 0.8347301029378951

=== Experiment 2853 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068053800479724175
rmse: 0.08249472739498215
mae: 0.03221886793294737
r2: 0.6931484089137439
pearson: 0.8403538656565497

=== Experiment 2694 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007356034582806624
rmse: 0.08576732817808086
mae: 0.03902845477600221
r2: 0.6683196382996645
pearson: 0.8190531023003361

=== Experiment 2973 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006217745222427386
rmse: 0.07885268050248759
mae: 0.039600446338149436
r2: 0.7196446045597034
pearson: 0.8509440859729472

=== Experiment 2730 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00642166778125642
rmse: 0.08013530920422295
mae: 0.0400309365212134
r2: 0.7104498261352843
pearson: 0.8484784518461197

=== Experiment 2974 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007224214788996081
rmse: 0.08499538098623996
mae: 0.03584967292911937
r2: 0.6742633347842538
pearson: 0.822933599506636

=== Experiment 2677 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005816876396398265
rmse: 0.07626844954762267
mae: 0.030700702583202066
r2: 0.7377196034894933
pearson: 0.8613520259616988

=== Experiment 2983 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00768856740208899
rmse: 0.08768447640311819
mae: 0.0460802235555084
r2: 0.653325879837109
pearson: 0.814625675228797

=== Experiment 2600 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005899073590240353
rmse: 0.07680542682805917
mae: 0.033962045480724616
r2: 0.7340133681969054
pearson: 0.8598356956813005

=== Experiment 2844 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006477281180668914
rmse: 0.08048155801591389
mae: 0.03146870187388058
r2: 0.7079422424331039
pearson: 0.8424079124442241

=== Experiment 2967 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006648330055296661
rmse: 0.08153729242068725
mae: 0.03806121621297375
r2: 0.7002297239605058
pearson: 0.8372086156006106

=== Experiment 2909 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00706785650781132
rmse: 0.08407054482880029
mae: 0.03849819901991169
r2: 0.6813134608643323
pearson: 0.8264553685742936

=== Experiment 2856 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005721499804189706
rmse: 0.07564059627071766
mae: 0.031135516163515668
r2: 0.7420200920537288
pearson: 0.862258254336723

=== Experiment 2985 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007477198239843674
rmse: 0.08647079414370885
mae: 0.036396101655148366
r2: 0.6628564223320677
pearson: 0.8147355163552417

=== Experiment 2518 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005698007835545905
rmse: 0.07548514976832135
mae: 0.02982224633524888
r2: 0.7430793345802716
pearson: 0.8640506498670851

=== Experiment 2846 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006276835343038056
rmse: 0.079226481324353
mae: 0.029383085946515928
r2: 0.7169802570289192
pearson: 0.8523710467945292

=== Experiment 2966 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007176063423285816
rmse: 0.08471164868709508
mae: 0.03854979853065486
r2: 0.6764344586710929
pearson: 0.8232914712862228

=== Experiment 2757 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005573182750936012
rmse: 0.07465375242367936
mae: 0.028244572896636157
r2: 0.7487076427056105
pearson: 0.8662966861312359

=== Experiment 2960 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067479615269669365
rmse: 0.08214597693719965
mae: 0.04160362704579972
r2: 0.6957373847540571
pearson: 0.8382940121745627

=== Experiment 2843 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006538945294044537
rmse: 0.08086374523879374
mae: 0.042051220837901325
r2: 0.7051618347014488
pearson: 0.8452339916804225

=== Experiment 2630 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007279407926256898
rmse: 0.08531944635460839
mae: 0.044434364523728224
r2: 0.671774700517521
pearson: 0.8247546607727603

=== Experiment 2711 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006820544026751877
rmse: 0.08258658502899777
mae: 0.03887007352744857
r2: 0.6924646717847555
pearson: 0.8346469824069614

=== Experiment 2652 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007643329283584933
rmse: 0.08742613615838764
mae: 0.041581312719387566
r2: 0.6553656466896385
pearson: 0.8124555568197719

=== Experiment 2969 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006785205730255589
rmse: 0.08237236023239584
mae: 0.04037227775816482
r2: 0.6940580600201993
pearson: 0.8355202645549594

=== Experiment 2895 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007189789720998891
rmse: 0.08479262775146723
mae: 0.038305448234686665
r2: 0.6758155459486158
pearson: 0.8392459325570727

=== Experiment 2823 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00715439887194646
rmse: 0.08458367970209418
mae: 0.03542132502575229
r2: 0.6774113037556861
pearson: 0.8251342115525411

=== Experiment 2931 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007496694261621741
rmse: 0.08658345258547814
mae: 0.035538712865088695
r2: 0.6619773552909499
pearson: 0.8334868004538047

=== Experiment 2828 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006973523440931517
rmse: 0.08350762504664778
mae: 0.040649753723317736
r2: 0.6855668973307847
pearson: 0.843580630169207

=== Experiment 2914 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059260253582711555
rmse: 0.07698068172126794
mae: 0.028091938246973713
r2: 0.7327981248387767
pearson: 0.8610987522937136

=== Experiment 2941 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006867125408051732
rmse: 0.08286812057752807
mae: 0.039379380696543044
r2: 0.6903643378039788
pearson: 0.8338346783803998

=== Experiment 2790 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008200190454222265
rmse: 0.09055490298278865
mae: 0.04109373610278984
r2: 0.6302570215989456
pearson: 0.8125488971822044

=== Experiment 2874 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0055338804283673
rmse: 0.07439005597771317
mae: 0.030282536290385572
r2: 0.7504797671319587
pearson: 0.8667503266543971

=== Experiment 2839 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005452708435551724
rmse: 0.07384245686291677
mae: 0.029509079097893007
r2: 0.754139776561487
pearson: 0.8684325858302798

=== Experiment 2915 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007919307962367743
rmse: 0.08899049366290616
mae: 0.04544318196780765
r2: 0.6429218895308273
pearson: 0.8074306589995156

=== Experiment 2928 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006204166665313311
rmse: 0.07876653264752302
mae: 0.03240989556793918
r2: 0.720256855723597
pearson: 0.8487861796306501

=== Experiment 2904 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00706788035072678
rmse: 0.08407068663170761
mae: 0.04823716394680574
r2: 0.6813123857977681
pearson: 0.8370005760223169

=== Experiment 2830 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00763011637035216
rmse: 0.0873505373214851
mae: 0.03850704382336207
r2: 0.6559614111318562
pearson: 0.8125113099991966

=== Experiment 2991 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005491793879664049
rmse: 0.07410663856675763
mae: 0.02712344969606639
r2: 0.7523774310892817
pearson: 0.867864235683752

=== Experiment 2978 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0071918838974046675
rmse: 0.08480497566419477
mae: 0.0387440824227165
r2: 0.675721120456197
pearson: 0.822746797506623

=== Experiment 2948 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006968942246437423
rmse: 0.08348019074269909
mae: 0.036934332637370226
r2: 0.6857734613741855
pearson: 0.8289852831109972

=== Experiment 2651 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0058273509812670175
rmse: 0.07633708784900703
mae: 0.029811698663821426
r2: 0.7372473090679788
pearson: 0.862879259495573

=== Experiment 2955 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005567659161335199
rmse: 0.07461674853097794
mae: 0.02688757354961942
r2: 0.7489566989295935
pearson: 0.8657681425345748

=== Experiment 2930 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006286781216915652
rmse: 0.07928922509972999
mae: 0.03258866348698302
r2: 0.7165318019532922
pearson: 0.8551099065614911

=== Experiment 2942 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0055946513480896205
rmse: 0.07479740201430542
mae: 0.03189767084232634
r2: 0.7477396331090067
pearson: 0.8662151237862175

=== Experiment 2921 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007573280017093817
rmse: 0.08702459432306374
mae: 0.041100855449220444
r2: 0.6585241372846828
pearson: 0.8135589096323541

=== Experiment 2901 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006030940796379226
rmse: 0.07765913208618305
mae: 0.029806448505475775
r2: 0.7280675339112994
pearson: 0.8598250595225182

=== Experiment 2993 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005817140019485182
rmse: 0.0762701777858501
mae: 0.03237604294011101
r2: 0.7377077168405337
pearson: 0.8618010692840623

=== Experiment 2945 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005673065175477085
rmse: 0.07531975289044093
mae: 0.029007600111013124
r2: 0.7442039881446705
pearson: 0.8667160341149164

=== Experiment 2626 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.008227031027649203
rmse: 0.09070298246281212
mae: 0.03748966354593456
r2: 0.6290467919565647
pearson: 0.8263323512200805

=== Experiment 2831 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.008168631742298944
rmse: 0.09038048319354651
mae: 0.05452699882997337
r2: 0.6316799900295167
pearson: 0.8164829562553152

=== Experiment 2734 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005823532618779042
rmse: 0.07631207387287442
mae: 0.029435276910559166
r2: 0.7374194773519717
pearson: 0.861904356433697

=== Experiment 2721 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006399928529975997
rmse: 0.07999955331110291
mae: 0.032830668488596886
r2: 0.7114300394073423
pearson: 0.844865551689606

=== Experiment 2968 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0065842705107306805
rmse: 0.08114351798345128
mae: 0.03958625315526209
r2: 0.703118140630224
pearson: 0.8419197870355295

=== Experiment 2594 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006390266067056912
rmse: 0.0799391397693077
mae: 0.034091981776262895
r2: 0.7118657156075887
pearson: 0.8475396286346009

=== Experiment 2712 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005278464413881029
rmse: 0.07265304132574926
mae: 0.02864731965417418
r2: 0.7619963628079597
pearson: 0.874718561916358

=== Experiment 2888 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.008093690069105328
rmse: 0.08996493799867439
mae: 0.03609807181301629
r2: 0.6350590770894642
pearson: 0.797749203378618

=== Experiment 2681 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00703102697345886
rmse: 0.08385121927234487
mae: 0.03453461430946352
r2: 0.682974088358367
pearson: 0.8291457683190827

=== Experiment 2567 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0075844446514529975
rmse: 0.08708871713059618
mae: 0.03812991923819054
r2: 0.6580207288353588
pearson: 0.8127356807488375

=== Experiment 2569 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006918702445460388
rmse: 0.08317873794101704
mae: 0.03511671696008052
r2: 0.6880387518880126
pearson: 0.8299550196087182

=== Experiment 2534 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006562488953667021
rmse: 0.08100919055062222
mae: 0.03574170159878707
r2: 0.704100261451428
pearson: 0.8399631757211937

=== Experiment 2996 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007671998759899125
rmse: 0.08758994668281929
mae: 0.04571812976504655
r2: 0.6540729526210329
pearson: 0.8179463814406976

=== Experiment 2743 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068136893747075865
rmse: 0.08254507480587553
mae: 0.03847958271867726
r2: 0.6927737450284693
pearson: 0.8371230031683278

=== Experiment 2944 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005785662414284694
rmse: 0.07606354195200676
mae: 0.02986358932655742
r2: 0.7391270282046701
pearson: 0.8602138935858226

=== Experiment 2990 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005873152151596308
rmse: 0.07663649360191467
mae: 0.029910870709476494
r2: 0.7351821544564676
pearson: 0.8634821015267515

=== Experiment 2870 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007428133020603541
rmse: 0.08618661741014982
mae: 0.039018069296030126
r2: 0.6650687514723508
pearson: 0.8216965983660488

=== Experiment 2815 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006555225865364306
rmse: 0.0809643493481193
mae: 0.0392640629146433
r2: 0.7044277509062677
pearson: 0.8415589984781533

=== Experiment 2509 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006154537582027649
rmse: 0.07845086093872806
mae: 0.030762398213978123
r2: 0.7224946092455176
pearson: 0.8540118095525522

=== Experiment 2725 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006555163834787158
rmse: 0.08096396627381318
mae: 0.03649389829385753
r2: 0.7044305478376897
pearson: 0.8394366488781094

=== Experiment 2932 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006641362577146796
rmse: 0.08149455550616125
mae: 0.0379372641319945
r2: 0.7005438844836613
pearson: 0.8380967324334487

=== Experiment 2857 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00556927848227688
rmse: 0.07462759866347624
mae: 0.027134093156160593
r2: 0.7488836844610507
pearson: 0.8661112669277934

=== Experiment 2562 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007385673766385161
rmse: 0.08593994278788625
mae: 0.041279191755787537
r2: 0.6669832205573107
pearson: 0.8348323164073798

=== Experiment 2894 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005577674888409975
rmse: 0.07468383284493355
mae: 0.028886127741936576
r2: 0.7485050942040504
pearson: 0.865547808663373

=== Experiment 2952 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006711987740480749
rmse: 0.08192672177306223
mae: 0.03741981816627974
r2: 0.6973594269534451
pearson: 0.8351781673129826

=== Experiment 2980 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005865238051697187
rmse: 0.07658484217974983
mae: 0.02870009736689164
r2: 0.7355389977376572
pearson: 0.8594472744186815

=== Experiment 2800 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006830749676767344
rmse: 0.08264834951024336
mae: 0.037053389076355706
r2: 0.692004503517408
pearson: 0.8353056183415349

=== Experiment 2781 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006481026219366648
rmse: 0.08050482109393603
mae: 0.03453045092039344
r2: 0.7077733802865406
pearson: 0.8416829353013332

=== Experiment 2593 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0055355106058507875
rmse: 0.07440101212920956
mae: 0.02881848118045371
r2: 0.7504062631467241
pearson: 0.8663356158047865

=== Experiment 2953 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007474512129653106
rmse: 0.08645526085585022
mae: 0.037696325233280606
r2: 0.6629775378583128
pearson: 0.8143710200131609

=== Experiment 2523 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00795158951470069
rmse: 0.08917168561096449
mae: 0.05021314598617798
r2: 0.6414663285443322
pearson: 0.8171623991518198

=== Experiment 2507 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006446342399007367
rmse: 0.08028911756276418
mae: 0.03044929672022622
r2: 0.7093372584810238
pearson: 0.8472089109596744

=== Experiment 2987 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006712690823512918
rmse: 0.08193101258688872
mae: 0.038419454012423936
r2: 0.6973277252489751
pearson: 0.8383192064539623

=== Experiment 2920 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007429173735201785
rmse: 0.08619265476362696
mae: 0.034383330878315854
r2: 0.6650218261091695
pearson: 0.8161662571748831

=== Experiment 2899 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006574583779639257
rmse: 0.08108380713582249
mae: 0.03529218971370131
r2: 0.7035549110716799
pearson: 0.8397756711595177

=== Experiment 2771 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005846141471318684
rmse: 0.0764600645521483
mae: 0.03393602390548191
r2: 0.7364000541418761
pearson: 0.8602315147769443

=== Experiment 2741 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005741575500339116
rmse: 0.07577318457303425
mae: 0.029722253203275376
r2: 0.7411148877503415
pearson: 0.867767369198305

=== Experiment 2850 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005864091890613755
rmse: 0.07657735886418227
mae: 0.031895509518783526
r2: 0.7355906776364451
pearson: 0.8597567821870958

=== Experiment 2835 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007201171510727457
rmse: 0.08485971665476769
mae: 0.036489871859206165
r2: 0.6753023460592636
pearson: 0.8244875900606254

=== Experiment 2814 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006098102964302169
rmse: 0.07809035128812118
mae: 0.035410419168659175
r2: 0.7250392213199846
pearson: 0.8542283902336218

=== Experiment 2937 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0067217701920597745
rmse: 0.08198640248272743
mae: 0.03858440092575079
r2: 0.6969183405173331
pearson: 0.8371076739074542

=== Experiment 2936 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0060539720228867
rmse: 0.07780727487122717
mae: 0.030859623959785736
r2: 0.7270290660448953
pearson: 0.852686449542379

=== Experiment 2845 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005893960105750591
rmse: 0.07677213104864675
mae: 0.03221848380739499
r2: 0.7342439329619321
pearson: 0.8570623130618112

=== Experiment 2685 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006673392087023839
rmse: 0.08169083233156484
mae: 0.033935397465740216
r2: 0.6990996879805709
pearson: 0.8364413213777264

=== Experiment 2957 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005883628014869846
rmse: 0.07670481089781687
mae: 0.030687236642275774
r2: 0.7347098023922438
pearson: 0.8621496250999442

=== Experiment 2977 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0073702713118769915
rmse: 0.0858502842853592
mae: 0.04766526003640096
r2: 0.6676777104519451
pearson: 0.836052403482868

=== Experiment 2926 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006198816357740568
rmse: 0.07873256224549388
mae: 0.031077155766168225
r2: 0.7204980987372014
pearson: 0.8618610845784953

=== Experiment 2640 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006065216670733079
rmse: 0.07787950096612765
mae: 0.033931404987458384
r2: 0.7265220498226501
pearson: 0.8523804631455091

=== Experiment 2837 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005202120613119834
rmse: 0.07212572781691588
mae: 0.03152340975940092
r2: 0.7654386711828057
pearson: 0.8756814066571154

=== Experiment 2852 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006651017768562788
rmse: 0.08155377225219436
mae: 0.036678073131888464
r2: 0.7001085361522892
pearson: 0.8488547561715598

=== Experiment 2910 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068742662153208015
rmse: 0.08291119475270393
mae: 0.03823082123651263
r2: 0.690042361947126
pearson: 0.831700853050846

=== Experiment 2704 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006333664532389775
rmse: 0.07958432340850662
mae: 0.037596336330619706
r2: 0.7144178538934893
pearson: 0.8461644840594817

=== Experiment 2838 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00583303048553576
rmse: 0.0763742789526406
mae: 0.027292423911051255
r2: 0.7369912227202421
pearson: 0.8594870787604827

=== Experiment 2779 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0070463253295814515
rmse: 0.0839423929226553
mae: 0.0392650792935028
r2: 0.6822842922141208
pearson: 0.833368595258745

=== Experiment 2986 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007149169940975802
rmse: 0.08455276424207432
mae: 0.036590734402333965
r2: 0.6776470739517246
pearson: 0.8308703596833362

=== Experiment 2924 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005919814240141632
rmse: 0.07694032908781735
mae: 0.027790967116578705
r2: 0.7330781814215148
pearson: 0.8583058856007695

=== Experiment 2639 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006803660012792064
rmse: 0.08248430161425908
mae: 0.03635833912609757
r2: 0.6932259645429805
pearson: 0.8418823250591596

=== Experiment 2543 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006130944927398636
rmse: 0.0783003507488864
mae: 0.03312003912340515
r2: 0.7235583916588183
pearson: 0.8508261709486854

=== Experiment 2981 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006882766317398363
rmse: 0.0829624391962915
mae: 0.033057403026483384
r2: 0.6896590960856297
pearson: 0.8499694480046666

=== Experiment 2988 ===
num_layers: 3
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005873384896587324
rmse: 0.07663801208660963
mae: 0.028513264996863616
r2: 0.7351716600872618
pearson: 0.8577445809571511

=== Experiment 2938 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006376364069695442
rmse: 0.07985213879224176
mae: 0.03736295077016977
r2: 0.7124925505498804
pearson: 0.8502880841322065

=== Experiment 2732 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068269211365666205
rmse: 0.08262518463862348
mae: 0.03784681029644071
r2: 0.6921771307099891
pearson: 0.8351672969370219

=== Experiment 2584 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007406600032735702
rmse: 0.08606160603158473
mae: 0.044967352214759164
r2: 0.666039664417919
pearson: 0.8248925161162242

=== Experiment 2707 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006093509138394167
rmse: 0.07806093221576442
mae: 0.03702984462195793
r2: 0.725246354908279
pearson: 0.8550719966072753

=== Experiment 2879 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068003078190167696
rmse: 0.08246397891817232
mae: 0.03804699732962088
r2: 0.6933771134848954
pearson: 0.8327470058907328

=== Experiment 2975 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005925877818378127
rmse: 0.07697972342362713
mae: 0.02890993675573203
r2: 0.7328047773476161
pearson: 0.8567215734593934

=== Experiment 2989 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0067310100168486405
rmse: 0.08204273287042942
mae: 0.03855703864858332
r2: 0.6965017208843609
pearson: 0.8350500116134204

=== Experiment 2824 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065822531773422404
rmse: 0.08113108638088264
mae: 0.03255325031626409
r2: 0.7032091013048124
pearson: 0.851148543139555

=== Experiment 2585 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006671237931653238
rmse: 0.08167764646250061
mae: 0.0402716834394936
r2: 0.699196817898115
pearson: 0.8402014135361772

=== Experiment 2971 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006446612926900303
rmse: 0.08029080225592657
mae: 0.0314607469058227
r2: 0.7093250604973997
pearson: 0.846715447939005

=== Experiment 2876 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006348327934008355
rmse: 0.07967639508667768
mae: 0.031058977094378423
r2: 0.7137566875683643
pearson: 0.8468562608387106

=== Experiment 2583 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006969461418995138
rmse: 0.08348330024019857
mae: 0.039323296897692775
r2: 0.6857500521120634
pearson: 0.8300172113957531

=== Experiment 2660 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005961771340386958
rmse: 0.07721250766803885
mae: 0.032884938166296486
r2: 0.7311863542381851
pearson: 0.8574791516403685

=== Experiment 2574 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006373673185774904
rmse: 0.07983528784801182
mae: 0.03087874611809133
r2: 0.7126138813215088
pearson: 0.8607489044468051

=== Experiment 2755 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007077813065797718
rmse: 0.08412973948490342
mae: 0.036316970450717134
r2: 0.680864524046942
pearson: 0.825173896879703

=== Experiment 2684 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00671701057808921
rmse: 0.08195737049277027
mae: 0.0319055149424404
r2: 0.6971329494163996
pearson: 0.8369530818476835

=== Experiment 2995 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006028604437727859
rmse: 0.07764408823424909
mae: 0.030894045774720442
r2: 0.7281728792945794
pearson: 0.8589674424405936

=== Experiment 2601 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065877763484254205
rmse: 0.08116511780577554
mae: 0.03796619589398946
r2: 0.7029600639516054
pearson: 0.8416524140461472

=== Experiment 2557 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0060242123183481214
rmse: 0.07761579941189888
mae: 0.0313107399845337
r2: 0.7283709180242923
pearson: 0.8628308598678911

=== Experiment 2563 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007403356321514861
rmse: 0.08604275868145361
mae: 0.040921708912563255
r2: 0.6661859219292001
pearson: 0.8187544693220532

=== Experiment 2745 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006996903532952491
rmse: 0.08364749567651437
mae: 0.037630351994635186
r2: 0.6845126992719244
pearson: 0.8302977519715498

=== Experiment 2552 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007125153263305534
rmse: 0.08441062292925894
mae: 0.03872661323003988
r2: 0.6787299753773288
pearson: 0.8330336621437531

=== Experiment 2782 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007025548405347242
rmse: 0.08381854451937973
mae: 0.03661110708695627
r2: 0.6832211145832177
pearson: 0.846647160194606

=== Experiment 2961 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007840619406550947
rmse: 0.08854727215759359
mae: 0.036149415020475556
r2: 0.6464699218791243
pearson: 0.8392119633206044

=== Experiment 2512 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059481247463846
rmse: 0.07712408668104019
mae: 0.029568007065211385
r2: 0.731801673155426
pearson: 0.8564355422122101

=== Experiment 2699 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007316207059333365
rmse: 0.08553482951016718
mae: 0.03751889441839369
r2: 0.6701154438036401
pearson: 0.8193936934286313

=== Experiment 2772 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0072520273135981235
rmse: 0.08515883579287661
mae: 0.03796261518172015
r2: 0.673009280291176
pearson: 0.8236342993659727

=== Experiment 2922 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007306662705952428
rmse: 0.08547901909797764
mae: 0.03697447725804599
r2: 0.6705457944967954
pearson: 0.8323538950481786

=== Experiment 2883 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007902600678697339
rmse: 0.08889657292999173
mae: 0.041534765184055376
r2: 0.6436752135980879
pearson: 0.8100885398341134

=== Experiment 2606 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006029284585104944
rmse: 0.0776484680151833
mae: 0.03183127344938292
r2: 0.7281422117487025
pearson: 0.8567833057208

=== Experiment 2508 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064244502875055245
rmse: 0.08015266862373033
mae: 0.03203883347328264
r2: 0.7103243641531867
pearson: 0.8437008643358462

=== Experiment 2943 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006572060637267511
rmse: 0.08106824678792253
mae: 0.03635662256328547
r2: 0.7036686784507021
pearson: 0.8424170269401984

=== Experiment 2791 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059918277741265015
rmse: 0.07740689745834348
mae: 0.031707554016111986
r2: 0.7298311228697172
pearson: 0.8577433679432974

=== Experiment 2893 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006889006980000113
rmse: 0.0830000420481828
mae: 0.03710279544873388
r2: 0.6893777073556425
pearson: 0.830483738562308

=== Experiment 2556 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005670889971249504
rmse: 0.07530531170674154
mae: 0.028665527605033884
r2: 0.7443020671459115
pearson: 0.86407523562018

=== Experiment 2891 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006785833488710471
rmse: 0.08237617063635862
mae: 0.03129011021934833
r2: 0.6940297546677663
pearson: 0.8455410527853863

=== Experiment 2947 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006525290540941009
rmse: 0.08077927049027497
mae: 0.04310351212515732
r2: 0.7057775215089704
pearson: 0.8499844932232665

=== Experiment 2668 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067869379190985185
rmse: 0.08238287394294107
mae: 0.029951730045790857
r2: 0.6939799563876662
pearson: 0.8333946275544487

=== Experiment 2858 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006022460446724879
rmse: 0.07760451305642527
mae: 0.03403434381883358
r2: 0.7284499091447266
pearson: 0.8538624711398198

=== Experiment 2617 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0068562771026205735
rmse: 0.0828026394665084
mae: 0.04009958048188116
r2: 0.6908534831211652
pearson: 0.8346609337536136

=== Experiment 2946 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006132685177251486
rmse: 0.07831146261724069
mae: 0.03419083425009435
r2: 0.7234799245588959
pearson: 0.8509055247886921

=== Experiment 2999 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006342209396014082
rmse: 0.07963798965326839
mae: 0.03222072241021021
r2: 0.7140325697535514
pearson: 0.8450258133086397

=== Experiment 2570 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064124470849682685
rmse: 0.08007775649310031
mae: 0.032208184838342875
r2: 0.7108655832725781
pearson: 0.8453573511541371

=== Experiment 2540 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005892882270713815
rmse: 0.07676511102521649
mae: 0.030367356681171298
r2: 0.7342925320693487
pearson: 0.8574837994625292

=== Experiment 2916 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006557997094330862
rmse: 0.08098146142377811
mae: 0.03567552409288375
r2: 0.7043027974118767
pearson: 0.8393944658919082

=== Experiment 2810 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006444007857447108
rmse: 0.08027457790264056
mae: 0.03214470539392725
r2: 0.7094425219324656
pearson: 0.8434607674516689

=== Experiment 2785 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007354999618534577
rmse: 0.08576129440799374
mae: 0.045628356641802884
r2: 0.6683663043831685
pearson: 0.8432848990375033

=== Experiment 2867 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006353759844183898
rmse: 0.07971047512205594
mae: 0.037195098599025915
r2: 0.7135117651293155
pearson: 0.8469777930381118

=== Experiment 2648 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0069604765383933275
rmse: 0.08342947044296355
mae: 0.0432543568155911
r2: 0.6861551764238507
pearson: 0.835386758298965

=== Experiment 2862 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005765382096142114
rmse: 0.07593011323672653
mae: 0.033685681664155316
r2: 0.7400414588236683
pearson: 0.8607070800552608

=== Experiment 2653 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007054788696606809
rmse: 0.08399278955128714
mae: 0.035531993279445465
r2: 0.6819026827199601
pearson: 0.8581118974587566

=== Experiment 2949 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006480380810707179
rmse: 0.08050081248476428
mae: 0.03677495602315638
r2: 0.7078024814789301
pearson: 0.8420201172271436

=== Experiment 2758 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005986531325901399
rmse: 0.07737267816161852
mae: 0.03597149397578476
r2: 0.7300699373890418
pearson: 0.8556745692435009

=== Experiment 2884 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005855702485995394
rmse: 0.07652256194087724
mae: 0.030589227356574537
r2: 0.7359689521982278
pearson: 0.8587628618677674

=== Experiment 2612 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006342377929435775
rmse: 0.07963904776826362
mae: 0.03078917732251343
r2: 0.7140249706557446
pearson: 0.8455617105919182

=== Experiment 2765 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007637683452432026
rmse: 0.08739384104404627
mae: 0.03855293049892385
r2: 0.6556202147314028
pearson: 0.8180023974249642

=== Experiment 2641 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064836975247654985
rmse: 0.08052141035007707
mae: 0.03469573679637947
r2: 0.7076529323018378
pearson: 0.8445742621494143

=== Experiment 2908 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008522815535676714
rmse: 0.09231909626765589
mae: 0.04132406438049164
r2: 0.615709998674322
pearson: 0.7897984098887613

=== Experiment 2735 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006778234420140044
rmse: 0.08233003352446812
mae: 0.031088895601745298
r2: 0.694372393325001
pearson: 0.85134617081439

=== Experiment 2935 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006441644843609577
rmse: 0.08025985823317643
mae: 0.030739001512356937
r2: 0.709549069186358
pearson: 0.8431951008320077

=== Experiment 2682 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007339395319648687
rmse: 0.08567027092083161
mae: 0.040213280894010474
r2: 0.669069895351956
pearson: 0.8187603156174993

=== Experiment 2822 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007199141585313821
rmse: 0.08484775533456275
mae: 0.04357978836299129
r2: 0.675393874502726
pearson: 0.8249064893816581

=== Experiment 2750 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005808922996251471
rmse: 0.07621629088489856
mae: 0.032641113745812134
r2: 0.7380782188015531
pearson: 0.862191154796995

=== Experiment 2918 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006641095287637853
rmse: 0.08149291556716973
mae: 0.036699422537028174
r2: 0.7005559364499765
pearson: 0.8382975915160231

=== Experiment 2587 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0071569751050632145
rmse: 0.08459890723326877
mae: 0.03954108991562203
r2: 0.6772951425383386
pearson: 0.8303055022686356

=== Experiment 2760 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00696981913763101
rmse: 0.08348544266895283
mae: 0.04026633415224805
r2: 0.6857339227362159
pearson: 0.8340534407106339

=== Experiment 2759 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006304545981424585
rmse: 0.07940117115902375
mae: 0.03312308513079663
r2: 0.7157307965404553
pearson: 0.8469900565631348

=== Experiment 2703 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005952319423806887
rmse: 0.07715127622928145
mae: 0.032044452967804365
r2: 0.7316125369966738
pearson: 0.8553500808978541

=== Experiment 2860 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006422147919897898
rmse: 0.08013830494774579
mae: 0.035782822912271896
r2: 0.7104281768952652
pearson: 0.843796955801597

=== Experiment 2638 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007223890734713045
rmse: 0.08499347465960574
mae: 0.03184379048362442
r2: 0.6742779462492487
pearson: 0.8235971352740293

=== Experiment 2933 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005853901361355338
rmse: 0.07651079245018534
mae: 0.030891531455860963
r2: 0.7360501641155122
pearson: 0.8600776407350724

=== Experiment 2644 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005662974222650315
rmse: 0.0752527356489471
mae: 0.02913896833134761
r2: 0.7446589847662616
pearson: 0.8634031798014943

=== Experiment 2708 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00628683260113201
rmse: 0.07928954912932731
mae: 0.03268751667959902
r2: 0.7165294850615926
pearson: 0.8471266881953229

=== Experiment 2733 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00655134982009044
rmse: 0.08094040906797074
mae: 0.034634736262704725
r2: 0.7046025200816886
pearson: 0.8398596648175514

=== Experiment 2564 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0066651486033742725
rmse: 0.08164036136234498
mae: 0.03546864140355879
r2: 0.6994713830300954
pearson: 0.8425685747659467

=== Experiment 2646 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006194171346754979
rmse: 0.07870305805211751
mae: 0.03291016682344333
r2: 0.7207075402381161
pearson: 0.8501620707677068

=== Experiment 2880 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005860027678466195
rmse: 0.07655081762114756
mae: 0.029399459916362274
r2: 0.735773931173382
pearson: 0.8638067821083276

=== Experiment 2636 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006246898775471048
rmse: 0.07903732520443141
mae: 0.03514403617634938
r2: 0.7183300836844246
pearson: 0.8573048665696682

=== Experiment 2526 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006307735897581243
rmse: 0.07942125595570271
mae: 0.0417178501752939
r2: 0.7155869646249411
pearson: 0.8535790616171616

=== Experiment 2854 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006418946764122464
rmse: 0.08011832976368431
mae: 0.03493946386825624
r2: 0.7105725156002429
pearson: 0.8445451238159294

=== Experiment 2548 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005832924522437702
rmse: 0.07637358524017124
mae: 0.02903370641999877
r2: 0.7369960005496956
pearson: 0.8599996478841971

=== Experiment 2869 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006870168783241228
rmse: 0.08288648130570647
mae: 0.032194497920126616
r2: 0.6902271133561761
pearson: 0.8512378810470466

=== Experiment 2679 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006842927674334187
rmse: 0.08272199027062989
mae: 0.033850740242612906
r2: 0.6914554029670659
pearson: 0.8377794268664851

=== Experiment 2769 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006264630621929483
rmse: 0.07914941959313083
mae: 0.03910720422209601
r2: 0.7175305625319963
pearson: 0.8493926807230583

=== Experiment 2522 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0077456468190259765
rmse: 0.0880093564288819
mae: 0.034858093928287814
r2: 0.6507521940499931
pearson: 0.8237400021193797

=== Experiment 2536 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00638544489509965
rmse: 0.07990897881402095
mae: 0.037499414442045846
r2: 0.7120831001291815
pearson: 0.8444836757624637

=== Experiment 2559 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.01127697280018679
rmse: 0.10619309205492977
mae: 0.051795302613094535
r2: 0.49152625981487663
pearson: 0.7023690496988961

=== Experiment 2963 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005672740234763664
rmse: 0.07531759578454203
mae: 0.03430433439348341
r2: 0.7442186395784213
pearson: 0.8640003170943851

=== Experiment 2656 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007121276415511974
rmse: 0.08438765558724791
mae: 0.03903912115127308
r2: 0.6789047807380093
pearson: 0.8287953742678679

=== Experiment 2817 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006132895757813962
rmse: 0.07831280711233611
mae: 0.033703791029237505
r2: 0.7234704295740174
pearson: 0.8554737181348856

=== Experiment 2807 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006391350298153181
rmse: 0.07994592108515093
mae: 0.03604902061474637
r2: 0.7118168281046644
pearson: 0.8464590406089476

=== Experiment 2691 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006512276965577849
rmse: 0.08069868007333111
mae: 0.039379092235161055
r2: 0.7063642978943827
pearson: 0.8449950029116441

=== Experiment 2770 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006572788176285537
rmse: 0.08107273386463254
mae: 0.0318220609787832
r2: 0.7036358740365936
pearson: 0.8436590088604251

=== Experiment 2547 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006213585320483116
rmse: 0.0788262984065795
mae: 0.03543555208016489
r2: 0.7198321727074577
pearson: 0.8549036778033009

=== Experiment 2919 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063042226436168785
rmse: 0.07939913503065936
mae: 0.03287919864888847
r2: 0.715745375699893
pearson: 0.8521229188592575

=== Experiment 2861 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006544869606527761
rmse: 0.0809003683955009
mae: 0.0326520661952593
r2: 0.7048947100590695
pearson: 0.8397464759444008

=== Experiment 2992 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006433206411603411
rmse: 0.08020727156314078
mae: 0.03069467275402093
r2: 0.709929554371478
pearson: 0.8527453053144342

=== Experiment 2982 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006285510946089895
rmse: 0.07928121433284113
mae: 0.03520698897048009
r2: 0.7165890779057369
pearson: 0.847222358932663

=== Experiment 2533 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005389538060124973
rmse: 0.07341347328743528
mae: 0.03323617721256942
r2: 0.7569881009860706
pearson: 0.871929560677721

=== Experiment 2887 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006376161010854401
rmse: 0.07985086731435295
mae: 0.03042738672120433
r2: 0.71250170638365
pearson: 0.8446973267953528

=== Experiment 2582 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005864316874145478
rmse: 0.07657882784520456
mae: 0.030567926284804738
r2: 0.7355805332280225
pearson: 0.8598066619743714

=== Experiment 2832 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063361818109969785
rmse: 0.07960013700363196
mae: 0.03866916342173367
r2: 0.7143043509090296
pearson: 0.8479828825185626

=== Experiment 2595 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005723914127696385
rmse: 0.0756565537656612
mae: 0.033422720229870945
r2: 0.7419112312694377
pearson: 0.862782963756486

=== Experiment 2717 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006569419166138851
rmse: 0.08105195349983152
mae: 0.035091006566739344
r2: 0.7037877812212929
pearson: 0.8482318968789961

=== Experiment 2609 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0077206292501314664
rmse: 0.08786711131095336
mae: 0.036038518617366955
r2: 0.6518802252203721
pearson: 0.8359319413960499

=== Experiment 2720 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00627010408394396
rmse: 0.07918398881051623
mae: 0.029744325871097287
r2: 0.7172837665388183
pearson: 0.8517494010321779

=== Experiment 2868 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006048801699915256
rmse: 0.07777404258436908
mae: 0.0320070252641206
r2: 0.7272621936320447
pearson: 0.8530569261123919

=== Experiment 2929 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0065622411923294535
rmse: 0.08100766131872622
mae: 0.038786261875570216
r2: 0.7041114329010909
pearson: 0.8444534360361612

=== Experiment 2959 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006171828293550503
rmse: 0.07856098455054203
mae: 0.03240522703337083
r2: 0.7217149786731762
pearson: 0.8530314056122267

=== Experiment 2997 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005187165806219276
rmse: 0.07202198140997841
mae: 0.027753090647423137
r2: 0.766112976843838
pearson: 0.8755904402018526

=== Experiment 2872 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006599335157576383
rmse: 0.08123629212104885
mae: 0.028864210479843052
r2: 0.7024388823344094
pearson: 0.8604003755081171

=== Experiment 2692 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006440941207481331
rmse: 0.08025547462622927
mae: 0.04146420072387493
r2: 0.7095807958296867
pearson: 0.8545347298153522

=== Experiment 2615 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063786182283951
rmse: 0.07986625212438042
mae: 0.03417557229048733
r2: 0.7123909115262439
pearson: 0.850546390928893

=== Experiment 2829 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00653091842661193
rmse: 0.08081409794467752
mae: 0.03147427342023375
r2: 0.7055237626210904
pearson: 0.8551663167570395

=== Experiment 2859 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005815032364809964
rmse: 0.07625635950404375
mae: 0.03355144672858018
r2: 0.7378027500621894
pearson: 0.8604753255916325

=== Experiment 2504 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006049019840424362
rmse: 0.07777544497091844
mae: 0.03157370100373798
r2: 0.7272523577725002
pearson: 0.8604826016246014

=== Experiment 2576 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00615392580216048
rmse: 0.0784469617140172
mae: 0.03437194028198219
r2: 0.7225221941304633
pearson: 0.8515511216082053

=== Experiment 2886 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005961952520714809
rmse: 0.07721368091675729
mae: 0.029715383126520663
r2: 0.7311781848969459
pearson: 0.8557457999105943

=== Experiment 2842 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00590734701178278
rmse: 0.0768592675725106
mae: 0.028145269463730496
r2: 0.7336403232609685
pearson: 0.8575723632060753

=== Experiment 508 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006731269262266274
rmse: 0.08204431279659959
mae: 0.0392367300594977
r2: 0.6964900316225826
pearson: 0.8355545275387265

=== Experiment 516 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006414354265846407
rmse: 0.08008966391393091
mae: 0.037058938496151
r2: 0.7107795893261974
pearson: 0.8434817514467788

=== Experiment 514 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006910426838523709
rmse: 0.08312897712905956
mae: 0.03789840545550175
r2: 0.6884118953624143
pearson: 0.829968466080543

=== Experiment 691 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007595383807218141
rmse: 0.08715149916793251
mae: 0.039087741271372815
r2: 0.657527487116599
pearson: 0.8200470730670472

=== Experiment 684 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00654950564471464
rmse: 0.08092901608641143
mae: 0.03775452762939118
r2: 0.7046856731376997
pearson: 0.8397900861118806

=== Experiment 704 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007404947781717367
rmse: 0.08605200626201209
mae: 0.037877161082028196
r2: 0.6661141636891244
pearson: 0.8292254230996459

=== Experiment 504 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007998423337206491
rmse: 0.08943390485272625
mae: 0.038538653323505624
r2: 0.6393546121006872
pearson: 0.8215302456304341

=== Experiment 599 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006963027991166004
rmse: 0.08344476011809253
mae: 0.044611154940093885
r2: 0.6860401325413108
pearson: 0.8324844362704064

=== Experiment 672 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007126647546955257
rmse: 0.08441947374246808
mae: 0.04014696729464863
r2: 0.6786625987852473
pearson: 0.8262039432462924

=== Experiment 549 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00663622215991799
rmse: 0.08146301099221652
mae: 0.036606320631384894
r2: 0.7007756636340379
pearson: 0.8380188454030817

=== Experiment 749 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00800347755445975
rmse: 0.08946215710824186
mae: 0.040761274958545785
r2: 0.6391267196692689
pearson: 0.8122557111469121

=== Experiment 576 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007287970486944653
rmse: 0.08536961102725403
mae: 0.040423660975586743
r2: 0.6713886184247813
pearson: 0.8203179034621795

=== Experiment 593 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007650457972784652
rmse: 0.08746689643965111
mae: 0.047640280886491915
r2: 0.655044217231199
pearson: 0.8157777013957029

=== Experiment 699 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006854771204693723
rmse: 0.08279354567050334
mae: 0.040418956188553075
r2: 0.6909213833958905
pearson: 0.8330062086357237

=== Experiment 561 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006944361720169576
rmse: 0.08333283698620596
mae: 0.031296289313457765
r2: 0.6868817864848857
pearson: 0.8451950244784554

=== Experiment 557 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006829449363649946
rmse: 0.08264048259569849
mae: 0.04438331996558864
r2: 0.69206313406356
pearson: 0.8369420452398663

=== Experiment 625 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007419460547844812
rmse: 0.08613629053914972
mae: 0.04442956310629237
r2: 0.6654597894519948
pearson: 0.8206131082424044

=== Experiment 594 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006653715590244537
rmse: 0.0815703107156307
mae: 0.04159584752257266
r2: 0.6999868925600612
pearson: 0.8399206454702773

=== Experiment 540 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007383144042682096
rmse: 0.08592522355328554
mae: 0.0342576534226859
r2: 0.6670972846856653
pearson: 0.8379972900685025

=== Experiment 535 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00729400908383189
rmse: 0.08540497107213309
mae: 0.04474786185509964
r2: 0.6711163407489256
pearson: 0.8234911313784067

=== Experiment 805 ===
num_layers: 1
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064790664440704014
rmse: 0.08049264838524324
mae: 0.03675904819214119
r2: 0.7078617456920854
pearson: 0.84196523065424

=== Experiment 727 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006790896802504931
rmse: 0.08240689778474208
mae: 0.03753602978640176
r2: 0.6938014520773088
pearson: 0.8346344842484243

=== Experiment 651 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006578135478270489
rmse: 0.08110570558395068
mae: 0.03365154058505439
r2: 0.7033947665436198
pearson: 0.8389468461380765

=== Experiment 635 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005758330200758658
rmse: 0.07588366227824443
mae: 0.027609484347881557
r2: 0.7403594256827319
pearson: 0.8679428822024318

=== Experiment 532 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006956464615099674
rmse: 0.08340542317559257
mae: 0.040252576670309966
r2: 0.686336072279379
pearson: 0.8307657467586987

=== Experiment 786 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007679954728704729
rmse: 0.08763535090763731
mae: 0.03913310272506008
r2: 0.6537142214892779
pearson: 0.822073773356228

=== Experiment 548 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00632314215963134
rmse: 0.07951818760278268
mae: 0.030068746509692997
r2: 0.7148923030499168
pearson: 0.8652333431535054

=== Experiment 551 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007142453410480111
rmse: 0.08451303692614597
mae: 0.03646516445005174
r2: 0.6779499193556042
pearson: 0.8349144840444908

=== Experiment 585 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0065844684455870365
rmse: 0.08114473763336127
mae: 0.0371005663665557
r2: 0.7031092158346687
pearson: 0.8435439740814679

=== Experiment 695 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006970464192351483
rmse: 0.08348930585620822
mae: 0.03720482057179844
r2: 0.6857048375027801
pearson: 0.8282976591607043

=== Experiment 520 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006403502427976203
rmse: 0.080021887180797
mae: 0.04180083237294205
r2: 0.7112688939195058
pearson: 0.8467188526685951

=== Experiment 526 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006879053526680631
rmse: 0.08294005984251898
mae: 0.04381773096001802
r2: 0.6898265041849682
pearson: 0.8372627711840147

=== Experiment 543 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071810995182449685
rmse: 0.08474136839964863
mae: 0.03952102094569518
r2: 0.6762073833659394
pearson: 0.8227766085711578

=== Experiment 580 ===
num_layers: 1
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006552672608090579
rmse: 0.08094858002516522
mae: 0.02982674559756108
r2: 0.7045428761529657
pearson: 0.8407617168273777

=== Experiment 615 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006429066258174444
rmse: 0.08018145831908051
mae: 0.03458120691364301
r2: 0.7101162320673691
pearson: 0.8435712360404495

=== Experiment 843 ===
num_layers: 1
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006635110864039394
rmse: 0.08145618984484479
mae: 0.03728774666647854
r2: 0.7008257714760848
pearson: 0.8377421919742123

=== Experiment 582 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006936599862715715
rmse: 0.08328625254335624
mae: 0.04494376243589006
r2: 0.6872317652212256
pearson: 0.8354142757196558

=== Experiment 558 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006677729137319903
rmse: 0.08171737353415064
mae: 0.03747235045166019
r2: 0.6989041322916034
pearson: 0.8401177525932241

=== Experiment 784 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006475447382047918
rmse: 0.08047016454592297
mae: 0.03486916990578627
r2: 0.708024927605808
pearson: 0.8449294824613107

=== Experiment 669 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007253607019441903
rmse: 0.08516811034326113
mae: 0.0350206783466565
r2: 0.6729380520499615
pearson: 0.820582632696631

=== Experiment 515 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00782920735974753
rmse: 0.08848280827227134
mae: 0.04424016110218992
r2: 0.6469844860466647
pearson: 0.8073617312265045

=== Experiment 885 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006740895813854148
rmse: 0.08210295861815303
mae: 0.042399723238829415
r2: 0.6960559746484546
pearson: 0.8383389791463342

=== Experiment 542 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006942111345205065
rmse: 0.08331933356193547
mae: 0.042335987420062456
r2: 0.6869832549015698
pearson: 0.8311414587560798

=== Experiment 563 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007181536871905801
rmse: 0.08474394888076553
mae: 0.03598565496691843
r2: 0.6761876632818667
pearson: 0.827259287918285

=== Experiment 832 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0075775227990590825
rmse: 0.08704896782305395
mae: 0.04779552886024241
r2: 0.6583328321132602
pearson: 0.8171948238727245

=== Experiment 810 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005891056483519684
rmse: 0.07675321806621326
mae: 0.029908220841454117
r2: 0.7343748560103416
pearson: 0.8601759187451119

=== Experiment 538 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006372455342369731
rmse: 0.0798276602586455
mae: 0.03362487816692999
r2: 0.7126687933446341
pearson: 0.8447089254650002

=== Experiment 788 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00756899635218584
rmse: 0.08699997903554828
mae: 0.04430162677762058
r2: 0.6587172858499983
pearson: 0.8244804353260963

=== Experiment 553 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006287029692039571
rmse: 0.07929079197510623
mae: 0.03816892242043058
r2: 0.716520598319317
pearson: 0.8483661628874745

=== Experiment 737 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006501612626063255
rmse: 0.08063257794504189
mae: 0.03849678941045687
r2: 0.706845148269361
pearson: 0.841930913441236

=== Experiment 571 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006449276391169519
rmse: 0.0803073869028841
mae: 0.03622019659329283
r2: 0.7092049660657811
pearson: 0.8424045755945965

=== Experiment 850 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0063946100140441
rmse: 0.07996630549202645
mae: 0.03379991081806481
r2: 0.7116698489498536
pearson: 0.8469143055566863

=== Experiment 711 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007309639658853892
rmse: 0.0854964306790283
mae: 0.03798973822564879
r2: 0.6704115650007256
pearson: 0.8206510651321232

=== Experiment 721 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006079316063341123
rmse: 0.07796996898384097
mae: 0.03571836195228165
r2: 0.7258863144155707
pearson: 0.8526827182197961

=== Experiment 729 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007638138706406205
rmse: 0.08739644561654784
mae: 0.04128576418398474
r2: 0.6555996875300795
pearson: 0.8194666365650649

=== Experiment 779 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006216734712318727
rmse: 0.07884627265964275
mae: 0.034584284914951365
r2: 0.7196901680157433
pearson: 0.8484347804539715

=== Experiment 654 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0062123616162869846
rmse: 0.07881853599431408
mae: 0.03391834975646699
r2: 0.7198873489910678
pearson: 0.8489752901474301

=== Experiment 573 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007933235535635002
rmse: 0.08906871243952616
mae: 0.038869005832776624
r2: 0.6422939013822002
pearson: 0.8104475288555208

=== Experiment 723 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00712142035788591
rmse: 0.08438850844686088
mae: 0.0359160624114476
r2: 0.6788982904397223
pearson: 0.82591476673552

=== Experiment 806 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006223627036276096
rmse: 0.07888996790642075
mae: 0.03704636899437043
r2: 0.7193793961620494
pearson: 0.8493938368394761

=== Experiment 753 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006888778346688083
rmse: 0.08299866472834418
mae: 0.03311199270825474
r2: 0.6893880163310527
pearson: 0.8486256957551809

=== Experiment 776 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006699424782849829
rmse: 0.08185001394532458
mae: 0.03696611775562569
r2: 0.6979258851836427
pearson: 0.8374783859109887

=== Experiment 670 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007403268020261202
rmse: 0.08604224555566412
mae: 0.03793359123721836
r2: 0.6661899033938633
pearson: 0.820406010746265

=== Experiment 640 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0075279392488699
rmse: 0.08676369775931579
mae: 0.04303449506344945
r2: 0.6605685325679009
pearson: 0.818274734214972

=== Experiment 502 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0071056856269749065
rmse: 0.08429522896922996
mae: 0.046093174613733134
r2: 0.6796077625311148
pearson: 0.8330573629251998

=== Experiment 510 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005552787613868715
rmse: 0.07451702901933702
mae: 0.028170477756300112
r2: 0.7496272504593888
pearson: 0.8664036194693421

=== Experiment 880 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006347380012353454
rmse: 0.0796704462919184
mae: 0.04020182095625442
r2: 0.7137994289385714
pearson: 0.8487017812855567

=== Experiment 862 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006649705895443961
rmse: 0.08154572886082974
mae: 0.03647213378624387
r2: 0.7001676879338177
pearson: 0.8370409164617191

=== Experiment 686 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006300746559882492
rmse: 0.07937724207783042
mae: 0.0336928310463656
r2: 0.715902110785535
pearson: 0.8494695919035027

=== Experiment 768 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007289130428877414
rmse: 0.08537640440354358
mae: 0.04979644622147142
r2: 0.6713363171535625
pearson: 0.8399638594398867

=== Experiment 676 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005851661737207088
rmse: 0.07649615504851918
mae: 0.029202320054604665
r2: 0.7361511477826228
pearson: 0.8617814967903844

=== Experiment 871 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007383535276144327
rmse: 0.08592750011576228
mae: 0.033422943158394815
r2: 0.6670796441410483
pearson: 0.8399282345183803

=== Experiment 800 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006659951869909623
rmse: 0.08160852816899483
mae: 0.03763470425536457
r2: 0.6997057014547591
pearson: 0.8404027070372593

=== Experiment 730 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007022980500334835
rmse: 0.08380322488028033
mae: 0.04306897454033266
r2: 0.6833369002900056
pearson: 0.830004211089986

=== Experiment 718 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007058490310884176
rmse: 0.08401482197138893
mae: 0.03594794219283374
r2: 0.6817357785613987
pearson: 0.8258484931016452

=== Experiment 648 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007238717345152534
rmse: 0.08508065200239437
mae: 0.03727361998372538
r2: 0.6736094209102224
pearson: 0.8220046158015861

=== Experiment 879 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007598606021519268
rmse: 0.08716998348926808
mae: 0.04746411187775004
r2: 0.6573821988919661
pearson: 0.8219765647311849

=== Experiment 616 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007355956613859029
rmse: 0.08576687363929636
mae: 0.0406226688986284
r2: 0.6683231538851948
pearson: 0.8196579272114618

=== Experiment 523 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005890292169164863
rmse: 0.07674823886686172
mae: 0.029400515010708377
r2: 0.7344093186082001
pearson: 0.871162174786478

=== Experiment 683 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006708811896811243
rmse: 0.08190733725870498
mae: 0.035017452963617274
r2: 0.6975026243467074
pearson: 0.8468680546678319

=== Experiment 679 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0065981133303909135
rmse: 0.08122877156765891
mae: 0.03981433614938097
r2: 0.7024939739844351
pearson: 0.8427217446491194

=== Experiment 567 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006816023033720086
rmse: 0.0825592092605064
mae: 0.03577244263944056
r2: 0.6926685213707175
pearson: 0.8323924182070637

=== Experiment 803 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006334779723226708
rmse: 0.07959132944753912
mae: 0.035705352826280325
r2: 0.714367570429485
pearson: 0.845484799845683

=== Experiment 708 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005547616417570535
rmse: 0.07448232285294636
mae: 0.028824020747411864
r2: 0.7498604174244561
pearson: 0.8681593807763026

=== Experiment 518 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0062495537145852
rmse: 0.07905411889702649
mae: 0.03540318831520651
r2: 0.7182103736482957
pearson: 0.8485770152075601

=== Experiment 650 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006694289834222316
rmse: 0.08181863989472274
mae: 0.03477396066555525
r2: 0.6981574177571935
pearson: 0.8358045471942871

=== Experiment 882 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006550188461148771
rmse: 0.08093323458968368
mae: 0.04023410766215929
r2: 0.7046548852452152
pearson: 0.8420059506545478

=== Experiment 574 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00658225988977842
rmse: 0.08113112774871566
mae: 0.03710042724372389
r2: 0.7032087986440195
pearson: 0.8407589637042678

=== Experiment 864 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007487361018566318
rmse: 0.08652953841646399
mae: 0.04366007890728753
r2: 0.6623981871124445
pearson: 0.8291311408625368

=== Experiment 707 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007120188409952452
rmse: 0.08438120886756986
mae: 0.0394016191816595
r2: 0.6789538384298202
pearson: 0.8272353368221235

=== Experiment 536 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006751101395339447
rmse: 0.08216508623094998
mae: 0.042178678138177786
r2: 0.6955958094711023
pearson: 0.8393643105349339

=== Experiment 828 ===
num_layers: 1
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0069781251731874955
rmse: 0.08353517326963233
mae: 0.04063898171079457
r2: 0.6853594072487399
pearson: 0.8303413178980169

=== Experiment 653 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007367191443782238
rmse: 0.08583234497427085
mae: 0.035906573008004046
r2: 0.6678165803487842
pearson: 0.8173829311815841

=== Experiment 870 ===
num_layers: 2
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007624404945822372
rmse: 0.0873178386460772
mae: 0.0490529321181769
r2: 0.6562189367501143
pearson: 0.8201430296999574

=== Experiment 732 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006962712173950159
rmse: 0.08344286772367163
mae: 0.04149320207839382
r2: 0.6860543726005717
pearson: 0.8297769344652266

=== Experiment 575 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006636179134271453
rmse: 0.08146274691091293
mae: 0.03512207055155039
r2: 0.700777603641503
pearson: 0.841472952143843

=== Experiment 649 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0061947434861575066
rmse: 0.07870669276597453
mae: 0.03234722753683684
r2: 0.7206817427242739
pearson: 0.8495855171225484

=== Experiment 519 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0062732575991270405
rmse: 0.07920389888841989
mae: 0.030104457468598095
r2: 0.7171415759271815
pearson: 0.8476306303973421

=== Experiment 655 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0058533851969374915
rmse: 0.07650741922805586
mae: 0.03072319071333446
r2: 0.7360734377419317
pearson: 0.8588472083629249

=== Experiment 765 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006330530650948702
rmse: 0.07956463190984234
mae: 0.03842489762276165
r2: 0.7145591592283407
pearson: 0.8482332312632561

=== Experiment 621 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007634712972525525
rmse: 0.08737684460156206
mae: 0.05121838747644961
r2: 0.6557541523629786
pearson: 0.8264412813597174

=== Experiment 757 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006340515432404344
rmse: 0.07962735354389434
mae: 0.03358274285525061
r2: 0.7141089498271441
pearson: 0.845109964243216

=== Experiment 873 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007522341259992908
rmse: 0.08673143178797932
mae: 0.047950922961043946
r2: 0.6608209434225013
pearson: 0.8217799950732629

=== Experiment 714 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005530621244930417
rmse: 0.07436814670899375
mae: 0.027359843403118884
r2: 0.7506267222786515
pearson: 0.8753766510489471

=== Experiment 792 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0072465127841949755
rmse: 0.0851264517303228
mae: 0.046747621717158294
r2: 0.6732579279948337
pearson: 0.828289648709323

=== Experiment 840 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007562737994044746
rmse: 0.08696400401341205
mae: 0.04109565082154155
r2: 0.6589994724640664
pearson: 0.8192303116826599

=== Experiment 545 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00742135838510856
rmse: 0.08614730631371222
mae: 0.04401345903643024
r2: 0.6653742168050745
pearson: 0.8222058992628364

=== Experiment 772 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007426515904420804
rmse: 0.08617723541876245
mae: 0.051121555456091525
r2: 0.6651416665292831
pearson: 0.8337431530215411

=== Experiment 895 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006285330577932373
rmse: 0.0792800768032699
mae: 0.030140246396516152
r2: 0.7165972106265734
pearson: 0.8665898208904753

=== Experiment 705 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007472171197841461
rmse: 0.08644172139564009
mae: 0.041293653033333176
r2: 0.663083089443377
pearson: 0.8152342936497463

=== Experiment 894 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006130123278344097
rmse: 0.07829510379547432
mae: 0.03263007907375015
r2: 0.723595439453052
pearson: 0.8519872298342832

=== Experiment 509 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00792119076093638
rmse: 0.08900107168420153
mae: 0.045045021914722976
r2: 0.6428369949720503
pearson: 0.8056998056770244

=== Experiment 719 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0071154992013868
rmse: 0.08435341843332018
mae: 0.03694601118297856
r2: 0.6791652727801669
pearson: 0.8249577508656738

=== Experiment 835 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0061426498278100115
rmse: 0.07837505871008972
mae: 0.03118012210016452
r2: 0.7230306228509901
pearson: 0.8513084702922387

=== Experiment 869 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007159156413058118
rmse: 0.08461179830885358
mae: 0.04498517315552623
r2: 0.6771967883209155
pearson: 0.84179998385974

=== Experiment 815 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005989565752059908
rmse: 0.07739228483550481
mae: 0.03550509380784779
r2: 0.7299331164491079
pearson: 0.8547600567189771

=== Experiment 564 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006215365801471779
rmse: 0.07883759129673977
mae: 0.02836178963019222
r2: 0.7197518916033285
pearson: 0.8534735072375972

=== Experiment 641 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006548191166318257
rmse: 0.08092089449776403
mae: 0.03831987458308982
r2: 0.7047449423900463
pearson: 0.8483125347461833

=== Experiment 804 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006958907207831461
rmse: 0.08342006477959281
mae: 0.03493417084503358
r2: 0.68622593684817
pearson: 0.828748949677016

=== Experiment 872 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007315066947997223
rmse: 0.08552816464765992
mae: 0.03825021090660162
r2: 0.6701668509219855
pearson: 0.8200641736694402

=== Experiment 789 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007357197114901556
rmse: 0.08577410515360423
mae: 0.03683108998141504
r2: 0.6682672202391735
pearson: 0.8186988460330547

=== Experiment 759 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007445797928822736
rmse: 0.08628903713000126
mae: 0.046788677529360695
r2: 0.664272248535668
pearson: 0.8238800955747274

=== Experiment 787 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006542631385820627
rmse: 0.0808865340203215
mae: 0.03175112208666167
r2: 0.7049956304456378
pearson: 0.8445779533627564

=== Experiment 796 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006799888115958594
rmse: 0.08246143411291483
mae: 0.03487197663502067
r2: 0.6933960377110628
pearson: 0.8454918210049478

=== Experiment 606 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006407792596470864
rmse: 0.08004868891162968
mae: 0.029692541574703545
r2: 0.7110754521103291
pearson: 0.8639756552991217

=== Experiment 942 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006339649225835581
rmse: 0.07962191423116868
mae: 0.037185348064292904
r2: 0.7141480067000829
pearson: 0.8453851363352661

=== Experiment 829 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006572072049213473
rmse: 0.08106831717269992
mae: 0.03535328333881474
r2: 0.7036681638910813
pearson: 0.841194056196014

=== Experiment 638 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007102620986395542
rmse: 0.08427704898960062
mae: 0.045636828947125
r2: 0.6797459458259869
pearson: 0.8326922359430772

=== Experiment 524 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007875320316921754
rmse: 0.08874300150953739
mae: 0.03675087436866073
r2: 0.6449052731035434
pearson: 0.8231616633953257

=== Experiment 919 ===
num_layers: 1
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.00698587408407594
rmse: 0.08358154152727706
mae: 0.03892403539139223
r2: 0.6850100122673364
pearson: 0.8292058834307511

=== Experiment 643 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005876591785488616
rmse: 0.07665893154413655
mae: 0.029975274520051227
r2: 0.7350270628781621
pearson: 0.8614555045534995

=== Experiment 846 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006856535325332037
rmse: 0.0828041987180121
mae: 0.041678625418115865
r2: 0.690841839972758
pearson: 0.8412967098831571

=== Experiment 701 ===
num_layers: 2
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006974256523707079
rmse: 0.08351201424769421
mae: 0.03978215743718024
r2: 0.685533842951089
pearson: 0.8301178493183244

=== Experiment 623 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006030735236388627
rmse: 0.07765780859893374
mae: 0.03428928369244578
r2: 0.7280768025208016
pearson: 0.8543245405908582

=== Experiment 642 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005984467416605389
rmse: 0.07735933955641937
mae: 0.031585296001867574
r2: 0.7301629981508031
pearson: 0.8654039491643025

=== Experiment 529 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006463359229372018
rmse: 0.08039501992892358
mae: 0.034530985466485656
r2: 0.7085699770895615
pearson: 0.8440344883737365

=== Experiment 590 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006826055605553688
rmse: 0.08261994677772244
mae: 0.043123065473802166
r2: 0.692216157122413
pearson: 0.8404744313281306

=== Experiment 668 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007137261672228723
rmse: 0.08448231573666008
mae: 0.046572196915434086
r2: 0.6781840125483004
pearson: 0.8309236952810084

=== Experiment 511 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007654983214400992
rmse: 0.08749276092569598
mae: 0.03945258963543503
r2: 0.6548401760784293
pearson: 0.8156227067821851

=== Experiment 745 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006087879967556377
rmse: 0.07802486762280585
mae: 0.03255494694846961
r2: 0.7255001717437674
pearson: 0.8521529092786408

=== Experiment 660 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007109367880705094
rmse: 0.08431706755280982
mae: 0.04059686355364773
r2: 0.6794417313311043
pearson: 0.827824045377469

=== Experiment 608 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006997142377534585
rmse: 0.08364892334952426
mae: 0.03821738893643831
r2: 0.6845019298748414
pearson: 0.8283337567804747

=== Experiment 946 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0063652963130427405
rmse: 0.07978280712686625
mae: 0.034261496449516496
r2: 0.7129915908260585
pearson: 0.8462701795339865

=== Experiment 982 ===
num_layers: 1
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006896575167854626
rmse: 0.08304562100348595
mae: 0.03811273691251333
r2: 0.6890364610962967
pearson: 0.8336386480076613

=== Experiment 837 ===
num_layers: 1
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006928537247828869
rmse: 0.08323783543454784
mae: 0.033116815825647426
r2: 0.6875953049778454
pearson: 0.8298959982442901

=== Experiment 656 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007070427628878209
rmse: 0.0840858348883937
mae: 0.036823608337385544
r2: 0.6811975301470631
pearson: 0.8266064398139727

=== Experiment 566 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007154237716208507
rmse: 0.08458272705587416
mae: 0.04131861272842035
r2: 0.6774185701969803
pearson: 0.8257125060199927

=== Experiment 734 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006628097848811192
rmse: 0.08141313069039412
mae: 0.03549837356700556
r2: 0.7011419852460603
pearson: 0.8374946005271999

=== Experiment 713 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007222540765534836
rmse: 0.08498553268371527
mae: 0.04498943341462722
r2: 0.6743388157653273
pearson: 0.829841732379913

=== Experiment 892 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008115568582247231
rmse: 0.09008645060300262
mae: 0.05794848422125944
r2: 0.6340725845613657
pearson: 0.8231272245113219

=== Experiment 521 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00642000530697677
rmse: 0.0801249356129337
mae: 0.035604805088020955
r2: 0.7105247863688426
pearson: 0.8455176645281232

=== Experiment 663 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006952666010700741
rmse: 0.08338264813917066
mae: 0.03371135117046516
r2: 0.6865073496798333
pearson: 0.8431840060516888

=== Experiment 825 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006384388074735941
rmse: 0.07990236588947752
mae: 0.03411622875604173
r2: 0.7121307516942389
pearson: 0.8464396971682981

=== Experiment 941 ===
num_layers: 1
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005827129215933195
rmse: 0.07633563529527475
mae: 0.028529595771344
r2: 0.7372573083692744
pearson: 0.8605472408371333

=== Experiment 781 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006975310918980833
rmse: 0.08351832684495561
mae: 0.03368729011641336
r2: 0.6854863007322702
pearson: 0.8322231838435782

=== Experiment 743 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.008425936075189202
rmse: 0.09179289773827386
mae: 0.05614998318481483
r2: 0.6200782509077924
pearson: 0.8072561348058789

=== Experiment 618 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00596161327790297
rmse: 0.07721148410633595
mae: 0.033517508852098366
r2: 0.7311934812060195
pearson: 0.8551536027892864

=== Experiment 963 ===
num_layers: 1
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006672619262325421
rmse: 0.08168610201451298
mae: 0.04151418661529832
r2: 0.6991345343060765
pearson: 0.8388731656620625

=== Experiment 986 ===
num_layers: 2
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006224189328747583
rmse: 0.07889353160270861
mae: 0.03278472354400217
r2: 0.7193540426419938
pearson: 0.8481661973731076

=== Experiment 830 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007114754634958278
rmse: 0.08434900494349816
mae: 0.04013922436203884
r2: 0.6791988449527202
pearson: 0.8255880344471919

=== Experiment 819 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007176215873404471
rmse: 0.08471254850023384
mae: 0.04439046093538936
r2: 0.6764275847623411
pearson: 0.8297087817321835

=== Experiment 834 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007902044773163247
rmse: 0.08889344617666281
mae: 0.048687950604078836
r2: 0.6437002791339472
pearson: 0.8253317768809435

=== Experiment 513 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006646024772371962
rmse: 0.08152315482347308
mae: 0.04088861376900497
r2: 0.7003336681529477
pearson: 0.8396427687916538

=== Experiment 965 ===
num_layers: 1
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007531374069647081
rmse: 0.08678348961436778
mae: 0.0442486213561842
r2: 0.6604136580108382
pearson: 0.8171729823473557

=== Experiment 547 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006607069701873274
rmse: 0.08128388340792579
mae: 0.03653059451799542
r2: 0.7020901351362959
pearson: 0.8383424906595274

=== Experiment 733 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006690417796918759
rmse: 0.08179497415439875
mae: 0.04344714491003454
r2: 0.698332006214997
pearson: 0.8420326004325247

=== Experiment 689 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008211309781724957
rmse: 0.09061627768632387
mae: 0.04166798216567795
r2: 0.6297556560158394
pearson: 0.7936835930242325

=== Experiment 636 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005714416584996026
rmse: 0.07559376022527274
mae: 0.028701218824252535
r2: 0.7423394712896085
pearson: 0.8663131205436863

=== Experiment 770 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062928049331774155
rmse: 0.07932720172284798
mae: 0.033593488672417626
r2: 0.7162601952382897
pearson: 0.8503309970725997

=== Experiment 856 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006902523179242349
rmse: 0.083081424995256
mae: 0.04156254470984881
r2: 0.6887682678807443
pearson: 0.834710873232816

=== Experiment 853 ===
num_layers: 1
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.008219478508123098
rmse: 0.09066133965546229
mae: 0.04190589600091541
r2: 0.6293873317378788
pearson: 0.7941245597207176

=== Experiment 722 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007180372095559544
rmse: 0.08473707627455378
mae: 0.04646590037312199
r2: 0.6762401825346622
pearson: 0.8355979401193535

=== Experiment 816 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006788087671710044
rmse: 0.08238985175196059
mae: 0.02974642762501386
r2: 0.6939281145484573
pearson: 0.8529233474344509

=== Experiment 908 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006971550171490283
rmse: 0.08349580930496023
mae: 0.04183719349373204
r2: 0.6856558711813876
pearson: 0.838489514086646

=== Experiment 904 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00741157465537466
rmse: 0.08609050270137038
mae: 0.04045192747162996
r2: 0.6658153608726272
pearson: 0.821998949599537

=== Experiment 903 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006207305041871333
rmse: 0.07878645214674496
mae: 0.0318171494807505
r2: 0.7201153477059009
pearson: 0.849146963708377

=== Experiment 647 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006664915886051351
rmse: 0.08163893609088343
mae: 0.03623368383081791
r2: 0.6994818761517592
pearson: 0.8370246160641873

=== Experiment 798 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007554512435651894
rmse: 0.08691669825558201
mae: 0.04709521557314382
r2: 0.659370359271655
pearson: 0.8206208622412975

=== Experiment 659 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00783941033775705
rmse: 0.08854044464399899
mae: 0.04833195133145255
r2: 0.6465244382588895
pearson: 0.8259828377089498

=== Experiment 645 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007072109446080457
rmse: 0.08409583489139315
mae: 0.034853617429403294
r2: 0.6811216977496382
pearson: 0.8398657395840088

=== Experiment 764 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006744042186969323
rmse: 0.08212211752609234
mae: 0.03638390169733367
r2: 0.695914106069516
pearson: 0.8345275765565023

=== Experiment 974 ===
num_layers: 3
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0066589313595642715
rmse: 0.08160227545580008
mae: 0.03833615668083316
r2: 0.6997517158170701
pearson: 0.8374643720366897

=== Experiment 929 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006431409688526234
rmse: 0.08019607028106947
mae: 0.035508861001431476
r2: 0.7100105678242294
pearson: 0.8452303497351696

=== Experiment 938 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006614087048378198
rmse: 0.08132703762204915
mae: 0.03967240351071378
r2: 0.7017737260709593
pearson: 0.8411093606049573

=== Experiment 681 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006160897673225777
rmse: 0.07849138598104748
mae: 0.02951299799316615
r2: 0.7222078355326842
pearson: 0.850198054668382

=== Experiment 944 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005613360327296797
rmse: 0.07492236199758251
mae: 0.030441886224095438
r2: 0.7468960534710065
pearson: 0.866602154094728

=== Experiment 823 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007263305201184062
rmse: 0.0852250268476582
mae: 0.04609422854429492
r2: 0.6725007653037038
pearson: 0.8266581368947298

=== Experiment 971 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00572818106350124
rmse: 0.07568474789216939
mae: 0.029965091153795635
r2: 0.7417188370120189
pearson: 0.861823033810486

=== Experiment 884 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00692619127110326
rmse: 0.08322374223202932
mae: 0.03601929435985059
r2: 0.6877010840358597
pearson: 0.8468919873123808

=== Experiment 847 ===
num_layers: 2
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0073118342479721035
rmse: 0.08550926410613123
mae: 0.043926009973070806
r2: 0.6703126119432978
pearson: 0.8239706100942481

=== Experiment 537 ===
num_layers: 3
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007767293331621234
rmse: 0.0881322491011164
mae: 0.04604352248097545
r2: 0.6497761623244316
pearson: 0.8149192476963407

=== Experiment 766 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006939801668943345
rmse: 0.083305472022811
mae: 0.042992779511882205
r2: 0.6870873971876443
pearson: 0.8355379430745388

=== Experiment 702 ===
num_layers: 3
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.008092088414630813
rmse: 0.08995603600999108
mae: 0.0474959547789833
r2: 0.6351312949847779
pearson: 0.8069876120971152

=== Experiment 975 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007026522734571462
rmse: 0.08382435645187777
mae: 0.03796213022658919
r2: 0.6831771825073326
pearson: 0.8279030515747327

=== Experiment 886 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0074357588201242174
rmse: 0.08623084610581191
mae: 0.04906691727214051
r2: 0.6647249075283345
pearson: 0.8276152093381587

=== Experiment 725 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005915386378706401
rmse: 0.0769115490593344
mae: 0.027460590982805108
r2: 0.7332778317447783
pearson: 0.8705378886994107

=== Experiment 956 ===
num_layers: 3
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006010727430094665
rmse: 0.07752888126430475
mae: 0.03229933488942455
r2: 0.7289789457004875
pearson: 0.8544932601957431

=== Experiment 985 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00551319964220524
rmse: 0.07425092351079035
mae: 0.02781706001124996
r2: 0.751412254677696
pearson: 0.8670765898564784

=== Experiment 664 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006951756520592041
rmse: 0.08337719424754014
mae: 0.04473002330157605
r2: 0.6865483581885374
pearson: 0.8377289257474007

=== Experiment 782 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006969181548607064
rmse: 0.0834816240175469
mae: 0.03477796491082436
r2: 0.6857626713446813
pearson: 0.8321285193660697

=== Experiment 674 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006549870895328053
rmse: 0.08093127266593583
mae: 0.03402038774232727
r2: 0.7046692041482998
pearson: 0.8396127646117538

=== Experiment 793 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006889830794605569
rmse: 0.08300500463589873
mae: 0.03749256786553331
r2: 0.689340561917671
pearson: 0.8440474780160232

=== Experiment 783 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008414713321088292
rmse: 0.09173174652806024
mae: 0.053203170124470844
r2: 0.6205842799506907
pearson: 0.8100742270201993

=== Experiment 517 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.0070738183908782394
rmse: 0.08410599497585318
mae: 0.04363124142551189
r2: 0.6810446421808702
pearson: 0.8349893642331303

=== Experiment 748 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00586672138691941
rmse: 0.07659452582867401
mae: 0.03267680433412358
r2: 0.735472114805353
pearson: 0.8591607271894813

=== Experiment 923 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006286875465621763
rmse: 0.07928981943239474
mae: 0.031960330850025344
r2: 0.7165275523206158
pearson: 0.8473181504690143

=== Experiment 912 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006934285017986655
rmse: 0.0832723544640516
mae: 0.03336214154320783
r2: 0.687336140551218
pearson: 0.8303561038467164

=== Experiment 811 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0064882053404571914
rmse: 0.0805493968969178
mae: 0.031229926279174303
r2: 0.7074496768763407
pearson: 0.8552766950154669

=== Experiment 661 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006701229813790942
rmse: 0.08186103965740321
mae: 0.0379603927503743
r2: 0.6978444971329627
pearson: 0.8364343617132309

=== Experiment 860 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007832552621717804
rmse: 0.08850170971070448
mae: 0.04589159053532397
r2: 0.6468336496567874
pearson: 0.8116795310944868

=== Experiment 619 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00674321279051164
rmse: 0.08211706759566881
mae: 0.031993784264027665
r2: 0.6959515031907477
pearson: 0.8514917995464074

=== Experiment 883 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005683917127047209
rmse: 0.07539175768641562
mae: 0.02720890997786836
r2: 0.7437146784246828
pearson: 0.8625112730563833

=== Experiment 979 ===
num_layers: 2
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007042149493437942
rmse: 0.08391751601088977
mae: 0.04302444174760444
r2: 0.6824725788279056
pearson: 0.8351876226634201

=== Experiment 586 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007703475481399576
rmse: 0.08776944503299298
mae: 0.04134098930967125
r2: 0.6526536811025423
pearson: 0.8095026968613477

=== Experiment 957 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007121101174300418
rmse: 0.08438661727015971
mae: 0.041099585085799195
r2: 0.6789126822871085
pearson: 0.8280489948878322

=== Experiment 675 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005895202056682167
rmse: 0.07678021917578881
mae: 0.029514901761724326
r2: 0.7341879339410515
pearson: 0.8635458097586897

=== Experiment 934 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008299360002485209
rmse: 0.09110082328105058
mae: 0.05425352868370544
r2: 0.6257855103156255
pearson: 0.816203118236122

=== Experiment 855 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064051615684539995
rmse: 0.08003225330111605
mae: 0.03070969393610391
r2: 0.7111940840056055
pearson: 0.8612957894597575

=== Experiment 634 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005838082307892705
rmse: 0.0764073445939113
mae: 0.028754414600963035
r2: 0.7367634382736445
pearson: 0.863801430878589

=== Experiment 968 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.005638439195698024
rmse: 0.07508954118715884
mae: 0.0315505120129287
r2: 0.7457652583328493
pearson: 0.8637377098710276

=== Experiment 572 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006980868605649934
rmse: 0.08355159247824026
mae: 0.04532797882857248
r2: 0.6852357070864854
pearson: 0.8348811322912413

=== Experiment 715 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007305283887063583
rmse: 0.08547095346995717
mae: 0.04341441264369414
r2: 0.6706079648336287
pearson: 0.8249181641574571

=== Experiment 877 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005834592689442888
rmse: 0.07638450555867261
mae: 0.02959488142849146
r2: 0.7369207836336482
pearson: 0.8592701139733542

=== Experiment 817 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007511138992333717
rmse: 0.08666682751972474
mae: 0.036939279314846564
r2: 0.6613260487407592
pearson: 0.8137744915453022

=== Experiment 609 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005660105754737049
rmse: 0.07523367434026501
mae: 0.029827823447717432
r2: 0.7447883227219616
pearson: 0.8634179344186248

=== Experiment 831 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006330187049221083
rmse: 0.07956247261882378
mae: 0.03249401836539246
r2: 0.7145746520790082
pearson: 0.847752140665339

=== Experiment 541 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064960254393339216
rmse: 0.08059792453490301
mae: 0.04233358871905584
r2: 0.7070970720598777
pearson: 0.8465598184823996

=== Experiment 720 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0054267339373460875
rmse: 0.07366636910657459
mae: 0.030248062374258287
r2: 0.755310955253328
pearson: 0.8694116123213969

=== Experiment 813 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007208850175999024
rmse: 0.08490494788879517
mae: 0.035442765215873674
r2: 0.6749561184218078
pearson: 0.8217396871102105

=== Experiment 578 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.008213617491334351
rmse: 0.0906290102082901
mae: 0.04587714974929814
r2: 0.6296516024052525
pearson: 0.823953166968767

=== Experiment 887 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005968776253728396
rmse: 0.07725785561176543
mae: 0.02729436580775322
r2: 0.7308705057787177
pearson: 0.8733613279587691

=== Experiment 988 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006625444810339371
rmse: 0.08139683538282905
mae: 0.042182135396925384
r2: 0.7012616095830633
pearson: 0.8420384740794872

=== Experiment 990 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006093454424347491
rmse: 0.0780605817576803
mae: 0.028323095406751264
r2: 0.7252488219405622
pearson: 0.8659990932597227

=== Experiment 874 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0056523863419946615
rmse: 0.07518235392693329
mae: 0.029786684205323155
r2: 0.7451363876449424
pearson: 0.8636111578690031

=== Experiment 950 ===
num_layers: 3
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006103369204528385
rmse: 0.0781240629033615
mae: 0.035436039069246035
r2: 0.7248017688660342
pearson: 0.852993654068085

=== Experiment 993 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006293460117700979
rmse: 0.07933133124876311
mae: 0.035389266839882534
r2: 0.716230653256491
pearson: 0.8610707645321558

=== Experiment 960 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007133057162402166
rmse: 0.0844574281067223
mae: 0.04705978273648218
r2: 0.6783735920458337
pearson: 0.8325152253619468

=== Experiment 562 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006801891767724913
rmse: 0.08247358224137541
mae: 0.03881512503158728
r2: 0.6933056939347941
pearson: 0.8393716726629016

=== Experiment 692 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006012558523444246
rmse: 0.0775406894697503
mae: 0.03413945115883241
r2: 0.7288963825072806
pearson: 0.853993013850796

=== Experiment 531 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006319111929319442
rmse: 0.07949284200051877
mae: 0.03527817672519486
r2: 0.7150740243608402
pearson: 0.8457307735004037

=== Experiment 591 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007884561461063495
rmse: 0.08879505313396403
mae: 0.0422173172680431
r2: 0.644488593981531
pearson: 0.8047372683644514

=== Experiment 530 ===
num_layers: 1
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006547222249508333
rmse: 0.08091490746153229
mae: 0.03458688259386857
r2: 0.7047886304225529
pearson: 0.8398577839546519

=== Experiment 984 ===
num_layers: 2
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006709344185054833
rmse: 0.08191058652613124
mae: 0.03790543896115451
r2: 0.6974786237040818
pearson: 0.8359442156992118

=== Experiment 969 ===
num_layers: 2
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006585865629075984
rmse: 0.08115334638248742
mae: 0.04281245698036883
r2: 0.7030462174460996
pearson: 0.8433206439594819

=== Experiment 607 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.0067996989943563055
rmse: 0.08246028737735701
mae: 0.036316929137108316
r2: 0.6934045651208713
pearson: 0.8328119314952362

=== Experiment 994 ===
num_layers: 3
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006240930940954933
rmse: 0.07899956291622716
mae: 0.0374003015681623
r2: 0.7185991707161115
pearson: 0.849177862559194

=== Experiment 624 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007144068170299881
rmse: 0.08452258970417247
mae: 0.04583822174856771
r2: 0.6778771105460409
pearson: 0.8320910603975714

=== Experiment 760 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006142974629718527
rmse: 0.07837713078263664
mae: 0.02906031712399659
r2: 0.7230159776758914
pearson: 0.8713938269842701

=== Experiment 620 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007821173702851288
rmse: 0.08843739991005665
mae: 0.050423138838891574
r2: 0.6473467201002334
pearson: 0.8159075511986917

=== Experiment 992 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008562485849253927
rmse: 0.09253370115397917
mae: 0.0560453797262049
r2: 0.6139212816953656
pearson: 0.8135223316920372

=== Experiment 777 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007342190677026282
rmse: 0.0856865839967161
mae: 0.04076473213118984
r2: 0.6689438539181312
pearson: 0.8284897026021544

=== Experiment 700 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0064719904187932895
rmse: 0.08044868189593468
mae: 0.03714091012090973
r2: 0.7081808005574333
pearson: 0.8422163784669482

=== Experiment 600 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006521810982099519
rmse: 0.0807577301693127
mae: 0.03141566486487629
r2: 0.7059344132856622
pearson: 0.8484083163786937

=== Experiment 927 ===
num_layers: 2
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00648863791099353
rmse: 0.08055208197801923
mae: 0.035833421893251484
r2: 0.7074301724612462
pearson: 0.8437529881086065

=== Experiment 901 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.008187421093389198
rmse: 0.09048436933188625
mae: 0.03919152474836729
r2: 0.630832786458683
pearson: 0.7956976600665403

=== Experiment 910 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006388999303180645
rmse: 0.07993121607470166
mae: 0.04027335623877671
r2: 0.7119228334332239
pearson: 0.8472224420157128

=== Experiment 821 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006540239430253411
rmse: 0.08087174679857861
mae: 0.03556098694716601
r2: 0.7051034826693836
pearson: 0.8397827645013823

=== Experiment 603 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006711672060073282
rmse: 0.08192479514819236
mae: 0.03385274064116116
r2: 0.6973736608440761
pearson: 0.839281316747284

=== Experiment 898 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005670573467065902
rmse: 0.07530321020425293
mae: 0.0330066879571429
r2: 0.744316338180246
pearson: 0.863564347439469

=== Experiment 875 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006803422888110811
rmse: 0.08248286420894228
mae: 0.041864209125149474
r2: 0.6932366563904917
pearson: 0.8366358562562932

=== Experiment 690 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00665116776673267
rmse: 0.08155469187442664
mae: 0.03663789176375003
r2: 0.7001017728008285
pearson: 0.8370350476053916

=== Experiment 970 ===
num_layers: 2
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006510439315466093
rmse: 0.08068729339534257
mae: 0.038780071115207294
r2: 0.7064471567291102
pearson: 0.84269287572056

=== Experiment 987 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006922893490015273
rmse: 0.08320392713096704
mae: 0.04365239635795182
r2: 0.6878497795336527
pearson: 0.8335932438564309

=== Experiment 824 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006567391898364533
rmse: 0.08103944655761497
mae: 0.034017147797310916
r2: 0.7038791898329683
pearson: 0.8479436339434785

=== Experiment 839 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00805371995032623
rmse: 0.08974252030295467
mae: 0.04414953918789018
r2: 0.636861312153019
pearson: 0.8074353609689137

=== Experiment 841 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007280335232717384
rmse: 0.08532488050221568
mae: 0.03462873813709305
r2: 0.6717328886773564
pearson: 0.8220162170296375

=== Experiment 943 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00670138973804149
rmse: 0.08186201645477278
mae: 0.03780045136452647
r2: 0.6978372862188937
pearson: 0.843017235780273

=== Experiment 771 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006569720514135999
rmse: 0.08105381245898306
mae: 0.03625663537527288
r2: 0.703774193572735
pearson: 0.83962892223534

=== Experiment 673 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.007315632483367269
rmse: 0.08553147071907082
mae: 0.03844814160237951
r2: 0.6701413511810613
pearson: 0.8193765791098939

=== Experiment 698 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006536799938375451
rmse: 0.08085047890009961
mae: 0.03566570883606631
r2: 0.7052585678443246
pearson: 0.841169913338108

=== Experiment 755 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0057110524702702
rmse: 0.07557150567687665
mae: 0.02912378010421741
r2: 0.7424911577419361
pearson: 0.8635233980506695

=== Experiment 973 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.008681665001947074
rmse: 0.09317545278638079
mae: 0.041094887236828755
r2: 0.6085475461551884
pearson: 0.782943154516875

=== Experiment 763 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005934428014615638
rmse: 0.0770352387846993
mae: 0.028269904364875614
r2: 0.732419252762496
pearson: 0.8572983092203572

=== Experiment 930 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.008223897617042564
rmse: 0.09068570789844761
mae: 0.04563089104801134
r2: 0.6291880760616986
pearson: 0.8125614925911478

=== Experiment 629 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.0073266061538567375
rmse: 0.08559559657982843
mae: 0.04866179228690798
r2: 0.6696465532085727
pearson: 0.8297585825462672

=== Experiment 742 ===
num_layers: 3
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0057896736533024394
rmse: 0.07608990506829694
mae: 0.03140192871814996
r2: 0.7389461632028413
pearson: 0.8606809031220054

=== Experiment 597 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007411443806428122
rmse: 0.08608974274806565
mae: 0.03662117436588755
r2: 0.6658212607940355
pearson: 0.8169404933132596

=== Experiment 767 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.005951703963796955
rmse: 0.07714728746882131
mae: 0.03499366017517717
r2: 0.731640287817637
pearson: 0.8573526242662441

=== Experiment 773 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006070471928303447
rmse: 0.07791323333236433
mae: 0.03576380451001692
r2: 0.7262850925718181
pearson: 0.8542885632227044

=== Experiment 503 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005632980103893871
rmse: 0.0750531818372404
mae: 0.02948395449604167
r2: 0.7460114063795684
pearson: 0.864470427292571

=== Experiment 646 ===
num_layers: 2
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006611880382375813
rmse: 0.08131346987046988
mae: 0.034747503975691936
r2: 0.7018732236697789
pearson: 0.83917897208961

=== Experiment 888 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00731671324107516
rmse: 0.08553778838078034
mae: 0.03650005502679597
r2: 0.6700926202916901
pearson: 0.8198130064884632

=== Experiment 932 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006607205158470796
rmse: 0.08128471663523713
mae: 0.036752992966634965
r2: 0.7020840274579326
pearson: 0.8381717982455329

=== Experiment 556 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005643068855760134
rmse: 0.0751203624575929
mae: 0.029744059975081862
r2: 0.745556508998318
pearson: 0.8649827613713577

=== Experiment 863 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006411088202268441
rmse: 0.08006927127349443
mae: 0.036292985133614813
r2: 0.7109268546954233
pearson: 0.8484568259593094

=== Experiment 909 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006739016885886622
rmse: 0.08209151530996746
mae: 0.037072783095291065
r2: 0.6961406946835309
pearson: 0.8389770161834594

=== Experiment 685 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059012631101257075
rmse: 0.07681967918525635
mae: 0.03090962606958564
r2: 0.733914643708956
pearson: 0.8602552754702008

=== Experiment 588 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007093961633703943
rmse: 0.0842256589983358
mae: 0.036741046894638654
r2: 0.6801363922275785
pearson: 0.8254271632564854

=== Experiment 754 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00638899345934716
rmse: 0.07993117951930373
mae: 0.03736180812194376
r2: 0.7119230969291038
pearson: 0.8494916127233313

=== Experiment 881 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005767334188017578
rmse: 0.07594296667906501
mae: 0.03347891104095786
r2: 0.739953439860185
pearson: 0.8672934112043439

=== Experiment 550 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006322158774419781
rmse: 0.0795120039643058
mae: 0.03163074568778369
r2: 0.7149366434562849
pearson: 0.846850880663921

=== Experiment 997 ===
num_layers: 3
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006499370143489057
rmse: 0.08061867118409394
mae: 0.040855144078686095
r2: 0.7069462608216395
pearson: 0.8442434637259448

=== Experiment 924 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005262613933104962
rmse: 0.07254387591730237
mae: 0.02547445620928067
r2: 0.7627110540098225
pearson: 0.8883767506516441

=== Experiment 533 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007071405682477123
rmse: 0.08409165049205018
mae: 0.04352514239878349
r2: 0.6811534301407625
pearson: 0.845779144126606

=== Experiment 500 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006826822098656598
rmse: 0.08262458531609462
mae: 0.04197605171959036
r2: 0.6921815962857621
pearson: 0.8369652238088254

=== Experiment 527 ===
num_layers: 1
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00992924815364275
rmse: 0.09964561281683579
mae: 0.044397726327853326
r2: 0.5522945709485674
pearson: 0.7656373328462888

=== Experiment 906 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006919032167056297
rmse: 0.083180719923888
mae: 0.038455816231945925
r2: 0.6880238848863746
pearson: 0.833372573799582

=== Experiment 928 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006676794435832532
rmse: 0.08171165422283734
mae: 0.03667453406164265
r2: 0.6989462775702753
pearson: 0.836514723612981

=== Experiment 631 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067595032490758425
rmse: 0.08221619821589808
mae: 0.03006173206623706
r2: 0.695216973584067
pearson: 0.8483893959155496

=== Experiment 925 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005627938006669691
rmse: 0.07501958415420397
mae: 0.03223275152175062
r2: 0.74623875232421
pearson: 0.8653748837888335

=== Experiment 658 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006209800878273181
rmse: 0.07880228980348973
mae: 0.03525928399860391
r2: 0.7200028115410433
pearson: 0.8570447306873402

=== Experiment 583 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0066084223683989425
rmse: 0.08129220361387027
mae: 0.03665860637687888
r2: 0.7020291439980075
pearson: 0.8388695606455819

=== Experiment 712 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006749455949925187
rmse: 0.08215507257574049
mae: 0.03912139031685882
r2: 0.6956700018806155
pearson: 0.8357188195009293

=== Experiment 596 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005347096492177982
rmse: 0.07312384352711489
mae: 0.028149231702856374
r2: 0.7589017726048378
pearson: 0.8724416525184311

=== Experiment 595 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.00612700922328948
rmse: 0.07827521461669383
mae: 0.03212081495600166
r2: 0.7237358508248644
pearson: 0.8559654135962128

=== Experiment 554 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006007439255981652
rmse: 0.07750767223947351
mae: 0.027354725614105915
r2: 0.729127208024007
pearson: 0.8548151759452928

=== Experiment 931 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.006099210934974582
rmse: 0.07809744512450188
mae: 0.03331178605161471
r2: 0.7249892634100208
pearson: 0.863560520701238

=== Experiment 506 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005900743572186395
rmse: 0.07681629756885185
mae: 0.031409830954901234
r2: 0.7339380694459736
pearson: 0.8571231307910695

=== Experiment 983 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006119701295037608
rmse: 0.0782285197037347
mae: 0.03118454641735369
r2: 0.7240653620932761
pearson: 0.8589968839025958

=== Experiment 838 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00674593947037086
rmse: 0.08213366831191006
mae: 0.03722763026815539
r2: 0.6958285583959991
pearson: 0.8354993305843013

=== Experiment 966 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.007236660913799127
rmse: 0.0850685659559342
mae: 0.038848810104880484
r2: 0.6737021444948429
pearson: 0.8231262401875985

=== Experiment 601 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006530078519712263
rmse: 0.08080890124059517
mae: 0.034758379496966474
r2: 0.7055616336535252
pearson: 0.8407697489824859

=== Experiment 785 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.009409227772516284
rmse: 0.09700117407803002
mae: 0.04845229770232459
r2: 0.5757420610550867
pearson: 0.7602642502101598

=== Experiment 568 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006033607906494943
rmse: 0.0776763020907596
mae: 0.030701929853271474
r2: 0.7279472750900666
pearson: 0.8574353371406896

=== Experiment 738 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005779408852196743
rmse: 0.07602242335125041
mae: 0.03068198717936115
r2: 0.7394089985668126
pearson: 0.8615315406008122

=== Experiment 565 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007650313655715867
rmse: 0.08746607145468388
mae: 0.04231999919659316
r2: 0.6550507244243119
pearson: 0.8162380704364754

=== Experiment 859 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0068421007734751
rmse: 0.08271699204803751
mae: 0.03790601115093395
r2: 0.691492687562855
pearson: 0.832092300331489

=== Experiment 917 ===
num_layers: 3
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006670501309333256
rmse: 0.08167313701170817
mae: 0.035751702799481214
r2: 0.6992300318743108
pearson: 0.8365239190604832

=== Experiment 534 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005602640016943133
rmse: 0.07485078501220367
mae: 0.029012473379000342
r2: 0.7473794275464838
pearson: 0.8658896712973396

=== Experiment 918 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007304156555685935
rmse: 0.08546435839392895
mae: 0.03603627170255704
r2: 0.6706587957092704
pearson: 0.8301988243304463

=== Experiment 848 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006496133453180135
rmse: 0.08059859461045295
mae: 0.03755265607816857
r2: 0.7070922017630925
pearson: 0.8420337054685223

=== Experiment 978 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007168891076443135
rmse: 0.08466930421612744
mae: 0.036223083052126674
r2: 0.6767578566334381
pearson: 0.8454633228325974

=== Experiment 955 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.00775661986172807
rmse: 0.08807167457093154
mae: 0.04112954671392203
r2: 0.650257424384161
pearson: 0.8379577671242142

=== Experiment 791 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005894880143413387
rmse: 0.0767781228177232
mae: 0.028914743250404523
r2: 0.734202448868656
pearson: 0.8573182490259164

=== Experiment 741 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006979305821057389
rmse: 0.08354223974168629
mae: 0.043676632111118295
r2: 0.6853061723559293
pearson: 0.833027446648554

=== Experiment 857 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005652285805835597
rmse: 0.07518168530856166
mae: 0.03256988531385592
r2: 0.7451409207761053
pearson: 0.8690413037907878

=== Experiment 899 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0064945705865939
rmse: 0.08058889865604257
mae: 0.040117576082397306
r2: 0.7071626707295957
pearson: 0.8522493393629067

=== Experiment 639 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006878210790325285
rmse: 0.08293497929296953
mae: 0.036415672004111006
r2: 0.6898645027963142
pearson: 0.8334394930880678

=== Experiment 678 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006078506084087157
rmse: 0.07796477463628787
mae: 0.029449779420441767
r2: 0.7259228360236328
pearson: 0.8646263924738267

=== Experiment 922 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007107837407501597
rmse: 0.08430799136203873
mae: 0.040124105347986896
r2: 0.6795107396942373
pearson: 0.8327077687616844

=== Experiment 945 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006424390869580447
rmse: 0.08015229796818334
mae: 0.0377560463016117
r2: 0.7103270432812767
pearson: 0.8438911143554265

=== Experiment 703 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005647963846346606
rmse: 0.07515293637873777
mae: 0.028098388886381515
r2: 0.745335796027224
pearson: 0.8700836970485789

=== Experiment 677 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007233754986981438
rmse: 0.0850514843314415
mae: 0.03963149770868532
r2: 0.6738331714560561
pearson: 0.8441766310268832

=== Experiment 865 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00595032150276415
rmse: 0.07713832706744521
mae: 0.030089638903485007
r2: 0.7317026223771386
pearson: 0.8580304019051831

=== Experiment 896 ===
num_layers: 2
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006632228433687769
rmse: 0.08143849479016523
mae: 0.03369498541687001
r2: 0.7009557389919849
pearson: 0.837664964556197

=== Experiment 802 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.00754498984627596
rmse: 0.08686190100542332
mae: 0.043278131019878485
r2: 0.6597997286354035
pearson: 0.8212582357074779

=== Experiment 820 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006688662802632648
rmse: 0.08178424544270521
mae: 0.03803547376214305
r2: 0.6984111381349257
pearson: 0.8382131036971672

=== Experiment 915 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.0074286853967174896
rmse: 0.08618982188586706
mae: 0.048756224364937324
r2: 0.6650438450764922
pearson: 0.8299609929334444

=== Experiment 617 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005608067841106156
rmse: 0.07488703386505674
mae: 0.027358758905727917
r2: 0.7471346893439412
pearson: 0.8685910606988057

=== Experiment 916 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005638582106627572
rmse: 0.07509049278455676
mae: 0.02948922691540675
r2: 0.7457588145419358
pearson: 0.8657760695533187

=== Experiment 948 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.007329162574435841
rmse: 0.08561052840881103
mae: 0.046433289034360656
r2: 0.6695312853298278
pearson: 0.8279041761434691

=== Experiment 842 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.005939195146049922
rmse: 0.07706617381218508
mae: 0.03249035371138466
r2: 0.732204304904299
pearson: 0.8562096699038333

=== Experiment 989 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.007244002569167396
rmse: 0.08511170641672858
mae: 0.04546335881100296
r2: 0.6733711124855972
pearson: 0.8300666227050274

=== Experiment 528 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008571695733325262
rmse: 0.09258345280515985
mae: 0.053807336055891504
r2: 0.6135060120761691
pearson: 0.7987678580919466

=== Experiment 836 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006480591452311664
rmse: 0.08050212079387514
mae: 0.03818738182909223
r2: 0.7077929837416941
pearson: 0.8438121654748642

=== Experiment 926 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00664547823831013
rmse: 0.08151980273718852
mae: 0.032699050337058416
r2: 0.7003583111332476
pearson: 0.8403387631646648

=== Experiment 961 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006592525489148524
rmse: 0.0811943685802687
mae: 0.038455028356763225
r2: 0.7027459272866572
pearson: 0.8424686532698953

=== Experiment 671 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006765410205478074
rmse: 0.08225211368395388
mae: 0.037724436507057243
r2: 0.6949506315197407
pearson: 0.8408662838244916

=== Experiment 630 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005671181089257298
rmse: 0.07530724460008677
mae: 0.03175950436797821
r2: 0.744288940763071
pearson: 0.8652782715495703

=== Experiment 728 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006675217464110257
rmse: 0.08170200403974347
mae: 0.03705700918171451
r2: 0.6990173825311543
pearson: 0.8367890819446824

=== Experiment 998 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005967201688939054
rmse: 0.07724766461802618
mae: 0.027530438921460704
r2: 0.7309415022120502
pearson: 0.8666728280233598

=== Experiment 709 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0062159324286012434
rmse: 0.07884118485031312
mae: 0.032656903600702385
r2: 0.7197263426354525
pearson: 0.8568734283209292

=== Experiment 905 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006956723409234346
rmse: 0.08340697458387006
mae: 0.03803817343585332
r2: 0.6863244033657507
pearson: 0.8315331889217678

=== Experiment 706 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006696717620133352
rmse: 0.08183347493619804
mae: 0.03947642687878056
r2: 0.6980479499590152
pearson: 0.843255422845929

=== Experiment 680 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007574291873757618
rmse: 0.0870304077535985
mae: 0.03863193420929081
r2: 0.6584785131130642
pearson: 0.8374969978274559

=== Experiment 769 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006547892876735098
rmse: 0.0809190513830649
mae: 0.035282512434034866
r2: 0.7047583921360637
pearson: 0.8426914910138467

=== Experiment 858 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006940176885169199
rmse: 0.08330772404266726
mae: 0.04086411806320477
r2: 0.6870704788531661
pearson: 0.8381199181679371

=== Experiment 746 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0075133793933262385
rmse: 0.08667975192238518
mae: 0.04794155629229802
r2: 0.6612250300460822
pearson: 0.8272615095490333

=== Experiment 539 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006115917773814401
rmse: 0.07820433347209348
mae: 0.037465094273492254
r2: 0.7242359593999754
pearson: 0.854842050942813

=== Experiment 939 ===
num_layers: 2
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006039518675206564
rmse: 0.07771434021598951
mae: 0.03427775071610281
r2: 0.7276807611303878
pearson: 0.8539935806364009

=== Experiment 893 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006762305577668453
rmse: 0.08223323888591798
mae: 0.040104687557599744
r2: 0.6950906178212254
pearson: 0.8363998499362568

=== Experiment 610 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006795494394322786
rmse: 0.08243478873826746
mae: 0.03221382029141665
r2: 0.6935941486857966
pearson: 0.8338165405360941

=== Experiment 964 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006333382996729941
rmse: 0.07958255460042697
mae: 0.03468712865552917
r2: 0.714430548212478
pearson: 0.8455988732850069

=== Experiment 911 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.005909768456872745
rmse: 0.07687501841868231
mae: 0.029633209512607336
r2: 0.7335311413676215
pearson: 0.8652375268280011

=== Experiment 826 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007367537599849883
rmse: 0.08583436141691672
mae: 0.04165395471737654
r2: 0.667800972324049
pearson: 0.8197110454932918

=== Experiment 726 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006532808911136156
rmse: 0.08082579360041048
mae: 0.03387596301038668
r2: 0.7054385215059591
pearson: 0.8400430438982417

=== Experiment 665 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006036656172698146
rmse: 0.07769592121017774
mae: 0.02911157279462127
r2: 0.7278098301086087
pearson: 0.8570989131615077

=== Experiment 693 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006356222278284454
rmse: 0.07972591973934484
mae: 0.037531484561524935
r2: 0.7134007350595215
pearson: 0.8490695573118107

=== Experiment 688 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006636369283826045
rmse: 0.08146391399770848
mae: 0.02953452173793411
r2: 0.7007690298817777
pearson: 0.8596050248531825

=== Experiment 794 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006117187320885469
rmse: 0.07821244990975203
mae: 0.030684335433317646
r2: 0.7241787160813113
pearson: 0.8517946748263262

=== Experiment 852 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006362854622810115
rmse: 0.07976750355132167
mae: 0.03639946145450646
r2: 0.7131016855639775
pearson: 0.8499830959388697

=== Experiment 951 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059276020925206185
rmse: 0.07699092214359182
mae: 0.03145302460914039
r2: 0.7327270305854423
pearson: 0.856249772079107

=== Experiment 949 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006574240794052518
rmse: 0.0810816921015621
mae: 0.029989939099227463
r2: 0.7035703761408264
pearson: 0.8594978368588428

=== Experiment 920 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007019240863093324
rmse: 0.08378090989654698
mae: 0.04392968926999388
r2: 0.6835055188872852
pearson: 0.833159981133253

=== Experiment 967 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006508891565421695
rmse: 0.080677701785696
mae: 0.037938719341092
r2: 0.7065169440973952
pearson: 0.8457596292151147

=== Experiment 581 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.00772489485158079
rmse: 0.08789138098574166
mae: 0.03989243139258015
r2: 0.6516878911284569
pearson: 0.8098580408012792

=== Experiment 750 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006859994082578991
rmse: 0.08282508124100448
mae: 0.040451713450636535
r2: 0.6906858861308083
pearson: 0.8344665908770716

=== Experiment 954 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006347700764010279
rmse: 0.07967245925669848
mae: 0.033059344557122734
r2: 0.7137849663875387
pearson: 0.844865620703139

=== Experiment 849 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005629931299077035
rmse: 0.07503286812508926
mae: 0.029458331034065723
r2: 0.7461488756468779
pearson: 0.8648727626084749

=== Experiment 827 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007356014199534433
rmse: 0.08576720934911217
mae: 0.038100809461137725
r2: 0.6683205573724362
pearson: 0.8181993603057831

=== Experiment 687 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006603169095236246
rmse: 0.08125988613846469
mae: 0.03654692689529733
r2: 0.7022660117727713
pearson: 0.8380317823813647

=== Experiment 682 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007033484393129686
rmse: 0.08386587144440631
mae: 0.037348156979404974
r2: 0.6828632843870024
pearson: 0.8265277888595481

=== Experiment 736 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.007381397328171237
rmse: 0.08591505879746132
mae: 0.033841842810313834
r2: 0.667176043274172
pearson: 0.8329120589093513

=== Experiment 972 ===
num_layers: 1
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006153471000172405
rmse: 0.07844406287395117
mae: 0.03340956911105687
r2: 0.7225427009519318
pearson: 0.8501689435261881

=== Experiment 587 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007920722650360123
rmse: 0.08899844184231612
mae: 0.03841288069088416
r2: 0.6428581018719024
pearson: 0.829130346685221

=== Experiment 774 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.007530960822724571
rmse: 0.08678110867420727
mae: 0.037730007359905414
r2: 0.6604322911326903
pearson: 0.8130586489932022

=== Experiment 657 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006055082240390577
rmse: 0.07781440895098142
mae: 0.03347968064098146
r2: 0.7269790068262897
pearson: 0.8549820508284051

=== Experiment 724 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.007172931941277022
rmse: 0.08469316348606316
mae: 0.040868502249340175
r2: 0.6765756558165985
pearson: 0.8233603210123099

=== Experiment 833 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006808955374112591
rmse: 0.08251639457776007
mae: 0.03388827407742871
r2: 0.6929871990317047
pearson: 0.8330480872481879

=== Experiment 613 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.0064717609221510985
rmse: 0.08044725552901788
mae: 0.03359703685276814
r2: 0.7081911484600205
pearson: 0.8418174699334545

=== Experiment 808 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006797087765756238
rmse: 0.08244445260753593
mae: 0.034821429589021374
r2: 0.693522304269159
pearson: 0.8467155572549808

=== Experiment 797 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005979379471481173
rmse: 0.07732644742571051
mae: 0.03176552577022943
r2: 0.7303924113570781
pearson: 0.855712356400724

=== Experiment 612 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006668935502437644
rmse: 0.08166355063574963
mae: 0.03116145650781342
r2: 0.6993006334180696
pearson: 0.8404326307288766

=== Experiment 799 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006061031891486461
rmse: 0.07785262931646214
mae: 0.03375729857764742
r2: 0.726710739677018
pearson: 0.8568291121759868

=== Experiment 958 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006115885688356632
rmse: 0.0782041283332065
mae: 0.03751010883402346
r2: 0.7242374061191443
pearson: 0.8547396757724102

=== Experiment 947 ===
num_layers: 1
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.007104078797605992
rmse: 0.08428569746763677
mae: 0.03854579568272706
r2: 0.6796802137601394
pearson: 0.826245206437067

=== Experiment 756 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006182452186860611
rmse: 0.07862857105951125
mae: 0.033217843089929026
r2: 0.7212359519997564
pearson: 0.8495639973824851

=== Experiment 844 ===
num_layers: 3
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.009830213022654393
rmse: 0.09914743074156987
mae: 0.04539765848898173
r2: 0.5567600214161381
pearson: 0.747460044601445

=== Experiment 889 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.00601427036461097
rmse: 0.07755172702532788
mae: 0.033792011180236683
r2: 0.7288191963425119
pearson: 0.8565630313013194

=== Experiment 867 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006205714647699369
rmse: 0.07877635843131725
mae: 0.02979437845342209
r2: 0.7201870578791287
pearson: 0.8593211266505294

=== Experiment 570 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006000369263554804
rmse: 0.07746205047347252
mae: 0.030968644138933486
r2: 0.7294459908707921
pearson: 0.8557098568843954

=== Experiment 505 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005964685476196193
rmse: 0.07723137624175937
mae: 0.02972738562670597
r2: 0.7310549571371558
pearson: 0.8561166076005812

=== Experiment 716 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006215589268671805
rmse: 0.07883900854698647
mae: 0.03190418622149678
r2: 0.7197418155656347
pearson: 0.8486296879354749

=== Experiment 546 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.0058561940606661166
rmse: 0.07652577383252074
mae: 0.028028305924130994
r2: 0.7359467873126835
pearson: 0.8677506425219214

=== Experiment 878 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006781989835142093
rmse: 0.08235283744438981
mae: 0.0344481785237892
r2: 0.694203063315443
pearson: 0.8334664202299917

=== Experiment 897 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007083803020338658
rmse: 0.08416533146336833
mae: 0.0383355642522709
r2: 0.6805944396330714
pearson: 0.8278318092730731

=== Experiment 900 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.007334717642321299
rmse: 0.08564296609950696
mae: 0.033008126362744646
r2: 0.6692808097638447
pearson: 0.8204992190816275

=== Experiment 604 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.00835524862465171
rmse: 0.09140704909716597
mae: 0.0403248999474495
r2: 0.6232655169406018
pearson: 0.8065163511527293

=== Experiment 627 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00649513452414758
rmse: 0.08059239743392413
mae: 0.03297382588633747
r2: 0.7071372430335081
pearson: 0.845805744702864

=== Experiment 996 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0059573634331903285
rmse: 0.07718395839285731
mae: 0.036018838196633325
r2: 0.7313851048336127
pearson: 0.8565468566055899

=== Experiment 854 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.0069265234349672085
rmse: 0.08322573781569742
mae: 0.035626895941291606
r2: 0.6876861069134308
pearson: 0.829592197202246

=== Experiment 902 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006561369244600791
rmse: 0.08100227925559127
mae: 0.030363550996349038
r2: 0.7041507486404027
pearson: 0.8489853878445698

=== Experiment 953 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00764117077880647
rmse: 0.08741379055278675
mae: 0.040495084168451524
r2: 0.6554629727200687
pearson: 0.8104260885013564

=== Experiment 818 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005814031319868234
rmse: 0.07624979553984544
mae: 0.034659651707488866
r2: 0.7378478867380185
pearson: 0.8597626315600525

=== Experiment 710 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006562770460340891
rmse: 0.08101092803036447
mae: 0.034156149992550934
r2: 0.7040875684394037
pearson: 0.839595222512038

=== Experiment 622 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.005873123699978417
rmse: 0.07663630797460443
mae: 0.031680973179847166
r2: 0.7351834373273958
pearson: 0.857707511408418

=== Experiment 544 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.007739494739139949
rmse: 0.08797439820277232
mae: 0.037073306031765556
r2: 0.6510295886243165
pearson: 0.8162561370975511

=== Experiment 637 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006110030715688615
rmse: 0.07816668545927104
mae: 0.03229393967835406
r2: 0.7245014042597746
pearson: 0.851211295131253

=== Experiment 717 ===
num_layers: 3
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005754436361042159
rmse: 0.07585800129875661
mae: 0.030961947820021266
r2: 0.740534997201739
pearson: 0.8605531664381857

=== Experiment 812 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006625034993681802
rmse: 0.08139431794469318
mae: 0.03803734359926618
r2: 0.701280088035779
pearson: 0.8403434648915785

=== Experiment 667 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.00826525161726037
rmse: 0.09091342924596107
mae: 0.04184055261930736
r2: 0.6273234423931653
pearson: 0.8118435120266746

=== Experiment 795 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.005713626174806295
rmse: 0.07558853203235458
mae: 0.028132932489076425
r2: 0.7423751105371775
pearson: 0.8623143292893043

=== Experiment 697 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006274240455782068
rmse: 0.07921010324309688
mae: 0.03700638671477873
r2: 0.7170972593531946
pearson: 0.8482574396014474

=== Experiment 512 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006739813691547447
rmse: 0.08209636832130546
mae: 0.03741231534112109
r2: 0.6961047670669853
pearson: 0.8399318860216347

=== Experiment 801 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005940882348468976
rmse: 0.07707711948736133
mae: 0.03128171050173453
r2: 0.7321282296898184
pearson: 0.8560859620869303

=== Experiment 666 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006921177480303795
rmse: 0.08319361442024138
mae: 0.03550245480666447
r2: 0.6879271536562696
pearson: 0.8294796716745765

=== Experiment 936 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006217737594217073
rmse: 0.07885263213246006
mae: 0.034960728051715066
r2: 0.7196449485123491
pearson: 0.8491853032690507

=== Experiment 962 ===
num_layers: 2
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006543142066151699
rmse: 0.08088969072849579
mae: 0.03271798723767488
r2: 0.7049726040942776
pearson: 0.8401867782050241

=== Experiment 981 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006885657144133655
rmse: 0.08297985987053519
mae: 0.03435760508471126
r2: 0.6895287499804856
pearson: 0.8322732812224763

=== Experiment 866 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005414507081699857
rmse: 0.07358333426598619
mae: 0.02882128782495573
r2: 0.755862258792966
pearson: 0.8709227966967115

=== Experiment 959 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067308595871748665
rmse: 0.0820418160889608
mae: 0.0351632973335681
r2: 0.6965085036921422
pearson: 0.8364974738048422

=== Experiment 913 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006119323331716708
rmse: 0.07822610390219309
mae: 0.03983010862329937
r2: 0.7240824042930609
pearson: 0.8547309068434791

=== Experiment 937 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006423017630705723
rmse: 0.08014373107552283
mae: 0.037552935231503035
r2: 0.7103889620177294
pearson: 0.8452280073962976

=== Experiment 940 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006051013012174421
rmse: 0.07778825754684586
mae: 0.030348998811518143
r2: 0.7271624865355517
pearson: 0.8543011263896687

=== Experiment 952 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006716985441185189
rmse: 0.08195721713909758
mae: 0.03732576501961264
r2: 0.6971340828283381
pearson: 0.83730234925197

=== Experiment 747 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.005579899127816641
rmse: 0.07469872239748576
mae: 0.03237887060847716
r2: 0.7484048042281666
pearson: 0.8680921960877649

=== Experiment 626 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006785635661673472
rmse: 0.08237496987358157
mae: 0.03892211085781918
r2: 0.6940386746017941
pearson: 0.8489985062049634

=== Experiment 662 ===
num_layers: 2
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0076394439479536055
rmse: 0.08740391265815052
mae: 0.033940273084018605
r2: 0.655540834763188
pearson: 0.8139890211361939

=== Experiment 552 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.009435608286105525
rmse: 0.09713705928277593
mae: 0.04358112273722594
r2: 0.5745525753083001
pearson: 0.7581958785754406

=== Experiment 822 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006808944838878148
rmse: 0.08251633074051554
mae: 0.03554188351167198
r2: 0.6929876740607888
pearson: 0.8440585773921889

=== Experiment 807 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.00627326214908992
rmse: 0.0792039276115138
mae: 0.032800785737117776
r2: 0.7171413707713576
pearson: 0.8588686589851742

=== Experiment 914 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006008129570093157
rmse: 0.0775121253101291
mae: 0.03685437742498721
r2: 0.7290960820645498
pearson: 0.8556007827406463

=== Experiment 611 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.006322616261035063
rmse: 0.07951488075219042
mae: 0.031137000094986232
r2: 0.7149160155861571
pearson: 0.8473350546070159

=== Experiment 758 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006035334884031511
rmse: 0.077687417797424
mae: 0.03116736138339122
r2: 0.7278694064330439
pearson: 0.853227709383978

=== Experiment 814 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.008149230674437171
rmse: 0.09027308942557118
mae: 0.03852851386048181
r2: 0.632554775640341
pearson: 0.8134178646731116

=== Experiment 790 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005684941553022001
rmse: 0.07539855139869732
mae: 0.031417904116917306
r2: 0.7436684875083639
pearson: 0.8641230916691767

=== Experiment 761 ===
num_layers: 2
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006653815861397547
rmse: 0.08157092534351652
mae: 0.03956474041823292
r2: 0.6999823713779046
pearson: 0.8392376369850959

=== Experiment 569 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.00717183766439205
rmse: 0.08468670299635032
mae: 0.0352878940986248
r2: 0.676624996279714
pearson: 0.8234416370589963

=== Experiment 592 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.0072548733652462415
rmse: 0.08517554440827627
mae: 0.04468948742913928
r2: 0.6728809530750054
pearson: 0.8252926330134178

=== Experiment 851 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006420831217518287
rmse: 0.0801300893392631
mae: 0.0359045799246681
r2: 0.7104875464260367
pearson: 0.8435102177434204

=== Experiment 995 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.006728743671982872
rmse: 0.08202891972946415
mae: 0.03687309816887091
r2: 0.6966039093768639
pearson: 0.8363021905289546

=== Experiment 991 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.0063274791097945255
rmse: 0.07954545310572142
mae: 0.03360509318698821
r2: 0.7146967518758964
pearson: 0.8522142172384718

=== Experiment 696 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
mse: 0.006847468021003583
rmse: 0.08274942912796186
mae: 0.03836794364978804
r2: 0.6912506807341019
pearson: 0.8333833029827382

=== Experiment 555 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006682348978586342
rmse: 0.08174563583816778
mae: 0.03645223346131202
r2: 0.6986958256822471
pearson: 0.8365737774608952

=== Experiment 644 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006645711853783282
rmse: 0.08152123560019979
mae: 0.040020302781495105
r2: 0.7003477775143854
pearson: 0.8505294884731835

=== Experiment 907 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.009733107301126927
rmse: 0.09865651170159488
mae: 0.04685928807472451
r2: 0.5611384756603148
pearson: 0.7495663503785808

=== Experiment 762 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.006037848450926343
rmse: 0.07770359355220545
mae: 0.029483762940492618
r2: 0.7277560708080612
pearson: 0.8588781420215034

=== Experiment 633 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00588848575121752
rmse: 0.07673646949930339
mae: 0.035434433693721235
r2: 0.7344907691983806
pearson: 0.8623895917243048

=== Experiment 740 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.007641093929091995
rmse: 0.08741335097736498
mae: 0.03690332106954406
r2: 0.6554664378398702
pearson: 0.8110249008357926

=== Experiment 752 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006782411464084356
rmse: 0.08235539729783566
mae: 0.033526036414171255
r2: 0.6941840522520095
pearson: 0.8410432875570942

=== Experiment 933 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006896429538288155
rmse: 0.08304474419424841
mae: 0.03153513231877498
r2: 0.6890430274693524
pearson: 0.8316322590994448

=== Experiment 891 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.0067275692567210525
rmse: 0.08202176087308205
mae: 0.03176753705828943
r2: 0.6966568632441195
pearson: 0.8426665880324504

=== Experiment 632 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.00677776696474819
rmse: 0.08232719456381464
mae: 0.03596290074349391
r2: 0.6943934706828767
pearson: 0.8345383666656314

=== Experiment 522 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.006292276576407217
rmse: 0.07932387141590617
mae: 0.03191482637922835
r2: 0.7162840186125103
pearson: 0.8485177260569647

=== Experiment 560 ===
num_layers: 3
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.0060499296709374
rmse: 0.07778129383686928
mae: 0.03424952568158117
r2: 0.7272113339151142
pearson: 0.8539637190136639

=== Experiment 559 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007009182962450332
rmse: 0.08372086336421963
mae: 0.03939958191258823
r2: 0.6839590252004571
pearson: 0.8286979028252036

=== Experiment 751 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005770830772191272
rmse: 0.07596598431002702
mae: 0.031580987654941335
r2: 0.7397957804187575
pearson: 0.8602631138449894

=== Experiment 584 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.005918773543013549
rmse: 0.0769335657760223
mae: 0.029962663186362457
r2: 0.7331251059969742
pearson: 0.8585947184491505

=== Experiment 744 ===
num_layers: 3
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.008828026845505531
rmse: 0.09395758003219076
mae: 0.05588316053867126
r2: 0.6019481550479111
pearson: 0.8053746317273701

=== Experiment 731 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006136032345366679
rmse: 0.07833283057164907
mae: 0.03211574783298855
r2: 0.7233290022217831
pearson: 0.8614401129962784

=== Experiment 861 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006227779059356971
rmse: 0.07891627879821102
mae: 0.032200551627578865
r2: 0.7191921832687778
pearson: 0.8482519989724054

=== Experiment 589 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.006567857201284609
rmse: 0.08104231734892955
mae: 0.030828654673883592
r2: 0.7038582095290975
pearson: 0.8402474810149053

=== Experiment 780 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.005912946503932118
rmse: 0.07689568586034017
mae: 0.035333656865654706
r2: 0.7333878446244797
pearson: 0.858199000654082

=== Experiment 999 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
mse: 0.0058412279157172425
rmse: 0.07642792628167562
mae: 0.030395055725403745
r2: 0.736621604201324
pearson: 0.860772478772809

=== Experiment 977 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006337960275491846
rmse: 0.0796113074600075
mae: 0.03330503777328137
r2: 0.7142241607277182
pearson: 0.845690434165533

=== Experiment 976 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005741787042111023
rmse: 0.07577458044826789
mae: 0.030124847330671483
r2: 0.7411053494249529
pearson: 0.8610937084096997

=== Experiment 809 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
mse: 0.006769211206514206
rmse: 0.08227521623498905
mae: 0.03327607073906699
r2: 0.6947792460559701
pearson: 0.8387422257685194

=== Experiment 579 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
mse: 0.008528326468601872
rmse: 0.0923489386436134
mae: 0.04211580239559826
r2: 0.6154615131342738
pearson: 0.7854315136010745

=== Experiment 614 ===
num_layers: 3
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006765967782526863
rmse: 0.08225550305315057
mae: 0.030947319502160203
r2: 0.6949254906160192
pearson: 0.8344093679412082

=== Experiment 501 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.008004356689465714
rmse: 0.08946707041960028
mae: 0.03327381137865491
r2: 0.639087079858787
pearson: 0.833259636445845

=== Experiment 694 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
mse: 0.006034315880142289
rmse: 0.07768085916197302
mae: 0.03126452244872584
r2: 0.7279153528699108
pearson: 0.8532649343383322

=== Experiment 525 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
mse: 0.007245152204359344
rmse: 0.08511845983310168
mae: 0.032776483733618404
r2: 0.6733192759407854
pearson: 0.8401883973460271

=== Experiment 652 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.006620464474859908
rmse: 0.08136623670085712
mae: 0.03270075594030427
r2: 0.701486170717819
pearson: 0.8472654428720715

=== Experiment 605 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
mse: 0.006107978100333083
rmse: 0.07815355462378588
mae: 0.03029937660272909
r2: 0.7245939557827306
pearson: 0.8529230995885256

=== Experiment 739 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
mse: 0.005716763788398195
rmse: 0.07560928374477698
mae: 0.030435162198099715
r2: 0.7422336369212881
pearson: 0.8623550427951776

=== Experiment 845 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
mse: 0.008007466422691597
rmse: 0.08948444793756956
mae: 0.03259054587692595
r2: 0.6389468633563301
pearson: 0.8289076084319837

=== Experiment 735 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005795993231971456
rmse: 0.07613142079306977
mae: 0.028718600864353686
r2: 0.7386612161821146
pearson: 0.8604521773485895

=== Experiment 628 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
mse: 0.005587983450989453
rmse: 0.07475281567265177
mae: 0.027320165829416355
r2: 0.7480402856544872
pearson: 0.868517011813286

=== Experiment 1 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.005806387127633033
rmse: 0.07619965306766845
mae: 0.030252991648864847
r2: 0.7381925600014374
pearson: 0.8602801302044083

=== Experiment 1 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
mse: 0.00593693753160486
rmse: 0.07705152517377485
mae: 0.030940236516812946
r2: 0.7323060997459747
pearson: 0.8567441648401265

=== Experiment 0 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006500916890726527
rmse: 0.08062826359736719
mae: 0.028442643299144366
r2: 0.7068765186694776
pearson: 0.8425041073231484

=== Experiment 0 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
mse: 0.006500916890726527
rmse: 0.08062826359736719
mae: 0.028442643299144366
r2: 0.7068765186694776
pearson: 0.8425041073231484

=== Experiment 1 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006452742963410871
rmse: 0.08032896715015618
mae: 0.030490408926167303
r2: 0.7090486598491736
pearson: 0.8494670239862516

=== Experiment 0 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006500916890726527
rmse: 0.08062826359736719
mae: 0.028442643299144366
r2: 0.7068765186694776
pearson: 0.8425041073231484

=== Experiment 1002 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006890947911900003
rmse: 0.0830117335796573
mae: 0.03269124849656498
r2: 0.6892901915905458
pearson: 0.8373481295947083

=== Experiment 1014 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00635202073771198
rmse: 0.0796995654800701
mae: 0.0344847692429719
r2: 0.7135901806746949
pearson: 0.8459596904821328

=== Experiment 1035 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005741115601699921
rmse: 0.0757701498065031
mae: 0.0301769762412484
r2: 0.7411356243775659
pearson: 0.8613646107093822

=== Experiment 1058 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005654665351807078
rmse: 0.07519750894682002
mae: 0.02863565965282051
r2: 0.7450336280955695
pearson: 0.8660299851274383

=== Experiment 1089 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006212069110008714
rmse: 0.07881668040464984
mae: 0.032794056021158646
r2: 0.7199005379704131
pearson: 0.8523438080096835

=== Experiment 1009 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006884151525090749
rmse: 0.08297078717892671
mae: 0.04299189780136888
r2: 0.6895966376804565
pearson: 0.8366897406800412

=== Experiment 1053 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007021869639834779
rmse: 0.08379659682728637
mae: 0.03495599660436603
r2: 0.683386988501026
pearson: 0.8306107897237793

=== Experiment 1013 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006077443828953563
rmse: 0.07795796193432435
mae: 0.03663271515634199
r2: 0.7259707326400726
pearson: 0.855202705886121

=== Experiment 1016 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007411169159059077
rmse: 0.0860881476108011
mae: 0.04398959981154611
r2: 0.6658336445230242
pearson: 0.8183854082140867

=== Experiment 1024 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006465883116085157
rmse: 0.08041071518202755
mae: 0.03077421273237273
r2: 0.7084561761485135
pearson: 0.8435047063795609

=== Experiment 1087 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006684005730919142
rmse: 0.08175576879290623
mae: 0.0378473477064406
r2: 0.6986211234487538
pearson: 0.8389941901802102

=== Experiment 1020 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007451372450681331
rmse: 0.08632133253536654
mae: 0.04220543287217241
r2: 0.6640208957985987
pearson: 0.8244499486112175

=== Experiment 1040 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007495842755629388
rmse: 0.08657853518990367
mae: 0.048304310957837875
r2: 0.6620157493213688
pearson: 0.824681206193754

=== Experiment 1052 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00794907113009982
rmse: 0.0891575635047292
mae: 0.03404170284521151
r2: 0.6415798813975597
pearson: 0.8190669258304291

=== Experiment 1033 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007837124884770043
rmse: 0.0885275374376247
mae: 0.03722952029911391
r2: 0.6466274883281686
pearson: 0.8147680418181422

=== Experiment 1044 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006924958286028285
rmse: 0.08321633425012355
mae: 0.034177345143776565
r2: 0.6877566787902121
pearson: 0.8344158188033933

=== Experiment 1018 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0060051461502770385
rmse: 0.07749287806164537
mae: 0.031181585175436722
r2: 0.729230603150955
pearson: 0.8638857028607011

=== Experiment 1063 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005851420283223576
rmse: 0.07649457682230536
mae: 0.035956397035174295
r2: 0.7361620348364675
pearson: 0.8610205091788314

=== Experiment 1043 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007755920651516705
rmse: 0.08806770492931393
mae: 0.03684872163609486
r2: 0.6502889514648456
pearson: 0.8087065120175515

=== Experiment 1047 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006401403345317502
rmse: 0.0800087704274819
mae: 0.04051659462455362
r2: 0.7113635406327097
pearson: 0.8486378173423014

=== Experiment 1067 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007422465190908535
rmse: 0.08615372998836751
mae: 0.04302488680580846
r2: 0.665324311418696
pearson: 0.8257474196830232

=== Experiment 1022 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006227617983326062
rmse: 0.07891525824152172
mae: 0.027140483227411763
r2: 0.7191994461161165
pearson: 0.8487044575057378

=== Experiment 1124 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007804580987283155
rmse: 0.08834353958996184
mae: 0.04112362108993914
r2: 0.6480948783421879
pearson: 0.8115255215605033

=== Experiment 1139 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0064008906807775335
rmse: 0.08000556656119331
mae: 0.03183751277702529
r2: 0.7113866564511759
pearson: 0.8497999484655672

=== Experiment 1026 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005293381741854986
rmse: 0.07275563031034084
mae: 0.028818406140352432
r2: 0.7613237470552321
pearson: 0.8765120005401696

=== Experiment 1037 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0056958114380274505
rmse: 0.07547059982554433
mae: 0.029367656767955802
r2: 0.7431783691776703
pearson: 0.8623299179635663

=== Experiment 1143 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005559282583484554
rmse: 0.07456059672162337
mae: 0.02706780227913549
r2: 0.7493343951380653
pearson: 0.8734529582772922

=== Experiment 1025 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005934374715375597
rmse: 0.07703489284327977
mae: 0.02951385768100312
r2: 0.7324216560017704
pearson: 0.8564788755273063

=== Experiment 1099 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005673580480034515
rmse: 0.07532317359242449
mae: 0.028832348765548885
r2: 0.744180753288979
pearson: 0.866476132373244

=== Experiment 1104 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006689746900222665
rmse: 0.08179087296405795
mae: 0.03395418437474789
r2: 0.6983622566517398
pearson: 0.8364140976740861

=== Experiment 1005 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007605671024584167
rmse: 0.08721049836220503
mae: 0.044117597590519184
r2: 0.6570636410133157
pearson: 0.825196346872979

=== Experiment 1126 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067221575771444705
rmse: 0.08198876494462196
mae: 0.03899156370684416
r2: 0.6969008734943654
pearson: 0.8431956384231458

=== Experiment 1106 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007170047958967913
rmse: 0.08467613571111941
mae: 0.03744004443238075
r2: 0.6767056933095787
pearson: 0.8249447836752425

=== Experiment 1129 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006229222497949601
rmse: 0.0789254236475776
mae: 0.031187853235961654
r2: 0.7191270992579419
pearson: 0.8490043229756286

=== Experiment 1096 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005931262949605418
rmse: 0.07701469307609697
mae: 0.028546651638441688
r2: 0.732561964150755
pearson: 0.8559559462329266

=== Experiment 1165 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007008861528943358
rmse: 0.08371894366834401
mae: 0.03514647277226366
r2: 0.6839735184958127
pearson: 0.8324039010529678

=== Experiment 1001 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0060331499025761725
rmse: 0.07767335387747958
mae: 0.03186704282696399
r2: 0.727967926285181
pearson: 0.8584318744242123

=== Experiment 1060 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00746504495488814
rmse: 0.08640049163568538
mae: 0.04140951786420332
r2: 0.6634044086016428
pearson: 0.8291377318790375

=== Experiment 1151 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006338466993105443
rmse: 0.0796144898439062
mae: 0.03448509853017142
r2: 0.7142013130535443
pearson: 0.8474590831106636

=== Experiment 1102 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006337643527957662
rmse: 0.07960931809755478
mae: 0.029918023177028633
r2: 0.7142384427346231
pearson: 0.8508302047720266

=== Experiment 1086 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00747721490413762
rmse: 0.08647089050158799
mae: 0.0357747962466031
r2: 0.6628556709463881
pearson: 0.8167641822685076

=== Experiment 1046 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005965411497565524
rmse: 0.07723607639934543
mae: 0.029559736914011196
r2: 0.7310222211531592
pearson: 0.8635704096976704

=== Experiment 1019 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008021075521685754
rmse: 0.08956045735527345
mae: 0.034732404317065625
r2: 0.6383332350725061
pearson: 0.8244351604271692

=== Experiment 1028 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006592433620171496
rmse: 0.0811938028433913
mae: 0.04101107213844188
r2: 0.7027500696183969
pearson: 0.8426723782194727

=== Experiment 1032 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006866732804373241
rmse: 0.08286575169738848
mae: 0.039807043541606205
r2: 0.6903820401310437
pearson: 0.8390002006837365

=== Experiment 1158 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007667384021782196
rmse: 0.08756359986765161
mae: 0.03857777492926815
r2: 0.6542810291316237
pearson: 0.8366062400987747

=== Experiment 1045 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0059253278373745406
rmse: 0.07697615109483287
mae: 0.03393248480460812
r2: 0.7328295757490026
pearson: 0.8620728430639556

=== Experiment 1142 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006899670074882342
rmse: 0.08306425268960374
mae: 0.045068413836114644
r2: 0.6888969131005898
pearson: 0.8373948565939112

=== Experiment 1076 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005928012308686539
rmse: 0.07699358615291627
mae: 0.027840636199795116
r2: 0.7327085341190709
pearson: 0.856206695731539

=== Experiment 1121 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007725864835218535
rmse: 0.0878968988942075
mae: 0.04443527850658573
r2: 0.6516441549931535
pearson: 0.8108112915590956

=== Experiment 1159 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006782405991071951
rmse: 0.08235536406981607
mae: 0.035977617337580224
r2: 0.69418429902773
pearson: 0.8382638557765725

=== Experiment 1155 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006454067825757727
rmse: 0.08033721320632006
mae: 0.03120645233621705
r2: 0.7089889223890704
pearson: 0.8456593202460576

=== Experiment 1182 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006478552495973268
rmse: 0.08048945580616922
mae: 0.0341998682408816
r2: 0.7078849193855792
pearson: 0.8472407111226115

=== Experiment 1066 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0074274088198203105
rmse: 0.08618241595488206
mae: 0.04642681799786302
r2: 0.6651014053669215
pearson: 0.828756753194859

=== Experiment 1171 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00642000530697677
rmse: 0.0801249356129337
mae: 0.035604805088020955
r2: 0.7105247863688426
pearson: 0.8455176645281232

=== Experiment 1107 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006882418708157012
rmse: 0.08296034419020339
mae: 0.04257129841179842
r2: 0.6896747696333277
pearson: 0.8343904426885873

=== Experiment 1249 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006529356967167607
rmse: 0.08080443655621644
mae: 0.03517508731308914
r2: 0.7055941681401842
pearson: 0.8404227093657926

=== Experiment 1134 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006824762081523899
rmse: 0.08261211824861954
mae: 0.036585167915432565
r2: 0.6922744815515929
pearson: 0.8368662076269332

=== Experiment 1179 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008006394638212795
rmse: 0.08947845907375024
mae: 0.0463441948659524
r2: 0.638995189646751
pearson: 0.8084791274529527

=== Experiment 1054 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0072506966119934395
rmse: 0.08515102237785192
mae: 0.036146592496465464
r2: 0.6730692810408445
pearson: 0.8544124473474393

=== Experiment 1041 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005465010498657058
rmse: 0.07392570932129808
mae: 0.027590995736766655
r2: 0.7535850819506194
pearson: 0.8692229822583346

=== Experiment 1079 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007000185704753399
rmse: 0.08366711244421789
mae: 0.037854108021597256
r2: 0.6843647075900157
pearson: 0.8402785755793767

=== Experiment 1090 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005618106693558958
rmse: 0.07495403053578212
mae: 0.02879908126654006
r2: 0.7466820419052812
pearson: 0.8650530330994702

=== Experiment 1049 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068767143343801485
rmse: 0.08292595693979146
mae: 0.03085419090180135
r2: 0.6899319773362396
pearson: 0.8435527222306123

=== Experiment 1186 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006795586205353746
rmse: 0.0824353456070498
mae: 0.03640823768179287
r2: 0.6935900089668198
pearson: 0.8371440824726315

=== Experiment 1094 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007525034395661203
rmse: 0.0867469561175561
mae: 0.03460787882178452
r2: 0.6606995111205576
pearson: 0.8317705039248882

=== Experiment 1174 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007027119030565532
rmse: 0.0838279131946247
mae: 0.035423800000211496
r2: 0.6831502957834099
pearson: 0.8268924093591922

=== Experiment 1172 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006229443822302195
rmse: 0.078926825745764
mae: 0.03226965482299255
r2: 0.7191171198402959
pearson: 0.8510132191319787

=== Experiment 1116 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007114009549002143
rmse: 0.08434458814293981
mae: 0.039411313296606465
r2: 0.679232440550545
pearson: 0.8350977433100738

=== Experiment 1217 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006373767753512208
rmse: 0.07983588011359434
mae: 0.03165113507474228
r2: 0.7126096173038563
pearson: 0.8445641150317459

=== Experiment 1173 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007909063699768942
rmse: 0.0889329168517987
mae: 0.05206148702962936
r2: 0.6433837988225622
pearson: 0.825967885355071

=== Experiment 1260 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007153235333955662
rmse: 0.0845768013935007
mae: 0.04558329458957535
r2: 0.6774637671715791
pearson: 0.8314745734916122

=== Experiment 1059 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00566684944163215
rmse: 0.07527847927284498
mae: 0.027477294643787124
r2: 0.7444842528479849
pearson: 0.864713201474445

=== Experiment 1093 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005786682047180942
rmse: 0.07607024416406813
mae: 0.029036939057582603
r2: 0.739081053406153
pearson: 0.8647674797929548

=== Experiment 1068 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007200112582142612
rmse: 0.08485347713642978
mae: 0.03551505230320147
r2: 0.6753500926830842
pearson: 0.8234927094258433

=== Experiment 1206 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007177860699201897
rmse: 0.08472225622114828
mae: 0.035725556207492515
r2: 0.6763534202910784
pearson: 0.824408610946689

=== Experiment 1240 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007676587945362624
rmse: 0.08761613975382973
mae: 0.0379426817316619
r2: 0.6538660282683388
pearson: 0.8346719051286391

=== Experiment 1150 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007166974721861762
rmse: 0.0846579867576696
mae: 0.04547506278303705
r2: 0.6768442642180599
pearson: 0.830099525453196

=== Experiment 1042 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008057224115809763
rmse: 0.08976204162010668
mae: 0.04696267385968442
r2: 0.6367033108736677
pearson: 0.8312575962676234

=== Experiment 1170 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006147062189743294
rmse: 0.07840320267529442
mae: 0.036212541513464834
r2: 0.7228316713934473
pearson: 0.8596569064203946

=== Experiment 1210 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0069753697634398594
rmse: 0.08351867912892216
mae: 0.035720084539365926
r2: 0.6854836474615122
pearson: 0.8284406131534021

=== Experiment 1004 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006771887946603935
rmse: 0.08229148161628842
mae: 0.03466293995626152
r2: 0.6946585530234475
pearson: 0.834196752132381

=== Experiment 1242 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006856425754948982
rmse: 0.08280353709201668
mae: 0.032608446457695055
r2: 0.6908467804531042
pearson: 0.8364018469450153

=== Experiment 1237 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006182512862277428
rmse: 0.07862895689424748
mae: 0.03547324871483656
r2: 0.7212332161719175
pearson: 0.854083086054478

=== Experiment 1224 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0058664350352802645
rmse: 0.07659265653625198
mae: 0.02642925660601788
r2: 0.735485026274729
pearson: 0.877177589977211

=== Experiment 1219 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063889418843511014
rmse: 0.07993085689739039
mae: 0.030908311457889144
r2: 0.7119254224229761
pearson: 0.8568029086781218

=== Experiment 1048 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007406589405918371
rmse: 0.08606154429196801
mae: 0.03932299367774814
r2: 0.6660401435764352
pearson: 0.8175532262083792

=== Experiment 1287 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00787462651951193
rmse: 0.08873909239738667
mae: 0.053725173440314064
r2: 0.6449365561233888
pearson: 0.8209333915040395

=== Experiment 1235 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007632781001083583
rmse: 0.08736578850490381
mae: 0.04179825013485169
r2: 0.6558412641049701
pearson: 0.8256447733858989

=== Experiment 1030 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006753501974884349
rmse: 0.08217969320266624
mae: 0.03539811624044893
r2: 0.6954875683959986
pearson: 0.8417454429708838

=== Experiment 1137 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007905117554074654
rmse: 0.08891072800328796
mae: 0.04893563615476846
r2: 0.6435617287950861
pearson: 0.821664223373354

=== Experiment 1320 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007065560282988752
rmse: 0.08405688718355415
mae: 0.04452882610501475
r2: 0.681416996630936
pearson: 0.834163294682379

=== Experiment 1250 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006113936652235518
rmse: 0.07819166613032055
mae: 0.028270158131513633
r2: 0.724325287300009
pearson: 0.8563479511813646

=== Experiment 1265 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006573493124271611
rmse: 0.08107708137489664
mae: 0.03351048978816459
r2: 0.703604088242173
pearson: 0.8416178946146151

=== Experiment 1036 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006914703185687009
rmse: 0.08315469430938345
mae: 0.0372617807652349
r2: 0.6882190767509853
pearson: 0.8334740166815423

=== Experiment 1314 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0058518361610101115
rmse: 0.07649729512218136
mae: 0.03256805758891629
r2: 0.7361432830901143
pearson: 0.8674830603528986

=== Experiment 1192 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006428445019464506
rmse: 0.08017758427057095
mae: 0.031750549956664244
r2: 0.7101442434473723
pearson: 0.854519016127856

=== Experiment 1166 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006836351017031086
rmse: 0.08268222914889925
mae: 0.03351866102506838
r2: 0.6917519415502543
pearson: 0.8447841959704965

=== Experiment 1097 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0069612378947403655
rmse: 0.08343403319233923
mae: 0.042393937845504014
r2: 0.6861208472012613
pearson: 0.8322265206857561

=== Experiment 1091 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006319160597514448
rmse: 0.0794931481167682
mae: 0.040366268298384037
r2: 0.7150718299333478
pearson: 0.8516984862158368

=== Experiment 1197 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005998357376342986
rmse: 0.07744906310823253
mae: 0.029909234844775208
r2: 0.7295367059796736
pearson: 0.8577984293086245

=== Experiment 1084 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006931898634972301
rmse: 0.08325802444793115
mae: 0.03261540068095361
r2: 0.6874437415110076
pearson: 0.8411506118644173

=== Experiment 1083 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007359867359876837
rmse: 0.08578966930742207
mae: 0.04902521782294359
r2: 0.6681468200685028
pearson: 0.8291930317384152

=== Experiment 1130 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0056707069875348475
rmse: 0.07530409675133783
mae: 0.02796624841974877
r2: 0.7443103178010682
pearson: 0.8634056148181126

=== Experiment 1232 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006616156513120417
rmse: 0.08133975973114511
mae: 0.03135513366886192
r2: 0.7016804148165738
pearson: 0.8474922819844682

=== Experiment 1258 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007085976188386467
rmse: 0.08417824058737784
mae: 0.04586666561574874
r2: 0.6804964524422801
pearson: 0.834877146915446

=== Experiment 1276 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006034072571392985
rmse: 0.07767929306702646
mae: 0.03162868453715848
r2: 0.7279263235543286
pearson: 0.8534194846057178

=== Experiment 1252 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005939922412896741
rmse: 0.07707089212469738
mae: 0.027899219628904365
r2: 0.7321715127622705
pearson: 0.8558220065764698

=== Experiment 1301 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0073708288282529726
rmse: 0.0858535312509216
mae: 0.04960594307101244
r2: 0.6676525722839339
pearson: 0.8305222587441747

=== Experiment 1297 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065758675395716625
rmse: 0.08109172300285439
mae: 0.03152057697825873
r2: 0.7034970269013536
pearson: 0.847590087395378

=== Experiment 1225 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006822370645238037
rmse: 0.08259764309735501
mae: 0.03681447735253946
r2: 0.6923823103611708
pearson: 0.8527775241246616

=== Experiment 1180 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007707682089214314
rmse: 0.08779340572739114
mae: 0.03864423156363512
r2: 0.6524640070076453
pearson: 0.8124970074426485

=== Experiment 1274 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0061004827959884635
rmse: 0.07810558748251283
mae: 0.03324086498384146
r2: 0.7249319157566936
pearson: 0.8552135681422232

=== Experiment 1308 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005954672007557533
rmse: 0.07716652128713289
mae: 0.031424697456563976
r2: 0.7315064600308085
pearson: 0.8613795392071505

=== Experiment 1303 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00835940391896809
rmse: 0.09142977588820882
mae: 0.052873666879574246
r2: 0.6230781565487611
pearson: 0.7997729445142275

=== Experiment 1103 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057569866413260075
rmse: 0.07587480900355538
mae: 0.028795335786761717
r2: 0.7404200061862045
pearson: 0.8623665187570267

=== Experiment 1131 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006440898925356016
rmse: 0.0802552112037344
mae: 0.03518017337971285
r2: 0.7095827023121073
pearson: 0.8429802891152068

=== Experiment 1140 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006306519059985469
rmse: 0.07941359493175881
mae: 0.033839620957706616
r2: 0.7156418312965633
pearson: 0.8487007148239986

=== Experiment 1148 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00695765795778851
rmse: 0.08341257673629625
mae: 0.03158924575897162
r2: 0.6862822649827698
pearson: 0.849664914921455

=== Experiment 1214 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006813497599565561
rmse: 0.08254391315878816
mae: 0.04371268081787979
r2: 0.6927823920852166
pearson: 0.8414380308120657

=== Experiment 1021 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006749659772215968
rmse: 0.08215631303932747
mae: 0.03564174394420955
r2: 0.6956608116232332
pearson: 0.8419239068766762

=== Experiment 1064 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007175028924457436
rmse: 0.08470554246598883
mae: 0.0371691629951972
r2: 0.6764811037679468
pearson: 0.8225947997223257

=== Experiment 1324 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007864416896878001
rmse: 0.08868154766848627
mae: 0.04030700134601496
r2: 0.6453969035143541
pearson: 0.8124387165496634

=== Experiment 1105 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007509625640891326
rmse: 0.08665809622240339
mae: 0.041492615130266114
r2: 0.6613942850912196
pearson: 0.8275105504537809

=== Experiment 1184 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0068821632746085305
rmse: 0.08295880468406311
mae: 0.032746387943121666
r2: 0.6896862870196039
pearson: 0.8508951510384762

=== Experiment 1145 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006414408022611998
rmse: 0.08008999951686852
mae: 0.037372841506400696
r2: 0.7107771654572971
pearson: 0.8501932418997727

=== Experiment 1074 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0064965138680189525
rmse: 0.08060095451059468
mae: 0.027075914874845723
r2: 0.7070750490254436
pearson: 0.8481987880472579

=== Experiment 1295 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007897623758951377
rmse: 0.08886857576754213
mae: 0.05456541899138252
r2: 0.6438996207189234
pearson: 0.8236693655527347

=== Experiment 1291 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008153527607912159
rmse: 0.09029688592588428
mae: 0.03521263400313441
r2: 0.632361028801179
pearson: 0.82622931890599

=== Experiment 1065 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006076357439505976
rmse: 0.0779509938327022
mae: 0.036843368124071234
r2: 0.7260197174621075
pearson: 0.8532423833502178

=== Experiment 1304 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008410493348132507
rmse: 0.09170874193953653
mae: 0.05261025158787275
r2: 0.6207745566739149
pearson: 0.802186443089313

=== Experiment 1163 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006085877340531413
rmse: 0.07801203330596769
mae: 0.02975440770572655
r2: 0.7255904693148996
pearson: 0.8535855366872036

=== Experiment 1321 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005705283655568126
rmse: 0.07553332811129221
mae: 0.029689537631547955
r2: 0.742751271057803
pearson: 0.8756795141492392

=== Experiment 1332 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0075029057138130024
rmse: 0.08661931490039045
mae: 0.041298965138290816
r2: 0.661697283645515
pearson: 0.8207573304839199

=== Experiment 1227 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006331723558351772
rmse: 0.07957212802452736
mae: 0.03207624827894411
r2: 0.7145053715585749
pearson: 0.8523295911630341

=== Experiment 1160 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006339308839232852
rmse: 0.07961977668414331
mae: 0.029821147654951752
r2: 0.7141633545821844
pearson: 0.8540455325666619

=== Experiment 1127 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005804154133677336
rmse: 0.07618499940065195
mae: 0.02857465347933881
r2: 0.738293244716084
pearson: 0.8641596209273786

=== Experiment 1311 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007774958159602111
rmse: 0.08817572318729294
mae: 0.0480443742143736
r2: 0.6494305586043225
pearson: 0.8312786764995744

=== Experiment 1328 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005930993044717321
rmse: 0.07701294076138972
mae: 0.028325192379531985
r2: 0.7325741340433651
pearson: 0.8629301801768678

=== Experiment 1284 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00848217230519024
rmse: 0.09209870957396873
mae: 0.042307916684081175
r2: 0.6175425840485034
pearson: 0.8045365518107984

=== Experiment 1369 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006618105134524662
rmse: 0.0813517371328029
mae: 0.0332691897899375
r2: 0.7015925523351101
pearson: 0.8390198551187328

=== Experiment 1334 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007010595612735467
rmse: 0.08372929960733858
mae: 0.04415025898843105
r2: 0.6838953294208552
pearson: 0.8338170080072964

=== Experiment 1255 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006405297433474519
rmse: 0.08003310211077988
mae: 0.03963377773313197
r2: 0.7111879579116289
pearson: 0.8500202560739646

=== Experiment 1277 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006825272567976533
rmse: 0.08261520784926062
mae: 0.038017795721313134
r2: 0.6922514639421258
pearson: 0.8340774668416581

=== Experiment 1330 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006130143140611888
rmse: 0.07829523063770799
mae: 0.034193617031022326
r2: 0.7235945438721394
pearson: 0.8543667851703584

=== Experiment 1062 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005924306403269345
rmse: 0.07696951606492888
mae: 0.0340123054435694
r2: 0.7328756317632383
pearson: 0.8566634208129978

=== Experiment 1296 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005651683622613824
rmse: 0.07517768034871669
mae: 0.0302927526474967
r2: 0.7451680729525338
pearson: 0.8660431488123781

=== Experiment 1354 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006779540415560282
rmse: 0.08233796460661559
mae: 0.03285142165184971
r2: 0.6943135065663151
pearson: 0.8432176833337759

=== Experiment 1361 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005704672002941804
rmse: 0.07552927911043375
mae: 0.028535234535474185
r2: 0.7427788502055149
pearson: 0.8619126645077331

=== Experiment 1257 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00870237970220761
rmse: 0.09328654620151616
mae: 0.05624472162918537
r2: 0.6076135294376774
pearson: 0.8134651197292712

=== Experiment 1373 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006912402001063955
rmse: 0.08314085638880535
mae: 0.04328491130668744
r2: 0.6883228361528102
pearson: 0.8346661641718764

=== Experiment 1351 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006143038756989808
rmse: 0.07837753987584586
mae: 0.031206155177056345
r2: 0.7230130862054538
pearson: 0.8514065681848185

=== Experiment 1123 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006836197290725103
rmse: 0.08268129952247427
mae: 0.04014961285270558
r2: 0.691758873001731
pearson: 0.8383900652929152

=== Experiment 1271 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008272776408983069
rmse: 0.0909548042105697
mae: 0.05397059767124551
r2: 0.6269841528463027
pearson: 0.8204742033484003

=== Experiment 1220 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.010117200492883229
rmse: 0.10058429545850202
mae: 0.04668121702673864
r2: 0.5438198826963652
pearson: 0.7793750979386285

=== Experiment 1279 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007107332727002441
rmse: 0.08430499823262225
mae: 0.04672697213064193
r2: 0.6795334955158173
pearson: 0.8326364590389392

=== Experiment 1029 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006536735917089793
rmse: 0.08085008297515713
mae: 0.033641270932096054
r2: 0.705261454535917
pearson: 0.8498480333659513

=== Experiment 1307 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007429592225560434
rmse: 0.08619508237457885
mae: 0.05096059555492985
r2: 0.6650029565630897
pearson: 0.8332725634123983

=== Experiment 1410 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062211045890887265
rmse: 0.0788739791635285
mae: 0.03922157069110686
r2: 0.7194931321955139
pearson: 0.8553191207467228

=== Experiment 1360 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006832253073240953
rmse: 0.08265744414897519
mae: 0.034386340615954705
r2: 0.6919367160321085
pearson: 0.8388296436118766

=== Experiment 1120 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006567809723296
rmse: 0.08104202442742901
mae: 0.03238689666685478
r2: 0.703860350290707
pearson: 0.8444180136025079

=== Experiment 1195 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006093217064511161
rmse: 0.07805906138630646
mae: 0.03257610294457649
r2: 0.7252595243911123
pearson: 0.8526692922322322

=== Experiment 1082 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006537313965654795
rmse: 0.0808536577135184
mae: 0.0351160273730574
r2: 0.7052353905805397
pearson: 0.8508019037846204

=== Experiment 1352 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007117013126116243
rmse: 0.08436239165716108
mae: 0.04227197414859508
r2: 0.6790970105804457
pearson: 0.8261354325267724

=== Experiment 1144 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006224350395707325
rmse: 0.07889455238295813
mae: 0.028778595394650292
r2: 0.71934678020367
pearson: 0.8485573692390753

=== Experiment 1203 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007136562071686273
rmse: 0.08447817512047874
mae: 0.03175104367983112
r2: 0.6782155572288417
pearson: 0.8345226165775573

=== Experiment 1230 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0058821476948682895
rmse: 0.07669516083083919
mae: 0.031013857698513603
r2: 0.7347765493695755
pearson: 0.867979640589391

=== Experiment 1125 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006058446780976223
rmse: 0.07783602495616168
mae: 0.0295334734310958
r2: 0.7268273011721305
pearson: 0.8551936730585652

=== Experiment 1017 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0067518988021728074
rmse: 0.08216993855524542
mae: 0.03778656181336911
r2: 0.6955598547479517
pearson: 0.8422895307806099

=== Experiment 1398 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006207305041871333
rmse: 0.07878645214674496
mae: 0.0318171494807505
r2: 0.7201153477059009
pearson: 0.849146963708377

=== Experiment 1072 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007308899246381422
rmse: 0.08549210049110632
mae: 0.03860151791394016
r2: 0.6704449498732288
pearson: 0.8305346754502227

=== Experiment 1348 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0058802231541947225
rmse: 0.0766826131153257
mae: 0.031510152836733966
r2: 0.7348633260614914
pearson: 0.8610740802488143

=== Experiment 1420 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00701385547353282
rmse: 0.08374876401197107
mae: 0.03843688707326596
r2: 0.683748343732277
pearson: 0.8380902904709715

=== Experiment 1286 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006380015092994242
rmse: 0.07987499666976045
mae: 0.038903208524377786
r2: 0.7123279275162757
pearson: 0.848887297346903

=== Experiment 1200 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006383132665959986
rmse: 0.07989450961086116
mae: 0.032200884716495945
r2: 0.7121873575233992
pearson: 0.8440774277632851

=== Experiment 1315 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0071270632732656014
rmse: 0.0844219359720304
mae: 0.04679044586046474
r2: 0.6786438538688889
pearson: 0.841298777980963

=== Experiment 1147 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007733104615986015
rmse: 0.08793807261923595
mae: 0.03984226744881877
r2: 0.651317716464819
pearson: 0.8313313236393052

=== Experiment 1417 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007507424062228378
rmse: 0.08664539261973701
mae: 0.049820303928248356
r2: 0.6614935533041431
pearson: 0.8234145688111423

=== Experiment 1365 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006577823297646022
rmse: 0.08110378103174982
mae: 0.03798655591007331
r2: 0.7034088426305758
pearson: 0.8392172676742103

=== Experiment 1342 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005763186706237794
rmse: 0.0759156552118059
mae: 0.02966274641297885
r2: 0.7401404479881892
pearson: 0.8655338458163164

=== Experiment 1088 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006113277165215845
rmse: 0.0781874488982461
mae: 0.03254748444847817
r2: 0.7243550232794636
pearson: 0.8546660841151846

=== Experiment 1327 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00553862374458869
rmse: 0.07442193053521717
mae: 0.028140839129616332
r2: 0.7502658930912295
pearson: 0.8676226309913716

=== Experiment 1353 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0072001302356374496
rmse: 0.08485358115976868
mae: 0.03773266270398586
r2: 0.675349296694772
pearson: 0.8257479503805291

=== Experiment 1289 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007361157109125047
rmse: 0.08579718590446336
mae: 0.03850130848800523
r2: 0.6680886658425631
pearson: 0.8197437906589019

=== Experiment 1298 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006393802866676059
rmse: 0.07996125853609395
mae: 0.029050477664434696
r2: 0.7117062428694259
pearson: 0.8508707011144482

=== Experiment 1397 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006902123819387407
rmse: 0.08307902153604968
mae: 0.03386283156387457
r2: 0.688786274840832
pearson: 0.8324886708836557

=== Experiment 1290 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007815600322858784
rmse: 0.08840588398324392
mae: 0.03560576825220941
r2: 0.6475980213510633
pearson: 0.8423071583382232

=== Experiment 1272 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007642658591305982
rmse: 0.08742230030893709
mae: 0.04817818388596575
r2: 0.655395887909302
pearson: 0.8192909938447475

=== Experiment 1201 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006992046691269079
rmse: 0.08361845903428906
mae: 0.03582854104780216
r2: 0.6847316921257698
pearson: 0.8285704790392093

=== Experiment 1323 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005603694843979376
rmse: 0.07485783087947029
mae: 0.029097902746717686
r2: 0.7473318658596845
pearson: 0.8660053193486855

=== Experiment 1362 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007398312428941638
rmse: 0.08601344330360015
mae: 0.03811495319781123
r2: 0.6664133488253399
pearson: 0.825816880164756

=== Experiment 1356 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0070509062306975635
rmse: 0.08396967447059422
mae: 0.035988540713554625
r2: 0.682077741399004
pearson: 0.8306567584550908

=== Experiment 1135 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007625227585377787
rmse: 0.08732254912322353
mae: 0.03497990681654358
r2: 0.6561818442946319
pearson: 0.8108924086517592

=== Experiment 1434 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006630760293184612
rmse: 0.08142948049192389
mae: 0.03882371137857177
r2: 0.7010219368010943
pearson: 0.8428136807531386

=== Experiment 1470 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006874041155540722
rmse: 0.08290983750762464
mae: 0.03603444370934098
r2: 0.6900525097935534
pearson: 0.8347632293009183

=== Experiment 1341 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007109002271826154
rmse: 0.08431489946519627
mae: 0.03253728181985571
r2: 0.6794582164745389
pearson: 0.8251697332776362

=== Experiment 1233 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005968636771948794
rmse: 0.07725695290359823
mae: 0.035620583345363346
r2: 0.7308767949507694
pearson: 0.8558583317190841

=== Experiment 1254 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007045761567406809
rmse: 0.08393903482532312
mae: 0.02933277420519647
r2: 0.6823097120024446
pearson: 0.8513205264465892

=== Experiment 1416 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0075638075062693065
rmse: 0.08697015296220484
mae: 0.048251196985056095
r2: 0.6589512486285892
pearson: 0.8215832992237622

=== Experiment 1469 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006925412365426196
rmse: 0.0832190625123006
mae: 0.03128030293047367
r2: 0.6877362045500159
pearson: 0.834698746898244

=== Experiment 1475 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0062464703649733905
rmse: 0.0790346149795986
mae: 0.03410875508394833
r2: 0.718349400525206
pearson: 0.8489455085845458

=== Experiment 1370 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006212076740026939
rmse: 0.07881672880821011
mae: 0.03040418393454442
r2: 0.7199001939362497
pearson: 0.8564737107792837

=== Experiment 1482 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0060616919086046534
rmse: 0.07785686808885041
mae: 0.03319871074159362
r2: 0.7266809797956545
pearson: 0.8539550342433369

=== Experiment 1350 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006734122337066756
rmse: 0.08206169835597334
mae: 0.03343410311809639
r2: 0.696361387735563
pearson: 0.8374935053887161

=== Experiment 1431 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007843437422867805
rmse: 0.08856318322456462
mae: 0.04047289398473209
r2: 0.646342858763701
pearson: 0.8197304508621778

=== Experiment 1312 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00611205101880467
rmse: 0.07817960743572885
mae: 0.03627618585207011
r2: 0.7244103096814751
pearson: 0.8531864743582006

=== Experiment 1078 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005729719392064478
rmse: 0.07569490994818924
mae: 0.02921609586657197
r2: 0.7416494744541726
pearson: 0.861920836345823

=== Experiment 1267 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006679855798118847
rmse: 0.08173038479120753
mae: 0.04009122968184843
r2: 0.6988082420921939
pearson: 0.8417117824796643

=== Experiment 1430 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007128007469108101
rmse: 0.08442752791067674
mae: 0.04764668933475365
r2: 0.678601280493923
pearson: 0.8337150076986626

=== Experiment 1266 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008050072748690082
rmse: 0.08972219763631563
mae: 0.04265139176745399
r2: 0.6370257628695368
pearson: 0.8141722470565654

=== Experiment 1457 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007823746796139446
rmse: 0.08845194625410707
mae: 0.054177752560938576
r2: 0.6472307004563753
pearson: 0.8338832394030257

=== Experiment 1388 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006751683167654394
rmse: 0.08216862641942115
mae: 0.03209062520362022
r2: 0.6955695776134789
pearson: 0.8450948541052438

=== Experiment 1247 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006649745477336428
rmse: 0.08154597155798947
mae: 0.04158415336536528
r2: 0.7001659032037075
pearson: 0.8414063040796614

=== Experiment 1081 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006472177387274284
rmse: 0.08044984392324378
mae: 0.03038086585623197
r2: 0.7081723702309164
pearson: 0.8496207367444721

=== Experiment 1449 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007495983850219524
rmse: 0.086579350021928
mae: 0.048877828186360654
r2: 0.6620093874283999
pearson: 0.8308271282744105

=== Experiment 1128 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006944792528419702
rmse: 0.08333542181101444
mae: 0.03532858322532305
r2: 0.6868623615305007
pearson: 0.8405036633009089

=== Experiment 1426 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005329188733092478
rmse: 0.07300129268096886
mae: 0.02800765191330375
r2: 0.7597092255801957
pearson: 0.8717282290364105

=== Experiment 1216 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00642153290768155
rmse: 0.08013446766330672
mae: 0.03332366192267296
r2: 0.7104559075254127
pearson: 0.8432045256832539

=== Experiment 1115 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062625681286291686
rmse: 0.07913638940859741
mae: 0.02957284530783355
r2: 0.7176235594471347
pearson: 0.8506936539289539

=== Experiment 1055 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005720778875154601
rmse: 0.07563583063042674
mae: 0.028331917099927342
r2: 0.7420525984266166
pearson: 0.8647668936783631

=== Experiment 1326 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006492085039425524
rmse: 0.08057347602918422
mae: 0.032132428261649124
r2: 0.7072747429574447
pearson: 0.8436065935585524

=== Experiment 1415 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007164629342370134
rmse: 0.0846441335378308
mae: 0.03568053297058246
r2: 0.676950016347057
pearson: 0.8382625549044513

=== Experiment 1390 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009056321655458818
rmse: 0.09516470803537842
mae: 0.04000389963748197
r2: 0.5916544425473476
pearson: 0.8184208527712846

=== Experiment 1419 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007702411350547838
rmse: 0.08776338274330496
mae: 0.0357965065232596
r2: 0.6527016622942872
pearson: 0.8267641781579932

=== Experiment 1269 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0059558597655541045
rmse: 0.0771742169740264
mae: 0.028062438181993422
r2: 0.7314529045455154
pearson: 0.8601163155386664

=== Experiment 1288 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006064190902273433
rmse: 0.07787291507496963
mae: 0.028407673916971517
r2: 0.7265683012710205
pearson: 0.8581423545892183

=== Experiment 1333 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005821167722965087
rmse: 0.07629657740007141
mae: 0.029891378396578017
r2: 0.7375261094633527
pearson: 0.8666489789366226

=== Experiment 1229 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0059649509719238184
rmse: 0.07723309505596561
mae: 0.029303007692527583
r2: 0.7310429860516509
pearson: 0.86253907434549

=== Experiment 1389 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005709428941351117
rmse: 0.07556076323960152
mae: 0.02950933448861443
r2: 0.7425643619463278
pearson: 0.862190840584929

=== Experiment 1436 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0072123765650609585
rmse: 0.08492571203740926
mae: 0.04060179818223296
r2: 0.6747971150910876
pearson: 0.826404771167934

=== Experiment 1337 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006806401240157797
rmse: 0.08250091660192509
mae: 0.03185084524928189
r2: 0.6931023638075661
pearson: 0.8452244889241319

=== Experiment 1006 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006077126204687963
rmse: 0.07795592475680065
mae: 0.03549932185549924
r2: 0.7259850541784112
pearson: 0.8527601071969249

=== Experiment 1221 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006218558653344851
rmse: 0.07885783824924984
mae: 0.0329284435525037
r2: 0.7196079273176532
pearson: 0.8492083073494006

=== Experiment 1110 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005894913405870293
rmse: 0.07677833943157597
mae: 0.029997666940739694
r2: 0.7342009490791142
pearson: 0.8579058443409078

=== Experiment 1381 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005948032518456757
rmse: 0.07712348875962989
mae: 0.02826441290924288
r2: 0.7318058316720999
pearson: 0.8562947553865478

=== Experiment 1493 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007951148849526211
rmse: 0.08916921469613945
mae: 0.05255215421025336
r2: 0.641486197943098
pearson: 0.817066421544141

=== Experiment 1461 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006494481641285707
rmse: 0.08058834680824335
mae: 0.03385893611864196
r2: 0.7071666812343943
pearson: 0.8417703519991272

=== Experiment 1495 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006670835779276608
rmse: 0.08167518459897478
mae: 0.03230470247906435
r2: 0.6992149507717765
pearson: 0.837501829298159

=== Experiment 1379 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005953516689861596
rmse: 0.07715903505009375
mae: 0.028351366992814094
r2: 0.731558552797224
pearson: 0.8585608238015925

=== Experiment 1211 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005880865197048119
rmse: 0.07668679936630632
mae: 0.02901683599536886
r2: 0.7348343766318165
pearson: 0.8611877425101853

=== Experiment 1490 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005856848577200957
rmse: 0.07653005015809775
mae: 0.027703207900342853
r2: 0.7359172754502021
pearson: 0.8581481213226393

=== Experiment 1246 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059950912558456105
rmse: 0.077427974633498
mae: 0.030056410488576477
r2: 0.7296839739153707
pearson: 0.8600211896248904

=== Experiment 1459 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007325829991018782
rmse: 0.08559106256507616
mae: 0.045332865271977
r2: 0.6696815500493207
pearson: 0.8290462902354191

=== Experiment 1473 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006350229162611226
rmse: 0.07968832513368082
mae: 0.02914687474985555
r2: 0.7136709620074553
pearson: 0.8451753414389978

=== Experiment 1117 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006086370577397874
rmse: 0.07801519452900105
mae: 0.03065162410629057
r2: 0.725568229481681
pearson: 0.8527617984210738

=== Experiment 1456 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006254678913227488
rmse: 0.07908652801348336
mae: 0.029944423988347246
r2: 0.7179792806972873
pearson: 0.8536518252333177

=== Experiment 1313 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005540789364279606
rmse: 0.07443647872031298
mae: 0.02800866840380035
r2: 0.7501682462525282
pearson: 0.8664264935969899

=== Experiment 1424 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006207925110345704
rmse: 0.07879038716966495
mae: 0.03383128763065775
r2: 0.7200873890913045
pearson: 0.84914069345851

=== Experiment 1429 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006469991755141271
rmse: 0.0804362589578933
mae: 0.0287228264235545
r2: 0.7082709194218237
pearson: 0.8577868943363522

=== Experiment 1169 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00662276087887832
rmse: 0.08138034700637692
mae: 0.034801215260147005
r2: 0.7013826268713536
pearson: 0.839224365148844

=== Experiment 1401 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006868964461602621
rmse: 0.08287921610152102
mae: 0.034654118083209874
r2: 0.6902814156887984
pearson: 0.8370201429263746

=== Experiment 1418 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006577727509120336
rmse: 0.08110319049902005
mae: 0.0357494951927315
r2: 0.703413161693039
pearson: 0.8388878057394455

=== Experiment 1161 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005694509177083726
rmse: 0.07546197172804144
mae: 0.02919013255348098
r2: 0.7432370875504539
pearson: 0.862866176875331

=== Experiment 1485 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0060333349501046326
rmse: 0.0776745450588842
mae: 0.03144175705597366
r2: 0.7279595825735685
pearson: 0.8571074807183693

=== Experiment 1178 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007274502991387951
rmse: 0.08529069698031522
mae: 0.037178370935464666
r2: 0.6719958618719348
pearson: 0.8474871556264943

=== Experiment 1376 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006638533770140226
rmse: 0.08147719785400223
mae: 0.03571041136001107
r2: 0.7006714341465345
pearson: 0.8449563216646695

=== Experiment 1438 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065223105182400515
rmse: 0.08076082291705584
mae: 0.04306227428482886
r2: 0.7059118894209483
pearson: 0.8459858031002023

=== Experiment 1428 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005717115927051332
rmse: 0.07561161238230099
mae: 0.030764198754725792
r2: 0.7422177591444064
pearson: 0.8639647648279365

=== Experiment 1310 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005769919597100929
rmse: 0.07595998681609238
mae: 0.031659062714197515
r2: 0.7398368649025424
pearson: 0.8633178016725135

=== Experiment 1478 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007336196282845764
rmse: 0.08565159825038739
mae: 0.04425579176102714
r2: 0.6692141385133961
pearson: 0.8223119888645039

=== Experiment 1339 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008699086647756283
rmse: 0.09326889432043399
mae: 0.04151306739458158
r2: 0.6077620118135036
pearson: 0.7961622113442612

=== Experiment 1101 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007730818066797689
rmse: 0.08792507075230414
mae: 0.03824312329159049
r2: 0.6514208159613334
pearson: 0.8244670958842596

=== Experiment 1223 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006234573245185222
rmse: 0.07895931385964054
mae: 0.031295453887155406
r2: 0.7188858364201318
pearson: 0.8634068252228355

=== Experiment 1215 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0061315581461898645
rmse: 0.07830426646224244
mae: 0.03182305611417245
r2: 0.72353074189342
pearson: 0.8529650062575073

=== Experiment 1479 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0074717010193279835
rmse: 0.08643900172565613
mae: 0.03953063562096097
r2: 0.6631042895856084
pearson: 0.8151374233167408

=== Experiment 1414 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007199367671535029
rmse: 0.08484908762936128
mae: 0.04405151001947634
r2: 0.6753836803745199
pearson: 0.8295824693535228

=== Experiment 1183 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006294734963680854
rmse: 0.07933936578824445
mae: 0.030804230756789813
r2: 0.7161731710123611
pearson: 0.8470152327246044

=== Experiment 1453 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006663427682465534
rmse: 0.08162982103658892
mae: 0.03186595976749312
r2: 0.6995489785964352
pearson: 0.8468406079905495

=== Experiment 1270 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008491949782831382
rmse: 0.09215177579857797
mae: 0.05612060616156001
r2: 0.6171017218850614
pearson: 0.8145047398052639

=== Experiment 1273 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007644679370119796
rmse: 0.0874338571156494
mae: 0.03596140559096852
r2: 0.6553047718819999
pearson: 0.8357131579894083

=== Experiment 1236 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00725061636186356
rmse: 0.08515055115419723
mae: 0.04027635690563904
r2: 0.6730728994838799
pearson: 0.8440499138151332

=== Experiment 1450 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00638002856686603
rmse: 0.0798750810132048
mae: 0.03151564695009009
r2: 0.7123273199853271
pearson: 0.845015993850995

=== Experiment 1335 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007840039188577605
rmse: 0.0885439957793729
mae: 0.03593022390216625
r2: 0.646496083652169
pearson: 0.8351442803088354

=== Experiment 1499 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006283812830328144
rmse: 0.07927050416345378
mae: 0.033411489696353866
r2: 0.7166656451980362
pearson: 0.8474143343502353

=== Experiment 1494 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00623600911204189
rmse: 0.07896840578384427
mae: 0.03025800154557137
r2: 0.718821093815538
pearson: 0.8538957657786452

=== Experiment 1208 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006945501195324851
rmse: 0.08333967359742207
mae: 0.040235817590902195
r2: 0.6868304080516557
pearson: 0.8326530306528401

=== Experiment 1486 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005841222775302457
rmse: 0.07642789265250258
mae: 0.029949827800753937
r2: 0.7366218359803642
pearson: 0.8631552981615972

=== Experiment 1367 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005643770873157619
rmse: 0.0751250349294935
mae: 0.030154853786050186
r2: 0.7455248553428468
pearson: 0.8647772818692836

=== Experiment 1204 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006795810328428325
rmse: 0.0824367049828432
mae: 0.03895242947073919
r2: 0.6935799033560311
pearson: 0.8425650047544989

=== Experiment 1359 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007092590459398885
rmse: 0.08421751872026915
mae: 0.03498868223960114
r2: 0.6801982178734935
pearson: 0.8402652508455031

=== Experiment 1467 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007006286237934484
rmse: 0.08370356168010107
mae: 0.032101454971788304
r2: 0.6840896372339306
pearson: 0.8388647471145414

=== Experiment 1317 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007542634805223056
rmse: 0.08684834371030374
mae: 0.043694796975693784
r2: 0.6599059163999461
pearson: 0.8243576852534711

=== Experiment 1402 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007088089637310941
rmse: 0.08419079306735945
mae: 0.049036534311868954
r2: 0.6804011579604756
pearson: 0.8415883930427009

=== Experiment 1285 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006359514215630618
rmse: 0.07974656240635465
mae: 0.029851286744535157
r2: 0.7132523030534754
pearson: 0.8502432431443354

=== Experiment 1465 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00590988395363492
rmse: 0.0768757696132853
mae: 0.029749440047309814
r2: 0.7335259336694491
pearson: 0.8621412557238596

=== Experiment 1245 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006451479453499486
rmse: 0.08032110216810702
mae: 0.03241993261276184
r2: 0.7091056309549646
pearson: 0.8486693469361668

=== Experiment 1437 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006298978680198967
rmse: 0.07936610536116137
mae: 0.028419926361146656
r2: 0.7159818237023616
pearson: 0.8541330834256728

=== Experiment 1349 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006432904145033897
rmse: 0.08020538725692868
mae: 0.03490817560073593
r2: 0.7099431834380596
pearson: 0.8481739719414263

=== Experiment 1322 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006054540166154325
rmse: 0.07781092575052892
mae: 0.03483595730957191
r2: 0.7270034487150172
pearson: 0.8550194447007488

=== Experiment 1404 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005569706083004561
rmse: 0.07463046350522393
mae: 0.027681778730361085
r2: 0.748864404132441
pearson: 0.8681534601099172

=== Experiment 1329 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006840266304470974
rmse: 0.08270590247661272
mae: 0.0357616608796575
r2: 0.6915754029628396
pearson: 0.8382959305214844

=== Experiment 1407 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006651507257990909
rmse: 0.08155677321958557
mae: 0.03022420198633243
r2: 0.7000864652894158
pearson: 0.838288599749254

=== Experiment 1427 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006445099504911568
rmse: 0.08028137707408592
mae: 0.03496244155445315
r2: 0.7093933000287638
pearson: 0.8472884441731673

=== Experiment 1408 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00761563750054959
rmse: 0.0872676200004881
mae: 0.04674794242993376
r2: 0.6566142569986162
pearson: 0.8168765700416108

=== Experiment 1345 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005757018694498642
rmse: 0.07587502022733597
mae: 0.028519338814601308
r2: 0.740418560922758
pearson: 0.8691373574758716

=== Experiment 1393 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005318201271865022
rmse: 0.07292599860039643
mae: 0.02825507892155454
r2: 0.7602046453709903
pearson: 0.8736083735720933

=== Experiment 1488 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006345641757917146
rmse: 0.07965953651583184
mae: 0.031440985473616
r2: 0.7138778060660402
pearson: 0.8470688398866153

=== Experiment 1491 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007295006784037954
rmse: 0.08541081186850968
mae: 0.031692836907426886
r2: 0.6710713548857555
pearson: 0.8329792325605997

=== Experiment 1050 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006155060040703756
rmse: 0.07845419071473338
mae: 0.03297842754210244
r2: 0.7224710518137636
pearson: 0.8586110819750883

=== Experiment 1412 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006922877324910709
rmse: 0.08320382998943443
mae: 0.0341339174571742
r2: 0.6878505084111026
pearson: 0.8327107004186092

=== Experiment 1394 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006309448224540755
rmse: 0.07943203525367304
mae: 0.031216312147233694
r2: 0.7155097565559894
pearson: 0.846166994597357

=== Experiment 1231 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006587283643917148
rmse: 0.0811620825503951
mae: 0.030024731917463625
r2: 0.7029822797810275
pearson: 0.8514403686130079

=== Experiment 1241 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007428514101307914
rmse: 0.08618882816994274
mae: 0.03509659259388797
r2: 0.6650515687111171
pearson: 0.8288541604664676

=== Experiment 1207 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006148434034575587
rmse: 0.07841195084026151
mae: 0.036862919138787795
r2: 0.7227698155137543
pearson: 0.852068640660698

=== Experiment 1460 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0070533113604310015
rmse: 0.0839839946682164
mae: 0.03712724610954741
r2: 0.681969295157902
pearson: 0.8316602498225144

=== Experiment 1435 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006294858878244083
rmse: 0.07934014669915908
mae: 0.029071766322063725
r2: 0.7161675837592442
pearson: 0.8589014012688484

=== Experiment 1468 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005408342395361572
rmse: 0.07354143318811221
mae: 0.026108025654288956
r2: 0.7561402217866733
pearson: 0.8703912723541927

=== Experiment 1281 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005819981707526042
rmse: 0.07628880460150128
mae: 0.028224738169991284
r2: 0.7375795863774939
pearson: 0.8591315241614246

=== Experiment 1422 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00571654970307706
rmse: 0.07560786799716719
mae: 0.02931856609621764
r2: 0.742243289934192
pearson: 0.8647906305996427

=== Experiment 1141 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006806084042195171
rmse: 0.08249899418899101
mae: 0.028653135950257973
r2: 0.6931166661240913
pearson: 0.8371169290063023

=== Experiment 1364 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005824634014860395
rmse: 0.07631928992633773
mae: 0.029663032349510395
r2: 0.7373698158874249
pearson: 0.8607366811687547

=== Experiment 1406 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006163807138994446
rmse: 0.07850991745629622
mae: 0.028397914958480073
r2: 0.7220766490017287
pearson: 0.849824348027985

=== Experiment 1268 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005751187683200516
rmse: 0.07583658538726883
mae: 0.029733162061686608
r2: 0.7406814786557663
pearson: 0.86110700965053

=== Experiment 1484 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006565921356590681
rmse: 0.08103037304980572
mae: 0.04009665353209983
r2: 0.7039454959143161
pearson: 0.8455969404998394

=== Experiment 1487 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006381693986300233
rmse: 0.07988550548316155
mae: 0.032675072015306686
r2: 0.7122522269560461
pearson: 0.8464473853917045

=== Experiment 1193 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006448292617439943
rmse: 0.0803012616179842
mae: 0.0353668116260235
r2: 0.7092493239902558
pearson: 0.8468990061136524

=== Experiment 1075 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006067865622591624
rmse: 0.07789650584327659
mae: 0.029865771899361005
r2: 0.7264026097492412
pearson: 0.8651254840007024

=== Experiment 1347 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0057591570296172
rmse: 0.07588911008581666
mae: 0.02894185236604874
r2: 0.7403221443334156
pearson: 0.8643066315417306

=== Experiment 1039 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006093636375472124
rmse: 0.07806174719715236
mae: 0.033120213261737544
r2: 0.7252406178444352
pearson: 0.8520460335073763

=== Experiment 1382 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00619597117625743
rmse: 0.0787144915263856
mae: 0.027338719948626553
r2: 0.7206263867180162
pearson: 0.8524997751994834

=== Experiment 1452 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006002049793359083
rmse: 0.07747289715351481
mae: 0.028458261898340286
r2: 0.7293702165215747
pearson: 0.8594318469527551

=== Experiment 1380 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007445332151020074
rmse: 0.08628633814816847
mae: 0.03715316496141786
r2: 0.6642932502517855
pearson: 0.8166395628287445

=== Experiment 1439 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0068199815998985485
rmse: 0.0825831798848806
mae: 0.04124760424734857
r2: 0.692490031364029
pearson: 0.8371856070598289

=== Experiment 1411 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006193540856160328
rmse: 0.07869905244766501
mae: 0.027882428980837582
r2: 0.7207359687815289
pearson: 0.8570114154897273

=== Experiment 1471 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007315062060915169
rmse: 0.0855281360776392
mae: 0.03734799583346765
r2: 0.6701670712783643
pearson: 0.8277236120237225

=== Experiment 1368 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00881360941662597
rmse: 0.09388082560686165
mae: 0.04114436474862557
r2: 0.6025982305704947
pearson: 0.8292280629454222

=== Experiment 1383 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005917076148845068
rmse: 0.07692253342711138
mae: 0.03048558573025824
r2: 0.7332016407529507
pearson: 0.8599758298878696

=== Experiment 1392 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006680416873947252
rmse: 0.08173381719916947
mae: 0.031542775883011985
r2: 0.6987829434300394
pearson: 0.8451315440535313

=== Experiment 1190 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006380628206577579
rmse: 0.07987883453442206
mae: 0.03530425095261293
r2: 0.7123002824946549
pearson: 0.8443812220807646

=== Experiment 1318 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005756984905706061
rmse: 0.07587479756616199
mae: 0.028684827407302084
r2: 0.740420084444544
pearson: 0.8608092961067356

=== Experiment 1357 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006076301780672958
rmse: 0.0779506368201887
mae: 0.029354004448568057
r2: 0.726022227094391
pearson: 0.8566817085068671

=== Experiment 1302 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0072282308438864785
rmse: 0.08501900283987385
mae: 0.03453135230736501
r2: 0.6740822526368466
pearson: 0.8279675648132413

=== Experiment 1238 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006339173594209633
rmse: 0.07961892736158678
mae: 0.036266608988480255
r2: 0.7141694527207554
pearson: 0.8475058555625242

=== Experiment 1489 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006559804421802691
rmse: 0.08099261955143006
mae: 0.03402869782177032
r2: 0.7042213058116379
pearson: 0.8529841879877991

=== Experiment 1492 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006265194178916443
rmse: 0.07915297959594726
mae: 0.03320423486778981
r2: 0.7175051519954947
pearson: 0.8494242967411514

=== Experiment 1283 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006000465189487561
rmse: 0.07746266965117818
mae: 0.027978358666869044
r2: 0.7294416656127043
pearson: 0.8550569067270541

=== Experiment 1385 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006492171778074777
rmse: 0.08057401428546784
mae: 0.03583494635063267
r2: 0.7072708319499266
pearson: 0.8411469533164084

=== Experiment 1421 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005737098881540475
rmse: 0.07574363921505538
mae: 0.02943364081482141
r2: 0.7413167365216616
pearson: 0.861891234178523

=== Experiment 1023 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007031456439746721
rmse: 0.0838537801160253
mae: 0.042895737339112304
r2: 0.6829547239124698
pearson: 0.8379329213501565

=== Experiment 1445 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005689647052660791
rmse: 0.07542974912235086
mae: 0.0322613316734329
r2: 0.7434563186006995
pearson: 0.8627554319327951

=== Experiment 1399 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00688391251901527
rmse: 0.08296934686385853
mae: 0.028382106106811522
r2: 0.6896074143592054
pearson: 0.8366298580666218

=== Experiment 1228 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006967472614566469
rmse: 0.08347138799952034
mae: 0.0345916405702756
r2: 0.6858397264284124
pearson: 0.8348940050945319

=== Experiment 1440 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006066878761037243
rmse: 0.07789017114525583
mae: 0.029653146033770218
r2: 0.7264471069023773
pearson: 0.8572277675977638

=== Experiment 1425 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006520482833271509
rmse: 0.08074950670605678
mae: 0.043664755437782726
r2: 0.7059942989317542
pearson: 0.8486936629786155

=== Experiment 1444 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006230829637042205
rmse: 0.07893560436863839
mae: 0.03703333269742486
r2: 0.7190546340636125
pearson: 0.8532378987207626

=== Experiment 1466 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005976250189611212
rmse: 0.07730621055006649
mae: 0.031431975718894826
r2: 0.7305335092992922
pearson: 0.8548259415110857

=== Experiment 1446 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006238728911698425
rmse: 0.07898562471550392
mae: 0.03519976684300491
r2: 0.7186984592461001
pearson: 0.8538350915071047

=== Experiment 1098 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007663150323100614
rmse: 0.0875394215373886
mae: 0.04776023809776325
r2: 0.6544719247417832
pearson: 0.8427485554700752

=== Experiment 1199 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0058623259147392946
rmse: 0.07656582733007784
mae: 0.03404517455023968
r2: 0.7356703047113592
pearson: 0.8585912790308284

=== Experiment 1243 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007143830581929604
rmse: 0.08452118421987238
mae: 0.03430104425922291
r2: 0.6778878233010861
pearson: 0.8344875717121173

=== Experiment 1213 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006544792118334422
rmse: 0.08089988948283194
mae: 0.03947352320178481
r2: 0.704898203967602
pearson: 0.8430136294521182

=== Experiment 1292 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006918931130437695
rmse: 0.0831801125897152
mae: 0.03031883437166732
r2: 0.6880284405830377
pearson: 0.840340595510688

=== Experiment 1441 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0072975994067695565
rmse: 0.08542598788875407
mae: 0.03989775358198328
r2: 0.6709544546678881
pearson: 0.8235619458287058

=== Experiment 1409 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00613109126422351
rmse: 0.0783012852016077
mae: 0.030148139330686417
r2: 0.7235517933957925
pearson: 0.8506454032975657

=== Experiment 1386 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007654517486316956
rmse: 0.08749009936168181
mae: 0.04941116852145032
r2: 0.654861175552756
pearson: 0.8200442969796657

=== Experiment 1234 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009548076351117806
rmse: 0.09771425868888228
mae: 0.03865388185120003
r2: 0.5694814397578771
pearson: 0.7848182171074746

=== Experiment 1168 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00651500130666893
rmse: 0.08071555802117043
mae: 0.04029950422230053
r2: 0.7062414585536603
pearson: 0.8475668775437317

=== Experiment 1474 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005770931393003268
rmse: 0.07596664658258431
mae: 0.03139321683726444
r2: 0.7397912434706314
pearson: 0.8606602334042995

=== Experiment 1164 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00843402489868355
rmse: 0.09183694735063633
mae: 0.04903654320477284
r2: 0.6197135294165959
pearson: 0.7953841810967801

=== Experiment 1483 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007144435568933197
rmse: 0.08452476305162408
mae: 0.03695342881056407
r2: 0.6778605447033692
pearson: 0.8237679248286177

=== Experiment 1248 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006904608290719274
rmse: 0.08309397264976127
mae: 0.042868870038430235
r2: 0.6886742511219774
pearson: 0.8341240729175119

=== Experiment 1458 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005566375421854577
rmse: 0.07460814581434508
mae: 0.028556114728273786
r2: 0.7490145821777578
pearson: 0.8667977510261989

=== Experiment 1085 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0076018195984343135
rmse: 0.08718841435898644
mae: 0.03914538438170004
r2: 0.6572373001232702
pearson: 0.8415308256471646

=== Experiment 1152 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005952319423806887
rmse: 0.07715127622928145
mae: 0.032044452967804365
r2: 0.7316125369966738
pearson: 0.8553500808978541

=== Experiment 1114 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007308939807450449
rmse: 0.0854923377119286
mae: 0.04076755429949595
r2: 0.6704431209924787
pearson: 0.8508825214638426

=== Experiment 1294 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006861925814684992
rmse: 0.08283674193668526
mae: 0.039064314608231954
r2: 0.690598785180369
pearson: 0.8326431108295781

=== Experiment 1462 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006614158092433861
rmse: 0.0813274744009296
mae: 0.0330160719671012
r2: 0.7017705227257582
pearson: 0.845894943458685

=== Experiment 1251 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005836020642082467
rmse: 0.07639385212229101
mae: 0.029487584340653506
r2: 0.7368563978776195
pearson: 0.8636811083927012

=== Experiment 1299 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006471666291096655
rmse: 0.08044666737097725
mae: 0.03579458419627344
r2: 0.7081954153326151
pearson: 0.8485860749741645

=== Experiment 1387 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006202595083167439
rmse: 0.07875655581072244
mae: 0.0386130701236719
r2: 0.7203277176708481
pearson: 0.8535062976538701

=== Experiment 1343 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006549446934481847
rmse: 0.08092865335888054
mae: 0.030304789708982708
r2: 0.704688320356256
pearson: 0.8620357603736643

=== Experiment 1391 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00591483488867959
rmse: 0.07690796375330444
mae: 0.02926570728497061
r2: 0.733302698187358
pearson: 0.8570080973171345

=== Experiment 1443 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006221583703103728
rmse: 0.07887701631719932
mae: 0.03180645396949459
r2: 0.7194715291554517
pearson: 0.848476860299879

=== Experiment 1331 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00700053112317892
rmse: 0.08366917666129457
mae: 0.042057511133988
r2: 0.6843491328252393
pearson: 0.8310895565800012

=== Experiment 1336 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006976146210523568
rmse: 0.08352332734346476
mae: 0.034887722254285755
r2: 0.6854486378042484
pearson: 0.8454158224219538

=== Experiment 1451 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00695619131304868
rmse: 0.0834037847645338
mae: 0.046119642905997196
r2: 0.6863483953485687
pearson: 0.8398568316758203

=== Experiment 1244 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006981682194574221
rmse: 0.08355646111806209
mae: 0.041209362874747475
r2: 0.6851990227199773
pearson: 0.8387342563236565

=== Experiment 1476 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005624719491536381
rmse: 0.07499812992026123
mae: 0.02933882605909891
r2: 0.7463838737549942
pearson: 0.8648661304388834

=== Experiment 1080 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006244614533359339
rmse: 0.0790228734820453
mae: 0.03580040805759687
r2: 0.7184330791558697
pearson: 0.8505288941918521

=== Experiment 1366 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007011131827664133
rmse: 0.08373250162072153
mae: 0.03162505623897031
r2: 0.6838711517257279
pearson: 0.836209923534204

=== Experiment 1119 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.01216293348170943
rmse: 0.11028569028531957
mae: 0.040005620595444706
r2: 0.4515786826261369
pearson: 0.723809819030478

=== Experiment 1263 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005889938718573076
rmse: 0.0767459361697613
mae: 0.032278209945616934
r2: 0.7344252555398183
pearson: 0.8602845243130695

=== Experiment 1481 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006311979567079523
rmse: 0.07944796767117157
mae: 0.03201603959636953
r2: 0.7153956194350455
pearson: 0.8602907928462006

=== Experiment 1051 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006170631818019542
rmse: 0.07855336923404077
mae: 0.03025259193492027
r2: 0.7217689272282579
pearson: 0.8495848445707602

=== Experiment 1423 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006907613410900745
rmse: 0.08311205334306658
mae: 0.03327632894527439
r2: 0.6885387515756496
pearson: 0.8342662491388783

=== Experiment 1372 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006019490206578736
rmse: 0.0775853736639757
mae: 0.03213520961585512
r2: 0.7285838359656135
pearson: 0.8609438699222417

=== Experiment 1187 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007199268204539308
rmse: 0.08484850148670457
mae: 0.04353498009283448
r2: 0.6753881652975781
pearson: 0.8264608682249257

=== Experiment 1154 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00610978712514869
rmse: 0.07816512729567253
mae: 0.03174340672731948
r2: 0.7245123876500075
pearson: 0.8546280022336932

=== Experiment 1275 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007086035058123695
rmse: 0.08417859025977861
mae: 0.03584513430441896
r2: 0.6804937980317391
pearson: 0.8277038598871336

=== Experiment 1156 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0061775954477162115
rmse: 0.07859768093090413
mae: 0.034017054347812485
r2: 0.721454940230163
pearson: 0.8531697258188226

=== Experiment 1464 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005754866752190284
rmse: 0.07586083806675407
mae: 0.03214651001825677
r2: 0.7405155910542995
pearson: 0.8609464962303354

=== Experiment 1363 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006516947782279308
rmse: 0.0807276147441463
mae: 0.030867772664330145
r2: 0.7061536928250975
pearson: 0.8475000538342887

=== Experiment 1073 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006862346727836621
rmse: 0.08283928251642829
mae: 0.03219658265870607
r2: 0.6905798063916201
pearson: 0.8490689294488811

=== Experiment 1222 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005987455919496014
rmse: 0.07737865286689873
mae: 0.03181599039236178
r2: 0.7300282478708056
pearson: 0.8605891253015616

=== Experiment 1133 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007006321347168762
rmse: 0.08370377140349629
mae: 0.030041110371572203
r2: 0.6840880541740098
pearson: 0.846265777819867

=== Experiment 1003 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006648798143487943
rmse: 0.0815401627634379
mae: 0.032215150861923426
r2: 0.7002086180699822
pearson: 0.8386380101035701

=== Experiment 1196 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006224200979840152
rmse: 0.07889360544328135
mae: 0.0302453019394946
r2: 0.7193535172993579
pearson: 0.8483130371049155

=== Experiment 1316 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006019526161261879
rmse: 0.07758560537407617
mae: 0.029047376192570105
r2: 0.7285822147847754
pearson: 0.8632637050988572

=== Experiment 1346 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011341806267502764
rmse: 0.10649791672846359
mae: 0.042835706853433936
r2: 0.48860294730899034
pearson: 0.7639298530750018

=== Experiment 1146 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006718589244780339
rmse: 0.08196700095026253
mae: 0.03097557272928316
r2: 0.6970617680301208
pearson: 0.8350463693059826

=== Experiment 1413 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006221244970316764
rmse: 0.07887486906687557
mae: 0.02986927568515507
r2: 0.7194868024677282
pearson: 0.8564766442653717

=== Experiment 1480 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00636176987720527
rmse: 0.07976070384095961
mae: 0.032109073935342694
r2: 0.7131505962658669
pearson: 0.8509837994159415

=== Experiment 1477 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0057734783994674915
rmse: 0.07598340871182005
mae: 0.03101830807619075
r2: 0.7396764000701828
pearson: 0.8609532617292518

=== Experiment 1198 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006709588340975652
rmse: 0.08191207689331075
mae: 0.03276393232622481
r2: 0.6974676148210734
pearson: 0.8441323647184038

=== Experiment 1038 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006382135417109931
rmse: 0.07988826833215207
mae: 0.03274667412648068
r2: 0.7122323230351252
pearson: 0.8448978281465759

=== Experiment 1077 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006599001345795695
rmse: 0.08123423752209222
mae: 0.03801099770395232
r2: 0.7024539337606788
pearson: 0.8464411233975562

=== Experiment 1325 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006246904802700478
rmse: 0.07903736333342908
mae: 0.032737870223784736
r2: 0.7183298119193025
pearson: 0.8522024809434441

=== Experiment 1282 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00724052061649078
rmse: 0.08509124876560914
mae: 0.04147730801395808
r2: 0.6735281121992613
pearson: 0.8300546995606821

=== Experiment 1442 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007394798125979637
rmse: 0.08599301207644512
mae: 0.03414838263163048
r2: 0.6665718071991334
pearson: 0.8319272914390011

=== Experiment 1189 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007291782523182984
rmse: 0.08539193476659833
mae: 0.04419412861612156
r2: 0.6712167353886003
pearson: 0.833265971452891

=== Experiment 1136 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006996678258044964
rmse: 0.08364614909274044
mae: 0.038825278272114724
r2: 0.6845228568183538
pearson: 0.8314098625723764

=== Experiment 1405 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006420580975960092
rmse: 0.08012852785344363
mae: 0.030988740894595773
r2: 0.710498829707756
pearson: 0.8442652748595146

=== Experiment 1071 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005793289617894854
rmse: 0.07611366249166344
mae: 0.03425068181144331
r2: 0.738783120950877
pearson: 0.8608209375430865

=== Experiment 1202 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006631417696189276
rmse: 0.0814335170319278
mae: 0.03354046359402567
r2: 0.7009922947889593
pearson: 0.8386155929190104

=== Experiment 1309 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007355343005407834
rmse: 0.08576329637675918
mae: 0.04295417408758129
r2: 0.6683508212201895
pearson: 0.8388426013955951

=== Experiment 1340 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006753104507275723
rmse: 0.0821772748834842
mae: 0.03451390689334228
r2: 0.6955054900355326
pearson: 0.8361031557351416

=== Experiment 1358 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006051499058211856
rmse: 0.07779138164483168
mae: 0.029327124330641066
r2: 0.7271405709336491
pearson: 0.8605834274331414

=== Experiment 1205 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006044638711156668
rmse: 0.07774727462205133
mae: 0.030215080852125657
r2: 0.7274499009628979
pearson: 0.8545984922769757

=== Experiment 1497 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006077563876298556
rmse: 0.07795873187974876
mae: 0.038692374994810284
r2: 0.7259653197581236
pearson: 0.85606074101361

=== Experiment 1403 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006794030637006104
rmse: 0.08242590998591465
mae: 0.03564771335712496
r2: 0.6936601488589571
pearson: 0.8420326491615909

=== Experiment 1000 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005949527823628884
rmse: 0.07713318237716427
mae: 0.02836750415913945
r2: 0.7317384090200227
pearson: 0.8564301231236078

=== Experiment 1377 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006793003584045064
rmse: 0.08241967959198254
mae: 0.03748148255127784
r2: 0.6937064582249297
pearson: 0.8332936319035606

=== Experiment 1375 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006767683667063864
rmse: 0.08226593260313692
mae: 0.03793573666257725
r2: 0.6948481221374644
pearson: 0.8441251192595238

=== Experiment 1455 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007482361362124328
rmse: 0.08650064370930616
mae: 0.03253868678378378
r2: 0.6626236194208969
pearson: 0.8189296775905932

=== Experiment 1188 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007105653056549824
rmse: 0.08429503577643124
mae: 0.03710437287427593
r2: 0.6796092311172464
pearson: 0.8275957236228221

=== Experiment 1061 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006254561879127013
rmse: 0.0790857880982861
mae: 0.03211786102804151
r2: 0.717984557713367
pearson: 0.8493868964007397

=== Experiment 1138 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006453370459529768
rmse: 0.08033287284499271
mae: 0.036326302216755965
r2: 0.7090203663253547
pearson: 0.8448654455534907

=== Experiment 1371 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005931749580829556
rmse: 0.07701785235144873
mae: 0.030731484431989704
r2: 0.7325400221630414
pearson: 0.8575932467101935

=== Experiment 1300 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0059069784573673265
rmse: 0.07685686994255833
mae: 0.029782551864500614
r2: 0.7336569412173481
pearson: 0.8617731698691539

=== Experiment 1008 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009798853593583425
rmse: 0.09898915896997725
mae: 0.036667243010077455
r2: 0.5581740042706076
pearson: 0.7601576386875724

=== Experiment 1157 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007387387715705593
rmse: 0.08594991399475391
mae: 0.033537670419557165
r2: 0.6669059393368191
pearson: 0.8221932992134333

=== Experiment 1177 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006404251549826196
rmse: 0.08002656777487209
mae: 0.030942542498825318
r2: 0.7112351163450032
pearson: 0.8565066923102266

=== Experiment 1454 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006098192621108864
rmse: 0.07809092534417084
mae: 0.031027578190292178
r2: 0.7250351787340317
pearson: 0.8544777305103479

=== Experiment 1280 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006136888756824942
rmse: 0.07833829687212342
mae: 0.03005088652473784
r2: 0.7232903870060655
pearson: 0.8556002764715723

=== Experiment 1069 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006316237311085207
rmse: 0.07947475895581695
mae: 0.030721313336180996
r2: 0.7152036396318056
pearson: 0.8598437288825915

=== Experiment 1070 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006431031360900671
rmse: 0.08019371147977047
mae: 0.030877687296527047
r2: 0.7100276264503516
pearson: 0.8508613542265199

=== Experiment 1447 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006577517909390088
rmse: 0.08110189830941128
mae: 0.03203231858542961
r2: 0.7034226124526243
pearson: 0.8388810608668472

=== Experiment 1262 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006940496180729373
rmse: 0.0833096403829075
mae: 0.03139757261900992
r2: 0.6870560819568904
pearson: 0.8524074293224806

=== Experiment 1496 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010498819839360051
rmse: 0.1024637489035027
mae: 0.04677567876867875
r2: 0.5266128343272442
pearson: 0.7259646415570837

=== Experiment 1396 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00795204294857709
rmse: 0.08917422805147847
mae: 0.033413681837926076
r2: 0.6414458834104169
pearson: 0.8305872461702589

=== Experiment 1100 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067578441796119415
rmse: 0.0822061079215647
mae: 0.031924240605666815
r2: 0.6952917802959819
pearson: 0.8504408356669749

=== Experiment 1239 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062341067568786315
rmse: 0.07895635982540375
mae: 0.0292146553288935
r2: 0.7189068701725588
pearson: 0.8530494535044897

=== Experiment 1432 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006413702858679027
rmse: 0.08008559707387482
mae: 0.032309879264840476
r2: 0.7108089609886683
pearson: 0.8486068932678713

=== Experiment 1400 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006074773408690536
rmse: 0.07794083274311697
mae: 0.030282374911439345
r2: 0.7260911407143904
pearson: 0.862229378899113

=== Experiment 1010 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00602772602691672
rmse: 0.07763843137851717
mae: 0.028954162122600543
r2: 0.7282124864514355
pearson: 0.8542526555262802

=== Experiment 1209 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006088066152229266
rmse: 0.07802606072479416
mae: 0.03194575209475866
r2: 0.7254917767588129
pearson: 0.8524850479988171

=== Experiment 1191 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006289087022843235
rmse: 0.07930376424132234
mae: 0.03045413011276038
r2: 0.7164278341788805
pearson: 0.8501925571160426

=== Experiment 1031 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005367406068965085
rmse: 0.0732625830077338
mae: 0.028190453309657117
r2: 0.7579860227264357
pearson: 0.8814801117805894

=== Experiment 1259 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005884142451658548
rmse: 0.07670816417864887
mae: 0.028803328450057047
r2: 0.7346866066638622
pearson: 0.8596532447392274

=== Experiment 1015 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008598546107760565
rmse: 0.09272834576201909
mae: 0.0380167854586637
r2: 0.6122953405106348
pearson: 0.7894320084931487

=== Experiment 1256 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006934936695538014
rmse: 0.08327626730070227
mae: 0.0323776002828865
r2: 0.6873067566972522
pearson: 0.8426429828887335

=== Experiment 1112 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006049754435596885
rmse: 0.07778016736673228
mae: 0.035027486539178365
r2: 0.727219235199491
pearson: 0.856993908348904

=== Experiment 1185 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0059368414364086635
rmse: 0.07705090159374298
mae: 0.03187747235657212
r2: 0.7323104326360764
pearson: 0.8591383087524306

=== Experiment 1092 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005875677824776475
rmse: 0.07665297009755379
mae: 0.031152799756640405
r2: 0.7350682729644078
pearson: 0.8577901378016254

=== Experiment 1472 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006485554415639049
rmse: 0.08053293994161054
mae: 0.038914461688033704
r2: 0.7075692059096296
pearson: 0.8444926373596126

=== Experiment 1212 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006559211097564781
rmse: 0.08098895663956154
mae: 0.030472937699762517
r2: 0.7042480585403833
pearson: 0.8408366612821503

=== Experiment 1056 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008968139265447515
rmse: 0.09470026011288203
mae: 0.03371696514584444
r2: 0.5956305477009189
pearson: 0.8151655189105034

=== Experiment 1218 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005447030178270418
rmse: 0.07380399838945324
mae: 0.027594025990737445
r2: 0.7543958066831088
pearson: 0.8700821719739201

=== Experiment 1162 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005850652900536401
rmse: 0.07648956072913742
mae: 0.02840393019073467
r2: 0.7361966357840812
pearson: 0.8617789626905689

=== Experiment 1261 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005717298351942565
rmse: 0.07561281870121339
mae: 0.028439092852843406
r2: 0.7422095336863519
pearson: 0.8621976119950912

=== Experiment 1278 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005882524920615954
rmse: 0.07669762004531792
mae: 0.033587349218174016
r2: 0.7347595404266429
pearson: 0.8577618598984555

=== Experiment 1007 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0067169096893489625
rmse: 0.08195675499523492
mae: 0.03192556034172889
r2: 0.6971374984452927
pearson: 0.8362797215976553

=== Experiment 1344 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005722706423406617
rmse: 0.07564857185305363
mae: 0.028531293776852946
r2: 0.741965686124312
pearson: 0.8620216036237249

=== Experiment 1153 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005821031967151374
rmse: 0.07629568773627625
mae: 0.02890730930049886
r2: 0.7375322306332421
pearson: 0.8617612621409939

=== Experiment 1108 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005804804499840757
rmse: 0.07618926761585752
mae: 0.03187097940191692
r2: 0.7382639199920235
pearson: 0.8683486296491847

=== Experiment 1264 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0052761992212358895
rmse: 0.07263745054196141
mae: 0.027345384182091852
r2: 0.7620984993473415
pearson: 0.8796011731882792

=== Experiment 1095 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0062478322398535
rmse: 0.07904323019622553
mae: 0.031033915948902507
r2: 0.718287994186269
pearson: 0.84858814418248

=== Experiment 1463 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005747814496529706
rmse: 0.07581434228778686
mae: 0.029678376948913326
r2: 0.7408335741581
pearson: 0.8609546491004216

=== Experiment 1504 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006660799580956068
rmse: 0.08161372176880594
mae: 0.03507318898700197
r2: 0.699667478536781
pearson: 0.8368340390800065

=== Experiment 1501 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007076110265888229
rmse: 0.08411961879305106
mae: 0.03385201716851136
r2: 0.6809413025454012
pearson: 0.8321393424556056

=== Experiment 1506 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006420115182034872
rmse: 0.08012562125833954
mae: 0.038928333132197325
r2: 0.7105198321508326
pearson: 0.8465900677315197

=== Experiment 1508 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065579264106327756
rmse: 0.08098102500359436
mae: 0.037597842999303084
r2: 0.7043059845087133
pearson: 0.8415882050379097

=== Experiment 1502 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005863084703523551
rmse: 0.07657078230972668
mae: 0.02971777949821942
r2: 0.7356360912590455
pearson: 0.8578844794749655

=== Experiment 1576 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0068323464366037935
rmse: 0.08265800890781119
mae: 0.03965587103236331
r2: 0.6919325063191704
pearson: 0.8338794044141818

=== Experiment 1515 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006705565198414965
rmse: 0.08188751552230025
mae: 0.03539315590723829
r2: 0.6976490165484144
pearson: 0.8363014224137808

=== Experiment 1517 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067949579636341845
rmse: 0.08243153500714508
mae: 0.04181602817604442
r2: 0.6936183361094446
pearson: 0.8364280955438714

=== Experiment 1588 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006371900075469126
rmse: 0.07982418227247384
mae: 0.031231384197209444
r2: 0.7126938300847909
pearson: 0.8447519789448733

=== Experiment 1539 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006208786750112508
rmse: 0.07879585490438255
mae: 0.02794431868301547
r2: 0.7200485381334559
pearson: 0.8492435700978994

=== Experiment 1536 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0077028495965685
rmse: 0.08776587945533561
mae: 0.03657728696553263
r2: 0.6526819019741025
pearson: 0.8150739427738002

=== Experiment 1531 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007646928665310909
rmse: 0.08744671900826759
mae: 0.04724535816232886
r2: 0.655203352151823
pearson: 0.8299536353762826

=== Experiment 1585 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007421365849770357
rmse: 0.08614734963868799
mae: 0.03900846940176735
r2: 0.6653738802267593
pearson: 0.8235349363948976

=== Experiment 1568 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007932764117647482
rmse: 0.08906606602768241
mae: 0.033894774760198844
r2: 0.6423151574117706
pearson: 0.8187900886000222

=== Experiment 1609 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006031197197299115
rmse: 0.07766078287848452
mae: 0.03599444486746193
r2: 0.7280559729066723
pearson: 0.8538649119142235

=== Experiment 1524 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006724470343630474
rmse: 0.08200286789881482
mae: 0.042832143656761586
r2: 0.6967965918714378
pearson: 0.842177566132979

=== Experiment 1500 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006781434382902148
rmse: 0.0823494649824888
mae: 0.038351857074648325
r2: 0.6942281084124688
pearson: 0.8408375788168728

=== Experiment 1590 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0058662592472039565
rmse: 0.07659150897589077
mae: 0.030946050904168625
r2: 0.7354929524817198
pearson: 0.8586290247344963

=== Experiment 1624 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006939459174167784
rmse: 0.08330341634151497
mae: 0.04293477987222653
r2: 0.6871028401263309
pearson: 0.8337142336347098

=== Experiment 1520 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006131321839904663
rmse: 0.07830275754981214
mae: 0.03155271853692345
r2: 0.7235413968398134
pearson: 0.8512007290779579

=== Experiment 1527 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006459669223946511
rmse: 0.0803720674360596
mae: 0.03447036206247143
r2: 0.708736357810102
pearson: 0.8420925796421729

=== Experiment 1534 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00693955506426098
rmse: 0.0833039918867096
mae: 0.0442671670042524
r2: 0.6870985164842334
pearson: 0.8355559612871064

=== Experiment 1607 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006602362437810893
rmse: 0.08125492254510426
mae: 0.03807089103875092
r2: 0.7023023836010434
pearson: 0.8408010667429975

=== Experiment 1618 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005749831232188314
rmse: 0.07582764161035416
mae: 0.030325942225127668
r2: 0.7407426404348867
pearson: 0.8617566178113104

=== Experiment 1505 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0055694807889483154
rmse: 0.07462895409255255
mae: 0.02852244111722988
r2: 0.7488745625422777
pearson: 0.8654114660610863

=== Experiment 1606 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0068099466921928515
rmse: 0.08252240115382521
mae: 0.040386486583012124
r2: 0.6929425009357781
pearson: 0.8346623620437547

=== Experiment 1652 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006284495541132981
rmse: 0.07927481025605158
mae: 0.03311448666105129
r2: 0.7166348620683302
pearson: 0.8485575823944399

=== Experiment 1542 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006684332888205168
rmse: 0.08175776958922722
mae: 0.029519824839677546
r2: 0.6986063720707197
pearson: 0.8546973281609362

=== Experiment 1565 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007261096524641576
rmse: 0.0852120679519138
mae: 0.029590746802556474
r2: 0.6726003535568916
pearson: 0.8207471892404474

=== Experiment 1705 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005884872118699762
rmse: 0.07671292015495018
mae: 0.03218749690090783
r2: 0.7346537062981249
pearson: 0.8619692943796194

=== Experiment 1545 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006793487259028347
rmse: 0.08242261375998912
mae: 0.036936555565280894
r2: 0.6936846495328151
pearson: 0.8339881261979073

=== Experiment 1593 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006165378544443538
rmse: 0.07851992450609933
mae: 0.029767476242249388
r2: 0.7220057950216576
pearson: 0.8512829120931784

=== Experiment 1617 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00716284097813907
rmse: 0.08463356886093762
mae: 0.043683175535989306
r2: 0.6770306529031165
pearson: 0.8309113998438344

=== Experiment 1594 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062423999699127986
rmse: 0.07900886007222734
mae: 0.03179232653733142
r2: 0.7185329328469704
pearson: 0.8476890900537134

=== Experiment 1584 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006055541496853049
rmse: 0.07781735986817498
mae: 0.03146874370905878
r2: 0.726958299154531
pearson: 0.8559698284488764

=== Experiment 1713 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006626930655253138
rmse: 0.0814059620375138
mae: 0.03526278998253233
r2: 0.7011946134898719
pearson: 0.8374410622866817

=== Experiment 1686 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008311630570674279
rmse: 0.09116814449507174
mae: 0.03957630868461259
r2: 0.6252322357966704
pearson: 0.8069090399633901

=== Experiment 1717 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006759980355326507
rmse: 0.08221909969907544
mae: 0.039020458363511853
r2: 0.6951954610732151
pearson: 0.8401300680206866

=== Experiment 1580 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006694874029190935
rmse: 0.08182220987721449
mae: 0.03530299768900437
r2: 0.6981310766631823
pearson: 0.8379716096843836

=== Experiment 1644 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059750631239194165
rmse: 0.07729853248231441
mae: 0.0291810267412291
r2: 0.7305870335688679
pearson: 0.8598226788295152

=== Experiment 1532 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0055974980146630115
rmse: 0.07481642877512272
mae: 0.028507795132256188
r2: 0.7476112781661262
pearson: 0.8652189541250481

=== Experiment 1667 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007861775768991318
rmse: 0.08866665533892275
mae: 0.037527952687302
r2: 0.6455159908083155
pearson: 0.8106780787471984

=== Experiment 1662 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00606786722756868
rmse: 0.07789651614525954
mae: 0.031309106277344435
r2: 0.7264025373815322
pearson: 0.854356518431175

=== Experiment 1511 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007065041766346465
rmse: 0.08405380280716908
mae: 0.03581494304555246
r2: 0.68144037631812
pearson: 0.8347632902650803

=== Experiment 1589 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006060545516102286
rmse: 0.07784950556106497
mae: 0.03394635256444418
r2: 0.7267326701290203
pearson: 0.853017707236289

=== Experiment 1674 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005788796394876645
rmse: 0.07608414023222346
mae: 0.032401331675264045
r2: 0.7389857183991495
pearson: 0.8608883329735426

=== Experiment 1620 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006465335581320491
rmse: 0.0804073104967483
mae: 0.03835957776627564
r2: 0.7084808642500637
pearson: 0.8442317876195462

=== Experiment 1521 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006971054313304579
rmse: 0.08349283989243976
mae: 0.03063862557898983
r2: 0.6856782292087371
pearson: 0.8329697198972514

=== Experiment 1544 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006384327592486461
rmse: 0.07990198741261985
mae: 0.034159596588139586
r2: 0.7121334788122478
pearson: 0.843941414307052

=== Experiment 1598 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007064579687943156
rmse: 0.08405105405611019
mae: 0.0353150412147824
r2: 0.681461211229949
pearson: 0.8259093861147582

=== Experiment 1663 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006775091215730239
rmse: 0.0823109422600072
mae: 0.030761749463981396
r2: 0.694514119028409
pearson: 0.8462999818155262

=== Experiment 1553 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062207544457967496
rmse: 0.07887175949474406
mae: 0.03712274057484679
r2: 0.7195089200024392
pearson: 0.8497384646401417

=== Experiment 1635 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005978388156092414
rmse: 0.07732003722252347
mae: 0.03431899522613042
r2: 0.730437109331646
pearson: 0.8548059100623515

=== Experiment 1790 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006106251378796684
rmse: 0.07814250686276122
mae: 0.032768354301706173
r2: 0.7246718128968184
pearson: 0.8546293471392065

=== Experiment 1633 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005862576434852785
rmse: 0.07656746329122302
mae: 0.03208718901819537
r2: 0.7356590088697041
pearson: 0.8597101947274686

=== Experiment 1619 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007653454108386588
rmse: 0.08748402201766096
mae: 0.030377997263241235
r2: 0.6549091227956079
pearson: 0.8194053598269784

=== Experiment 1605 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006850235602019441
rmse: 0.08276615009784762
mae: 0.030953252002433967
r2: 0.6911258917242025
pearson: 0.8329070019257291

=== Experiment 1575 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006589580842122782
rmse: 0.08117623323438199
mae: 0.03580884008819405
r2: 0.7028787001250097
pearson: 0.8394813510928674

=== Experiment 1712 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006372787513117101
rmse: 0.07982974078072094
mae: 0.035308171681253185
r2: 0.7126538159118349
pearson: 0.8453678918246742

=== Experiment 1562 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005880568971872513
rmse: 0.07668486794585039
mae: 0.029651847622929058
r2: 0.7348477332946057
pearson: 0.8614437023274097

=== Experiment 1676 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006516507334929963
rmse: 0.0807248867136397
mae: 0.0378257916602949
r2: 0.7061735524022239
pearson: 0.8484904126877236

=== Experiment 1622 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007633175044153534
rmse: 0.08736804360951167
mae: 0.03996427893753784
r2: 0.6558234968763776
pearson: 0.8239070558603755

=== Experiment 1561 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007989451708526002
rmse: 0.08938373290776125
mae: 0.05273751043982956
r2: 0.639759138889175
pearson: 0.8296202890985762

=== Experiment 1807 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006903860188268767
rmse: 0.0830894709832044
mae: 0.04434038921791669
r2: 0.6887079827322058
pearson: 0.8356123331597817

=== Experiment 1695 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0056698375850259275
rmse: 0.07529832391910146
mae: 0.029611047947220984
r2: 0.7443495187775421
pearson: 0.863561755767194

=== Experiment 1514 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006448512277000444
rmse: 0.08030262932806399
mae: 0.028751551363644955
r2: 0.7092394196373524
pearson: 0.8467810352585866

=== Experiment 1718 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007303824541087984
rmse: 0.08546241595630201
mae: 0.04588490807859655
r2: 0.6706737661013613
pearson: 0.8250663098913423

=== Experiment 1806 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007259218329475128
rmse: 0.0852010465280511
mae: 0.03462276807016059
r2: 0.6726850405502898
pearson: 0.8248001265634772

=== Experiment 1538 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005585742639025555
rmse: 0.07473782602555118
mae: 0.02757281653653726
r2: 0.7481413228796825
pearson: 0.8677214998206831

=== Experiment 1689 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005757126197789892
rmse: 0.07587572864750554
mae: 0.029367666497695306
r2: 0.7404137136466713
pearson: 0.8632240205857917

=== Experiment 1739 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007301371076001312
rmse: 0.0854480606918689
mae: 0.0394444159267285
r2: 0.6707843917622668
pearson: 0.8429542040013682

=== Experiment 1645 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00674673132452944
rmse: 0.0821384886915351
mae: 0.040605987440654426
r2: 0.6957928540405104
pearson: 0.8396801815794808

=== Experiment 1836 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007114349923067034
rmse: 0.08434660587757538
mae: 0.035896936070874866
r2: 0.6792170932337691
pearson: 0.8311577427131408

=== Experiment 1757 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00756929470201051
rmse: 0.08700169367322978
mae: 0.033883406754929964
r2: 0.6587038333877178
pearson: 0.8243700449456319

=== Experiment 1776 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006816959088959244
rmse: 0.08256487805937367
mae: 0.03965536485536075
r2: 0.6926263150519734
pearson: 0.8345223944381477

=== Experiment 1551 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006498125546494578
rmse: 0.08061095177762496
mae: 0.03532252515714816
r2: 0.7070023791523314
pearson: 0.8411204096074122

=== Experiment 1785 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006014963141143139
rmse: 0.0775561934415501
mae: 0.0308766472590859
r2: 0.7287879593535906
pearson: 0.8560924758508297

=== Experiment 1754 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070351185007674856
rmse: 0.08387561326611857
mae: 0.042646459135586776
r2: 0.6827896031928398
pearson: 0.8404753256997015

=== Experiment 1678 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007980102328784138
rmse: 0.08933141848635416
mae: 0.052232726110174335
r2: 0.6401806983067586
pearson: 0.8151151763716119

=== Experiment 1543 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006702312785751901
rmse: 0.08186765408726392
mae: 0.032931319172043436
r2: 0.6977956664038956
pearson: 0.8366289416063463

=== Experiment 1736 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007418846015002061
rmse: 0.08613272325314034
mae: 0.04596298645470734
r2: 0.6654874984673977
pearson: 0.82095474084053

=== Experiment 1711 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007130046073886811
rmse: 0.08443960015233853
mae: 0.045155388455157
r2: 0.6785093607017095
pearson: 0.832393085982339

=== Experiment 1621 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0071950086530069264
rmse: 0.08482339684902349
mae: 0.04027111518255606
r2: 0.6755802266014013
pearson: 0.8235557197650504

=== Experiment 1602 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007044184312335733
rmse: 0.08392963905758044
mae: 0.032859018680638064
r2: 0.6823808297393965
pearson: 0.8362621380300375

=== Experiment 1835 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005681454933009337
rmse: 0.07537542658591947
mae: 0.028169507595764046
r2: 0.7438256976701559
pearson: 0.8627328153841269

=== Experiment 1643 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0062584266711562635
rmse: 0.07911021850024347
mae: 0.03480673265095524
r2: 0.7178102959417931
pearson: 0.8529742671757065

=== Experiment 1803 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0076745492409577626
rmse: 0.08760450468416428
mae: 0.05348975006696999
r2: 0.6539579525526517
pearson: 0.8302100547926421

=== Experiment 1537 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006186146496829328
rmse: 0.07865205971129635
mae: 0.02886083001799828
r2: 0.7210693771892571
pearson: 0.8504376163018117

=== Experiment 1737 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006941862820013834
rmse: 0.08331784214688852
mae: 0.03524238762969947
r2: 0.686994460793058
pearson: 0.8319661318864348

=== Experiment 1779 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006656121604510527
rmse: 0.08158505748303746
mae: 0.03389057487635578
r2: 0.6998784064357756
pearson: 0.8427076492656145

=== Experiment 1777 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007594280030500902
rmse: 0.08714516642075396
mae: 0.03650949389787813
r2: 0.6575772559229701
pearson: 0.8263377626080476

=== Experiment 1522 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067649250190373105
rmse: 0.08224916424522082
mae: 0.03577497621162339
r2: 0.6949725083628078
pearson: 0.8340208146663203

=== Experiment 1824 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006593764592564255
rmse: 0.08120199869808781
mae: 0.030652436943546454
r2: 0.7026900566590132
pearson: 0.8614181752518086

=== Experiment 1647 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00573080846420824
rmse: 0.07570210343318236
mae: 0.028527388783894752
r2: 0.7416003686705477
pearson: 0.8613025336087973

=== Experiment 1893 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0070632039076051105
rmse: 0.08404286946317999
mae: 0.04912749234023525
r2: 0.6815232445598642
pearson: 0.8402565289672737

=== Experiment 1762 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007729157410635613
rmse: 0.08791562665781102
mae: 0.04244198535754409
r2: 0.6514956942167708
pearson: 0.8350099160847824

=== Experiment 1613 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005919581982138969
rmse: 0.07693881973450703
mae: 0.028687631027932686
r2: 0.7330886538326302
pearson: 0.8605754715809787

=== Experiment 1694 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0073771273047055635
rmse: 0.08589020494040961
mae: 0.04484698517338803
r2: 0.6673685767528582
pearson: 0.831855866404057

=== Experiment 1771 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059366358809019686
rmse: 0.07704956768796285
mae: 0.031680954823966725
r2: 0.7323197010434014
pearson: 0.8563652053299831

=== Experiment 1604 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0065824118258745925
rmse: 0.08113206410461028
mae: 0.04067054135789978
r2: 0.7032019479123154
pearson: 0.8422000900571203

=== Experiment 1550 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007426484232661288
rmse: 0.08617705165913538
mae: 0.04706178711603612
r2: 0.6651430945949798
pearson: 0.8301086489962423

=== Experiment 1548 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00653065027479576
rmse: 0.08081243886182225
mae: 0.036820998549122276
r2: 0.705535853468459
pearson: 0.841842091691896

=== Experiment 1603 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006328703657953476
rmse: 0.07955314989334789
mae: 0.029326963455787888
r2: 0.7146415375383741
pearson: 0.8536505071792074

=== Experiment 1540 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006031150245757603
rmse: 0.07766048059185317
mae: 0.029852464178871965
r2: 0.728058089931014
pearson: 0.8633980506600071

=== Experiment 1503 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006066409765239
rmse: 0.07788716046460417
mae: 0.02708014556160066
r2: 0.7264682537164993
pearson: 0.8528172412782338

=== Experiment 1648 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006309198277733111
rmse: 0.07943046190053984
mae: 0.03180323097913596
r2: 0.715521026547536
pearson: 0.8472881830630236

=== Experiment 1684 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007361101443955194
rmse: 0.0857968615041086
mae: 0.032494100235419376
r2: 0.6680911757605716
pearson: 0.8429584727712547

=== Experiment 1847 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006680356300274301
rmse: 0.08173344664379632
mae: 0.03349656699441509
r2: 0.6987856746702923
pearson: 0.8443767123450293

=== Experiment 1554 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00535890188989247
rmse: 0.0732045209662113
mae: 0.029677208850746314
r2: 0.7583694724178445
pearson: 0.8793032312194788

=== Experiment 1825 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007320289378222038
rmse: 0.08555868967102077
mae: 0.04694440950814939
r2: 0.6699313738417159
pearson: 0.827493019536315

=== Experiment 1656 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006876601552850369
rmse: 0.08292527692356758
mae: 0.031248543241663217
r2: 0.6899370626057784
pearson: 0.8442567810149403

=== Experiment 1822 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006092662558931344
rmse: 0.0780555094719863
mae: 0.03216301710209415
r2: 0.7252845268036495
pearson: 0.8521732514893215

=== Experiment 1808 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061803723332647525
rmse: 0.07861534413372973
mae: 0.03657637421864799
r2: 0.7213297316829801
pearson: 0.8511214388848676

=== Experiment 1581 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006895603822953722
rmse: 0.08303977253674122
mae: 0.04201054058189
r2: 0.6890802586103569
pearson: 0.8396680673670099

=== Experiment 1828 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006129129253319033
rmse: 0.07828875559950504
mae: 0.03032727424327068
r2: 0.7236402596039284
pearson: 0.8538525216643295

=== Experiment 1699 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005902031721940586
rmse: 0.07682468172365302
mae: 0.030967791286889198
r2: 0.7338799873405152
pearson: 0.8579709749459392

=== Experiment 1891 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0072534488537834355
rmse: 0.08516718178842972
mae: 0.04088092938505622
r2: 0.6729451836698879
pearson: 0.8222250814490409

=== Experiment 1523 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005858201813659127
rmse: 0.07653889085725718
mae: 0.035056190118788175
r2: 0.7358562586139081
pearson: 0.8578405573209649

=== Experiment 1690 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006624312800507953
rmse: 0.08138988143809986
mae: 0.028065013290980496
r2: 0.7013126514081263
pearson: 0.8378354561752325

=== Experiment 1731 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007391937384437068
rmse: 0.0859763768975936
mae: 0.04971644036215579
r2: 0.6667007967761792
pearson: 0.8294406456259942

=== Experiment 1932 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006688469621547356
rmse: 0.08178306439323092
mae: 0.0338648082295538
r2: 0.698419848585036
pearson: 0.8361677226014869

=== Experiment 1765 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006325537322303562
rmse: 0.07953324664757225
mae: 0.03228184576574612
r2: 0.7147843062192178
pearson: 0.8489290650137918

=== Experiment 1792 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008917175985752153
rmse: 0.09443079998470919
mae: 0.05471939148283626
r2: 0.5979284595517281
pearson: 0.8013597827099498

=== Experiment 1743 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006568370985121137
rmse: 0.0810454871360592
mae: 0.03101392006947323
r2: 0.703835043242042
pearson: 0.8457541197695482

=== Experiment 1608 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007055321732995942
rmse: 0.08399596259937701
mae: 0.04367634858694377
r2: 0.6818786483437813
pearson: 0.8317947401228224

=== Experiment 1507 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005632318748639735
rmse: 0.0750487757970757
mae: 0.029673248523530985
r2: 0.7460412265968926
pearson: 0.8638675481274423

=== Experiment 1815 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006607489910756597
rmse: 0.08128646818970915
mae: 0.03526549125326602
r2: 0.702071188102695
pearson: 0.838842690664208

=== Experiment 1535 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0069575089110997445
rmse: 0.08341168330096058
mae: 0.03268447403641513
r2: 0.6862889854323656
pearson: 0.8468821687037293

=== Experiment 1751 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00598569622876793
rmse: 0.0773672813841092
mae: 0.027146681075235532
r2: 0.7301075915512357
pearson: 0.8547977158704226

=== Experiment 1947 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006530473417148995
rmse: 0.0808113446067382
mae: 0.041599943448392886
r2: 0.7055438279019106
pearson: 0.843168535862194

=== Experiment 1782 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006497844868487037
rmse: 0.08060921081667427
mae: 0.040477964453423894
r2: 0.7070150348001558
pearson: 0.8464584369212695

=== Experiment 1708 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008389339513766894
rmse: 0.09159333771496099
mae: 0.03519379856945761
r2: 0.6217283737549458
pearson: 0.8258315331648214

=== Experiment 1587 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006347061056948023
rmse: 0.07966844454957071
mae: 0.02809204991212935
r2: 0.7138138104974194
pearson: 0.858353831908548

=== Experiment 1668 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006501916244469189
rmse: 0.08063446065094743
mae: 0.03081155685243642
r2: 0.7068314582490702
pearson: 0.8503798920052047

=== Experiment 1964 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007005869925863632
rmse: 0.08370107481904657
mae: 0.041301290535322874
r2: 0.6841084085619724
pearson: 0.8372061872078452

=== Experiment 1839 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007666798586368082
rmse: 0.0875602568884313
mae: 0.03638544051742183
r2: 0.6543074261567763
pearson: 0.8146916253792725

=== Experiment 1843 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006644039889979668
rmse: 0.08151098018046199
mae: 0.03357704126199756
r2: 0.7004231656264028
pearson: 0.8454881853787931

=== Experiment 1882 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006854473700428468
rmse: 0.08279174898761632
mae: 0.032118884680721875
r2: 0.6909347977322693
pearson: 0.8386328934603515

=== Experiment 1978 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006454228418993227
rmse: 0.08033821269478944
mae: 0.0367000980599691
r2: 0.7089816813107641
pearson: 0.8425611356859845

=== Experiment 1861 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057721456688664
rmse: 0.0759746383266574
mae: 0.03346890022723186
r2: 0.7397364923064064
pearson: 0.8641191654998739

=== Experiment 1823 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006407782505794139
rmse: 0.08004862588323512
mae: 0.02970038436432652
r2: 0.7110759070945014
pearson: 0.8435636477402683

=== Experiment 1610 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007165189335554399
rmse: 0.08464744139992891
mae: 0.045562036427834296
r2: 0.6769247665008504
pearson: 0.8269607065661562

=== Experiment 1856 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006323710926059039
rmse: 0.07952176385153337
mae: 0.03367554357704129
r2: 0.714866657622025
pearson: 0.8456100760602343

=== Experiment 1963 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066320162762666494
rmse: 0.08143719221747917
mae: 0.033485122436768475
r2: 0.7009653050767257
pearson: 0.8376754126112804

=== Experiment 1938 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0074327136598289255
rmse: 0.0862131872733454
mae: 0.04488477616204878
r2: 0.6648622124657717
pearson: 0.8292800894476497

=== Experiment 1939 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006764268887858209
rmse: 0.08224517546858423
mae: 0.03365787356222017
r2: 0.695002093028888
pearson: 0.8392878476111048

=== Experiment 1853 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005746199386698376
rmse: 0.07580368979606715
mae: 0.02669223432292127
r2: 0.7409063987495303
pearson: 0.8616152807396935

=== Experiment 1949 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005294903157876102
rmse: 0.07276608521746997
mae: 0.028219627657611247
r2: 0.7612551470764665
pearson: 0.8731068582043043

=== Experiment 1573 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005458799159167459
rmse: 0.07388368669176883
mae: 0.028063918519635314
r2: 0.7538651485143859
pearson: 0.8687327336023647

=== Experiment 1559 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006099630516530252
rmse: 0.07810013134771447
mae: 0.030268274902563248
r2: 0.7249703446623477
pearson: 0.8528505878555951

=== Experiment 1837 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006068976565947453
rmse: 0.07790363641029507
mae: 0.032594104294416594
r2: 0.7263525178022906
pearson: 0.8529650092935155

=== Experiment 1909 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006761501343634728
rmse: 0.0822283487833407
mae: 0.03814881339055577
r2: 0.6951268803798354
pearson: 0.834988908340561

=== Experiment 1924 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007628659966588653
rmse: 0.08734220037638538
mae: 0.046291466196054884
r2: 0.6560270797365404
pearson: 0.818088991752391

=== Experiment 1851 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00694752478885717
rmse: 0.08335181335074343
mae: 0.03425955882827858
r2: 0.686739165109935
pearson: 0.8522912293586621

=== Experiment 1709 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006037676066846364
rmse: 0.07770248430292537
mae: 0.029780191388579964
r2: 0.7277638435303555
pearson: 0.853277189161143

=== Experiment 1786 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005448748612792726
rmse: 0.07381563935097173
mae: 0.02909563854984106
r2: 0.7543183232268387
pearson: 0.8694905750356998

=== Experiment 1906 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006617485541730342
rmse: 0.08134792893325768
mae: 0.03846943806535232
r2: 0.7016204895015024
pearson: 0.8450824268344085

=== Experiment 1820 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006685274922184363
rmse: 0.08176353051443146
mae: 0.03628196923683665
r2: 0.6985638961732187
pearson: 0.8362589955765604

=== Experiment 1768 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006202068660987349
rmse: 0.07875321365498267
mae: 0.03059281444883023
r2: 0.7203514538152522
pearson: 0.8499540634036288

=== Experiment 1756 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006675106729587911
rmse: 0.08170132636370055
mae: 0.04004191813285855
r2: 0.6990223755020288
pearson: 0.8407680296540893

=== Experiment 1903 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008579991350495525
rmse: 0.0926282427259393
mae: 0.04219534378929426
r2: 0.6131319663491421
pearson: 0.7966781796635367

=== Experiment 1814 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00595641083969266
rmse: 0.07717778721687128
mae: 0.030095658170496045
r2: 0.7314280568551541
pearson: 0.8582595623991138

=== Experiment 1921 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007304312859993718
rmse: 0.08546527283051121
mae: 0.04717906711704058
r2: 0.6706517480168273
pearson: 0.8283491315121854

=== Experiment 1886 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008143403054307155
rmse: 0.09024080592673779
mae: 0.043467667200674596
r2: 0.6328175404670717
pearson: 0.8179729085114199

=== Experiment 1628 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00610710768915007
rmse: 0.07814798582913107
mae: 0.0317192044125644
r2: 0.7246332022398752
pearson: 0.8520181412273165

=== Experiment 1626 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0062353792004171205
rmse: 0.07896441730562646
mae: 0.029852164506056093
r2: 0.7188494962534535
pearson: 0.8568152297372381

=== Experiment 1961 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007917420969649931
rmse: 0.08897989081612728
mae: 0.04506984916141504
r2: 0.6430069732019441
pearson: 0.8108300201060898

=== Experiment 1665 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006595647354311691
rmse: 0.08121359094579977
mae: 0.03323593506161239
r2: 0.702605163760488
pearson: 0.8385064184159559

=== Experiment 1827 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005417537077113123
rmse: 0.07360392025641789
mae: 0.027374325998933066
r2: 0.7557256376333847
pearson: 0.8740667739607731

=== Experiment 1677 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006020614627893271
rmse: 0.0775926196741241
mae: 0.031999910188558016
r2: 0.7285331363034367
pearson: 0.8535760986916259

=== Experiment 1960 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007755132662510209
rmse: 0.08806323104741393
mae: 0.05195240486402299
r2: 0.6503244815423279
pearson: 0.8239482505509049

=== Experiment 1985 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006651256479362292
rmse: 0.08155523575689234
mae: 0.03157626492765761
r2: 0.7000977727874041
pearson: 0.837709362237486

=== Experiment 1766 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0075453048578003265
rmse: 0.08686371427587199
mae: 0.03656638236832813
r2: 0.6597855249044171
pearson: 0.8184710205219942

=== Experiment 1948 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059807585304471325
rmse: 0.07733536403513681
mae: 0.03003547484827303
r2: 0.7303302301952732
pearson: 0.8598310524347903

=== Experiment 1976 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005491025582103517
rmse: 0.07410145465578606
mae: 0.027387524354628358
r2: 0.7524120732881324
pearson: 0.8685650670556342

=== Experiment 1572 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008952499747208461
rmse: 0.09461765029426836
mae: 0.039025962164599734
r2: 0.5963357266948396
pearson: 0.8092300179749501

=== Experiment 1830 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006899057039121884
rmse: 0.08306056247776007
mae: 0.030344783613500945
r2: 0.6889245546132112
pearson: 0.8413681339355366

=== Experiment 1888 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007092570695757268
rmse: 0.08421740138330835
mae: 0.0326716404758783
r2: 0.6801991090073953
pearson: 0.8453940775601214

=== Experiment 1679 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0081705096710472
rmse: 0.09039087161349424
mae: 0.041223181978892263
r2: 0.6315953150487976
pearson: 0.8204427916160518

=== Experiment 1894 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006329376547629135
rmse: 0.07955737896404792
mae: 0.032382845461646464
r2: 0.7146111972390601
pearson: 0.8524526695231961

=== Experiment 1725 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007372045576117081
rmse: 0.08586061714265208
mae: 0.041918845515679405
r2: 0.6675977096582731
pearson: 0.8228131814974321

=== Experiment 1977 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006738517640384665
rmse: 0.08208847446739807
mae: 0.040127901840629795
r2: 0.6961632054434794
pearson: 0.8367488999539172

=== Experiment 1872 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0071964426541908585
rmse: 0.08483184929135318
mae: 0.0419860395855178
r2: 0.67551556811917
pearson: 0.8284046850769591

=== Experiment 1710 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006500502452608066
rmse: 0.08062569350156354
mae: 0.035643692675992544
r2: 0.7068952055018308
pearson: 0.8468804447255676

=== Experiment 1890 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007864226341715384
rmse: 0.08868047328310434
mae: 0.038515942645109
r2: 0.6454054955627667
pearson: 0.824859881650785

=== Experiment 1666 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006214984853797151
rmse: 0.07883517523160045
mae: 0.035678624980213916
r2: 0.7197690683663096
pearson: 0.8501850608003801

=== Experiment 1693 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006939672334926969
rmse: 0.08330469575556332
mae: 0.030833041855975615
r2: 0.6870932288015186
pearson: 0.8403609335317712

=== Experiment 1907 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00541169898107561
rmse: 0.07356425070015742
mae: 0.03172745998487741
r2: 0.7559888748141737
pearson: 0.8789416805946181

=== Experiment 1864 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.010769620208408093
rmse: 0.10377678068049757
mae: 0.04225338550725055
r2: 0.5144025648752253
pearson: 0.8131586938819594

=== Experiment 1871 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006549630580553081
rmse: 0.08092978796804723
mae: 0.037702680507734954
r2: 0.7046800398357327
pearson: 0.8502817635546246

=== Experiment 1564 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005248319931066176
rmse: 0.072445289226189
mae: 0.03038269323080687
r2: 0.7633555642704799
pearson: 0.8744307271393186

=== Experiment 1867 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006450420772199153
rmse: 0.08031451159161185
mae: 0.035002020966737483
r2: 0.7091533664288354
pearson: 0.8432117586424677

=== Experiment 1826 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00777387669190744
rmse: 0.08816959051684112
mae: 0.03809662325120047
r2: 0.649479321506685
pearson: 0.8277237087234718

=== Experiment 1566 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00667663279618438
rmse: 0.08171066513120781
mae: 0.034833808923243954
r2: 0.6989535658308674
pearson: 0.8393328203647694

=== Experiment 1829 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007365033884052371
rmse: 0.08581977560010497
mae: 0.036326089683101276
r2: 0.6679138637673891
pearson: 0.823010586656391

=== Experiment 1990 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0055338762666629
rmse: 0.07439002800552572
mae: 0.030282537626469257
r2: 0.7504799547813784
pearson: 0.8667504180164441

=== Experiment 1953 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006643370983493108
rmse: 0.08150687690920999
mae: 0.03736817193893283
r2: 0.7004533263254735
pearson: 0.8379323343948918

=== Experiment 1601 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005913117324999448
rmse: 0.0768967965847697
mae: 0.027802846894501573
r2: 0.7333801423777381
pearson: 0.8574753272639953

=== Experiment 1750 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008338316718319623
rmse: 0.0913143839617813
mae: 0.039844345974648476
r2: 0.6240289691448138
pearson: 0.8281102700201788

=== Experiment 1741 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007500992402436984
rmse: 0.08660826982706088
mae: 0.039073002948993095
r2: 0.661783554013347
pearson: 0.8200844596083623

=== Experiment 1926 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0076417079006688175
rmse: 0.08741686279356413
mae: 0.04345912609705822
r2: 0.6554387541317008
pearson: 0.8152208340932047

=== Experiment 1770 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006679116716901638
rmse: 0.08172586320683091
mae: 0.0361008712835927
r2: 0.6988415669389834
pearson: 0.8498535280079185

=== Experiment 1697 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00668083490858087
rmse: 0.0817363744521426
mae: 0.04242991635160242
r2: 0.6987640944324007
pearson: 0.8435138108963537

=== Experiment 1929 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007475064051628977
rmse: 0.08645845274829395
mae: 0.04691892455802978
r2: 0.66295265193934
pearson: 0.8214903775399073

=== Experiment 1767 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008237688584040049
rmse: 0.09076171320573477
mae: 0.04028694759869619
r2: 0.6285662474296507
pearson: 0.8093039911461588

=== Experiment 1844 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005151699696308255
rmse: 0.0717753418404138
mae: 0.0264890390744225
r2: 0.7677121281298972
pearson: 0.8774608414093626

=== Experiment 1981 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0076275975847095705
rmse: 0.08733611844311362
mae: 0.03637498712991613
r2: 0.6560749820678784
pearson: 0.8302640231962705

=== Experiment 1627 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006135083634917086
rmse: 0.07832677470007997
mae: 0.02962995859157836
r2: 0.7233717791583958
pearson: 0.8556742687287217

=== Experiment 1933 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007006026280616098
rmse: 0.08370200882067347
mae: 0.040858628243740154
r2: 0.6841013585950007
pearson: 0.830181118275678

=== Experiment 1730 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006382432088752549
rmse: 0.07989012510162034
mae: 0.03079326356600006
r2: 0.712218946241335
pearson: 0.8560578990268635

=== Experiment 1952 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0070302837158251665
rmse: 0.08384678715267012
mae: 0.03202681851927588
r2: 0.6830076015179376
pearson: 0.8373376962577853

=== Experiment 1895 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0067882840553120894
rmse: 0.08239104353819103
mae: 0.04095134784827034
r2: 0.6939192596982762
pearson: 0.8386618263752245

=== Experiment 1914 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006991661435272118
rmse: 0.08361615534854565
mae: 0.03060313129714577
r2: 0.6847490631491088
pearson: 0.8411771053212937

=== Experiment 1586 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006556127689786195
rmse: 0.08096991842521638
mae: 0.03524717040971018
r2: 0.7043870880400082
pearson: 0.845420729089179

=== Experiment 1704 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006719285781477304
rmse: 0.08197124972499385
mae: 0.036168098156409985
r2: 0.6970303614970239
pearson: 0.8449996440751495

=== Experiment 1975 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007645906683590814
rmse: 0.08744087535924383
mae: 0.03764285056413829
r2: 0.6552494328577737
pearson: 0.8179835150556533

=== Experiment 1769 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006468336080476911
rmse: 0.08042596645659231
mae: 0.03330243451114046
r2: 0.7083455730637216
pearson: 0.8421129618411032

=== Experiment 1998 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008418745921154372
rmse: 0.0917537242903762
mae: 0.05514989269257456
r2: 0.6204024517885942
pearson: 0.8080230913540795

=== Experiment 1897 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006738013541703852
rmse: 0.08208540395042137
mae: 0.03412003149767067
r2: 0.6961859350311265
pearson: 0.8416422283289872

=== Experiment 1533 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006596561084169173
rmse: 0.08121921622478989
mae: 0.029548012637957845
r2: 0.7025639640833774
pearson: 0.8384631450377323

=== Experiment 1804 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005704909274926273
rmse: 0.07553084982261403
mae: 0.03228605826422943
r2: 0.7427681517161666
pearson: 0.8635096878072175

=== Experiment 1946 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00652933287169021
rmse: 0.08080428745858854
mae: 0.04292564662442633
r2: 0.7055952545946549
pearson: 0.8462619672722174

=== Experiment 1696 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005886564032734816
rmse: 0.0767239469314165
mae: 0.030028765376456573
r2: 0.7345774186389525
pearson: 0.8575647736303657

=== Experiment 1556 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0067565787814517
rmse: 0.08219841106403274
mae: 0.03438688042221449
r2: 0.6953488365420828
pearson: 0.8416333289906841

=== Experiment 1955 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006442634883275499
rmse: 0.08026602570998205
mae: 0.035072231909275754
r2: 0.7095044287335714
pearson: 0.8424293239569765

=== Experiment 1722 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005387525512643229
rmse: 0.0733997650721256
mae: 0.028993490148158226
r2: 0.7570788458662325
pearson: 0.8712715449837091

=== Experiment 1905 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0075020940760936105
rmse: 0.08661462968860174
mae: 0.03421494395956331
r2: 0.6617338800330534
pearson: 0.8407547149874061

=== Experiment 1962 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007481472231307327
rmse: 0.0864955041103717
mae: 0.04754339226751344
r2: 0.662663709938099
pearson: 0.8238598719858983

=== Experiment 1980 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007967118928480423
rmse: 0.08925871906139154
mae: 0.04350691719570654
r2: 0.6407661141120218
pearson: 0.8194898708251662

=== Experiment 1703 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006269021478757039
rmse: 0.07917715250472852
mae: 0.037993457439349426
r2: 0.7173325807302056
pearson: 0.8503023791369426

=== Experiment 1599 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0055437060724144
rmse: 0.07445606806979804
mae: 0.02906443512733262
r2: 0.7500367331664717
pearson: 0.866087847021641

=== Experiment 1876 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0073560925252055855
rmse: 0.08576766596570987
mae: 0.032196660889584124
r2: 0.6683170257023977
pearson: 0.8199049499257067

=== Experiment 1956 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007462015480361948
rmse: 0.08638295827512478
mae: 0.0349954075836979
r2: 0.6635410062746552
pearson: 0.8181610712334096

=== Experiment 1734 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006118647646161964
rmse: 0.07822178498450393
mae: 0.03116271248172116
r2: 0.7241128706573323
pearson: 0.8562961528007778

=== Experiment 1702 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00665630783917926
rmse: 0.08158619882786096
mae: 0.03089614892206945
r2: 0.6998700091965304
pearson: 0.8426405352507188

=== Experiment 1993 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005816876396398265
rmse: 0.07626844954762267
mae: 0.030700702583202066
r2: 0.7377196034894933
pearson: 0.8613520259616988

=== Experiment 1849 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006658238860748088
rmse: 0.08159803221124935
mae: 0.036188113016955216
r2: 0.6997829402838998
pearson: 0.8479627023626441

=== Experiment 1720 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0065385356098887435
rmse: 0.08086121202337214
mae: 0.040156978177747216
r2: 0.7051803071797177
pearson: 0.8431865190126739

=== Experiment 1833 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008094717784884827
rmse: 0.08997064957465199
mae: 0.04834146261803226
r2: 0.635012737837299
pearson: 0.8194604438327432

=== Experiment 1982 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00810508140908273
rmse: 0.09002822562442697
mae: 0.03957821515431062
r2: 0.6345454465835949
pearson: 0.7977982936574607

=== Experiment 1794 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007547843513508913
rmse: 0.08687832591336525
mae: 0.03887238300791154
r2: 0.6596710580358649
pearson: 0.8213785817056043

=== Experiment 1838 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068792189356328675
rmse: 0.08294105699611543
mae: 0.037191584612998686
r2: 0.6898190459681138
pearson: 0.8325150657405054

=== Experiment 1923 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006984533993685921
rmse: 0.08357352447806614
mae: 0.03293311892125432
r2: 0.6850704363532035
pearson: 0.8308361410590362

=== Experiment 1859 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076803483106718745
rmse: 0.08763759644508672
mae: 0.03731900521502615
r2: 0.6536964750516081
pearson: 0.8210177928796649

=== Experiment 1819 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006549834989717429
rmse: 0.08093105083784238
mae: 0.0368169674649916
r2: 0.7046708231164798
pearson: 0.8420455497842085

=== Experiment 1727 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006218382481472267
rmse: 0.07885672121938793
mae: 0.038057987981149524
r2: 0.7196158708298489
pearson: 0.8537154557657318

=== Experiment 1941 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0069743100009673304
rmse: 0.08351233442412762
mae: 0.03803114783274408
r2: 0.6855314316849614
pearson: 0.8371631144348234

=== Experiment 1724 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010046706738434283
rmse: 0.10023326163721444
mae: 0.04302983053928541
r2: 0.546998415057794
pearson: 0.800326836255506

=== Experiment 1816 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0073046727208065584
rmse: 0.08546737810888175
mae: 0.032698386298479966
r2: 0.6706355220511633
pearson: 0.8285464180153457

=== Experiment 1957 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006187398384160662
rmse: 0.0786600176974342
mae: 0.03521998256940657
r2: 0.721012930140485
pearson: 0.8499871715588397

=== Experiment 1761 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008161854506600494
rmse: 0.09034298260850421
mae: 0.045978124959236234
r2: 0.6319855726042702
pearson: 0.816297450329948

=== Experiment 1999 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006185661943122251
rmse: 0.07864897928849586
mae: 0.0328948005826607
r2: 0.7210912255026409
pearson: 0.8494193779075261

=== Experiment 1918 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0057799624285604445
rmse: 0.07602606413961231
mae: 0.030314687104341032
r2: 0.739384038052221
pearson: 0.8599290697606939

=== Experiment 1691 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007745478268437738
rmse: 0.08800839885168767
mae: 0.029492703725301123
r2: 0.6507597939218318
pearson: 0.8229251121302869

=== Experiment 1845 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006554663334235559
rmse: 0.08096087533022082
mae: 0.029825033209113325
r2: 0.7044531151872742
pearson: 0.8421338422565544

=== Experiment 1852 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008021164665299651
rmse: 0.08956095502672831
mae: 0.04043537994276569
r2: 0.6383292156261906
pearson: 0.8049349379035496

=== Experiment 1582 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006420983265580922
rmse: 0.08013103809124728
mae: 0.039476659731481324
r2: 0.7104806906458141
pearson: 0.8524102677689006

=== Experiment 1799 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006311864113283479
rmse: 0.07944724106778964
mae: 0.028969499115844018
r2: 0.715400825195894
pearson: 0.85210713284866

=== Experiment 1723 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007248749503861398
rmse: 0.08513958834679317
mae: 0.04945395036893096
r2: 0.6731570752895305
pearson: 0.8334423000561504

=== Experiment 1655 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006705433437385064
rmse: 0.08188671099381795
mae: 0.030050482454631936
r2: 0.6976549575952602
pearson: 0.836660591599167

=== Experiment 1874 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006258095811864731
rmse: 0.07910812734393813
mae: 0.03200497662416651
r2: 0.7178252142416259
pearson: 0.8486647795886912

=== Experiment 1865 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0063347626707298856
rmse: 0.07959122232212473
mae: 0.03037679353729265
r2: 0.7143683393190614
pearson: 0.8545583325971807

=== Experiment 1781 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00625591347880315
rmse: 0.07909433278562472
mae: 0.036332587901213964
r2: 0.7179236146788546
pearson: 0.8496158393583089

=== Experiment 1987 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005721240645950955
rmse: 0.07563888316171091
mae: 0.028585706798922094
r2: 0.7420317773846499
pearson: 0.8627963403531933

=== Experiment 1995 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005797066705005579
rmse: 0.07613847059802015
mae: 0.027404539394151804
r2: 0.7386128137554777
pearson: 0.8677864805069696

=== Experiment 1591 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006062414427569787
rmse: 0.07786150799701858
mae: 0.028413086696624963
r2: 0.7266484017335215
pearson: 0.8577090745279247

=== Experiment 1516 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006138457161462713
rmse: 0.07834830669173848
mae: 0.03738593733152756
r2: 0.7232196683312562
pearson: 0.8519976085228008

=== Experiment 1868 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00722068322434046
rmse: 0.08497460340796219
mae: 0.036888721689639595
r2: 0.6744225714802152
pearson: 0.8320632932535378

=== Experiment 1810 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0068346959091903645
rmse: 0.08267221969434693
mae: 0.03183577414636574
r2: 0.6918265696343238
pearson: 0.8380860535887392

=== Experiment 1899 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006206756222615514
rmse: 0.07878296911525684
mae: 0.032693879369824555
r2: 0.7201400937245921
pearson: 0.8488998390116672

=== Experiment 1552 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006015051410344523
rmse: 0.07755676250556441
mae: 0.02966663897514809
r2: 0.7287839793341506
pearson: 0.8547573399151112

=== Experiment 1685 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008424615159543567
rmse: 0.09178570236994195
mae: 0.04578417693882895
r2: 0.6201378104128747
pearson: 0.7890492680039611

=== Experiment 1659 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006995270273663493
rmse: 0.08363773235605741
mae: 0.028653700553633046
r2: 0.684586342214413
pearson: 0.8359229960995402

=== Experiment 1857 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006530361084810044
rmse: 0.08081064957547392
mae: 0.03756761815528754
r2: 0.7055488929176327
pearson: 0.841161227487244

=== Experiment 1866 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006465574341171336
rmse: 0.08040879517298674
mae: 0.03907437620896267
r2: 0.7084700986734754
pearson: 0.8442342526427307

=== Experiment 1518 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007450076593422112
rmse: 0.08631382620080118
mae: 0.04122948125692815
r2: 0.6640793254320663
pearson: 0.8378789328564469

=== Experiment 1991 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005487459610031908
rmse: 0.07407738933056367
mae: 0.029612895406292646
r2: 0.752572861399337
pearson: 0.867630855604445

=== Experiment 1747 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0096847942861695
rmse: 0.09841135242526393
mae: 0.043617964507134184
r2: 0.5633168882405613
pearson: 0.7592769723591084

=== Experiment 1623 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00730335372916921
rmse: 0.08545966141501621
mae: 0.03973352783394829
r2: 0.6706949948035588
pearson: 0.8434349706665112

=== Experiment 1629 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070749346316857364
rmse: 0.08411263063111114
mae: 0.03972993746437509
r2: 0.6809943113741552
pearson: 0.8326757943725367

=== Experiment 1798 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0059597479520628795
rmse: 0.07719940383230223
mae: 0.03019904155434719
r2: 0.7312775879271565
pearson: 0.8558993792959589

=== Experiment 1793 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070654253719224175
rmse: 0.08405608468113666
mae: 0.029544394150177435
r2: 0.6814230797115381
pearson: 0.8394417160684461

=== Experiment 1943 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0063099560191574484
rmse: 0.07943523159881545
mae: 0.032666176415338324
r2: 0.7154868603202202
pearson: 0.8472326030381961

=== Experiment 1649 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006016877663329415
rmse: 0.07756853526610784
mae: 0.029648209538050484
r2: 0.7287016343908572
pearson: 0.8563546448951179

=== Experiment 1745 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006104195615427476
rmse: 0.07812935181753063
mae: 0.03074577651879088
r2: 0.7247645063623254
pearson: 0.8533959565735624

=== Experiment 1881 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005858815902210424
rmse: 0.07654290236338326
mae: 0.033665475768147365
r2: 0.7358285696314113
pearson: 0.8585617817015937

=== Experiment 1942 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005978098216705092
rmse: 0.07731816226932124
mae: 0.03304806257907016
r2: 0.7304501825710081
pearson: 0.8550542337571658

=== Experiment 1931 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006995077818542679
rmse: 0.08363658182005454
mae: 0.03113438734378208
r2: 0.6845950199311045
pearson: 0.8434212082587921

=== Experiment 1973 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006941416601561979
rmse: 0.08331516429535489
mae: 0.03530210752544373
r2: 0.6870145805866565
pearson: 0.8415249105031077

=== Experiment 1706 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0063071252695779635
rmse: 0.07941741162728715
mae: 0.03339230781472677
r2: 0.7156144975728509
pearson: 0.8476239961891822

=== Experiment 1984 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006989275300721398
rmse: 0.08360188574859659
mae: 0.03783513080623702
r2: 0.6848566529057827
pearson: 0.8334339395741868

=== Experiment 1800 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006169761162663952
rmse: 0.07854782723070035
mae: 0.03178246627605579
r2: 0.7218081846950369
pearson: 0.8568676164155072

=== Experiment 1967 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005842263220553209
rmse: 0.07643469906104955
mae: 0.027119213415848095
r2: 0.7365749227619427
pearson: 0.8671923382459097

=== Experiment 1742 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005860850728175268
rmse: 0.07655619327118654
mae: 0.0312528217656592
r2: 0.7357368202242519
pearson: 0.8625381140614805

=== Experiment 1664 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006163996877575814
rmse: 0.0785111258203308
mae: 0.03173658249760894
r2: 0.722068093772605
pearson: 0.8497459506726871

=== Experiment 1940 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007817963014393662
rmse: 0.08841924572395798
mae: 0.040710205080505796
r2: 0.6474914886296557
pearson: 0.8139524637114672

=== Experiment 1528 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006562782611603002
rmse: 0.08101100302800232
mae: 0.03782492889217377
r2: 0.7040870205443434
pearson: 0.8392782509214899

=== Experiment 1639 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00634568932882573
rmse: 0.07965983510418366
mae: 0.03252707377742923
r2: 0.7138756611147097
pearson: 0.8473009023142739

=== Experiment 1773 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007036166950093918
rmse: 0.08388186305807661
mae: 0.03335818007838653
r2: 0.682742329074169
pearson: 0.8402804496292952

=== Experiment 1854 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006365643782500199
rmse: 0.07978498469323786
mae: 0.035154943529389276
r2: 0.7129759235811548
pearson: 0.8508275033291897

=== Experiment 1935 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008002245345600748
rmse: 0.08945527008287855
mae: 0.03716375565884488
r2: 0.6391822794243849
pearson: 0.8193740803427662

=== Experiment 1873 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006829592838527638
rmse: 0.08264135065769217
mae: 0.03793732245008329
r2: 0.6920566648444729
pearson: 0.8358245021250891

=== Experiment 1900 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006413328980201855
rmse: 0.08008326279692814
mae: 0.03403697349345593
r2: 0.7108258190046475
pearson: 0.8464401712702383

=== Experiment 1950 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006832094278800666
rmse: 0.08265648358598778
mae: 0.03174837806151554
r2: 0.6919438760035301
pearson: 0.8457058776928994

=== Experiment 1997 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005693100134676388
rmse: 0.07545263504130513
mae: 0.031190687085901874
r2: 0.743300620652434
pearson: 0.8679318440166764

=== Experiment 1885 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00841990565028968
rmse: 0.09176004386599693
mae: 0.04105613040043313
r2: 0.6203501601123191
pearson: 0.8308856847078306

=== Experiment 1831 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005857358877042975
rmse: 0.07653338406893409
mae: 0.02819213529084825
r2: 0.7358942662549257
pearson: 0.8585042298417813

=== Experiment 1733 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062867411778912115
rmse: 0.07928897261215592
mae: 0.03669197740406257
r2: 0.7165336072952817
pearson: 0.847175080894576

=== Experiment 1913 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00652937508044761
rmse: 0.08080454863711331
mae: 0.03317558285813716
r2: 0.7055933514203611
pearson: 0.8444422550340934

=== Experiment 1625 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006784389096574695
rmse: 0.08236740311904155
mae: 0.03177644977626505
r2: 0.6940948816734429
pearson: 0.8462205010589435

=== Experiment 1763 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006817978200778586
rmse: 0.08257104941066563
mae: 0.03998802511685205
r2: 0.6925803637485848
pearson: 0.8422281570005189

=== Experiment 1878 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0070326500960909205
rmse: 0.08386089730077374
mae: 0.031182606386224186
r2: 0.6829009024732783
pearson: 0.8388429996637461

=== Experiment 1937 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006625788668580184
rmse: 0.08139894758889812
mae: 0.03260906288802711
r2: 0.7012461051663286
pearson: 0.8388775518462894

=== Experiment 1928 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00604836254045825
rmse: 0.07777121922959836
mae: 0.03123547012398946
r2: 0.7272819951386722
pearson: 0.8634418603139078

=== Experiment 1916 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0073792984495490085
rmse: 0.08590284308187365
mae: 0.03229711516941385
r2: 0.6672706807874612
pearson: 0.8313189135772455

=== Experiment 1616 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005707630088929096
rmse: 0.07554885895186701
mae: 0.030525305420045404
r2: 0.7426454714103011
pearson: 0.8639288076037825

=== Experiment 1966 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008712857268341829
rmse: 0.0933426872783392
mae: 0.04382209762678295
r2: 0.6071411005922152
pearson: 0.7797618486559659

=== Experiment 1883 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006209319671825625
rmse: 0.0787992364926566
mae: 0.028692865529892357
r2: 0.7200245089279684
pearson: 0.8560917047307982

=== Experiment 1719 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005947307753023264
rmse: 0.07711878988303217
mae: 0.032568674947472294
r2: 0.7318385110265023
pearson: 0.8601912038738219

=== Experiment 1788 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006533116606516513
rmse: 0.08082769702593606
mae: 0.03466625446268049
r2: 0.7054246476566857
pearson: 0.8402388387744508

=== Experiment 1969 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007331487368348779
rmse: 0.08562410506597297
mae: 0.03713525029039853
r2: 0.6694264613955134
pearson: 0.8336159893546677

=== Experiment 1821 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006467396463186865
rmse: 0.080420124739936
mae: 0.034497590682259396
r2: 0.7083879399937725
pearson: 0.8437412922836448

=== Experiment 1809 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008745806963788783
rmse: 0.09351901926233393
mae: 0.042474687862939654
r2: 0.605655413326782
pearson: 0.8011408846014355

=== Experiment 1858 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005883007072922781
rmse: 0.07670076318344414
mae: 0.03354595018573007
r2: 0.7347378003913396
pearson: 0.8583711325416895

=== Experiment 1994 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005497928978122253
rmse: 0.07414802072963414
mae: 0.027435807540143107
r2: 0.7521008022000646
pearson: 0.8685031700887201

=== Experiment 1749 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0068177796099717775
rmse: 0.08256984685689914
mae: 0.028981213906557463
r2: 0.6925893181206573
pearson: 0.8432666385235134

=== Experiment 1637 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0069827789565412995
rmse: 0.08356302385948763
mae: 0.0437997132170724
r2: 0.6851495702056035
pearson: 0.8316079500577853

=== Experiment 1682 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007984857892122799
rmse: 0.089358032051533
mae: 0.036752913218229516
r2: 0.639966272048902
pearson: 0.8495763255245607

=== Experiment 1614 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006470632014788356
rmse: 0.08044023877878755
mae: 0.034596981141714826
r2: 0.7082420503961365
pearson: 0.8444544025805105

=== Experiment 1755 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006348584883429645
rmse: 0.07967800752673002
mae: 0.03824825921937686
r2: 0.7137451018320502
pearson: 0.8593071899858594

=== Experiment 1735 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005771716689747376
rmse: 0.07597181510104505
mae: 0.02909329600739046
r2: 0.7397558347860751
pearson: 0.8603154511668856

=== Experiment 1797 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007243811595762003
rmse: 0.08511058451075285
mae: 0.03837259514198924
r2: 0.6733797233923926
pearson: 0.8456369516718317

=== Experiment 1631 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0061955734727946
rmse: 0.0787119652454098
mae: 0.03171462474376784
r2: 0.7206443189921125
pearson: 0.8504594245980313

=== Experiment 1912 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057588994114275835
rmse: 0.07588741273378335
mae: 0.03031095897540036
r2: 0.7403337602241988
pearson: 0.8607350055367777

=== Experiment 1653 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006823022957205969
rmse: 0.08260159173530476
mae: 0.031037931892001484
r2: 0.6923528979016411
pearson: 0.8434961722593067

=== Experiment 1525 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.012308264261374876
rmse: 0.11094261697551071
mae: 0.05605366689190557
r2: 0.44502578173598983
pearson: 0.6988282313214886

=== Experiment 1863 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005873949659099117
rmse: 0.07664169660895509
mae: 0.02947239817787232
r2: 0.7351461951941758
pearson: 0.8580643727929483

=== Experiment 1996 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006937440071864324
rmse: 0.08329129649527808
mae: 0.035648196353216324
r2: 0.6871938805605192
pearson: 0.8424819405314035

=== Experiment 1636 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006115652176628923
rmse: 0.07820263535603467
mae: 0.029844642240625826
r2: 0.7242479350601704
pearson: 0.855895533480525

=== Experiment 1574 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007399061094185382
rmse: 0.08601779521811392
mae: 0.045976994934191384
r2: 0.6663795918390123
pearson: 0.8223409874920375

=== Experiment 1958 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00562013468126131
rmse: 0.07496755752498083
mae: 0.028573098294691256
r2: 0.7465906008323677
pearson: 0.8683232947983142

=== Experiment 1748 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008545732263637534
rmse: 0.09244312988879992
mae: 0.03986531968615659
r2: 0.614676693496992
pearson: 0.7969869155369628

=== Experiment 1759 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005977733336976943
rmse: 0.07731580263424123
mae: 0.02945620074139551
r2: 0.7304666348373546
pearson: 0.8547505231888678

=== Experiment 1983 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006216947119967924
rmse: 0.07884761962144402
mae: 0.03312739884060884
r2: 0.7196805906483288
pearson: 0.8490476991556546

=== Experiment 1968 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006716848844116574
rmse: 0.08195638379111522
mae: 0.03590847439016203
r2: 0.6971402419300411
pearson: 0.8354330416175292

=== Experiment 1630 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006280238180841883
rmse: 0.07924795379593016
mae: 0.038428438212449074
r2: 0.7168268245700482
pearson: 0.850917192723812

=== Experiment 1986 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006988178886918473
rmse: 0.0835953281405036
mae: 0.03423704114790786
r2: 0.684906089721588
pearson: 0.8289204947152006

=== Experiment 1529 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006698869707834033
rmse: 0.08184662306921424
mae: 0.030373614065682387
r2: 0.6979509132717978
pearson: 0.8480802477290144

=== Experiment 1596 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0069526897009296275
rmse: 0.08338279019635664
mae: 0.04032437588856457
r2: 0.6865062814978398
pearson: 0.834404585962014

=== Experiment 1687 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006123648601435027
rmse: 0.07825374496747761
mae: 0.03506355168404582
r2: 0.7238873797851587
pearson: 0.8528143186918539

=== Experiment 1681 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006534328627234776
rmse: 0.08083519423638924
mae: 0.03826498497508807
r2: 0.7053699981759505
pearson: 0.8416918356644862

=== Experiment 1774 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007006268296199909
rmse: 0.08370345450577239
mae: 0.04587505864943504
r2: 0.6840904462188457
pearson: 0.8383164440678715

=== Experiment 1721 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006696685308375014
rmse: 0.08183327751211615
mae: 0.042973050565081446
r2: 0.6980494068819771
pearson: 0.8427300758205829

=== Experiment 1670 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006574544962030373
rmse: 0.08108356776826223
mae: 0.030064913037628316
r2: 0.7035566613405795
pearson: 0.8397522082944887

=== Experiment 1563 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006058502724225838
rmse: 0.07783638432138172
mae: 0.033304066652968194
r2: 0.7268247787156279
pearson: 0.8603285351230898

=== Experiment 1869 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0059692444885597285
rmse: 0.07726088589033735
mae: 0.03963812056709345
r2: 0.730849393276259
pearson: 0.8591786327198091

=== Experiment 1732 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006456604166902884
rmse: 0.08035299724903162
mae: 0.04127350148966967
r2: 0.7088745598831644
pearson: 0.8468181048022217

=== Experiment 1796 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006425266687497585
rmse: 0.0801577612430486
mae: 0.03969133117030084
r2: 0.71028755303687
pearson: 0.8455018919299975

=== Experiment 1660 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007162497634998832
rmse: 0.0846315404267158
mae: 0.04437367775246654
r2: 0.6770461340941929
pearson: 0.8428740727678644

=== Experiment 1541 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007168308897921312
rmse: 0.08466586619128934
mae: 0.045059397314675474
r2: 0.6767841068067508
pearson: 0.8315559052467048

=== Experiment 1612 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006688072346193873
rmse: 0.08178063552084854
mae: 0.04366269196884033
r2: 0.6984377615558703
pearson: 0.8433058535921696

=== Experiment 1692 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006197108290066127
rmse: 0.07872171422210092
mae: 0.03496037683196322
r2: 0.7205751147568641
pearson: 0.8512137944081272

=== Experiment 1530 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006170872487932872
rmse: 0.07855490110701478
mae: 0.028867372397181162
r2: 0.7217580755277928
pearson: 0.8585289746120368

=== Experiment 1557 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006171058355888762
rmse: 0.07855608414304242
mae: 0.02810464111988162
r2: 0.7217496948234685
pearson: 0.8517473360742785

=== Experiment 1783 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006526867125856447
rmse: 0.08078902849927364
mae: 0.030555304866036477
r2: 0.7057064339890413
pearson: 0.8458075822022816

=== Experiment 1965 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006885702444030408
rmse: 0.08298013282726419
mae: 0.044724375980345184
r2: 0.6895267074280789
pearson: 0.8385509018266958

=== Experiment 1746 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006146677665467408
rmse: 0.07840075041393041
mae: 0.02998981463494779
r2: 0.7228490094238049
pearson: 0.8600423509491942

=== Experiment 1640 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.012528652282649006
rmse: 0.11193146243415658
mae: 0.049604826570152145
r2: 0.4350885828569253
pearson: 0.6829006964220151

=== Experiment 1780 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00827703180709489
rmse: 0.09097819412966433
mae: 0.0401509723607634
r2: 0.6267922788183846
pearson: 0.8353880646084162

=== Experiment 1558 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006742540834235239
rmse: 0.08211297604054574
mae: 0.03172049323535056
r2: 0.6959818014035
pearson: 0.8377666325827106

=== Experiment 1632 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006613135169368796
rmse: 0.08132118524326115
mae: 0.04559144832556793
r2: 0.7018166458765391
pearson: 0.8475318806915647

=== Experiment 1892 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006639786820326661
rmse: 0.08148488706703017
mae: 0.03535829382713452
r2: 0.7006149346651378
pearson: 0.8403873476941056

=== Experiment 1908 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00520576558952864
rmse: 0.07215099160461094
mae: 0.03176319194306344
r2: 0.7652743208008095
pearson: 0.8755535860406406

=== Experiment 1707 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005432359615383901
rmse: 0.07370454270520849
mae: 0.030360594907646102
r2: 0.7550572959066535
pearson: 0.8728807041874523

=== Experiment 1634 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0062341088083834985
rmse: 0.07895637281678723
mae: 0.02996505330517609
r2: 0.7189067776711072
pearson: 0.8495094454583679

=== Experiment 1832 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006802745451288472
rmse: 0.08247875757604786
mae: 0.03273415858540688
r2: 0.6932672017186481
pearson: 0.8405107197450488

=== Experiment 1880 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006030478277982024
rmse: 0.07765615415394986
mae: 0.03329727603890775
r2: 0.7280883886622593
pearson: 0.8536670841753908

=== Experiment 1546 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006428670870528974
rmse: 0.08017899270088752
mae: 0.03491360559971733
r2: 0.7101340599222801
pearson: 0.8469138474863586

=== Experiment 1760 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.010469979080838958
rmse: 0.10232291571705215
mae: 0.0376440368362045
r2: 0.5279132514351719
pearson: 0.7611289213861591

=== Experiment 1698 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0071619214292543525
rmse: 0.08462813615609381
mae: 0.03420872236208268
r2: 0.677072114957604
pearson: 0.8355379714098125

=== Experiment 1595 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006178503648133106
rmse: 0.07860345824537943
mae: 0.035495751590531074
r2: 0.7214139898730947
pearson: 0.8522455242828169

=== Experiment 1875 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0058545611598075946
rmse: 0.07651510412858101
mae: 0.03174065041968753
r2: 0.7360204140936994
pearson: 0.8586753537304894

=== Experiment 1651 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006850981556804188
rmse: 0.08277065637533744
mae: 0.03892136919826374
r2: 0.6910922569512781
pearson: 0.8338871779518663

=== Experiment 1671 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005613461632388673
rmse: 0.07492303806165813
mae: 0.02977842109476657
r2: 0.7468914856690017
pearson: 0.8648205530152913

=== Experiment 1934 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006334645328336845
rmse: 0.07959048516208986
mae: 0.03833024939036954
r2: 0.7143736302359172
pearson: 0.849497134480091

=== Experiment 1758 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00562451404033985
rmse: 0.07499676019895693
mae: 0.029729120884821162
r2: 0.7463931374590197
pearson: 0.8653747543004212

=== Experiment 1970 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007260793986568483
rmse: 0.08521029272669167
mae: 0.038860095561698685
r2: 0.6726139948654501
pearson: 0.8237034935472993

=== Experiment 1919 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006806920917879974
rmse: 0.08250406606877005
mae: 0.03365008149538055
r2: 0.6930789317678008
pearson: 0.8336349904139626

=== Experiment 1974 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007176505311754165
rmse: 0.08471425683882357
mae: 0.04104987531485543
r2: 0.6764145341145451
pearson: 0.8236531499953629

=== Experiment 1951 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005649205254188791
rmse: 0.07516119513544732
mae: 0.027320451252283627
r2: 0.7452798214940053
pearson: 0.8636928456609794

=== Experiment 1510 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.012021454613208225
rmse: 0.10964239423329018
mae: 0.04792333415978641
r2: 0.4579579025372441
pearson: 0.6896429538977931

=== Experiment 1509 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011255137600553581
rmse: 0.10609023329484002
mae: 0.04192841496960498
r2: 0.49251079935593145
pearson: 0.7371578520786232

=== Experiment 1979 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008018162455925543
rmse: 0.08954419275377686
mae: 0.04087975544632779
r2: 0.6384645839254959
pearson: 0.8104517126015522

=== Experiment 1740 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006339526164655151
rmse: 0.07962114144280495
mae: 0.042836363254858185
r2: 0.7141535554745452
pearson: 0.8541922454584638

=== Experiment 1701 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005765024611629002
rmse: 0.07592775916375383
mae: 0.02952610723248789
r2: 0.7400575776430236
pearson: 0.8611490029597625

=== Experiment 1841 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.014850848932926552
rmse: 0.12186405923374845
mae: 0.04691778436604556
r2: 0.330381757972819
pearson: 0.6189179666619502

=== Experiment 1641 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007149770746486667
rmse: 0.08455631701112973
mae: 0.04935865543186304
r2: 0.6776199838956745
pearson: 0.8376404419519778

=== Experiment 1549 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008916303556432685
rmse: 0.09442618046089064
mae: 0.04352245087227886
r2: 0.5979677970057575
pearson: 0.7875837970484822

=== Experiment 1954 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005944246580097582
rmse: 0.07709894020087164
mae: 0.02962306574728846
r2: 0.7319765379663962
pearson: 0.8575097043336081

=== Experiment 1638 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0117760819676109
rmse: 0.10851765740012498
mae: 0.047686268236642265
r2: 0.4690216471304671
pearson: 0.7354276382162372

=== Experiment 1887 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005638436572436655
rmse: 0.07508952371960188
mae: 0.0338517420173899
r2: 0.7457653766145498
pearson: 0.8650443725507417

=== Experiment 1661 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006213585395912365
rmse: 0.0788262988850318
mae: 0.032328964446652245
r2: 0.719832169306386
pearson: 0.8485585324189637

=== Experiment 1726 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006782612360943132
rmse: 0.08235661698335557
mae: 0.03088932754225132
r2: 0.6941749939010688
pearson: 0.8390329538594361

=== Experiment 1642 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006639289465697841
rmse: 0.08148183518857341
mae: 0.02966931883110556
r2: 0.7006373601664487
pearson: 0.86204913907108

=== Experiment 1971 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006832446456998449
rmse: 0.08265861393102626
mae: 0.030217273310499036
r2: 0.6919279964435976
pearson: 0.8413308754392224

=== Experiment 1812 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00629562584798185
rmse: 0.079344979979718
mae: 0.02994529944150649
r2: 0.7161330014313374
pearson: 0.8489713864148676

=== Experiment 1764 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005493383771522158
rmse: 0.07411736484469857
mae: 0.03144105471365896
r2: 0.7523057435651646
pearson: 0.86740192154708

=== Experiment 1615 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005164720109502222
rmse: 0.07186598715318827
mae: 0.028925146237206885
r2: 0.7671250434297033
pearson: 0.8760248535916546

=== Experiment 1801 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005505964808928013
rmse: 0.07420218870712651
mae: 0.029618257130871954
r2: 0.7517384701258725
pearson: 0.8698095228126669

=== Experiment 1855 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006164486202014373
rmse: 0.07851424203298643
mae: 0.029715698971541988
r2: 0.7220460303490387
pearson: 0.8519016315758676

=== Experiment 1669 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006489115516442001
rmse: 0.08055504649891279
mae: 0.03878659623820734
r2: 0.7074086374417847
pearson: 0.8428183696691216

=== Experiment 1775 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005635903686474577
rmse: 0.07507265605048603
mae: 0.031242609272836595
r2: 0.7458795833277714
pearson: 0.8644161522747363

=== Experiment 1848 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005890114223269186
rmse: 0.0767470795748554
mae: 0.037082194659961515
r2: 0.7344173421103162
pearson: 0.8589725463184251

=== Experiment 1672 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006492414710378706
rmse: 0.08057552178160998
mae: 0.029149010358116553
r2: 0.707259878239265
pearson: 0.8426319732901709

=== Experiment 1889 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0076463092273565325
rmse: 0.0874431771343913
mae: 0.03563930577073995
r2: 0.6552312823365505
pearson: 0.8105638213871553

=== Experiment 1592 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006696218214782094
rmse: 0.08183042352806257
mae: 0.032696397833856534
r2: 0.6980704679264984
pearson: 0.8539614621821814

=== Experiment 1570 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005897201097922802
rmse: 0.07679323601674044
mae: 0.03147679072530369
r2: 0.7340977980513567
pearson: 0.8572374552031348

=== Experiment 1896 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0057818152382541075
rmse: 0.07603824852174139
mae: 0.032553008022113576
r2: 0.7393004956786177
pearson: 0.8619973727095293

=== Experiment 1688 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006074559928655773
rmse: 0.07793946323048276
mae: 0.03721977642780415
r2: 0.7261007664351982
pearson: 0.8561369194874213

=== Experiment 1925 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062122462136755515
rmse: 0.07881780391304716
mae: 0.028629442386634955
r2: 0.7198925524440248
pearson: 0.8560116189826485

=== Experiment 1860 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007382512111775195
rmse: 0.08592154626038334
mae: 0.03521164321715788
r2: 0.6671257781721234
pearson: 0.8456001324067071

=== Experiment 1675 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007506355795029777
rmse: 0.08663922780721085
mae: 0.03397644523681855
r2: 0.6615417210019477
pearson: 0.8189261605297039

=== Experiment 1578 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006381950938206892
rmse: 0.07988711371808906
mae: 0.030443247394804855
r2: 0.7122406411076679
pearson: 0.8444644862409418

=== Experiment 1813 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006356284609925582
rmse: 0.07972631065040939
mae: 0.03153431410314788
r2: 0.7133979245532572
pearson: 0.8502569787986458

=== Experiment 1901 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005941439642964342
rmse: 0.07708073457722327
mae: 0.03272302781834931
r2: 0.7321031015263064
pearson: 0.8576296511928634

=== Experiment 1917 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.012639914972363555
rmse: 0.11242737643636248
mae: 0.04584427709190156
r2: 0.430071797148151
pearson: 0.6581303524299736

=== Experiment 1879 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007079126989448777
rmse: 0.08413754803563495
mae: 0.04390675584605581
r2: 0.6808052798078177
pearson: 0.8367675650839433

=== Experiment 1936 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00745105528741358
rmse: 0.0863194954075473
mae: 0.0320410244557898
r2: 0.6640351965507472
pearson: 0.8209840625047881

=== Experiment 1842 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007523704495542324
rmse: 0.08673929037951789
mae: 0.033655358663808956
r2: 0.660759475731584
pearson: 0.8416839242267867

=== Experiment 1811 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00793767219040385
rmse: 0.08909361475663591
mae: 0.035362436893961
r2: 0.6420938545713939
pearson: 0.8258053801294291

=== Experiment 1729 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008154289075432747
rmse: 0.09030110229356421
mae: 0.035352670042725506
r2: 0.6323266945658231
pearson: 0.8036436653868619

=== Experiment 1989 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008922039863156438
rmse: 0.09445655013368019
mae: 0.03701468660651592
r2: 0.597709149460325
pearson: 0.7933122062748742

=== Experiment 1513 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008589542404846261
rmse: 0.09267978422960566
mae: 0.034928081898765805
r2: 0.6127013135122081
pearson: 0.8169562454836158

=== Experiment 1840 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.012953241999429153
rmse: 0.11381231040370436
mae: 0.05142036936246898
r2: 0.4159440194035341
pearson: 0.6483908167807034

=== Experiment 1680 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00628139124247473
rmse: 0.07925522848667292
mae: 0.030980735593766816
r2: 0.7167748335285243
pearson: 0.8526330794575611

=== Experiment 1577 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.01105809428701254
rmse: 0.10515747375727766
mae: 0.046658811484572016
r2: 0.5013953956381025
pearson: 0.7081427010295444

=== Experiment 1922 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006804035273193403
rmse: 0.08248657632120152
mae: 0.04138955310979677
r2: 0.6932090442166488
pearson: 0.8418231404938499

=== Experiment 1597 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006131108534132805
rmse: 0.07830139548011138
mae: 0.029926457243882265
r2: 0.7235510147031834
pearson: 0.8645707099525812

=== Experiment 1650 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005637034049069913
rmse: 0.07508018413050085
mae: 0.02912390343251658
r2: 0.7458286157758585
pearson: 0.8690434676552239

=== Experiment 1683 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006646150393714113
rmse: 0.08152392528401778
mae: 0.030780825700166006
r2: 0.7003280039419203
pearson: 0.8374747971298904

=== Experiment 1716 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005994146101216304
rmse: 0.07742187094882365
mae: 0.030671017364947754
r2: 0.7297265905216117
pearson: 0.8547831732573608

=== Experiment 1834 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006193703177674568
rmse: 0.07870008372088665
mae: 0.030650321937872965
r2: 0.7207286497758949
pearson: 0.8501727608898254

=== Experiment 1884 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.011392323074214703
rmse: 0.10673482596704181
mae: 0.04332445777714977
r2: 0.486325166728496
pearson: 0.7435210109399509

=== Experiment 1902 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006270513931363564
rmse: 0.07918657671198802
mae: 0.03244862490825772
r2: 0.7172652866990561
pearson: 0.8515562201391181

=== Experiment 1752 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0067145992794281
rmse: 0.08194265848401613
mae: 0.03153570035336623
r2: 0.6972416738117351
pearson: 0.8407369407801962

=== Experiment 1930 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009511032944067176
rmse: 0.09752452483384462
mae: 0.0376424676668201
r2: 0.5711517106777396
pearson: 0.8275631551399029

=== Experiment 1728 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00616946094054283
rmse: 0.07854591612899317
mae: 0.028620553806422547
r2: 0.7218217215783405
pearson: 0.8607531510283265

=== Experiment 1945 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006602916817721104
rmse: 0.08125833383549717
mae: 0.03242512680798955
r2: 0.7022773868548933
pearson: 0.8478836891718796

=== Experiment 1846 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00710792649743397
rmse: 0.08430851972033414
mae: 0.032247186794177825
r2: 0.6795067226683981
pearson: 0.840764872400071

=== Experiment 1714 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005812827741912987
rmse: 0.07624190279572636
mae: 0.03538835498998736
r2: 0.7379021555382854
pearson: 0.8609305192592575

=== Experiment 1778 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006483442291544988
rmse: 0.08051982545649852
mae: 0.035917286768440485
r2: 0.7076644406554138
pearson: 0.8425424635344325

=== Experiment 1898 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00549832122247149
rmse: 0.07415066569135768
mae: 0.03238818950742537
r2: 0.7520831160749976
pearson: 0.868149773179272

=== Experiment 1910 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005850265119151052
rmse: 0.0764870258223645
mae: 0.030637826528521338
r2: 0.7362141206760708
pearson: 0.8588747624978658

=== Experiment 1972 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0065443369648581
rmse: 0.08089707636780269
mae: 0.030411501455711108
r2: 0.7049187266375211
pearson: 0.8488709990811895

=== Experiment 1571 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007040235649880718
rmse: 0.08390611211276994
mae: 0.04103669724495977
r2: 0.6825588731915538
pearson: 0.8290823298328844

=== Experiment 1904 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007024073795413007
rmse: 0.08380974761573386
mae: 0.037464814034294816
r2: 0.6832876040961269
pearson: 0.8348723097163164

=== Experiment 1795 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006854990946708266
rmse: 0.08279487270784505
mae: 0.03239679993181213
r2: 0.6909114753251707
pearson: 0.8350629974382434

=== Experiment 1992 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057011529259787105
rmse: 0.07550597940546637
mae: 0.030873275349507328
r2: 0.7429375238369167
pearson: 0.8643027422167165

=== Experiment 1877 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006127720344610565
rmse: 0.07827975692738554
mae: 0.029369222346588415
r2: 0.723703786677485
pearson: 0.8549163098876013

=== Experiment 1657 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006015437884403279
rmse: 0.07755925402170445
mae: 0.02945788432209795
r2: 0.7287665533889416
pearson: 0.8543027933434827

=== Experiment 1567 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005697029954870405
rmse: 0.07547867218539556
mae: 0.032484322029637996
r2: 0.7431234267895318
pearson: 0.863117535300991

=== Experiment 1611 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005916615540740483
rmse: 0.07691953939500992
mae: 0.028002433282830107
r2: 0.7332224093696573
pearson: 0.8601622468569479

=== Experiment 1805 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064925972571754165
rmse: 0.08057665454196654
mae: 0.03068808326383887
r2: 0.7072516472845464
pearson: 0.8411927776958797

=== Experiment 1870 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00502392778589789
rmse: 0.07087967117515354
mae: 0.027996618225584434
r2: 0.773473307333582
pearson: 0.8806121769732391

=== Experiment 1646 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005618508913134047
rmse: 0.07495671359614192
mae: 0.03303825767006494
r2: 0.7466639060016708
pearson: 0.8663068067435612

=== Experiment 2035 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0075073053485223105
rmse: 0.08664470756210278
mae: 0.047239131448320364
r2: 0.6614989060529
pearson: 0.8205038927209453

=== Experiment 2026 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006632892286236335
rmse: 0.08144257047905804
mae: 0.03620889051060956
r2: 0.7009258061727541
pearson: 0.8464172370202587

=== Experiment 2042 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008931710527849692
rmse: 0.09450772734464463
mae: 0.041413736343672465
r2: 0.5972731034456914
pearson: 0.7734869309626701

=== Experiment 2039 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007855952578805355
rmse: 0.08863381171316821
mae: 0.052333021411760244
r2: 0.6457785558908185
pearson: 0.8180883712630237

=== Experiment 2012 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006004275231170505
rmse: 0.07748725850854775
mae: 0.029593156440801585
r2: 0.7292698725101479
pearson: 0.8646807310703628

=== Experiment 2009 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007623630880219709
rmse: 0.08731340607386537
mae: 0.037224753729084355
r2: 0.6562538390274459
pearson: 0.8339508841225729

=== Experiment 2020 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008264798567692673
rmse: 0.09091093755809954
mae: 0.041338655683928305
r2: 0.6273438701987701
pearson: 0.8226926967270967

=== Experiment 2021 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006166600275320052
rmse: 0.07852770387143668
mae: 0.03339629552194533
r2: 0.7219507077141605
pearson: 0.849842164764249

=== Experiment 2073 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0069989642993229595
rmse: 0.08365981292904592
mae: 0.0447544029237778
r2: 0.6844197802232926
pearson: 0.8340234272462714

=== Experiment 2049 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0056062427846695125
rmse: 0.0748748474767696
mae: 0.028809898101226354
r2: 0.7472169803354006
pearson: 0.8648644606950208

=== Experiment 2003 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0068837749022107745
rmse: 0.08296851753653776
mae: 0.035756926846003394
r2: 0.6896136194403502
pearson: 0.834299279147754

=== Experiment 2019 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005692375215099182
rmse: 0.07544783108280305
mae: 0.028544584154425966
r2: 0.7433333069571086
pearson: 0.862616594616295

=== Experiment 2063 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006282093910957173
rmse: 0.0792596613098818
mae: 0.035264855223271635
r2: 0.716743150515919
pearson: 0.8506952007529748

=== Experiment 2096 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006712918746285571
rmse: 0.0819324035183002
mae: 0.03245271941129879
r2: 0.6973174483114726
pearson: 0.8461151332111257

=== Experiment 2083 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005648827948485215
rmse: 0.07515868511679283
mae: 0.02828590238844744
r2: 0.7452968340421147
pearson: 0.8638225031154916

=== Experiment 2043 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006388889497066538
rmse: 0.07993052919295943
mae: 0.032310502853518816
r2: 0.7119277845425799
pearson: 0.8471598206854944

=== Experiment 2029 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0059681248593028945
rmse: 0.07725363978029058
mae: 0.031419102931594244
r2: 0.730899876866663
pearson: 0.8550998631615028

=== Experiment 2015 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007669638832543111
rmse: 0.08757647419566006
mae: 0.036380720657912424
r2: 0.6541793607068322
pearson: 0.8134823067564089

=== Experiment 2034 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068085597997741745
rmse: 0.08251399759903875
mae: 0.03203918241970564
r2: 0.6930050353045183
pearson: 0.8425541013138558

=== Experiment 2116 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006499870402871345
rmse: 0.08062177375170647
mae: 0.034370497903833
r2: 0.7069237043462733
pearson: 0.8435570740206328

=== Experiment 2053 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005663428949727703
rmse: 0.07525575692083432
mae: 0.029747282683853828
r2: 0.7446384813224829
pearson: 0.8632018403122095

=== Experiment 2086 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005674705531924694
rmse: 0.07533064138798165
mae: 0.028831628305252485
r2: 0.744130025194424
pearson: 0.866915626636709

=== Experiment 2055 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007526854645472871
rmse: 0.08675744720468019
mae: 0.04515864470705659
r2: 0.6606174368577026
pearson: 0.8179926744170096

=== Experiment 2017 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006643449591442459
rmse: 0.08150735912445244
mae: 0.03772568285508623
r2: 0.7004497819276352
pearson: 0.8411484308418974

=== Experiment 2064 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006153791349293762
rmse: 0.07844610474264328
mae: 0.031320103571437406
r2: 0.7225282565510464
pearson: 0.8544498260481739

=== Experiment 2101 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006632543905161473
rmse: 0.08144043163663534
mae: 0.03471999173528267
r2: 0.7009415145220854
pearson: 0.8403566636975314

=== Experiment 2072 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007138340958860111
rmse: 0.08448870314343872
mae: 0.035847804720116644
r2: 0.6781353479891217
pearson: 0.8413645297824357

=== Experiment 2147 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00609466272229216
rmse: 0.07806832086251221
mae: 0.038230800106309
r2: 0.7251943403180496
pearson: 0.8531984849580784

=== Experiment 2010 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007612328889249856
rmse: 0.08724866124617532
mae: 0.04778912183309312
r2: 0.6567634408258901
pearson: 0.8221720177756129

=== Experiment 2094 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008091118987662413
rmse: 0.08995064751107917
mae: 0.040772611153404956
r2: 0.6351750060201056
pearson: 0.8066221387552504

=== Experiment 2070 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006097285371631507
rmse: 0.07808511619784853
mae: 0.03091342262554218
r2: 0.7250760862136547
pearson: 0.8552877563747736

=== Experiment 2084 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007472347633098033
rmse: 0.086442741934173
mae: 0.033480046007319776
r2: 0.6630751340553134
pearson: 0.8284549293165251

=== Experiment 2144 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006959224399258957
rmse: 0.0834219659277996
mae: 0.03679078918546389
r2: 0.6862116348263109
pearson: 0.8317140853503497

=== Experiment 2200 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007960424993381178
rmse: 0.08922121380804668
mae: 0.04604804611264972
r2: 0.6410679406994206
pearson: 0.8275561729394165

=== Experiment 2088 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006780900088660432
rmse: 0.08234622085232832
mae: 0.043710851659618126
r2: 0.6942521995046673
pearson: 0.8394293328663199

=== Experiment 2100 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00598148405284549
rmse: 0.07734005464728796
mae: 0.02947165425440748
r2: 0.730297516709658
pearson: 0.8617812949836644

=== Experiment 2118 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006691550405263478
rmse: 0.08180189732068248
mae: 0.0324506390493651
r2: 0.6982809374032328
pearson: 0.8467666977047374

=== Experiment 2185 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006021732360725074
rmse: 0.077599821911684
mae: 0.03334732125338293
r2: 0.7284827382220026
pearson: 0.8536204645341847

=== Experiment 2135 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006837813826021188
rmse: 0.08269107464546091
mae: 0.041615721450552645
r2: 0.6916859841367221
pearson: 0.8339841651206802

=== Experiment 2069 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00560448524743954
rmse: 0.07486311005722071
mae: 0.031133487182830776
r2: 0.7472962269155488
pearson: 0.8647349516876685

=== Experiment 2028 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006380623137151703
rmse: 0.0798788028024438
mae: 0.03319287555893802
r2: 0.7123005110728364
pearson: 0.8490646565084418

=== Experiment 2102 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006592379537102055
rmse: 0.08119346979346341
mae: 0.03240542579153473
r2: 0.7027525082001942
pearson: 0.8397800343100672

=== Experiment 2050 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00552961083696225
rmse: 0.074361353114116
mae: 0.027102167349917085
r2: 0.7506722811292215
pearson: 0.8666848169571543

=== Experiment 2037 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007061716186056241
rmse: 0.08403401802874977
mae: 0.038318391802276
r2: 0.6815903252697064
pearson: 0.8412373831772336

=== Experiment 2225 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0070379306763336934
rmse: 0.08389237555543229
mae: 0.03793006246133491
r2: 0.6826628034342925
pearson: 0.8278556796887094

=== Experiment 2136 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006650165808251719
rmse: 0.08154854878078284
mae: 0.03319388560213565
r2: 0.7001469506677387
pearson: 0.8463557543188303

=== Experiment 2195 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007065291838163549
rmse: 0.08405529036392384
mae: 0.03680008259707353
r2: 0.6814291006899528
pearson: 0.8257402947232809

=== Experiment 2163 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007384140897747237
rmse: 0.08593102407016477
mae: 0.04422556002371175
r2: 0.6670523369295276
pearson: 0.8290103532973596

=== Experiment 2040 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006624691591344182
rmse: 0.08139220841913666
mae: 0.029177844787822062
r2: 0.7012955718960394
pearson: 0.8548221508947546

=== Experiment 2231 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007381608509712851
rmse: 0.08591628780221391
mae: 0.04572852837375628
r2: 0.6671665211914087
pearson: 0.8226522656538104

=== Experiment 2013 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006077641121554037
rmse: 0.07795922730218685
mae: 0.030062254105247932
r2: 0.7259618368035522
pearson: 0.8565659078079668

=== Experiment 2204 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007542441098944729
rmse: 0.08684722850468361
mae: 0.04905266849965701
r2: 0.6599146505307794
pearson: 0.8394059743848473

=== Experiment 2150 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006414587134084954
rmse: 0.08009111769781312
mae: 0.0398704363282108
r2: 0.7107690893998145
pearson: 0.8487992750049733

=== Experiment 2131 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007262089121454783
rmse: 0.08521789202658549
mae: 0.04616821242627691
r2: 0.6725555978034594
pearson: 0.8331424600021713

=== Experiment 2126 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007678451352071228
rmse: 0.08762677303239706
mae: 0.04530531527006228
r2: 0.6537820080799959
pearson: 0.8294040959386837

=== Experiment 2140 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007091605508400296
rmse: 0.08421167085624352
mae: 0.031971611016693526
r2: 0.6802426288805099
pearson: 0.8441702559551859

=== Experiment 2182 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006430399236322191
rmse: 0.08018977014758298
mae: 0.028132293717875307
r2: 0.7100561286693758
pearson: 0.8493606174001559

=== Experiment 2191 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005981312325883454
rmse: 0.07733894443217759
mae: 0.02834352492055044
r2: 0.7303052598027935
pearson: 0.8568759049925736

=== Experiment 2260 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006096671152079428
rmse: 0.07808118308580773
mae: 0.0345313858331382
r2: 0.7251037811029193
pearson: 0.8533819241116449

=== Experiment 2038 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006036282466372411
rmse: 0.07769351624410116
mae: 0.02906963602586231
r2: 0.7278266803623556
pearson: 0.8535030293826129

=== Experiment 2240 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0064761165694561
rmse: 0.08047432242309406
mae: 0.04001072556213085
r2: 0.707994754240105
pearson: 0.8436134756213676

=== Experiment 2161 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007448481133319442
rmse: 0.08630458350122223
mae: 0.04109817630244893
r2: 0.66415126402588
pearson: 0.8216025132315506

=== Experiment 2261 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0071670364805252965
rmse: 0.08465835151079483
mae: 0.038825072402783135
r2: 0.6768414795471034
pearson: 0.8233586744664309

=== Experiment 2044 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006912354938552129
rmse: 0.08314057335953445
mae: 0.03066622800703315
r2: 0.6883249581807547
pearson: 0.8435073908604318

=== Experiment 2166 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006092670473473589
rmse: 0.07805556017013515
mae: 0.03355003945493772
r2: 0.7252841699404233
pearson: 0.8550776972240881

=== Experiment 2237 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007709413379490358
rmse: 0.0878032651983419
mae: 0.05085484643174527
r2: 0.6523859438910977
pearson: 0.8272109164539808

=== Experiment 2201 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006616273025685877
rmse: 0.08134047593717335
mae: 0.037964787894077444
r2: 0.7016751613162782
pearson: 0.8422247846094978

=== Experiment 2002 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008558216114852973
rmse: 0.09251062703740026
mae: 0.049671171802218445
r2: 0.6141138021402508
pearson: 0.8245306892123523

=== Experiment 2214 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005669901101043614
rmse: 0.07529874568041366
mae: 0.0291370488539707
r2: 0.74434665486826
pearson: 0.8653487842944712

=== Experiment 2259 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0064877391402879
rmse: 0.08054650296746532
mae: 0.03079716434834367
r2: 0.7074706976367837
pearson: 0.8454680799495725

=== Experiment 2243 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007173145937757367
rmse: 0.08469442684000741
mae: 0.03500955004776824
r2: 0.6765660068094872
pearson: 0.8294950163821148

=== Experiment 2229 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006751755489618762
rmse: 0.08216906650083572
mae: 0.035577780505316306
r2: 0.695566316647937
pearson: 0.8345649876834815

=== Experiment 2171 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006583237672084126
rmse: 0.08113715346303521
mae: 0.035071725186951876
r2: 0.7031647108702099
pearson: 0.8479703953509806

=== Experiment 2235 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007603046180498525
rmse: 0.0871954481638722
mae: 0.03914828384137385
r2: 0.6571819940778549
pearson: 0.8153968088127692

=== Experiment 2309 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007092007316074493
rmse: 0.08421405652309176
mae: 0.046911317429216316
r2: 0.6802245115493295
pearson: 0.8338856325831536

=== Experiment 2122 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007114731715460978
rmse: 0.0843488690822881
mae: 0.04003061671119056
r2: 0.679199878382766
pearson: 0.8319496147348636

=== Experiment 2099 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006896509264449576
rmse: 0.08304522421216994
mae: 0.03478620985010272
r2: 0.6890394326518241
pearson: 0.8419416655435581

=== Experiment 2205 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007026605936623748
rmse: 0.08382485273845548
mae: 0.042675787192584906
r2: 0.683173430963415
pearson: 0.8322075161994102

=== Experiment 2236 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006545263529251868
rmse: 0.08090280297524845
mae: 0.039080300109449406
r2: 0.7048769482568182
pearson: 0.8414397632589709

=== Experiment 2212 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0077820738044695505
rmse: 0.08821606318845537
mae: 0.052924105452229725
r2: 0.6491097173090847
pearson: 0.8239829011352383

=== Experiment 2197 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006291370731491526
rmse: 0.07931816142278846
mae: 0.035432061990568524
r2: 0.7163248627610661
pearson: 0.8474959015445703

=== Experiment 2247 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0071098273235385406
rmse: 0.08431979200364847
mae: 0.044709069341355344
r2: 0.6794210152559605
pearson: 0.8355240128594229

=== Experiment 2275 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00693832984533447
rmse: 0.0832966376592385
mae: 0.03641710029270178
r2: 0.6871537610663696
pearson: 0.8300387369685364

=== Experiment 2071 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0070107459474957464
rmse: 0.08373019734537682
mae: 0.03860160426322877
r2: 0.6838885508926816
pearson: 0.8303975411635639

=== Experiment 2106 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005807028151925764
rmse: 0.07620385916688055
mae: 0.027586174111527067
r2: 0.7381636564982146
pearson: 0.8594388283526101

=== Experiment 2142 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006863397691640969
rmse: 0.08284562566388746
mae: 0.03854854752920973
r2: 0.6905324188962465
pearson: 0.8316471821919925

=== Experiment 2111 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005950888493236701
rmse: 0.07714200213396526
mae: 0.02905504613980406
r2: 0.7316770570262827
pearson: 0.859278101230261

=== Experiment 2292 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007360572007856821
rmse: 0.08579377604381813
mae: 0.03832267137735607
r2: 0.6681150478012248
pearson: 0.8179574576214809

=== Experiment 2154 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00570391900426326
rmse: 0.07552429413283689
mae: 0.0280397356349159
r2: 0.7428128025845104
pearson: 0.862478675252599

=== Experiment 2262 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005931401742097498
rmse: 0.0770155941488313
mae: 0.02896459239326632
r2: 0.7325557060583743
pearson: 0.8569318725258962

=== Experiment 2090 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006437638303643301
rmse: 0.08023489455120697
mae: 0.03197685128506082
r2: 0.7097297223100241
pearson: 0.85035478038158

=== Experiment 2286 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007054753927302425
rmse: 0.08399258257312026
mae: 0.04525198962984826
r2: 0.6819042504525915
pearson: 0.832530136850388

=== Experiment 2085 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005128282193537888
rmse: 0.07161202548132463
mae: 0.026569667331520993
r2: 0.7687680130229819
pearson: 0.8786672315799285

=== Experiment 2321 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0070031332839625956
rmse: 0.08368472551166428
mae: 0.035382244901218794
r2: 0.6842318025408018
pearson: 0.8301693987618968

=== Experiment 2244 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006090476831934413
rmse: 0.07804150710957863
mae: 0.029500655712804493
r2: 0.7253830802719292
pearson: 0.8601557797900651

=== Experiment 2186 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007208984305247295
rmse: 0.08490573776398916
mae: 0.039940153650517556
r2: 0.6749500705930378
pearson: 0.8221754264691793

=== Experiment 2180 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005991771080243883
rmse: 0.07740653125055974
mae: 0.03209741712175695
r2: 0.7298336791719315
pearson: 0.8543376280246685

=== Experiment 2372 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006764645655246642
rmse: 0.08224746595030538
mae: 0.04102093316902165
r2: 0.6949851047531717
pearson: 0.8386740412343819

=== Experiment 2332 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006332128643736119
rmse: 0.079574673381272
mae: 0.03521914648661322
r2: 0.7144871064368863
pearson: 0.8494712979379626

=== Experiment 2155 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005948502407953341
rmse: 0.07712653504438885
mae: 0.026833676610570283
r2: 0.731784644561513
pearson: 0.8571646457756784

=== Experiment 2146 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008628760422749022
rmse: 0.09289112133432895
mae: 0.039777366798703834
r2: 0.6109329903461423
pearson: 0.8020504407684587

=== Experiment 2175 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006110024355549878
rmse: 0.07816664477608003
mae: 0.028699611994955767
r2: 0.7245016910356309
pearson: 0.8537592247699246

=== Experiment 2132 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006473416518555565
rmse: 0.08045754482057954
mae: 0.04032503274249977
r2: 0.7081164983468268
pearson: 0.8450907167541538

=== Experiment 2168 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007383746161505952
rmse: 0.08592872721916665
mae: 0.038565369670120454
r2: 0.6670701354129103
pearson: 0.8189952793102234

=== Experiment 2271 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007381400324167884
rmse: 0.08591507623326586
mae: 0.034630614760026886
r2: 0.6671759081860018
pearson: 0.8424990765856376

=== Experiment 2025 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005850255837833985
rmse: 0.0764869651498475
mae: 0.03721988504457589
r2: 0.7362145391665724
pearson: 0.8607248033398216

=== Experiment 2344 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005842326950062063
rmse: 0.07643511594850931
mae: 0.0283666753644809
r2: 0.736572049226438
pearson: 0.8585325198050902

=== Experiment 2431 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008716767944402357
rmse: 0.09336363287920173
mae: 0.040921604963174894
r2: 0.606964769929866
pearson: 0.7847094973724692

=== Experiment 2421 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00690575924244403
rmse: 0.08310089796412569
mae: 0.04147286024424353
r2: 0.6886223552152808
pearson: 0.8394327177153045

=== Experiment 2248 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006941934114327891
rmse: 0.08331826999120835
mae: 0.04417391322213157
r2: 0.6869912461638163
pearson: 0.8341278339444714

=== Experiment 2394 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0074208750998390905
rmse: 0.08614450127453922
mae: 0.04288208260810056
r2: 0.6653960079251645
pearson: 0.8206424929931773

=== Experiment 2289 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062998300303739015
rmse: 0.07937146861671328
mae: 0.033321814963960104
r2: 0.7159434366976818
pearson: 0.8461773706958812

=== Experiment 2159 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008186053858489528
rmse: 0.09047681392759986
mae: 0.051496079318279944
r2: 0.630894434478543
pearson: 0.8086497775257034

=== Experiment 2358 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007577239080727492
rmse: 0.08704733815992016
mae: 0.03776514625486444
r2: 0.6583456248479576
pearson: 0.8221557985973826

=== Experiment 2347 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005757623352058388
rmse: 0.07587900468547534
mae: 0.02978353832971918
r2: 0.7403912971795192
pearson: 0.8630658155292893

=== Experiment 2031 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006042049306553283
rmse: 0.07773062013488174
mae: 0.028979901207391204
r2: 0.7275666560767804
pearson: 0.8558497864420901

=== Experiment 2324 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007974935020968127
rmse: 0.08930249168398453
mae: 0.05075436175062025
r2: 0.6404136899418766
pearson: 0.8205757271111924

=== Experiment 2115 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005976429220459844
rmse: 0.07730736847454998
mae: 0.027422415206733643
r2: 0.730525436877125
pearson: 0.8586622207417403

=== Experiment 2312 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006532509454446138
rmse: 0.08082394109696792
mae: 0.03347311767262857
r2: 0.7054520238763111
pearson: 0.8413838579456142

=== Experiment 2206 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006222458949087604
rmse: 0.0788825642907709
mae: 0.029309389203874735
r2: 0.719432064699264
pearson: 0.8524874449784509

=== Experiment 2307 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006652262938800507
rmse: 0.08156140593933203
mae: 0.03179363801092542
r2: 0.7000523919743117
pearson: 0.8448217211856145

=== Experiment 2124 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006557859989916413
rmse: 0.08098061490206414
mae: 0.028298124130709933
r2: 0.7043089793895803
pearson: 0.8511519335308316

=== Experiment 2384 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006870057669364881
rmse: 0.08288581102556022
mae: 0.04144132527375836
r2: 0.6902321234319557
pearson: 0.8398247237871656

=== Experiment 2333 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005716687363555483
rmse: 0.07560877834984164
mae: 0.028833864103070584
r2: 0.7422370828838087
pearson: 0.8686140466332927

=== Experiment 2412 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006438392919869669
rmse: 0.08023959695729826
mae: 0.035248006857878156
r2: 0.7096956969965093
pearson: 0.8472246227273118

=== Experiment 2188 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007267497832046427
rmse: 0.08524962071497108
mae: 0.03396891585893923
r2: 0.6723117214234378
pearson: 0.8220568004453046

=== Experiment 2414 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007478556118802465
rmse: 0.086478645449628
mae: 0.051725414177439824
r2: 0.6627951961674641
pearson: 0.8326006153020393

=== Experiment 2279 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007172771083132598
rmse: 0.08469221382826521
mae: 0.03794655151732626
r2: 0.6765829088395316
pearson: 0.8266955141348558

=== Experiment 2233 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006389091566157561
rmse: 0.07993179321244809
mae: 0.031224177644442703
r2: 0.7119186733362031
pearson: 0.8534281460936616

=== Experiment 2445 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008159896213275624
rmse: 0.09033214385408787
mae: 0.054664504828931745
r2: 0.6320738711883779
pearson: 0.8125458378495625

=== Experiment 2018 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009185222223990267
rmse: 0.09583956502400387
mae: 0.04745636398572601
r2: 0.5858423726457433
pearson: 0.7861127893192056

=== Experiment 2284 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005653212766339391
rmse: 0.0751878498584671
mae: 0.030277618174970936
r2: 0.7450991245349756
pearson: 0.8634632624041352

=== Experiment 2362 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0067639193023865205
rmse: 0.0822430501768175
mae: 0.03956523466403179
r2: 0.6950178556839423
pearson: 0.8578398130688175

=== Experiment 2160 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006264822437693485
rmse: 0.07915063131582392
mae: 0.028584596215064356
r2: 0.717521913643622
pearson: 0.8555071501632163

=== Experiment 2406 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008145988871641583
rmse: 0.09025513210694217
mae: 0.03954458924250155
r2: 0.6327009471015674
pearson: 0.8168342645149087

=== Experiment 2222 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005744975463387579
rmse: 0.07579561638635561
mae: 0.027367461278493526
r2: 0.7409615849129245
pearson: 0.8713181424630283

=== Experiment 2343 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0060711084433304335
rmse: 0.07791731799369402
mae: 0.031069698942044675
r2: 0.726256392389403
pearson: 0.8570625875594007

=== Experiment 2221 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006043354684401483
rmse: 0.07773901648722785
mae: 0.03437402587854794
r2: 0.7275077971641457
pearson: 0.8551329866641245

=== Experiment 2335 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006297314615861473
rmse: 0.07935562119888845
mae: 0.03642556100474348
r2: 0.7160568556309288
pearson: 0.8472807656829497

=== Experiment 2336 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007784741897854632
rmse: 0.0882311843842903
mae: 0.04558460043010228
r2: 0.648989414152669
pearson: 0.8107767389964842

=== Experiment 2263 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007368444259676013
rmse: 0.0858396427047318
mae: 0.03382376529690822
r2: 0.6677600914315402
pearson: 0.8475932504890734

=== Experiment 2104 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005978874481254118
rmse: 0.07732318204299482
mae: 0.03785566906893404
r2: 0.7304151811441528
pearson: 0.8565935556383197

=== Experiment 2303 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008320233821722069
rmse: 0.09121531571902862
mae: 0.048359956297832025
r2: 0.6248443189935059
pearson: 0.8010504934391597

=== Experiment 2036 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062208825341346615
rmse: 0.07887257149437098
mae: 0.03323453464914719
r2: 0.7195031445556588
pearson: 0.8485685006944397

=== Experiment 2203 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0071562481559123555
rmse: 0.08459461067888636
mae: 0.04097689468047333
r2: 0.677327920355596
pearson: 0.8254250816745414

=== Experiment 2399 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0077378090984667955
rmse: 0.0879648173900611
mae: 0.04796611754930433
r2: 0.6511055934203618
pearson: 0.813848429928196

=== Experiment 2051 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007285552469558023
rmse: 0.08535544780245737
mae: 0.042681503421991944
r2: 0.6714976457644934
pearson: 0.8340421712176146

=== Experiment 2448 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007123896729399623
rmse: 0.08440317961664491
mae: 0.03443184599794406
r2: 0.6787866319381034
pearson: 0.8292058205855218

=== Experiment 2215 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007027015350397921
rmse: 0.08382729478158006
mae: 0.03073875109296133
r2: 0.6831549706765343
pearson: 0.8457838854895828

=== Experiment 2369 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0060177361711997645
rmse: 0.07757406893543592
mae: 0.030490501647031872
r2: 0.7286629246488414
pearson: 0.8589405196848764

=== Experiment 2239 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006544175295695703
rmse: 0.08089607713415839
mae: 0.03300988378105612
r2: 0.7049260162288975
pearson: 0.842422924915808

=== Experiment 2355 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006299672589033334
rmse: 0.0793704768099155
mae: 0.027310003399588827
r2: 0.7159505356584328
pearson: 0.8657550504043692

=== Experiment 2149 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006728782018379754
rmse: 0.08202915346619002
mae: 0.04316896210612625
r2: 0.6966021803547058
pearson: 0.8405254087641452

=== Experiment 2403 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007576579202501333
rmse: 0.08704354773618395
mae: 0.03912414378441354
r2: 0.6583753784667405
pearson: 0.8284480432715146

=== Experiment 2158 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065422859128374785
rmse: 0.0808843984513545
mae: 0.04012037472207291
r2: 0.7050112076703938
pearson: 0.8434775278456313

=== Experiment 2296 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0066113613216128315
rmse: 0.08131027808102019
mae: 0.0313895366279579
r2: 0.7018966278911256
pearson: 0.8444528736500779

=== Experiment 2396 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006208751776896169
rmse: 0.0787956329811251
mae: 0.033241817790709095
r2: 0.7200501150603879
pearson: 0.8535390773123261

=== Experiment 2318 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006521461564524128
rmse: 0.08075556677111571
mae: 0.028658033018959764
r2: 0.7059501683703462
pearson: 0.8403309105603118

=== Experiment 2223 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007761417909983416
rmse: 0.08809890981154885
mae: 0.051596670933520536
r2: 0.6500410825001101
pearson: 0.823317012170434

=== Experiment 2183 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007848516552184883
rmse: 0.08859185375746961
mae: 0.03879339735077704
r2: 0.6461138430582898
pearson: 0.8143676402808442

=== Experiment 2456 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005662779748834877
rmse: 0.07525144350000787
mae: 0.029802132729126407
r2: 0.7446677535050052
pearson: 0.8630756294416159

=== Experiment 2054 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0051875298033226625
rmse: 0.07202450835182884
mae: 0.029084790665140435
r2: 0.7660965643746537
pearson: 0.8768413862285886

=== Experiment 2484 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006501597124019293
rmse: 0.08063248181731289
mae: 0.042128248360590426
r2: 0.7068458472497003
pearson: 0.8497086980783835

=== Experiment 2452 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006336041470080393
rmse: 0.07959925546184708
mae: 0.029983109033103996
r2: 0.7143106788191897
pearson: 0.8471173992749105

=== Experiment 2472 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00889323930801663
rmse: 0.09430397291745789
mae: 0.061177879915931495
r2: 0.5990077538154852
pearson: 0.8087676740945849

=== Experiment 2495 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067498082681057845
rmse: 0.08215721677433933
mae: 0.04504361084222206
r2: 0.6956541160089194
pearson: 0.8425318873880036

=== Experiment 2430 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006475543744617832
rmse: 0.08047076329088616
mae: 0.032397446099219004
r2: 0.708020582659943
pearson: 0.8492487858727491

=== Experiment 2462 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005664555091690192
rmse: 0.07526323864736484
mae: 0.02936581095446774
r2: 0.7445877040770473
pearson: 0.8635161769276855

=== Experiment 2030 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0057811948149433965
rmse: 0.07603416873316493
mae: 0.03036587908691877
r2: 0.7393284702926284
pearson: 0.8600484958832388

=== Experiment 2283 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0065121655637165
rmse: 0.08069798983690052
mae: 0.030057186714856018
r2: 0.7063693209552793
pearson: 0.8469021169162274

=== Experiment 2375 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005449972258910538
rmse: 0.07382392741456213
mae: 0.026766120141430188
r2: 0.7542631495619545
pearson: 0.8715763644485762

=== Experiment 2192 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00644116022303752
rmse: 0.08025683910444967
mae: 0.030706816664500976
r2: 0.7095709205146541
pearson: 0.843145386424628

=== Experiment 2446 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066352198344636504
rmse: 0.08145685873186892
mae: 0.031558850495577204
r2: 0.7008208580476166
pearson: 0.8392822047100806

=== Experiment 2432 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005841902301173894
rmse: 0.07643233805905648
mae: 0.03093912230671718
r2: 0.7365911964579033
pearson: 0.8585519431186827

=== Experiment 2473 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0055063440348082585
rmse: 0.07420474401821125
mae: 0.029197445455444233
r2: 0.7517213709978422
pearson: 0.8675582194918156

=== Experiment 2385 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007371753576750042
rmse: 0.08585891669914104
mae: 0.034449568790034545
r2: 0.667610875781214
pearson: 0.8218840894233076

=== Experiment 2062 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005969147654143391
rmse: 0.07726025921612864
mae: 0.02889024493323877
r2: 0.7308537594974713
pearson: 0.8607200686366108

=== Experiment 2490 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0062560363250520135
rmse: 0.07909510936241262
mae: 0.034095960475289566
r2: 0.717918075595562
pearson: 0.8580540658677592

=== Experiment 2450 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00762213767575063
rmse: 0.0873048548234898
mae: 0.04604338367962835
r2: 0.6563211669597472
pearson: 0.8186762901498137

=== Experiment 2319 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0061325954927277495
rmse: 0.07831089000086609
mae: 0.033335134179867214
r2: 0.7234839683945979
pearson: 0.8517281035047684

=== Experiment 2441 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006927612816577762
rmse: 0.0832322822982631
mae: 0.037514161045685626
r2: 0.6876369871760845
pearson: 0.8353417217843572

=== Experiment 2411 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005297492516726555
rmse: 0.07278387538958443
mae: 0.027345392434605598
r2: 0.7611383940255635
pearson: 0.8726705796313747

=== Experiment 2294 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007070303729773488
rmse: 0.0840850981433303
mae: 0.03721634073061267
r2: 0.6812031167031627
pearson: 0.8381614604497953

=== Experiment 2381 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006722461070950929
rmse: 0.08199061574930956
mae: 0.03051129171320818
r2: 0.6968871890922099
pearson: 0.8477254657016551

=== Experiment 2440 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007757339739170154
rmse: 0.08807576136015036
mae: 0.044335099952229895
r2: 0.6502249654271398
pearson: 0.8316201333904748

=== Experiment 2156 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0055047243930397925
rmse: 0.07419382988523907
mae: 0.027227855967783782
r2: 0.7517943999323236
pearson: 0.8783508940560573

=== Experiment 2198 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006801475669700481
rmse: 0.08247105958880656
mae: 0.03718766192912411
r2: 0.6933244556115771
pearson: 0.8327548363020542

=== Experiment 2268 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006269421781423279
rmse: 0.0791796803569153
mae: 0.027941455197989577
r2: 0.7173145312591716
pearson: 0.8478707607516571

=== Experiment 2252 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006996646222077867
rmse: 0.08364595759555787
mae: 0.037453862244622674
r2: 0.68452430130601
pearson: 0.8360102128644469

=== Experiment 2293 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006352725059272266
rmse: 0.07970398396110615
mae: 0.030929544279583457
r2: 0.7135584231255372
pearson: 0.8485309851868416

=== Experiment 2425 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005845684927175188
rmse: 0.07645707898667845
mae: 0.03426895989656338
r2: 0.7364206395163662
pearson: 0.8584895831992164

=== Experiment 2208 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005664347058648569
rmse: 0.07526185659846937
mae: 0.029409874287637795
r2: 0.7445970841953317
pearson: 0.8637025543250074

=== Experiment 2380 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006115827576773489
rmse: 0.07820375679450117
mae: 0.029800948991377696
r2: 0.7242400263448514
pearson: 0.8516156067838414

=== Experiment 2423 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007382502227381031
rmse: 0.08592148874048348
mae: 0.032710622112873
r2: 0.6671262238551057
pearson: 0.8310323756267921

=== Experiment 2093 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005855168291781627
rmse: 0.07651907142524422
mae: 0.029967605451297386
r2: 0.7359930387802095
pearson: 0.8606509145605467

=== Experiment 2482 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006933620625291904
rmse: 0.08326836509318472
mae: 0.03752562460709185
r2: 0.6873660977254026
pearson: 0.8311385953748885

=== Experiment 2467 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007403914857818032
rmse: 0.08604600431058976
mae: 0.046733160831613614
r2: 0.6661607377731208
pearson: 0.8237776910381751

=== Experiment 2326 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006730578972480788
rmse: 0.08204010587804472
mae: 0.03430396657531406
r2: 0.6965211564851903
pearson: 0.8373169317866889

=== Experiment 2219 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005847233040958369
rmse: 0.07646720238741815
mae: 0.029316017558143184
r2: 0.7363508357472592
pearson: 0.8637998640375949

=== Experiment 2120 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0065607248371858246
rmse: 0.08099830144630087
mae: 0.03594552936051164
r2: 0.7041798046871182
pearson: 0.8403873262889613

=== Experiment 2361 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00572706552471457
rmse: 0.07567737789270033
mae: 0.03106907963191619
r2: 0.7417691361649237
pearson: 0.8615391382358604

=== Experiment 2345 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007563107839471636
rmse: 0.0869661304156488
mae: 0.047464680385369586
r2: 0.6589827962965492
pearson: 0.824933218129391

=== Experiment 2308 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006443637033013327
rmse: 0.08027226814419365
mae: 0.03687940012051079
r2: 0.7094592422429725
pearson: 0.8425669638911187

=== Experiment 2137 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006753072658027984
rmse: 0.08217708109946462
mae: 0.047056237766093585
r2: 0.6955069261040954
pearson: 0.8485343527838937

=== Experiment 2134 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006651632395159039
rmse: 0.08155754039424583
mae: 0.0287470631798037
r2: 0.7000808229095821
pearson: 0.8370772697404919

=== Experiment 2108 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00579783083390998
rmse: 0.07614348845377378
mae: 0.02945694759501598
r2: 0.738578359519497
pearson: 0.859430006200316

=== Experiment 2209 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007094367891767314
rmse: 0.08422807068767107
mae: 0.03972416176396621
r2: 0.680118074230309
pearson: 0.8342649783392697

=== Experiment 2076 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006104026600806982
rmse: 0.07812827017672272
mae: 0.03217692498702774
r2: 0.7247721271571745
pearson: 0.8517571025472576

=== Experiment 2400 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006930741535600327
rmse: 0.08325107528194652
mae: 0.03131666418788465
r2: 0.6874959146124116
pearson: 0.8330978452718569

=== Experiment 2364 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00631540891236446
rmse: 0.07946954707537007
mae: 0.031049020038664957
r2: 0.715240991765518
pearson: 0.8542652021271924

=== Experiment 2481 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006462325316140829
rmse: 0.0803885894648042
mae: 0.031412931358348384
r2: 0.7086165957820957
pearson: 0.8421569569286854

=== Experiment 2246 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006936317488013748
rmse: 0.08328455732015239
mae: 0.03367183080288877
r2: 0.6872444973722546
pearson: 0.8413978734996528

=== Experiment 2368 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006193769542698524
rmse: 0.0787005053522436
mae: 0.03019442188349583
r2: 0.7207256574061742
pearson: 0.8529236837812887

=== Experiment 2103 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005983019410067225
rmse: 0.07734998002628847
mae: 0.0268808073975383
r2: 0.7302282881282924
pearson: 0.8548673042016376

=== Experiment 2374 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006028182808319269
rmse: 0.07764137304504132
mae: 0.02808597718292467
r2: 0.7281918903790394
pearson: 0.8582380861566977

=== Experiment 2257 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005674816411606213
rmse: 0.07533137733777481
mae: 0.02923307026140142
r2: 0.744125025678386
pearson: 0.8637513623773021

=== Experiment 2496 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008545376860208585
rmse: 0.09244120758735568
mae: 0.03469768887170791
r2: 0.6146927184811781
pearson: 0.8372000978210857

=== Experiment 2359 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007920962806573685
rmse: 0.08899979104792148
mae: 0.04566899531796787
r2: 0.6428472733339339
pearson: 0.835204966798433

=== Experiment 2498 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007301354161235308
rmse: 0.08544796171492511
mae: 0.03509814373106898
r2: 0.6707851544416212
pearson: 0.8334757505652493

=== Experiment 2250 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006129062748088293
rmse: 0.07828833085516827
mae: 0.03237456631415744
r2: 0.7236432582955112
pearson: 0.851414094866417

=== Experiment 2458 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007271700670246335
rmse: 0.08527426733925267
mae: 0.03749666283732832
r2: 0.6721222172988145
pearson: 0.8227713331426636

=== Experiment 2338 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006667860617210195
rmse: 0.08165696918947087
mae: 0.03732064075465168
r2: 0.6993490995198821
pearson: 0.840197684047199

=== Experiment 2409 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007339369498611818
rmse: 0.0856701202205986
mae: 0.030356086631361474
r2: 0.6690710596111438
pearson: 0.8382105004417341

=== Experiment 2465 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007935492872314863
rmse: 0.08908138342164912
mae: 0.0503056399276862
r2: 0.6421921190648352
pearson: 0.820032697337074

=== Experiment 2434 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00663121922670125
rmse: 0.08143229842452716
mae: 0.029312231858082353
r2: 0.7010012436908212
pearson: 0.8407352814078571

=== Experiment 2295 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007678355984241329
rmse: 0.08762622886009262
mae: 0.035087305284293255
r2: 0.6537863081734715
pearson: 0.8136528352391513

=== Experiment 2081 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006187866112059092
rmse: 0.07866299073935018
mae: 0.02959437161552993
r2: 0.7209918404954077
pearson: 0.8552211512823661

=== Experiment 2278 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007624070734402163
rmse: 0.08731592486140294
mae: 0.03261839037138095
r2: 0.6562340061959537
pearson: 0.8337681149257783

=== Experiment 2488 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006401682511895499
rmse: 0.08001051500831312
mae: 0.02993560525761374
r2: 0.7113509531345784
pearson: 0.8468504023393181

=== Experiment 2455 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006837963214818718
rmse: 0.0826919779350979
mae: 0.03204028202177513
r2: 0.6916792482615923
pearson: 0.8320983468673551

=== Experiment 2325 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00669134499940528
rmse: 0.08180064180313795
mae: 0.0323457185858541
r2: 0.6982901990629728
pearson: 0.8372326987328216

=== Experiment 2402 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0063245837840030066
rmse: 0.07952725183233107
mae: 0.036331434215328064
r2: 0.7148273008415056
pearson: 0.8493451029868145

=== Experiment 2001 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005891194589928269
rmse: 0.07675411773923448
mae: 0.02929093773093857
r2: 0.7343686288531639
pearson: 0.8591768485656817

=== Experiment 2494 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0074187613920366794
rmse: 0.08613223201587591
mae: 0.04702778315484482
r2: 0.6654913140796614
pearson: 0.8393466745203869

=== Experiment 2487 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006470145413146709
rmse: 0.08043721410607597
mae: 0.030613420944740857
r2: 0.7082639910499886
pearson: 0.8463621091282576

=== Experiment 2258 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006568897645693979
rmse: 0.08104873623748848
mae: 0.031380168698305066
r2: 0.7038112963486136
pearson: 0.8405697716516055

=== Experiment 2352 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006422493780208774
rmse: 0.08014046281503978
mae: 0.0326524200591071
r2: 0.7104125822060725
pearson: 0.8438151699222612

=== Experiment 2429 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006507228536054857
rmse: 0.08066739450394352
mae: 0.027707961594270067
r2: 0.7065919293595908
pearson: 0.8471160551128979

=== Experiment 2311 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00895476465120668
rmse: 0.09462961825563221
mae: 0.04280975049636307
r2: 0.5962336031704204
pearson: 0.7931731033624131

=== Experiment 2437 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007920219967092452
rmse: 0.08899561768476273
mae: 0.03934492566785562
r2: 0.642880767639194
pearson: 0.806631897462837

=== Experiment 2059 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008650461114333373
rmse: 0.09300785512166901
mae: 0.03910161594747793
r2: 0.6099545157138088
pearson: 0.8111285694899365

=== Experiment 2306 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005795004067561543
rmse: 0.07612492408903632
mae: 0.029109239459335592
r2: 0.7387058171699932
pearson: 0.8627728966250423

=== Experiment 2119 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006398705125273896
rmse: 0.07999190662356971
mae: 0.03343270899735079
r2: 0.7114852021868984
pearson: 0.844368570793774

=== Experiment 2489 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006474821499200827
rmse: 0.080466275539513
mae: 0.029377757656936454
r2: 0.7080531483879113
pearson: 0.8512587766140797

=== Experiment 2471 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007575471644309934
rmse: 0.08703718541123635
mae: 0.04622279300853502
r2: 0.6584253177781171
pearson: 0.81817504692259

=== Experiment 2320 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006153014746023191
rmse: 0.07844115467038454
mae: 0.03265534651131379
r2: 0.7225632732507072
pearson: 0.8570265539807068

=== Experiment 2128 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0058314224430423695
rmse: 0.07636375084450979
mae: 0.029475068528882275
r2: 0.7370637286485154
pearson: 0.8605865296653153

=== Experiment 2014 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006797386690756577
rmse: 0.08244626547489327
mae: 0.029657886392138743
r2: 0.6935088258724604
pearson: 0.8559447638905916

=== Experiment 2427 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005941302092933037
rmse: 0.077079842325559
mae: 0.029985399992413314
r2: 0.7321093035966775
pearson: 0.8557557487400682

=== Experiment 2357 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00609179948107311
rmse: 0.0780499806602994
mae: 0.030638375268992435
r2: 0.7253234426044077
pearson: 0.8516766260029704

=== Experiment 2256 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006103504336372895
rmse: 0.07812492775275312
mae: 0.03509132578875758
r2: 0.7247956758306413
pearson: 0.8574999724677168

=== Experiment 2238 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00582814507352875
rmse: 0.07634228889369737
mae: 0.02859436178350572
r2: 0.7372115037974039
pearson: 0.860328229859862

=== Experiment 2499 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007803228496255318
rmse: 0.08833588453315741
mae: 0.04299295289142058
r2: 0.6481558615673563
pearson: 0.8243751121900923

=== Experiment 2439 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007393535021282254
rmse: 0.08598566753408532
mae: 0.04466266472551484
r2: 0.6666287600340035
pearson: 0.8194573986205469

=== Experiment 2449 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0059012631101257075
rmse: 0.07681967918525635
mae: 0.03090962606958564
r2: 0.733914643708956
pearson: 0.8602552754702008

=== Experiment 2270 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0061152699485224555
rmse: 0.07820019148648202
mae: 0.030431504959979358
r2: 0.7242651695572594
pearson: 0.8515119730750352

=== Experiment 2241 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007223907512401192
rmse: 0.08499357335940871
mae: 0.031483342638661496
r2: 0.6742771897506745
pearson: 0.8297962364566908

=== Experiment 2436 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007384228185246704
rmse: 0.0859315319614791
mae: 0.04906586979224627
r2: 0.6670484011745947
pearson: 0.827441519805745

=== Experiment 2245 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0054094496277862225
rmse: 0.07354896075259135
mae: 0.02900348523409714
r2: 0.7560902971639759
pearson: 0.8696230402826695

=== Experiment 2386 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007462817592472489
rmse: 0.08638760091860688
mae: 0.047967123465063516
r2: 0.6635048393926274
pearson: 0.8312180992591723

=== Experiment 2457 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005841780862099631
rmse: 0.07643154363284593
mae: 0.030261315061440624
r2: 0.7365966720923125
pearson: 0.8594115303715932

=== Experiment 2253 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007327456893071957
rmse: 0.08560056596233437
mae: 0.03436147959057969
r2: 0.6696081937517975
pearson: 0.8309299937050736

=== Experiment 2350 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0067462070355022085
rmse: 0.08213529713528897
mae: 0.03250903292697439
r2: 0.695816494001991
pearson: 0.8345511703698348

=== Experiment 2365 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005741575500339116
rmse: 0.07577318457303425
mae: 0.029722253203275376
r2: 0.7411148877503415
pearson: 0.867767369198305

=== Experiment 2232 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007711643407455815
rmse: 0.08781596328376644
mae: 0.04988214779248339
r2: 0.6522853929116457
pearson: 0.820780508972355

=== Experiment 2105 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006111869669373916
rmse: 0.07817844760145802
mae: 0.030749612782562323
r2: 0.7244184866474898
pearson: 0.853913522232337

=== Experiment 2398 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0059159353214652925
rmse: 0.07691511763928657
mae: 0.029579752051818505
r2: 0.7332530801573882
pearson: 0.8610887916750033

=== Experiment 2314 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00646730707475685
rmse: 0.08041956897892981
mae: 0.03073200759494792
r2: 0.708391970478739
pearson: 0.852218459121838

=== Experiment 2046 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005871270659030625
rmse: 0.07662421718380309
mae: 0.037053138887687354
r2: 0.7352669901281461
pearson: 0.8594602279773675

=== Experiment 2110 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00673373430147553
rmse: 0.08205933403017313
mae: 0.033729306958957754
r2: 0.6963788840895839
pearson: 0.8398903431107996

=== Experiment 2491 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005943980901905894
rmse: 0.07709721721246425
mae: 0.029626547854375858
r2: 0.7319885172791254
pearson: 0.8624012431854873

=== Experiment 2476 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006769158652626338
rmse: 0.08227489685576238
mae: 0.035611456606921645
r2: 0.6947816156876432
pearson: 0.8362429872424108

=== Experiment 2288 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006282308600889104
rmse: 0.07926101564381512
mae: 0.03617414516645009
r2: 0.7167334702413806
pearson: 0.8492945380446789

=== Experiment 2451 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.004931500634197181
rmse: 0.07022464406600563
mae: 0.029048507578602036
r2: 0.7776408069234666
pearson: 0.881872496194581

=== Experiment 2011 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005630102075698465
rmse: 0.07503400612854458
mae: 0.03228389502413805
r2: 0.746141175404182
pearson: 0.8662062231285393

=== Experiment 2048 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005755146277848847
rmse: 0.07586268040248016
mae: 0.030060634354539584
r2: 0.7405029873653837
pearson: 0.86275519972394

=== Experiment 2277 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006491698185613882
rmse: 0.0805710753658773
mae: 0.03720877652883346
r2: 0.7072921860255443
pearson: 0.8448393885573274

=== Experiment 2022 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0063954797418586794
rmse: 0.07997174339639396
mae: 0.032856020520321515
r2: 0.7116306333054908
pearson: 0.8455489995396867

=== Experiment 2497 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062821353615713565
rmse: 0.07925992279564342
mae: 0.03169445937155747
r2: 0.7167412815259693
pearson: 0.8527019782721916

=== Experiment 2480 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008047582608365085
rmse: 0.08970831961621556
mae: 0.039599916296940194
r2: 0.6371380422007962
pearson: 0.8199964744307984

=== Experiment 2187 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005711075774743905
rmse: 0.07557165986495139
mae: 0.030373099692606764
r2: 0.7424901069534737
pearson: 0.8618573056590473

=== Experiment 2304 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00569439394544466
rmse: 0.07546120821617329
mae: 0.02784835826671453
r2: 0.7432422832943422
pearson: 0.8654865599800747

=== Experiment 2305 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0058556230756599175
rmse: 0.07652204307034619
mae: 0.02965489385118948
r2: 0.7359725327753033
pearson: 0.8582803616092247

=== Experiment 2389 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008094317086933717
rmse: 0.08996842272116211
mae: 0.03952732998464969
r2: 0.6350308051315536
pearson: 0.8035479685390243

=== Experiment 2464 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005847633341012024
rmse: 0.07646981980501866
mae: 0.029213989324368184
r2: 0.7363327863940256
pearson: 0.859965996788045

=== Experiment 2371 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005744586734112781
rmse: 0.07579305201740316
mae: 0.032167539012325115
r2: 0.7409791125448324
pearson: 0.8664957046943154

=== Experiment 2351 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007649011557749076
rmse: 0.08745862769189255
mae: 0.037581798769541264
r2: 0.6551094354485373
pearson: 0.8163630026556352

=== Experiment 2251 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006487768677560963
rmse: 0.08054668632265988
mae: 0.034714934344749114
r2: 0.7074693658141418
pearson: 0.8418140660485196

=== Experiment 2139 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006500879190275818
rmse: 0.08062802980524712
mae: 0.03628969170263096
r2: 0.7068782185662068
pearson: 0.8470630054377104

=== Experiment 2433 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00580096273375776
rmse: 0.07616405145314789
mae: 0.0318793631387632
r2: 0.7384371435338153
pearson: 0.8625466039898143

=== Experiment 2298 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007032644353701285
rmse: 0.08386086306317915
mae: 0.03771230934784052
r2: 0.6829011613950995
pearson: 0.827266766968433

=== Experiment 2194 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00728965987042092
rmse: 0.08537950497877649
mae: 0.040448832988664446
r2: 0.6713124448673908
pearson: 0.8205061330023492

=== Experiment 2493 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006772499232852221
rmse: 0.08229519568512017
mae: 0.03518516795398971
r2: 0.6946309903955599
pearson: 0.8405722228634414

=== Experiment 2041 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007145723974393632
rmse: 0.08453238417549591
mae: 0.044599902732276535
r2: 0.6778024510682287
pearson: 0.8289656737174284

=== Experiment 2349 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007789617012305896
rmse: 0.08825880699570948
mae: 0.034030474195897165
r2: 0.6487695973877634
pearson: 0.8256663582676077

=== Experiment 2322 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007752102747597096
rmse: 0.08804602630213981
mae: 0.03580486805782257
r2: 0.6504610990721928
pearson: 0.8482467901471564

=== Experiment 2435 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006272005599934521
rmse: 0.07919599484781109
mae: 0.02910265737378225
r2: 0.7171980280197251
pearson: 0.86090238433373

=== Experiment 2199 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005736288358867473
rmse: 0.07573828859214785
mae: 0.029243786518390096
r2: 0.741353282632249
pearson: 0.8610299912257862

=== Experiment 2443 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006399620090232383
rmse: 0.07999762552871419
mae: 0.03036682028737553
r2: 0.711443946819627
pearson: 0.8477566827475693

=== Experiment 2404 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007532998222850148
rmse: 0.08679284661105516
mae: 0.03337193781177231
r2: 0.6603404256577564
pearson: 0.8296529270833709

=== Experiment 2408 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005608763297523206
rmse: 0.07489167709113748
mae: 0.03074680836452507
r2: 0.7471033315201902
pearson: 0.8643911563885198

=== Experiment 2078 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00554153000438993
rmse: 0.07444145353490843
mae: 0.027623656449552433
r2: 0.7501348511159344
pearson: 0.8696580548207933

=== Experiment 2033 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006140742591727547
rmse: 0.0783628903992671
mae: 0.036301787155636575
r2: 0.7231166192865095
pearson: 0.8525893222206138

=== Experiment 2337 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006048292875884191
rmse: 0.07777077134685106
mae: 0.03130931462337489
r2: 0.7272851362836548
pearson: 0.8597072678676814

=== Experiment 2348 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008284893404992376
rmse: 0.09102138982125232
mae: 0.03202747392014196
r2: 0.6264378028293409
pearson: 0.8292325428691804

=== Experiment 2485 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007151445103785693
rmse: 0.08456621727253556
mae: 0.03874210464312777
r2: 0.6775444878620024
pearson: 0.8259926063246856

=== Experiment 2373 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0056576565171387126
rmse: 0.07521739504355833
mae: 0.029738925266058928
r2: 0.7448987577672785
pearson: 0.8656290931656891

=== Experiment 2327 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006328560439843322
rmse: 0.07955224974721534
mae: 0.02945122247751745
r2: 0.714647995179926
pearson: 0.8496889330776715

=== Experiment 2210 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0066972460467686
rmse: 0.0818367035429006
mae: 0.03029650916597506
r2: 0.6980241234346103
pearson: 0.848225529276611

=== Experiment 2265 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0057047434960263025
rmse: 0.07552975238954715
mae: 0.02982817446856497
r2: 0.7427756266138015
pearson: 0.8622484729005804

=== Experiment 2313 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006537401841445442
rmse: 0.08085420113664746
mae: 0.02978480205146895
r2: 0.7052314282998166
pearson: 0.8443289843171927

=== Experiment 2145 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007137366143062968
rmse: 0.08448293403441294
mae: 0.0289382815914714
r2: 0.6781793020043655
pearson: 0.826622581617074

=== Experiment 2281 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006520393061394604
rmse: 0.0807489508377329
mae: 0.036578267282634316
r2: 0.7059983467061722
pearson: 0.8406208913674424

=== Experiment 2329 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005906178972535887
rmse: 0.07685166863859162
mae: 0.029819818068066495
r2: 0.7336929896365176
pearson: 0.859642581732349

=== Experiment 2323 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00521112904511603
rmse: 0.07218815030956278
mae: 0.029614798163231086
r2: 0.7650324849490113
pearson: 0.8761135825143598

=== Experiment 2178 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006365214519395273
rmse: 0.07978229452325418
mae: 0.037126195455312185
r2: 0.7129952788656226
pearson: 0.8479667263875362

=== Experiment 2330 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00669695811498696
rmse: 0.08183494433912056
mae: 0.03158003886047628
r2: 0.6980371061519159
pearson: 0.8490940479106144

=== Experiment 2228 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008828868684112898
rmse: 0.09396205981199486
mae: 0.04379253453430767
r2: 0.6019101969156264
pearson: 0.77992255966234

=== Experiment 2130 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0063650099509516824
rmse: 0.07978101247133733
mae: 0.03739073582114916
r2: 0.7130045027667067
pearson: 0.8470047093535368

=== Experiment 2479 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006677339601717558
rmse: 0.08171499006741394
mae: 0.0374462293131337
r2: 0.6989216962804652
pearson: 0.8478635920563199

=== Experiment 2419 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007882982396466252
rmse: 0.08878616106390823
mae: 0.039504239157902105
r2: 0.6445597933092211
pearson: 0.8257492134640356

=== Experiment 2310 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00573116788442543
rmse: 0.07570447730765618
mae: 0.02886401684116481
r2: 0.7415841625711475
pearson: 0.8614173193408193

=== Experiment 2382 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006121729582637897
rmse: 0.07824148249258764
mae: 0.03518657820494628
r2: 0.7239739074980966
pearson: 0.8510517835002618

=== Experiment 2346 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0057403852669141485
rmse: 0.07576533024354971
mae: 0.027721222593680622
r2: 0.7411685548516116
pearson: 0.8661151353053813

=== Experiment 2468 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007482704782567648
rmse: 0.08650262876102464
mae: 0.036012495562041585
r2: 0.6626081347442585
pearson: 0.8156381844395351

=== Experiment 2193 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006285931152428333
rmse: 0.07928386438884227
mae: 0.030315049717716112
r2: 0.7165701309868835
pearson: 0.848671591210907

=== Experiment 2360 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006409899975103636
rmse: 0.08006185093478439
mae: 0.028902033296855565
r2: 0.7109804313353056
pearson: 0.8512723356434344

=== Experiment 2006 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0062015586472509154
rmse: 0.07874997553809725
mae: 0.03761249741333201
r2: 0.7203744501101539
pearson: 0.8568568011965536

=== Experiment 2447 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007030173990379138
rmse: 0.08384613282900492
mae: 0.035182728805680144
r2: 0.683012548990005
pearson: 0.8352678883424081

=== Experiment 2254 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005935515786199097
rmse: 0.07704229868195196
mae: 0.027527332365499865
r2: 0.7323702056205627
pearson: 0.8564021452196048

=== Experiment 2218 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006489369974728665
rmse: 0.08055662588967256
mae: 0.04148464679349936
r2: 0.7073971640296348
pearson: 0.8507286203750826

=== Experiment 2216 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006894073154105009
rmse: 0.08303055554496193
mae: 0.04220940923757469
r2: 0.6891492757950466
pearson: 0.8360156268680492

=== Experiment 2092 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006957498294787316
rmse: 0.08341161966289419
mae: 0.048019537247159336
r2: 0.6862894641172206
pearson: 0.8405950742086411

=== Experiment 2391 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006592969163228552
rmse: 0.08119710070703604
mae: 0.03853446812771833
r2: 0.7027259222176647
pearson: 0.8411925956721253

=== Experiment 2057 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008171124518712622
rmse: 0.09039427259905697
mae: 0.054510009983929526
r2: 0.6315675918381789
pearson: 0.8183739760030446

=== Experiment 2459 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0056014881972821385
rmse: 0.07484309051129662
mae: 0.027216267264626816
r2: 0.7474313625880433
pearson: 0.8692037421995759

=== Experiment 2196 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006343287561253562
rmse: 0.07964475852969587
mae: 0.03171983893750172
r2: 0.7139839557574401
pearson: 0.8455649001139133

=== Experiment 2008 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006535252053570535
rmse: 0.08084090581859245
mae: 0.03584428625438834
r2: 0.7053283612889021
pearson: 0.8470265272557176

=== Experiment 2181 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005686753187849222
rmse: 0.07541056416609826
mae: 0.032174086520271115
r2: 0.7435868016913658
pearson: 0.8627543243384336

=== Experiment 2141 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005895189834494214
rmse: 0.07678013958371145
mae: 0.035630062648044664
r2: 0.7341884850341269
pearson: 0.8611899965122533

=== Experiment 2004 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0075007029351372295
rmse: 0.08660659868126233
mae: 0.03839744333449365
r2: 0.6617966059664887
pearson: 0.8378914565195816

=== Experiment 2123 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006455952236088927
rmse: 0.08034894047894425
mae: 0.031328398762853274
r2: 0.7089039551566291
pearson: 0.8421282126963529

=== Experiment 2387 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007646110180793362
rmse: 0.08744203897893371
mae: 0.047290877453368754
r2: 0.6552402572584768
pearson: 0.8175377757116098

=== Experiment 2460 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006383144471586475
rmse: 0.07989458349341634
mae: 0.029228340193920708
r2: 0.7121868252128967
pearson: 0.848912249824056

=== Experiment 2079 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006107271609102428
rmse: 0.07814903460121839
mae: 0.02833538100900583
r2: 0.7246258111613701
pearson: 0.8519543429581131

=== Experiment 2370 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006066823677002463
rmse: 0.07788981754377437
mae: 0.03368046777873237
r2: 0.7264495906172612
pearson: 0.8528483074651465

=== Experiment 2226 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006370708906841832
rmse: 0.07981672072217595
mae: 0.03619119029646544
r2: 0.7127475393539221
pearson: 0.8457510070787753

=== Experiment 2162 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059956206098875815
rmse: 0.07743139292229981
mae: 0.02951281001051336
r2: 0.7296601055746048
pearson: 0.8649952026204879

=== Experiment 2211 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0074104318162809
rmse: 0.0860838650170919
mae: 0.045632573771779254
r2: 0.6658668909843628
pearson: 0.8224930650535529

=== Experiment 2133 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005866113641118659
rmse: 0.07659055843326029
mae: 0.031376658652394994
r2: 0.7354995177960195
pearson: 0.8578210212347936

=== Experiment 2234 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008009265463582268
rmse: 0.08949449962753168
mae: 0.03520318960429274
r2: 0.6388657453943883
pearson: 0.8201273829483431

=== Experiment 2376 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007079015726503188
rmse: 0.08413688683629308
mae: 0.03345591171260857
r2: 0.6808102966050638
pearson: 0.8421333950607024

=== Experiment 2114 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007034360628738829
rmse: 0.08387109531142913
mae: 0.03615454786278657
r2: 0.6828237753090471
pearson: 0.8308793574390834

=== Experiment 2356 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005561000971465223
rmse: 0.0745721192636043
mae: 0.030850011138236672
r2: 0.7492569137803372
pearson: 0.8657714835741173

=== Experiment 2297 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008677274426685378
rmse: 0.09315188901297375
mae: 0.058745152749481395
r2: 0.608745515261298
pearson: 0.81742692349967

=== Experiment 2213 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006169165052513247
rmse: 0.07854403257099324
mae: 0.03068197470354205
r2: 0.721835063039364
pearson: 0.8535540058269576

=== Experiment 2074 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006312064207049894
rmse: 0.07944850034487683
mae: 0.036249748977069965
r2: 0.7153918030560344
pearson: 0.8513145795567265

=== Experiment 2264 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006201592520996021
rmse: 0.07875019060926787
mae: 0.031717771289840754
r2: 0.7203729227578961
pearson: 0.8488356585932152

=== Experiment 2407 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006265519019142583
rmse: 0.07915503154659584
mae: 0.03593986083701647
r2: 0.7174905050926711
pearson: 0.8488541775395197

=== Experiment 2291 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008181075541122457
rmse: 0.09044929817926979
mae: 0.039250735313841704
r2: 0.6311189046174992
pearson: 0.7978732489904022

=== Experiment 2276 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00590155617704556
rmse: 0.07682158666055759
mae: 0.03366934380563265
r2: 0.7339014294505285
pearson: 0.8575952903534302

=== Experiment 2454 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0062824158270595115
rmse: 0.07926169205271555
mae: 0.03572557861871577
r2: 0.7167286354605507
pearson: 0.8515389548130099

=== Experiment 2249 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006348771186966356
rmse: 0.07967917662078566
mae: 0.030580164212517868
r2: 0.7137367014875784
pearson: 0.8532654608563675

=== Experiment 2016 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005873659574587286
rmse: 0.07663980411370638
mae: 0.028421276616850538
r2: 0.7351592749771384
pearson: 0.8665585411099662

=== Experiment 2255 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006236830267578282
rmse: 0.07897360487896118
mae: 0.03341675784097184
r2: 0.7187840682738204
pearson: 0.8493428427858787

=== Experiment 2328 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005846448224274925
rmse: 0.07646207049429753
mae: 0.02981639264201442
r2: 0.7363862227860916
pearson: 0.8582337760319064

=== Experiment 2315 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006226150317885217
rmse: 0.07890595869695277
mae: 0.032838799362609424
r2: 0.7192656225048752
pearson: 0.848817032416795

=== Experiment 2113 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006699341745849693
rmse: 0.08184950669276934
mae: 0.032377857904533916
r2: 0.6979296292854316
pearson: 0.8408276595233254

=== Experiment 2492 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007345537569412776
rmse: 0.085706111622292
mae: 0.04015614692184964
r2: 0.6687929440135042
pearson: 0.8207775571698259

=== Experiment 2202 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066751860369500485
rmse: 0.08170181171155294
mae: 0.03240751287979528
r2: 0.699018799567976
pearson: 0.8384453444125953

=== Experiment 2478 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00613496222080753
rmse: 0.07832599964767466
mae: 0.031679326750250106
r2: 0.7233772536671574
pearson: 0.8585448752547465

=== Experiment 2077 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006962007959629229
rmse: 0.08343864787752273
mae: 0.04076266116250698
r2: 0.686086125314354
pearson: 0.8360859062930815

=== Experiment 2164 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007225729164797773
rmse: 0.08500428909647896
mae: 0.03258060386500769
r2: 0.6741950522458832
pearson: 0.8409911756351215

=== Experiment 2242 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006804107184526403
rmse: 0.08248701221723577
mae: 0.0355634012749783
r2: 0.6932058017662945
pearson: 0.8442477508088042

=== Experiment 2300 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007124361145641909
rmse: 0.08440593074921873
mae: 0.04012557857318219
r2: 0.6787656916141439
pearson: 0.8376965671696843

=== Experiment 2267 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006803465675424636
rmse: 0.08248312357946101
mae: 0.03177851233933108
r2: 0.6932347271293435
pearson: 0.8452758765194115

=== Experiment 2007 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006860734366670121
rmse: 0.08282955008129719
mae: 0.04209539798866299
r2: 0.6906525070469631
pearson: 0.8361495621663385

=== Experiment 2274 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005945912628167327
rmse: 0.07710974405460913
mae: 0.031063772806404467
r2: 0.7319014165922152
pearson: 0.8618888476787442

=== Experiment 2217 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006692837156121203
rmse: 0.08180976198548193
mae: 0.031079301365432212
r2: 0.6982229183734001
pearson: 0.8409489749822066

=== Experiment 2377 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005957983732083796
rmse: 0.07718797660311998
mae: 0.0308530003140666
r2: 0.7313571358295207
pearson: 0.8573746462143369

=== Experiment 2475 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006717254765106627
rmse: 0.08195886019892314
mae: 0.039677058700973364
r2: 0.697121939131259
pearson: 0.8403743890229427

=== Experiment 2426 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006396687159311873
rmse: 0.07997929206558328
mae: 0.031422111187506534
r2: 0.7115761913839512
pearson: 0.8436180721926259

=== Experiment 2477 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005933921909386831
rmse: 0.07703195382039087
mae: 0.030187611184654062
r2: 0.7324420728245087
pearson: 0.8573572551480197

=== Experiment 2089 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0069970926453023
rmse: 0.08364862608137864
mae: 0.038832677948290165
r2: 0.6845041722793062
pearson: 0.8328255928371359

=== Experiment 2109 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005594027040097232
rmse: 0.0747932285711563
mae: 0.029770276091170837
r2: 0.7477677828816045
pearson: 0.8648241910864924

=== Experiment 2173 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005993489212900282
rmse: 0.07741762856675656
mae: 0.031731218641224315
r2: 0.7297562093266625
pearson: 0.8565064589752233

=== Experiment 2148 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00802030449110362
rmse: 0.08955615272611715
mae: 0.03673718808004376
r2: 0.6383680005020981
pearson: 0.8088473873504287

=== Experiment 2463 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006418058891178555
rmse: 0.08011278856199275
mae: 0.03352919546894386
r2: 0.7106125494005011
pearson: 0.8450536699713265

=== Experiment 2127 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006210904222778766
rmse: 0.07880929020603324
mae: 0.03537837250957495
r2: 0.7199530622229056
pearson: 0.8521224370199454

=== Experiment 2317 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006643180506579603
rmse: 0.08150570842940759
mae: 0.04173105447543804
r2: 0.7004619148456681
pearson: 0.8426965349237296

=== Experiment 2129 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006993631888063412
rmse: 0.08362793724625409
mae: 0.03749581722125596
r2: 0.684660216300012
pearson: 0.8275218952160686

=== Experiment 2282 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005445485008095119
rmse: 0.07379352958149596
mae: 0.029337441120159454
r2: 0.7544654777262316
pearson: 0.869405776314882

=== Experiment 2379 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005981111074096362
rmse: 0.07733764331873814
mae: 0.028434192013934333
r2: 0.7303143341572957
pearson: 0.857150513594805

=== Experiment 2401 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006102501515926204
rmse: 0.07811850943231191
mae: 0.03377680373509715
r2: 0.7248408925632075
pearson: 0.8529190043380412

=== Experiment 2413 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0057821008172713505
rmse: 0.076040126362805
mae: 0.03285036857778135
r2: 0.739287619046423
pearson: 0.8609025961992657

=== Experiment 2098 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008041784106139925
rmse: 0.08967599514998384
mae: 0.036985773193280044
r2: 0.6373994941140128
pearson: 0.8007973627773297

=== Experiment 2341 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006559254376690386
rmse: 0.08098922383064543
mae: 0.03607709961906679
r2: 0.7042461071036583
pearson: 0.8413490535491119

=== Experiment 2442 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006409182984550337
rmse: 0.08005737308049982
mae: 0.03777252626364281
r2: 0.7110127601237823
pearson: 0.8447908672851827

=== Experiment 2023 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006608580122420003
rmse: 0.08129317389805864
mae: 0.03188374454783291
r2: 0.7020220309386309
pearson: 0.842830785535754

=== Experiment 2067 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007310634671859041
rmse: 0.08550224951344287
mae: 0.03576127453966011
r2: 0.6703667003022621
pearson: 0.8279346585076128

=== Experiment 2000 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006132962441916823
rmse: 0.07831323286595199
mae: 0.034073019395969906
r2: 0.7234674228171694
pearson: 0.8527112874107828

=== Experiment 2117 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0057318506415255386
rmse: 0.07570898653083093
mae: 0.029495718098519277
r2: 0.7415533773540081
pearson: 0.8615207778222898

=== Experiment 2366 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007529118695729246
rmse: 0.08677049438449251
mae: 0.03860889532492111
r2: 0.6605153518281275
pearson: 0.8203658748820316

=== Experiment 2125 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006324862228714035
rmse: 0.07952900243756383
mae: 0.03570226743516564
r2: 0.7148147458920382
pearson: 0.8528830482724442

=== Experiment 2483 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008478466764931445
rmse: 0.09207859015499448
mae: 0.038693122161286383
r2: 0.6177096652278394
pearson: 0.7870265943157945

=== Experiment 2153 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006264986649486867
rmse: 0.07915166864625703
mae: 0.03827027757679063
r2: 0.7175145094061335
pearson: 0.8505856869880977

=== Experiment 2091 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007920341048092187
rmse: 0.08899629794599428
mae: 0.03505050485956297
r2: 0.6428753081502075
pearson: 0.8532652141092326

=== Experiment 2422 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007448375178061087
rmse: 0.08630396965413055
mae: 0.036747373293408755
r2: 0.6641560415018448
pearson: 0.8195155324134877

=== Experiment 2143 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006154624145258232
rmse: 0.07845141264029751
mae: 0.028215546086807416
r2: 0.7224907061475498
pearson: 0.8623619467373334

=== Experiment 2138 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005560890249444499
rmse: 0.07457137687775718
mae: 0.029025677063779197
r2: 0.7492619061875192
pearson: 0.8656740887679603

=== Experiment 2395 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007193342062340871
rmse: 0.08481357239464019
mae: 0.037223985975331206
r2: 0.675655372441011
pearson: 0.822057356139073

=== Experiment 2157 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006311698331128559
rmse: 0.07944619771347498
mae: 0.030596999658231983
r2: 0.7154083002402929
pearson: 0.8459468679599518

=== Experiment 2177 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006685191747145077
rmse: 0.08176302188119686
mae: 0.03482289194136853
r2: 0.698567646499132
pearson: 0.8360774989187824

=== Experiment 2469 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008334260391402235
rmse: 0.09129217048248023
mae: 0.05636052619889603
r2: 0.6242118671401928
pearson: 0.8116143642361517

=== Experiment 2393 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006023636922758384
rmse: 0.07761209263225921
mae: 0.03494989329569737
r2: 0.7283968623581826
pearson: 0.8577863926812569

=== Experiment 2095 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009029466436850705
rmse: 0.09502350465464166
mae: 0.04424041308070617
r2: 0.5928653325345017
pearson: 0.8166009947400684

=== Experiment 2032 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00537280773937051
rmse: 0.073299438874868
mae: 0.02789793834616815
r2: 0.7577424637853121
pearson: 0.8706825737809348

=== Experiment 2301 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006314990594878821
rmse: 0.07946691509602484
mae: 0.03224073729808711
r2: 0.7152598535168297
pearson: 0.8542174271758328

=== Experiment 2005 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007104846747467312
rmse: 0.08429025298020709
mae: 0.03203649828680451
r2: 0.6796455872388931
pearson: 0.8395934205081323

=== Experiment 2082 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006400938166884748
rmse: 0.08000586332816333
mae: 0.037697421873073275
r2: 0.7113845153235019
pearson: 0.8451384409853084

=== Experiment 2290 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005985031812632216
rmse: 0.07736298735591986
mae: 0.030995183470176848
r2: 0.730137549782363
pearson: 0.8653682147060566

=== Experiment 2266 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006021982581968478
rmse: 0.07760143414891556
mae: 0.03681854300382976
r2: 0.7284714558562684
pearson: 0.8578083193297286

=== Experiment 2486 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006449623464104954
rmse: 0.08030954777674292
mae: 0.03397850657084949
r2: 0.7091893166998806
pearson: 0.8486586992814168

=== Experiment 2299 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0066721523106679035
rmse: 0.08168324375701484
mae: 0.03154748839270471
r2: 0.6991555889507928
pearson: 0.8498048382172789

=== Experiment 2461 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0059563528216244
rmse: 0.07717741134311516
mae: 0.02960219515097877
r2: 0.7314306728643163
pearson: 0.8557348997300996

=== Experiment 2112 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007671250825848528
rmse: 0.0875856770588007
mae: 0.03395195265763898
r2: 0.6541066766381834
pearson: 0.8474996323401619

=== Experiment 2167 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006326221043651939
rmse: 0.07953754486814349
mae: 0.03766938750197755
r2: 0.7147534775245483
pearson: 0.8610188118715483

=== Experiment 2410 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012115860617742277
rmse: 0.11007207010746312
mae: 0.04682366798213403
r2: 0.453701177344064
pearson: 0.7326111274028015

=== Experiment 2207 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006769211206514206
rmse: 0.08227521623498905
mae: 0.03327607073906699
r2: 0.6947792460559701
pearson: 0.8387422257685194

=== Experiment 2056 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0064917567973977915
rmse: 0.08057143909226018
mae: 0.03172164725482792
r2: 0.7072895432460049
pearson: 0.8487281221826583

=== Experiment 2383 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00690479793137597
rmse: 0.0830951137635419
mae: 0.03569220041019201
r2: 0.6886657003082344
pearson: 0.8304691411535021

=== Experiment 2367 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005566988600897646
rmse: 0.07461225503158074
mae: 0.03160886295693436
r2: 0.7489869342045867
pearson: 0.8664061986288574

=== Experiment 2061 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005465076545147331
rmse: 0.07392615602848109
mae: 0.0329143810928173
r2: 0.7535821039434423
pearson: 0.8687180145842994

=== Experiment 2065 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006320490056789661
rmse: 0.07950150977679393
mae: 0.033179037467146295
r2: 0.7150118851997682
pearson: 0.8462822925731596

=== Experiment 2444 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0082358244355959
rmse: 0.09075144315985229
mae: 0.03329275401340469
r2: 0.6286503010625228
pearson: 0.8105036965283564

=== Experiment 2397 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00630562041740695
rmse: 0.07940793674064923
mae: 0.02937001998036542
r2: 0.7156823506949059
pearson: 0.8649037132186799

=== Experiment 2058 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008050572130685264
rmse: 0.08972498052763937
mae: 0.0357694966723789
r2: 0.6370032459551689
pearson: 0.8469579189778763

=== Experiment 2390 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007220950975029708
rmse: 0.08497617886813756
mae: 0.03331295737300815
r2: 0.674410498719483
pearson: 0.8354995251924027

=== Experiment 2388 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007103457424879201
rmse: 0.08428201127689823
mae: 0.0319054238439159
r2: 0.6797082311829035
pearson: 0.8465751077125485

=== Experiment 2080 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006702104005608666
rmse: 0.08186637896968857
mae: 0.03348328596433284
r2: 0.6978050802086628
pearson: 0.8356495077562729

=== Experiment 2027 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005809415745851368
rmse: 0.07621952339034513
mae: 0.03376336112017844
r2: 0.7380560009389695
pearson: 0.8602749313511289

=== Experiment 2285 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007989661075574089
rmse: 0.0893849040698377
mae: 0.03651406274978678
r2: 0.639749698621126
pearson: 0.8169435512738444

=== Experiment 2354 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006228279098858591
rmse: 0.07891944689909193
mae: 0.03391360255385787
r2: 0.719169636707734
pearson: 0.8494474128647469

=== Experiment 2269 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006252579610454564
rmse: 0.07907325471013928
mae: 0.02987909864259482
r2: 0.7180739373353446
pearson: 0.8482936730894902

=== Experiment 2415 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006993252213045973
rmse: 0.08362566719043844
mae: 0.030506386995323754
r2: 0.6846773356794406
pearson: 0.8360667848398451

=== Experiment 2405 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011657378827922466
rmse: 0.10796934207413911
mae: 0.039471004029075246
r2: 0.4743739194537717
pearson: 0.7713039955286057

=== Experiment 2424 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00985134211478278
rmse: 0.0992539274526846
mae: 0.0472557534532411
r2: 0.5558073199516935
pearson: 0.7579640066011765

=== Experiment 2068 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008008146614957004
rmse: 0.08948824847407064
mae: 0.03364696485464734
r2: 0.6389161937864585
pearson: 0.802112775747745

=== Experiment 2334 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006502525790441925
rmse: 0.08063824024891618
mae: 0.032280733796160796
r2: 0.7068039740894412
pearson: 0.8645618488930287

=== Experiment 2176 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006213528123315783
rmse: 0.07882593560063707
mae: 0.03460907175944483
r2: 0.7198347517025583
pearson: 0.849522712915701

=== Experiment 2087 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00689749558453182
rmse: 0.08305116245141798
mae: 0.04120949639531831
r2: 0.6889949599134284
pearson: 0.8372133478020749

=== Experiment 2272 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006850322072479649
rmse: 0.08276667247437007
mae: 0.03483534733091987
r2: 0.6911219928092105
pearson: 0.8391936172072627

=== Experiment 2378 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005317580242279008
rmse: 0.07292174053242975
mae: 0.027911815404461793
r2: 0.7602326473216876
pearson: 0.8725989701977296

=== Experiment 2438 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007003889754551568
rmse: 0.08368924515462885
mae: 0.03302307976689356
r2: 0.6841976936148934
pearson: 0.8305096750130415

=== Experiment 2024 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005691166910462478
rmse: 0.07543982310731169
mae: 0.028921384183113717
r2: 0.743387788881362
pearson: 0.8641883362076468

=== Experiment 2273 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00666223875692466
rmse: 0.08162253829013565
mae: 0.03159955377782764
r2: 0.6996025867257871
pearson: 0.8375802778720716

=== Experiment 2174 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0058123201297114084
rmse: 0.07623857376493483
mae: 0.03053489698692346
r2: 0.7379250435490374
pearson: 0.8591247035059489

=== Experiment 2287 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007934716641188465
rmse: 0.0890770264500812
mae: 0.03306630009106362
r2: 0.642227118984679
pearson: 0.8272219351353562

=== Experiment 2470 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005516133694329189
rmse: 0.07427067856381271
mae: 0.03326081761540985
r2: 0.7512799595588022
pearson: 0.868258791048269

=== Experiment 2189 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0054404025220516706
rmse: 0.07375908433577298
mae: 0.03040883308916924
r2: 0.7546946447849575
pearson: 0.8694854406081884

=== Experiment 2416 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0058012688015016975
rmse: 0.07616606069307837
mae: 0.03151873605689473
r2: 0.7384233430739517
pearson: 0.8600109892306603

=== Experiment 2428 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006372805634424996
rmse: 0.07982985428036929
mae: 0.03506453176123969
r2: 0.7126529988300376
pearson: 0.8577242891462246

=== Experiment 2503 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007488746669362666
rmse: 0.08653754485402661
mae: 0.04793572207326637
r2: 0.6623357087279088
pearson: 0.825043119032275

=== Experiment 2520 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00785614508405331
rmse: 0.0886348976648211
mae: 0.03592383002505742
r2: 0.6457698759139162
pearson: 0.8138237792012386

=== Experiment 2505 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007744851945025136
rmse: 0.08800484046360822
mae: 0.04294680287547676
r2: 0.6507880345688402
pearson: 0.8178662184368065

=== Experiment 2508 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007873387483594082
rmse: 0.08873211078067557
mae: 0.040972937670289554
r2: 0.6449924237075833
pearson: 0.8115387839671454

=== Experiment 2507 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008151462450730456
rmse: 0.09028544982847711
mae: 0.04627874330564486
r2: 0.6324541458295567
pearson: 0.8252341478544574

=== Experiment 2527 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0064603262226917806
rmse: 0.0803761545652178
mae: 0.03546291956031227
r2: 0.708706734025845
pearson: 0.8522138412697982

=== Experiment 2573 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006922042877866371
rmse: 0.08319881536335942
mae: 0.04034221705966924
r2: 0.6878881332610636
pearson: 0.8315403106520128

=== Experiment 2551 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007551522277396669
rmse: 0.08689949526548855
mae: 0.04762308087676112
r2: 0.6595051841913135
pearson: 0.8273561973166619

=== Experiment 2542 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007172211995420721
rmse: 0.08468891306080578
mae: 0.04703670990375018
r2: 0.6766081178583869
pearson: 0.8323931287184863

=== Experiment 2504 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006054817832939571
rmse: 0.07781270997041274
mae: 0.03378453539002664
r2: 0.7269909288418808
pearson: 0.8615999779731446

=== Experiment 2549 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00816973118762782
rmse: 0.09038656530496011
mae: 0.054514403707349536
r2: 0.6316304165235406
pearson: 0.8171389026987443

=== Experiment 2545 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005781687888272492
rmse: 0.07603741110974578
mae: 0.03250767321299727
r2: 0.7393062378332377
pearson: 0.8623112177295591

=== Experiment 2581 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006771007598988865
rmse: 0.08228613248287263
mae: 0.034936170692491005
r2: 0.6946982475099404
pearson: 0.8438615295666234

=== Experiment 2541 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006101681315965758
rmse: 0.07811325954001509
mae: 0.034426091017394446
r2: 0.7248778750184264
pearson: 0.8541359956651302

=== Experiment 2539 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00760645723369347
rmse: 0.0872150057827979
mae: 0.04299390780784667
r2: 0.6570281911906143
pearson: 0.8380813763232895

=== Experiment 2548 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006380915231205972
rmse: 0.0798806311392566
mae: 0.03010727660269853
r2: 0.7122873406804899
pearson: 0.8459569971267756

=== Experiment 2536 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006594308830412756
rmse: 0.08120534976473383
mae: 0.0354846005233304
r2: 0.7026655172139615
pearson: 0.8450870604058971

=== Experiment 2533 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006323923442441391
rmse: 0.07952310005552721
mae: 0.03508857972091981
r2: 0.7148570753518804
pearson: 0.8494063656901341

=== Experiment 2558 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006080899734629222
rmse: 0.07798012397161999
mae: 0.02787028883889849
r2: 0.7258149073742173
pearson: 0.8541621579472745

=== Experiment 2634 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007963665551754661
rmse: 0.08923937220618856
mae: 0.04719161281956437
r2: 0.6409218253486391
pearson: 0.80463199856964

=== Experiment 2606 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006261251391895968
rmse: 0.07912806955749627
mae: 0.03343585947400519
r2: 0.7176829305268946
pearson: 0.8570293787875924

=== Experiment 2633 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007095127808198275
rmse: 0.08423258163085277
mae: 0.03895096179464175
r2: 0.6800838099329017
pearson: 0.8248203269836506

=== Experiment 2540 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005784386212308695
rmse: 0.07605515243761395
mae: 0.02998979327232299
r2: 0.7391845715900685
pearson: 0.8600709008605179

=== Experiment 2511 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007330049863282823
rmse: 0.08561571037655895
mae: 0.046163996713051686
r2: 0.669491277866243
pearson: 0.8338063209077098

=== Experiment 2574 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005991296570138801
rmse: 0.0774034661377564
mae: 0.03397050189372395
r2: 0.7298550746237218
pearson: 0.8552981358267188

=== Experiment 2571 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006645822988831378
rmse: 0.0815219172298553
mae: 0.03279443629799227
r2: 0.700342766483981
pearson: 0.8392694709307543

=== Experiment 2523 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007680814833968578
rmse: 0.08764025806653343
mae: 0.049427938978430896
r2: 0.6536754397214923
pearson: 0.8202844945337658

=== Experiment 2678 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006340515432404344
rmse: 0.07962735354389434
mae: 0.03358274285525061
r2: 0.7141089498271441
pearson: 0.845109964243216

=== Experiment 2561 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007713159576964523
rmse: 0.08782459551267244
mae: 0.0472287239251931
r2: 0.6522170294958152
pearson: 0.8207976922127688

=== Experiment 2635 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006098911520511399
rmse: 0.07809552817230574
mae: 0.03417054855321471
r2: 0.7250027638763832
pearson: 0.8555628242242921

=== Experiment 2544 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006618523300491199
rmse: 0.08135430720306823
mae: 0.03128664704186389
r2: 0.7015736974157281
pearson: 0.8437607238202439

=== Experiment 2657 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00684822937000411
rmse: 0.08275402932790711
mae: 0.03925589463299397
r2: 0.6912163518427636
pearson: 0.8484428422025887

=== Experiment 2699 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007113235778197614
rmse: 0.08434000105642407
mae: 0.04367678699697967
r2: 0.6792673295355585
pearson: 0.8378786564183476

=== Experiment 2695 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006870494638222044
rmse: 0.08288844695264862
mae: 0.03148272659158768
r2: 0.690212420698511
pearson: 0.8472633705561223

=== Experiment 2566 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009884785191276956
rmse: 0.09942225702163955
mae: 0.04326381376182188
r2: 0.5542993863519948
pearson: 0.7858188224782562

=== Experiment 2690 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006706542907017214
rmse: 0.0818934851317076
mae: 0.034042186440939766
r2: 0.6976049320978615
pearson: 0.8362654570578943

=== Experiment 2593 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005800449335201147
rmse: 0.07616068103162646
mae: 0.028657723286201555
r2: 0.7384602924487688
pearson: 0.8595621211306452

=== Experiment 2602 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00599477159737723
rmse: 0.07742591037486889
mae: 0.029897548755787956
r2: 0.7296983871750176
pearson: 0.8544217980381127

=== Experiment 2622 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006386616066991282
rmse: 0.07991630664007993
mae: 0.03504204085315784
r2: 0.71203029250406
pearson: 0.8511303587404514

=== Experiment 2518 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0074143443991826815
rmse: 0.08610658743198851
mae: 0.03965012122726442
r2: 0.6656904743433809
pearson: 0.8165304637888352

=== Experiment 2714 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006570298850694393
rmse: 0.08105737998908176
mae: 0.044633407507968646
r2: 0.7037481166318624
pearson: 0.8466186429433933

=== Experiment 2592 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006229385431328573
rmse: 0.07892645583914543
mae: 0.03143186639420031
r2: 0.7191197526635966
pearson: 0.8484803018452054

=== Experiment 2543 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006482259949055032
rmse: 0.08051248318773327
mae: 0.031169738511096588
r2: 0.7077177519578972
pearson: 0.8471941815440389

=== Experiment 2640 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006007545075356143
rmse: 0.07750835487453042
mae: 0.03223859715897748
r2: 0.729122436674986
pearson: 0.8638094685379342

=== Experiment 2552 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006192711160467005
rmse: 0.07869378095165466
mae: 0.03236255035732104
r2: 0.7207733793951637
pearson: 0.851461459918734

=== Experiment 2577 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0063898860221316195
rmse: 0.07993676264480329
mae: 0.03701421994661219
r2: 0.7118828516659996
pearson: 0.8455243945742786

=== Experiment 2646 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006916955654552625
rmse: 0.08316823705329231
mae: 0.03984843203290508
r2: 0.6881175139212221
pearson: 0.8372703526685549

=== Experiment 2528 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006050825700896525
rmse: 0.07778705355582331
mae: 0.03325538602497159
r2: 0.7271709323186302
pearson: 0.8549776218804396

=== Experiment 2572 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0077860097385736805
rmse: 0.08823836885716825
mae: 0.03486783127758498
r2: 0.6489322477726664
pearson: 0.8168448202995047

=== Experiment 2636 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005944444711500483
rmse: 0.0771002251066784
mae: 0.031249815117345463
r2: 0.7319676043086436
pearson: 0.8587924705147053

=== Experiment 2630 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00758859516496219
rmse: 0.08711254309777777
mae: 0.04002399234671419
r2: 0.65783358400801
pearson: 0.8278041229248421

=== Experiment 2691 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007563637952835373
rmse: 0.0869691781773024
mae: 0.04386166561150946
r2: 0.6589588937182991
pearson: 0.8299594105989793

=== Experiment 2684 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007519475593795332
rmse: 0.0867149098701909
mae: 0.04907683619177422
r2: 0.6609501550500738
pearson: 0.8311119280959585

=== Experiment 2579 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007385142023670917
rmse: 0.08593684904434719
mae: 0.03937340576601458
r2: 0.6670071966022579
pearson: 0.8250289346734797

=== Experiment 2682 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007853060099196296
rmse: 0.08861749318952944
mae: 0.04584339700895749
r2: 0.6459089765233132
pearson: 0.8256178910910001

=== Experiment 2584 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007732002243344561
rmse: 0.08793180450408465
mae: 0.04619240579781191
r2: 0.6513674219620311
pearson: 0.8132228202776316

=== Experiment 2658 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007992624719232453
rmse: 0.08940148052036082
mae: 0.05021609451727018
r2: 0.6396160692329698
pearson: 0.8190558724443577

=== Experiment 2605 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006078678588912914
rmse: 0.07796588092821702
mae: 0.027888425808792503
r2: 0.7259150578569649
pearson: 0.8532111983632148

=== Experiment 2760 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0070465808092548595
rmse: 0.08394391466482165
mae: 0.04325984832975665
r2: 0.682272772748092
pearson: 0.838149745818124

=== Experiment 2724 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066911530475627845
rmse: 0.0817994685041583
mae: 0.03622006946063816
r2: 0.6982988540870665
pearson: 0.8383917925416183

=== Experiment 2590 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0069748686638810954
rmse: 0.08351567914997217
mae: 0.04104262872577604
r2: 0.6855062418200661
pearson: 0.8309975400029592

=== Experiment 2720 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006678995633682119
rmse: 0.08172512241460467
mae: 0.03342072560132791
r2: 0.6988470265280586
pearson: 0.8377527882561773

=== Experiment 2660 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007937286168041915
rmse: 0.08909144834405777
mae: 0.054149818579130135
r2: 0.6421112601497923
pearson: 0.8182057985611737

=== Experiment 2700 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006918833360275986
rmse: 0.08317952488609193
mae: 0.03524758517553482
r2: 0.6880328489965973
pearson: 0.8306534779699234

=== Experiment 2708 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0074067471561819935
rmse: 0.0860624607839097
mae: 0.04694003820479841
r2: 0.6660330306864797
pearson: 0.8278320609635965

=== Experiment 2501 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006481026219366648
rmse: 0.08050482109393603
mae: 0.03453045092039344
r2: 0.7077733802865406
pearson: 0.8416829353013332

=== Experiment 2768 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0069692460501593724
rmse: 0.08348201033851169
mae: 0.03406062263541333
r2: 0.685759762998077
pearson: 0.828551103318913

=== Experiment 2641 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006931196028845081
rmse: 0.0832538048910984
mae: 0.03026005161814735
r2: 0.6874754217120433
pearson: 0.8342409472191452

=== Experiment 2734 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0068767081449152495
rmse: 0.08292591962055802
mae: 0.03548322180632889
r2: 0.6899322564164878
pearson: 0.8349614938223858

=== Experiment 2621 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0071983108068470825
rmse: 0.08484285949239972
mae: 0.03134513016884349
r2: 0.6754313339381377
pearson: 0.8314819725662025

=== Experiment 2753 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006896759501281307
rmse: 0.08304673082837943
mae: 0.03764828738415174
r2: 0.6890281495832189
pearson: 0.8362840617406624

=== Experiment 2608 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007041105524532813
rmse: 0.0839112955717692
mae: 0.03898770259086434
r2: 0.682519650926352
pearson: 0.8274725070777432

=== Experiment 2626 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007120024775416758
rmse: 0.08438023924721212
mae: 0.04584181140344059
r2: 0.6789612166390135
pearson: 0.8314743872201164

=== Experiment 2557 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006476184174248204
rmse: 0.08047474246152145
mae: 0.027537349060415037
r2: 0.7079917059697842
pearson: 0.8420269257925582

=== Experiment 2727 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0061876544217164126
rmse: 0.07866164517550095
mae: 0.029533976383859656
r2: 0.7210013855197869
pearson: 0.8643669388163611

=== Experiment 2546 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006611342057506686
rmse: 0.08131015962047232
mae: 0.03854832478808141
r2: 0.7018974965011928
pearson: 0.8446455901730658

=== Experiment 2595 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0061667810311999876
rmse: 0.07852885476816777
mae: 0.03082398388569543
r2: 0.7219425575110908
pearson: 0.8504380772861025

=== Experiment 2648 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006772069185589397
rmse: 0.08229258281029583
mae: 0.03147014508613685
r2: 0.6946503810373625
pearson: 0.8367900720416249

=== Experiment 2614 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006650921292967043
rmse: 0.08155318076547992
mae: 0.03175043294667579
r2: 0.7001128861944392
pearson: 0.8390139337860936

=== Experiment 2604 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005752068628010699
rmse: 0.07584239334310791
mae: 0.027717035984174257
r2: 0.7406417572420085
pearson: 0.86721550892543

=== Experiment 2798 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008322825330590804
rmse: 0.09122952006116662
mae: 0.055393692674438395
r2: 0.624727468999225
pearson: 0.8100576750583368

=== Experiment 2512 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0067297195573687695
rmse: 0.08203486793655958
mae: 0.035860278729083137
r2: 0.6965599071343337
pearson: 0.8348897417578665

=== Experiment 2767 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006815137326978262
rmse: 0.08255384501631806
mae: 0.03593140612179525
r2: 0.6927084574978772
pearson: 0.8404093307224093

=== Experiment 2596 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005921879033928353
rmse: 0.07695374606819574
mae: 0.029191138585054063
r2: 0.7329850807784567
pearson: 0.8607145170166991

=== Experiment 2597 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007828530356040215
rmse: 0.08847898256671025
mae: 0.05302107202676383
r2: 0.6470150118458554
pearson: 0.8274222061935207

=== Experiment 2531 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006487902393606642
rmse: 0.08054751637143533
mae: 0.03582468977284444
r2: 0.7074633366164949
pearson: 0.8462472812004249

=== Experiment 2643 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008002534898367835
rmse: 0.08945688849031043
mae: 0.041719307599779945
r2: 0.6391692236175589
pearson: 0.8066845668603191

=== Experiment 2524 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006056514387566972
rmse: 0.07782361073329207
mae: 0.030528418661303167
r2: 0.7269144319404423
pearson: 0.8543219859904405

=== Experiment 2775 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0065315785484981875
rmse: 0.08081818204153189
mae: 0.037869205390910585
r2: 0.7054939980157809
pearson: 0.8434730487025757

=== Experiment 2745 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005850325743092604
rmse: 0.0764874221234616
mae: 0.03126095085882698
r2: 0.736211387169229
pearson: 0.8581108170191946

=== Experiment 2642 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0066672063013638785
rmse: 0.08165296260004212
mae: 0.0339467867252673
r2: 0.6993786023334061
pearson: 0.8387884318201065

=== Experiment 2771 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008353190099830063
rmse: 0.09139578819524488
mae: 0.04447804282391296
r2: 0.6233583349187838
pearson: 0.8075405447816508

=== Experiment 2509 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0065715342757218225
rmse: 0.08106500031284662
mae: 0.034557081596632774
r2: 0.7036924118611279
pearson: 0.8473129612237155

=== Experiment 2781 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00616771487385602
rmse: 0.07853480039992475
mae: 0.03458123566795384
r2: 0.7219004509567459
pearson: 0.849760830702165

=== Experiment 2854 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007087099006113132
rmse: 0.08418490961041136
mae: 0.037835366015196506
r2: 0.6804458250851748
pearson: 0.825927899402293

=== Experiment 2609 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00669120248841015
rmse: 0.08179977071123214
mae: 0.03877184261876381
r2: 0.6982966248210193
pearson: 0.8431793907717269

=== Experiment 2757 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0063326938895541356
rmse: 0.07957822497111967
mae: 0.030292128624989822
r2: 0.7144616197517357
pearson: 0.8519855498318579

=== Experiment 2560 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005846849449324035
rmse: 0.07646469413607848
mae: 0.028507117716615838
r2: 0.7363681317252198
pearson: 0.8639774333782012

=== Experiment 2802 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007005956308874198
rmse: 0.08370159083837175
mae: 0.042923441486181386
r2: 0.6841045135900461
pearson: 0.8346764653424256

=== Experiment 2799 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0072097463288748964
rmse: 0.08491022511379237
mae: 0.03414139972374463
r2: 0.6749157112830617
pearson: 0.8406143136918912

=== Experiment 2870 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006905262606485386
rmse: 0.08309790976002576
mae: 0.039053005904375576
r2: 0.6886447483120699
pearson: 0.842798001435234

=== Experiment 2718 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006591387539685851
rmse: 0.08118736071388115
mae: 0.039577015538275334
r2: 0.7027972369270794
pearson: 0.8400029658260298

=== Experiment 2822 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007063042177906726
rmse: 0.08404190727194812
mae: 0.035882516493728034
r2: 0.6815305368807818
pearson: 0.8355287651480208

=== Experiment 2562 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005919094376774019
rmse: 0.07693565088289056
mae: 0.030697634974808482
r2: 0.7331106397439243
pearson: 0.857520541525044

=== Experiment 2810 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006644582600035183
rmse: 0.08151430917351371
mae: 0.03604488027887471
r2: 0.7003986950688645
pearson: 0.8474639496502496

=== Experiment 2764 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007460960461819159
rmse: 0.08637685142339445
mae: 0.04489534725986801
r2: 0.6635885765964004
pearson: 0.8192585913455036

=== Experiment 2550 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006818264883210985
rmse: 0.08257278536667505
mae: 0.036332646808715306
r2: 0.6925674373638864
pearson: 0.8412898571519768

=== Experiment 2698 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006711271504634685
rmse: 0.08192235045843524
mae: 0.03673268061145317
r2: 0.6973917217125045
pearson: 0.8389649021255998

=== Experiment 2563 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006478415264894448
rmse: 0.08048860332304474
mae: 0.03775621307787157
r2: 0.7078911070745232
pearson: 0.8486417536736145

=== Experiment 2653 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007225979968473874
rmse: 0.08500576432497901
mae: 0.03574789530627726
r2: 0.674183743618515
pearson: 0.8342027846931269

=== Experiment 2591 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006379399275252747
rmse: 0.07987114169243324
mae: 0.036883228424939345
r2: 0.7123556944671976
pearson: 0.8451549718770698

=== Experiment 2903 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006615396027688443
rmse: 0.08133508485080988
mae: 0.03454360772201526
r2: 0.7017147047699862
pearson: 0.8420926787108328

=== Experiment 2830 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007198107797135123
rmse: 0.08484166309741413
mae: 0.03795530816050206
r2: 0.6754404875566984
pearson: 0.8228042771308431

=== Experiment 2847 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007155769366453746
rmse: 0.0845917807263433
mae: 0.04800989646730692
r2: 0.6773495087615534
pearson: 0.8369370619297807

=== Experiment 2565 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005900950957810921
rmse: 0.07681764743736247
mae: 0.031227700819990985
r2: 0.733928718519441
pearson: 0.8584234673525788

=== Experiment 2749 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0071991133947798
rmse: 0.08484758921018204
mae: 0.034344248831198
r2: 0.6753951456014974
pearson: 0.8260389228706649

=== Experiment 2773 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006934976704015297
rmse: 0.08327650751571716
mae: 0.03424978956041916
r2: 0.6873049527326207
pearson: 0.8331656491492594

=== Experiment 2816 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005632485346121166
rmse: 0.07504988571690943
mae: 0.02708311532697435
r2: 0.7460337147897802
pearson: 0.8655630903928476

=== Experiment 2853 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00580896097210849
rmse: 0.07621654001664264
mae: 0.028513376215370603
r2: 0.7380765064868747
pearson: 0.8593035485159554

=== Experiment 2921 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007627443242755766
rmse: 0.08733523482968238
mae: 0.050617874073658156
r2: 0.6560819412786443
pearson: 0.825133367806462

=== Experiment 2776 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005586011988058419
rmse: 0.07473962796307203
mae: 0.02645769310300827
r2: 0.7481291780503403
pearson: 0.8653294897389975

=== Experiment 2842 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005858478602570642
rmse: 0.07654069899452606
mae: 0.028404556686223558
r2: 0.735843778323712
pearson: 0.8590843844831497

=== Experiment 2713 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005708112112292153
rmse: 0.07555204902775406
mae: 0.029182567259282227
r2: 0.7426237371890156
pearson: 0.8676776095886536

=== Experiment 2741 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005966056381443223
rmse: 0.07724025104466727
mae: 0.031831098578174975
r2: 0.7309931436229489
pearson: 0.8576910039739485

=== Experiment 2948 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007375187260484728
rmse: 0.08587891045236151
mae: 0.039758770041234605
r2: 0.667456052492897
pearson: 0.8229040065071166

=== Experiment 2783 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006846776142991809
rmse: 0.0827452484617202
mae: 0.038361269262590474
r2: 0.6912818772091341
pearson: 0.8316112209181233

=== Experiment 2564 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007155150143750139
rmse: 0.08458812058291719
mae: 0.030117614249460863
r2: 0.6773774292407183
pearson: 0.8417831435933657

=== Experiment 2716 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007204617452895024
rmse: 0.08488001798359272
mae: 0.04040263849149467
r2: 0.6751469700436031
pearson: 0.8251198400074233

=== Experiment 2839 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0072608724080670695
rmse: 0.08521075288992035
mae: 0.04414326400162938
r2: 0.6726104588745949
pearson: 0.8249866585469154

=== Experiment 2900 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006610085530106942
rmse: 0.08130243249809284
mae: 0.041033908646075784
r2: 0.7019541527686073
pearson: 0.8437222056685416

=== Experiment 2811 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005697848117442332
rmse: 0.07548409181703342
mae: 0.027626506309264838
r2: 0.7430865361992645
pearson: 0.8622860333333443

=== Experiment 2598 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006725322660759711
rmse: 0.0820080646080598
mae: 0.0364573677149012
r2: 0.696758161267214
pearson: 0.8361022473617767

=== Experiment 2663 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007638203541497887
rmse: 0.08739681654098098
mae: 0.03837964134307109
r2: 0.6555967641443317
pearson: 0.8109351621400755

=== Experiment 2929 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008587399883353192
rmse: 0.09266822477717587
mae: 0.04043164828156608
r2: 0.6127979188633348
pearson: 0.7883393307955368

=== Experiment 2710 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005765482191425894
rmse: 0.07593077236157876
mae: 0.026898266133243743
r2: 0.740036945571378
pearson: 0.861844821173606

=== Experiment 2632 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0056237075136776156
rmse: 0.07499138292949141
mae: 0.02886434414760995
r2: 0.7464295033912404
pearson: 0.8642612959170591

=== Experiment 2886 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006209702919496021
rmse: 0.07880166825325477
mae: 0.030687205121572927
r2: 0.7200072284591904
pearson: 0.8487064234055769

=== Experiment 2871 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006710366510338845
rmse: 0.08191682678387173
mae: 0.03638582140277216
r2: 0.6974325275069856
pearson: 0.8387636705910988

=== Experiment 2888 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006985764795854313
rmse: 0.08358088774267902
mae: 0.03306629120691865
r2: 0.6850149400251483
pearson: 0.8322506771576157

=== Experiment 2920 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006194013285808724
rmse: 0.07870205388557991
mae: 0.03203014209957003
r2: 0.7207146671366149
pearson: 0.8493982866622283

=== Experiment 2922 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006222650968058938
rmse: 0.07888378140060819
mae: 0.038115990448632356
r2: 0.7194234066483605
pearson: 0.8511488528180086

=== Experiment 2877 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0062752573716064915
rmse: 0.07921652208729245
mae: 0.03477941577266869
r2: 0.7170514070662501
pearson: 0.8498643024989853

=== Experiment 2834 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006374737171980316
rmse: 0.07984195120348898
mae: 0.04031878914957192
r2: 0.712565906651803
pearson: 0.8487247629683752

=== Experiment 2911 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0062863864291888135
rmse: 0.07928673551855199
mae: 0.03413895348902489
r2: 0.7165496027581362
pearson: 0.8469375173521214

=== Experiment 2916 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0072109162131497526
rmse: 0.08491711378249824
mae: 0.030878360061717982
r2: 0.674862961716014
pearson: 0.8292327336517514

=== Experiment 2902 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007303277229549041
rmse: 0.08545921383647898
mae: 0.036773750413884074
r2: 0.6706984441377619
pearson: 0.8361029325368406

=== Experiment 2937 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0063058079569968985
rmse: 0.07940911759361703
mae: 0.03236955590955172
r2: 0.7156738946173378
pearson: 0.8461421720485693

=== Experiment 2672 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00742613115839978
rmse: 0.08617500309486377
mae: 0.042712294030118794
r2: 0.6651590145580314
pearson: 0.8190771690226597

=== Experiment 2899 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005986810821673965
rmse: 0.07737448430635234
mae: 0.028052444370728888
r2: 0.7300573350476725
pearson: 0.875575555782208

=== Experiment 2795 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006412662348514682
rmse: 0.08007910057258812
mae: 0.029965126939858144
r2: 0.710855877134015
pearson: 0.8507766555132664

=== Experiment 2949 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006871834913434975
rmse: 0.08289653137155363
mae: 0.035183963772094186
r2: 0.6901519882790601
pearson: 0.8324745093259441

=== Experiment 2747 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0071526110280533866
rmse: 0.08457311054970951
mae: 0.04830922509599872
r2: 0.6774919168499345
pearson: 0.8349661552648098

=== Experiment 2530 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006715970831382692
rmse: 0.08195102703067664
mae: 0.04035639348348559
r2: 0.6971798311377688
pearson: 0.8392402269653598

=== Experiment 2803 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006247755929018294
rmse: 0.07904274747893253
mae: 0.030951018028869657
r2: 0.7182914350082414
pearson: 0.8479343140173893

=== Experiment 2887 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006656641468089192
rmse: 0.08158824344284654
mae: 0.036346670777200475
r2: 0.6998549660158231
pearson: 0.8380499403442037

=== Experiment 2850 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006479740480879543
rmse: 0.08049683522275608
mae: 0.028792365222090808
r2: 0.7078313536690273
pearson: 0.8422350421638749

=== Experiment 2841 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0061184870333445196
rmse: 0.07822075832759817
mae: 0.028542644762375977
r2: 0.72412011261858
pearson: 0.8680678501297133

=== Experiment 2765 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005977414429057564
rmse: 0.07731374023456351
mae: 0.02745415249185094
r2: 0.7304810142550773
pearson: 0.8556076560520582

=== Experiment 2978 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005633020015198054
rmse: 0.07505344772359264
mae: 0.032526983322137396
r2: 0.7460096067964292
pearson: 0.8682260766627596

=== Experiment 2868 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005769529080770444
rmse: 0.07595741623285013
mae: 0.029945977914315445
r2: 0.7398544731120045
pearson: 0.8612812467344649

=== Experiment 2843 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00832249768152948
rmse: 0.09122772430313868
mae: 0.052765926120795156
r2: 0.6247422425511906
pearson: 0.8034485772259534

=== Experiment 2874 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006307768473896256
rmse: 0.07942146104105777
mae: 0.030129519179811225
r2: 0.715585495773235
pearson: 0.849948558783955

=== Experiment 2970 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006614253400525257
rmse: 0.08132806035142641
mae: 0.033290426841937074
r2: 0.7017662253258656
pearson: 0.8404859011804476

=== Experiment 2792 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006117949584387945
rmse: 0.07821732278969887
mae: 0.028988279754122295
r2: 0.7241443459554825
pearson: 0.8519410414986174

=== Experiment 2820 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006236837207862918
rmse: 0.0789736488194823
mae: 0.030803713379463266
r2: 0.7187837553394407
pearson: 0.8508913416305558

=== Experiment 2719 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007824686328545832
rmse: 0.08845725707111787
mae: 0.05258454488899082
r2: 0.6471883373536911
pearson: 0.8286150287293367

=== Experiment 2880 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007702595268329558
rmse: 0.08776443054181778
mae: 0.04948542741045741
r2: 0.6526933695224579
pearson: 0.8173420874336872

=== Experiment 2891 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006908167175785892
rmse: 0.08311538471177217
mae: 0.03036344153009585
r2: 0.6885137825607087
pearson: 0.8444108380117843

=== Experiment 2692 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006891945736657739
rmse: 0.08301774350497453
mae: 0.03422161533133259
r2: 0.6892452001113959
pearson: 0.8329769396047961

=== Experiment 2769 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006422558017641397
rmse: 0.08014086359430747
mae: 0.027995137259946488
r2: 0.7104096857685085
pearson: 0.844331430035759

=== Experiment 2956 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005757899543091048
rmse: 0.07588082460734759
mae: 0.027424531213385325
r2: 0.7403788438474175
pearson: 0.865367153691505

=== Experiment 2892 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007200433711981055
rmse: 0.08485536937625723
mae: 0.041839091624931275
r2: 0.675335613080009
pearson: 0.8263099435144203

=== Experiment 2935 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005772071102551819
rmse: 0.07597414759345325
mae: 0.03021455034451471
r2: 0.7397398544687117
pearson: 0.8603474703583271

=== Experiment 2964 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0075706346352314245
rmse: 0.08700939394819059
mae: 0.044565213588868786
r2: 0.6586434163885364
pearson: 0.831592443014097

=== Experiment 2962 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006180006932483705
rmse: 0.07861302012061173
mae: 0.034744699628685524
r2: 0.7213462074433723
pearson: 0.8494001392703359

=== Experiment 2529 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006690975304573828
rmse: 0.08179838204129608
mae: 0.0317158633505191
r2: 0.6983068684402078
pearson: 0.8407847203833577

=== Experiment 2836 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0057665683366552485
rmse: 0.07593792423193597
mae: 0.032462300597691625
r2: 0.7399879717610356
pearson: 0.8604390461245849

=== Experiment 2846 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006130593283117249
rmse: 0.07829810523325101
mae: 0.027811648751088994
r2: 0.7235742471446955
pearson: 0.8516471759304285

=== Experiment 2917 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006924945561228516
rmse: 0.08321625779394623
mae: 0.04241062906394774
r2: 0.6877572525458331
pearson: 0.8335529531067833

=== Experiment 2945 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006986733755362094
rmse: 0.08358668407923653
mae: 0.04376177893019157
r2: 0.684971250067414
pearson: 0.8353091385023078

=== Experiment 2553 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006164855396953161
rmse: 0.07851659313134492
mae: 0.030203196336410568
r2: 0.7220293835117438
pearson: 0.8509187766877233

=== Experiment 2629 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007414721449155454
rmse: 0.08610877684159411
mae: 0.03660035431092703
r2: 0.665673473326061
pearson: 0.8282285842365315

=== Experiment 2628 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007121792931516987
rmse: 0.0843907159083094
mae: 0.032572202859827176
r2: 0.6788814912586791
pearson: 0.8444790648529837

=== Experiment 2855 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007396274195718441
rmse: 0.08600159414637872
mae: 0.049564299650549405
r2: 0.6665052518642788
pearson: 0.8356334555068152

=== Experiment 2676 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00673960419671999
rmse: 0.08209509240338297
mae: 0.038982271512807525
r2: 0.6961142130965496
pearson: 0.8401165978760466

=== Experiment 2796 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00613059387706437
rmse: 0.07829810902610848
mae: 0.030900029222119704
r2: 0.7235742203638813
pearson: 0.8532486923466122

=== Experiment 2777 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00756756238902803
rmse: 0.08699173747562483
mae: 0.04900445754841626
r2: 0.6587819426176487
pearson: 0.8234239252806972

=== Experiment 2994 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006139860876224631
rmse: 0.07835726434878026
mae: 0.03153006288165209
r2: 0.7231563754504633
pearson: 0.8535516421584916

=== Experiment 2689 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006306392340986792
rmse: 0.079412797085777
mae: 0.035121540033177405
r2: 0.7156475450004404
pearson: 0.8482838671047438

=== Experiment 2756 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007565161073048545
rmse: 0.0869779344032068
mae: 0.04906967911893055
r2: 0.6588902168982635
pearson: 0.8250162128593341

=== Experiment 2934 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005581313303628001
rmse: 0.07470818766124634
mae: 0.026593465238938113
r2: 0.7483410396632587
pearson: 0.8676854274369806

=== Experiment 2735 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006568133507465299
rmse: 0.08104402203410994
mae: 0.04124906864000763
r2: 0.7038457510050218
pearson: 0.8429804510206098

=== Experiment 2601 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006269299498212458
rmse: 0.07917890816506917
mae: 0.032547487568160414
r2: 0.7173200449553265
pearson: 0.8549210432935456

=== Experiment 2774 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005911868845941771
rmse: 0.07688867826892183
mae: 0.029537585260027432
r2: 0.7334364357489509
pearson: 0.8574984040792204

=== Experiment 2623 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007652384153888919
rmse: 0.0874779066615618
mae: 0.03830038816223938
r2: 0.6549573665729849
pearson: 0.8171251823305968

=== Experiment 2831 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006432810042875299
rmse: 0.08020480062237734
mae: 0.028327911695966993
r2: 0.7099474264629733
pearson: 0.8474817615485354

=== Experiment 2506 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006920485246999553
rmse: 0.08318945394098673
mae: 0.03038043041353178
r2: 0.6879583661512825
pearson: 0.8303891721766206

=== Experiment 2717 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006965826525194512
rmse: 0.08346152721580472
mae: 0.04167436799553449
r2: 0.6859139478736945
pearson: 0.8442700533315252

=== Experiment 2812 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0075318918627929506
rmse: 0.08678647280995438
mae: 0.03909386546212832
r2: 0.6603903109457891
pearson: 0.8130933853339747

=== Experiment 2960 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0057635342985003085
rmse: 0.07591794450918905
mae: 0.02751428043405914
r2: 0.7401247752060595
pearson: 0.8627607416153307

=== Experiment 2895 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0065284601069580915
rmse: 0.08079888679281473
mae: 0.03110591467584587
r2: 0.7056346071722929
pearson: 0.8464103880175764

=== Experiment 2525 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007158023238506076
rmse: 0.08460510172859599
mae: 0.038555187975415836
r2: 0.6772478826627188
pearson: 0.8472648523028851

=== Experiment 2955 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006957367704836285
rmse: 0.08341083685490923
mae: 0.04739613390111673
r2: 0.6862953523606354
pearson: 0.839552563095921

=== Experiment 2617 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006115075604206531
rmse: 0.07819894886893641
mae: 0.02793907830054645
r2: 0.7242739324569271
pearson: 0.8549695796313084

=== Experiment 2984 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006357764783530261
rmse: 0.07973559295277273
mae: 0.037584513115538676
r2: 0.7133311841768966
pearson: 0.8486864152865938

=== Experiment 2607 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006162743823991773
rmse: 0.07850314531273109
mae: 0.03507444781003985
r2: 0.7221245934071985
pearson: 0.8505619070145495

=== Experiment 2738 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007195696865110631
rmse: 0.08482745348712663
mae: 0.03844991924209524
r2: 0.6755491954205505
pearson: 0.8255713839380365

=== Experiment 2908 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007796932964631299
rmse: 0.08830024328749779
mae: 0.045766115410253425
r2: 0.6484397243174078
pearson: 0.8224013979331882

=== Experiment 2845 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007555211667887561
rmse: 0.0869207205900156
mae: 0.04638192780290875
r2: 0.6593388311979043
pearson: 0.8272901866588166

=== Experiment 2534 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007623234760861733
rmse: 0.08731113766789282
mae: 0.030896353593394956
r2: 0.6562716998749516
pearson: 0.8522551568169169

=== Experiment 2600 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0071624278253588055
rmse: 0.08463112799294835
mae: 0.04244704544197559
r2: 0.6770492817801361
pearson: 0.8290468632071879

=== Experiment 2824 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005623127931267692
rmse: 0.07498751850319953
mae: 0.02877514204258932
r2: 0.7464556365070101
pearson: 0.8662320799706055

=== Experiment 2914 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006226542392816877
rmse: 0.07890844310222371
mae: 0.034347835972527864
r2: 0.719247944018772
pearson: 0.8491079738446824

=== Experiment 2806 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006991042600727635
rmse: 0.08361245481821254
mae: 0.04659723274497605
r2: 0.6847769661263496
pearson: 0.8369939280205181

=== Experiment 2570 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063516687714223466
rmse: 0.07969735736787228
mae: 0.03265298804598227
r2: 0.7136060506797823
pearson: 0.85248827267212

=== Experiment 2981 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005772318809627377
rmse: 0.0759757777823128
mae: 0.029234979664750412
r2: 0.7397286854656991
pearson: 0.8640378140515047

=== Experiment 2701 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008138078087131849
rmse: 0.09021129689308234
mae: 0.0426431956175124
r2: 0.6330576408933091
pearson: 0.7998621538558226

=== Experiment 2821 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007980778970221925
rmse: 0.08933520565948189
mae: 0.04171461492903047
r2: 0.6401501888421417
pearson: 0.8017898031799251

=== Experiment 2815 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007852633325566356
rmse: 0.08861508520317721
mae: 0.047512644093230534
r2: 0.6459282195584498
pearson: 0.830777963649542

=== Experiment 2952 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007252774731630069
rmse: 0.0851632240561034
mae: 0.03591478345037424
r2: 0.6729755795410793
pearson: 0.8465892919488895

=== Experiment 2852 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006457123375911002
rmse: 0.0803562279845875
mae: 0.03658390597447034
r2: 0.7088511489775097
pearson: 0.8501169913979236

=== Experiment 2555 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007544308476633652
rmse: 0.08685797877359139
mae: 0.032554645187480705
r2: 0.6598304512926816
pearson: 0.8173496568789459

=== Experiment 2983 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00713394059576947
rmse: 0.08446265799611963
mae: 0.04926469784666726
r2: 0.6783337584241307
pearson: 0.8358859817966184

=== Experiment 2664 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007678145693690276
rmse: 0.08762502892262163
mae: 0.033318599735447776
r2: 0.6537957900818623
pearson: 0.8324872047676155

=== Experiment 2645 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006325496469193398
rmse: 0.07953298981676345
mae: 0.029634542532176556
r2: 0.7147861482679744
pearson: 0.8465531221539574

=== Experiment 2971 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006672194046436813
rmse: 0.08168349922987392
mae: 0.03340118757233734
r2: 0.699153707103342
pearson: 0.8431182304861563

=== Experiment 2679 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0059223731100065714
rmse: 0.07695695621583906
mae: 0.03417669991850075
r2: 0.7329628031055494
pearson: 0.8568807864542344

=== Experiment 2627 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007759099605227671
rmse: 0.08808575143136188
mae: 0.03194490959250199
r2: 0.6501456138411825
pearson: 0.8356000170217207

=== Experiment 2910 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00570074127222536
rmse: 0.07550325338834983
mae: 0.027598641827981267
r2: 0.7429560851234724
pearson: 0.877145929041149

=== Experiment 2748 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006074936533288874
rmse: 0.0779418792003944
mae: 0.02996283324570067
r2: 0.726083785498048
pearson: 0.8523341206490629

=== Experiment 2582 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006615192344531053
rmse: 0.08133383271757856
mae: 0.03666736664242754
r2: 0.7017238887539051
pearson: 0.8379453149076637

=== Experiment 2969 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006008963018330236
rmse: 0.07751750136795069
mae: 0.029653275234177403
r2: 0.7290585022503686
pearson: 0.8600275766518691

=== Experiment 2778 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006605121413794561
rmse: 0.08127189805704406
mae: 0.04101574046485993
r2: 0.7021779825882578
pearson: 0.8425741005534486

=== Experiment 2559 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.011520433667008367
rmse: 0.10733328312787403
mae: 0.04780244105012478
r2: 0.4805487164852137
pearson: 0.7030700481209571

=== Experiment 2933 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0062414028969095285
rmse: 0.07900254993928695
mae: 0.03557593521465938
r2: 0.7185778904298425
pearson: 0.8482975069082755

=== Experiment 2858 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005852356262504818
rmse: 0.07650069452302259
mae: 0.030236008343319187
r2: 0.7361198319426321
pearson: 0.8583919596107576

=== Experiment 2732 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006561307775126383
rmse: 0.08100189982417932
mae: 0.03979242778122505
r2: 0.7041535202719489
pearson: 0.8472095777679433

=== Experiment 2782 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006537795136930018
rmse: 0.08085663322776937
mae: 0.03586544504198683
r2: 0.7052136947795196
pearson: 0.8446833058967883

=== Experiment 2754 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005784526027036393
rmse: 0.07605607159876451
mae: 0.029995267691776736
r2: 0.7391782674055338
pearson: 0.8703216308094259

=== Experiment 2944 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006370890925719469
rmse: 0.07981786094427405
mae: 0.02978336589531412
r2: 0.712739332202842
pearson: 0.8543080446338194

=== Experiment 2809 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005769501910872707
rmse: 0.07595723738310067
mae: 0.03189727865728678
r2: 0.7398556981907352
pearson: 0.8621829407045419

=== Experiment 2575 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006907266209574733
rmse: 0.08310996456246827
mae: 0.03891003574615769
r2: 0.6885544067306244
pearson: 0.8401867713118076

=== Experiment 2702 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006439190849518859
rmse: 0.08024456897210464
mae: 0.03530409702649391
r2: 0.7096597186998229
pearson: 0.8443112982322969

=== Experiment 2743 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00601279495497551
rmse: 0.07754221401904585
mae: 0.03033348589738482
r2: 0.7288857219135974
pearson: 0.8601201809942681

=== Experiment 2786 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0059435484204257705
rmse: 0.07709441238135077
mae: 0.029535129200384605
r2: 0.7320080176787145
pearson: 0.8611673926762673

=== Experiment 2893 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008230643464095088
rmse: 0.09072289382562203
mae: 0.03358551157797289
r2: 0.6288839087871745
pearson: 0.8339056261075242

=== Experiment 2972 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005571744035207372
rmse: 0.07464411587799384
mae: 0.028780517581630572
r2: 0.7487725137645876
pearson: 0.8657819543674371

=== Experiment 2761 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005876441565052336
rmse: 0.0766579517405229
mae: 0.03022457658865345
r2: 0.7350338362515169
pearson: 0.8604528205038922

=== Experiment 2923 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007215916291626396
rmse: 0.08494654961578131
mae: 0.04194815614823827
r2: 0.6746375103782049
pearson: 0.8245696413381193

=== Experiment 2752 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006229573663465295
rmse: 0.07892764828287548
mae: 0.030449110915146144
r2: 0.7191112653593992
pearson: 0.8577379732536734

=== Experiment 2993 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005785662414284694
rmse: 0.07606354195200676
mae: 0.02986358932655742
r2: 0.7391270282046701
pearson: 0.8602138935858226

=== Experiment 2687 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006102374491741844
rmse: 0.07811769640575587
mae: 0.026311319177477303
r2: 0.7248466200277728
pearson: 0.8580053219437098

=== Experiment 2728 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006349997618700961
rmse: 0.07968687231094568
mae: 0.04224333627951267
r2: 0.713681402220459
pearson: 0.8545039537025677

=== Experiment 2918 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007527269622601959
rmse: 0.08675983876542163
mae: 0.047392513302733814
r2: 0.6605987257215974
pearson: 0.8255738626479421

=== Experiment 2829 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005641855514844263
rmse: 0.0751122860445897
mae: 0.027535079604730506
r2: 0.7456112180061824
pearson: 0.8638379085499908

=== Experiment 2928 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0062953875905250485
rmse: 0.07934347856330128
mae: 0.0292475997786002
r2: 0.7161437443551992
pearson: 0.8505390151756874

=== Experiment 2569 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006213070368167143
rmse: 0.07882303196507441
mae: 0.029425760802050286
r2: 0.7198553916807371
pearson: 0.8493367861382595

=== Experiment 2976 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007308666020349818
rmse: 0.08549073645927854
mae: 0.031907886116614256
r2: 0.6704554659323432
pearson: 0.8376505490466146

=== Experiment 2924 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007873026522703677
rmse: 0.08873007676489228
mae: 0.033223047429941976
r2: 0.6450086992752604
pearson: 0.8225176494907522

=== Experiment 2514 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005565960486910199
rmse: 0.07460536500085097
mae: 0.028509660962275425
r2: 0.749033291411773
pearson: 0.8660243508127214

=== Experiment 2881 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007236264596493283
rmse: 0.08506623652480039
mae: 0.04550428055452994
r2: 0.6737200142677306
pearson: 0.8303414821461708

=== Experiment 2919 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00749814316235055
rmse: 0.08659181925765591
mae: 0.039953601685804245
r2: 0.6619120249947981
pearson: 0.8161305002153901

=== Experiment 2618 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005882352934886667
rmse: 0.07669649884373254
mae: 0.028752716479304632
r2: 0.7347672951874797
pearson: 0.8581594174081201

=== Experiment 2961 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005661092522360014
rmse: 0.07524023207274161
mae: 0.030792747483075335
r2: 0.7447438298041515
pearson: 0.8656469861996429

=== Experiment 2968 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006526585868276392
rmse: 0.08078728778883712
mae: 0.04048651421935673
r2: 0.7057191157695352
pearson: 0.8518205371530075

=== Experiment 2997 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005980277169437737
rmse: 0.0773322518063307
mae: 0.031506132922625205
r2: 0.7303519345513236
pearson: 0.8553904526957364

=== Experiment 2721 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005991917220757205
rmse: 0.07740747522531144
mae: 0.028547935832912003
r2: 0.7298270897605073
pearson: 0.8684132703008492

=== Experiment 2926 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005826525526128446
rmse: 0.07633168101207025
mae: 0.028921157723506737
r2: 0.7372845284768663
pearson: 0.8623317635734653

=== Experiment 2770 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005992892044797084
rmse: 0.0774137716740186
mae: 0.02613832944312034
r2: 0.729783135373602
pearson: 0.8699918367285395

=== Experiment 2851 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006440828535363411
rmse: 0.08025477266408154
mae: 0.03308054460245302
r2: 0.7095858761658927
pearson: 0.8447037852345991

=== Experiment 2950 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006745288392413985
rmse: 0.08212970468967963
mae: 0.03360393089669912
r2: 0.6958579152145132
pearson: 0.8376978718110082

=== Experiment 2814 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006385172877838587
rmse: 0.07990727675148608
mae: 0.02847543736207276
r2: 0.712095365267759
pearson: 0.8506239866217781

=== Experiment 2568 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005937064391314638
rmse: 0.07705234838286655
mae: 0.03303090979453467
r2: 0.7323003796974961
pearson: 0.8564595941441973

=== Experiment 2878 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005496688193023201
rmse: 0.07413965331064883
mae: 0.03052099393723613
r2: 0.7521567486540695
pearson: 0.8674997150053837

=== Experiment 2869 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006536409550790798
rmse: 0.0808480646075761
mae: 0.03209033568958874
r2: 0.7052761702486938
pearson: 0.8526436321907286

=== Experiment 2631 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006291685599498683
rmse: 0.07932014623977117
mae: 0.03153095056139047
r2: 0.7163106655012074
pearson: 0.8479416129702314

=== Experiment 2967 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0058651383962292115
rmse: 0.07658419155562858
mae: 0.03168788236853097
r2: 0.7355434911588465
pearson: 0.8636308505439015

=== Experiment 2515 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007070431300597503
rmse: 0.08408585672155278
mae: 0.032218668180158484
r2: 0.6811973645908562
pearson: 0.8417803915698134

=== Experiment 2669 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005883621445050673
rmse: 0.07670476807246518
mae: 0.029590214199952537
r2: 0.7347100986224988
pearson: 0.8587124780396127

=== Experiment 2998 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006278467845820787
rmse: 0.07923678341414918
mae: 0.031720020934989525
r2: 0.7169066481969655
pearson: 0.8487141894950417

=== Experiment 2556 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006674216746329712
rmse: 0.08169587961659824
mae: 0.036190288168591454
r2: 0.6990625044554295
pearson: 0.8429762439070794

=== Experiment 2876 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007371877365743414
rmse: 0.08585963758218068
mae: 0.04627935260567037
r2: 0.6676052941899868
pearson: 0.8366344158231932

=== Experiment 2889 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005963976797041285
rmse: 0.07722678807927522
mae: 0.030899925229414465
r2: 0.7310869111683371
pearson: 0.8578887744729834

=== Experiment 2959 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005945872455382422
rmse: 0.0771094835631936
mae: 0.03177833739987218
r2: 0.7319032279654052
pearson: 0.8567142695631231

=== Experiment 2825 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006382658302542548
rmse: 0.07989154086974758
mae: 0.036756565213586986
r2: 0.7122087463611082
pearson: 0.8455222964522153

=== Experiment 2517 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007552361947846749
rmse: 0.0869043264046546
mae: 0.049275029489602704
r2: 0.6594673238202868
pearson: 0.8220205380528761

=== Experiment 2625 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007519596247508753
rmse: 0.08671560555925763
mae: 0.048626759783636925
r2: 0.6609447148272396
pearson: 0.823823627880748

=== Experiment 2940 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005787904869329276
rmse: 0.07607828119331611
mae: 0.02852325347416656
r2: 0.7390259168936899
pearson: 0.8649330056690463

=== Experiment 2587 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00637607725913485
rmse: 0.07985034288677069
mae: 0.03570472589000017
r2: 0.7125054827118225
pearson: 0.8583499573892726

=== Experiment 2647 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0061492290953134516
rmse: 0.07841702044399194
mae: 0.03646113496059023
r2: 0.722733966575017
pearson: 0.8521446330299103

=== Experiment 2966 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007077150157682797
rmse: 0.08412579959609773
mae: 0.03857680948703969
r2: 0.6808944142820754
pearson: 0.8479519134016242

=== Experiment 2611 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006906778525018598
rmse: 0.08310703053905005
mae: 0.04185348477280693
r2: 0.6885763962126141
pearson: 0.8376680414856743

=== Experiment 2704 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006598753695985923
rmse: 0.08123271321324878
mae: 0.029414201064294435
r2: 0.7024651001816025
pearson: 0.8467492138070105

=== Experiment 2616 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006785749002555053
rmse: 0.08237565782775305
mae: 0.036904865831801255
r2: 0.6940335641113288
pearson: 0.8435372538338816

=== Experiment 2958 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006577282850676577
rmse: 0.08110044913979562
mae: 0.03503216206634558
r2: 0.7034332111465591
pearson: 0.83960376747992

=== Experiment 2860 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00583936856669702
rmse: 0.07641576124528905
mae: 0.02890233807076792
r2: 0.7367054414302838
pearson: 0.860088915419079

=== Experiment 2744 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006296619208118976
rmse: 0.0793512394869732
mae: 0.03362134194271173
r2: 0.7160882112599656
pearson: 0.8490662085009204

=== Experiment 2740 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006544512666009192
rmse: 0.0808981623154024
mae: 0.02934442668054458
r2: 0.7049108043499502
pearson: 0.8441793641555654

=== Experiment 2915 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005855276655641185
rmse: 0.07651977950596293
mae: 0.033141764695239245
r2: 0.7359881527014767
pearson: 0.8607843593976335

=== Experiment 2975 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006720322337649352
rmse: 0.0819775721624479
mae: 0.03795144415035164
r2: 0.6969836236354495
pearson: 0.839823323773707

=== Experiment 2882 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007306662705952428
rmse: 0.08547901909797764
mae: 0.03697447725804599
r2: 0.6705457944967954
pearson: 0.8323538950481786

=== Experiment 2988 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0055902768935118625
rmse: 0.0747681542738074
mae: 0.027787625564036283
r2: 0.7479368753405731
pearson: 0.8658151079326155

=== Experiment 2578 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006733186300446342
rmse: 0.08205599490863749
mae: 0.040482735053526626
r2: 0.6964035932147963
pearson: 0.8500886260910893

=== Experiment 2500 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00627687508303179
rmse: 0.07922673212389736
mae: 0.031240935682912925
r2: 0.7169784651700928
pearson: 0.8483548464952346

=== Experiment 2898 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006429931862897133
rmse: 0.08018685592350615
mae: 0.0373183696201958
r2: 0.7100772023314049
pearson: 0.8443040162747577

=== Experiment 2890 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006012615737887795
rmse: 0.07754105840061634
mae: 0.033324024218132256
r2: 0.7288938027332029
pearson: 0.8547090522056104

=== Experiment 2800 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005677406572484044
rmse: 0.07534856715614467
mae: 0.030298485071574833
r2: 0.7440082364644217
pearson: 0.8647910168043538

=== Experiment 2936 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0054000537243321725
rmse: 0.07348505783036557
mae: 0.027796578960399346
r2: 0.7565139543152658
pearson: 0.8701635058149961

=== Experiment 2943 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006441587227336282
rmse: 0.080259499296571
mae: 0.033006223651735965
r2: 0.7095516670787618
pearson: 0.8446435530240954

=== Experiment 2737 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0066686756098101015
rmse: 0.08166195938017959
mae: 0.03057691921494003
r2: 0.6993123518622582
pearson: 0.8460547764946702

=== Experiment 2991 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006582834328102398
rmse: 0.08113466785599358
mae: 0.030061891062424698
r2: 0.7031828974728211
pearson: 0.846453116467994

=== Experiment 2680 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008082564408337425
rmse: 0.08990308341952141
mae: 0.04767334355778959
r2: 0.6355607282367111
pearson: 0.8449486042678549

=== Experiment 2615 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0055384998888100105
rmse: 0.07442109841174081
mae: 0.028543649898316727
r2: 0.7502714776937769
pearson: 0.8670876470842799

=== Experiment 2808 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006676684192481542
rmse: 0.08171097963236974
mae: 0.04323583685400386
r2: 0.6989512483944496
pearson: 0.8431128390919912

=== Experiment 2906 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008077360876141805
rmse: 0.08987413908428724
mae: 0.054622107696098245
r2: 0.6357953532131618
pearson: 0.8196328250645978

=== Experiment 2750 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0064609673395315155
rmse: 0.0803801426941475
mae: 0.03708666590946554
r2: 0.7086778263497184
pearson: 0.8523198648398553

=== Experiment 2671 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005705754894220621
rmse: 0.07553644745565297
mae: 0.028463694567848884
r2: 0.74273002311437
pearson: 0.8635377763902655

=== Experiment 2711 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0063947339034341015
rmse: 0.07996708012322384
mae: 0.0312679791784584
r2: 0.7116642628317864
pearson: 0.8498165435564006

=== Experiment 2973 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007753399124571444
rmse: 0.08805338792216597
mae: 0.053130109426453125
r2: 0.6504026460049983
pearson: 0.8274286904514713

=== Experiment 2739 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005618166013255113
rmse: 0.07495442624191792
mae: 0.028884995955001814
r2: 0.7466793672062901
pearson: 0.8643121764525151

=== Experiment 2746 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006554206158638451
rmse: 0.0809580518456222
mae: 0.03462883025385632
r2: 0.7044737290337294
pearson: 0.8396615611286248

=== Experiment 2925 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005796361391797372
rmse: 0.07613383867766929
mae: 0.028409297732471205
r2: 0.7386446160176036
pearson: 0.8601977293280262

=== Experiment 2594 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006622711606131512
rmse: 0.08138004427457331
mae: 0.036632640090268696
r2: 0.7013848485578206
pearson: 0.8397663799907411

=== Experiment 2848 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006292213840789149
rmse: 0.0793234759752064
mae: 0.0318706663076956
r2: 0.716286847333917
pearson: 0.8573939846466497

=== Experiment 2844 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00754653473212541
rmse: 0.08687079332045616
mae: 0.04055564002434162
r2: 0.6597300704124083
pearson: 0.8286349599425541

=== Experiment 2510 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006667725189366941
rmse: 0.08165613993673067
mae: 0.03330132179707584
r2: 0.6993552059017281
pearson: 0.8374366744959497

=== Experiment 2697 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006204500895026177
rmse: 0.07876865426694922
mae: 0.028210798630913636
r2: 0.7202417854529497
pearson: 0.8510009406659087

=== Experiment 2873 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005843798205297995
rmse: 0.0764447395528168
mae: 0.02952253905857346
r2: 0.7365057109753987
pearson: 0.8595133071735799

=== Experiment 2807 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006030901656566603
rmse: 0.07765888008828484
mae: 0.03752105731964641
r2: 0.7280692987082238
pearson: 0.8596288026480757

=== Experiment 2707 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00764117077880647
rmse: 0.08741379055278675
mae: 0.040495084168451524
r2: 0.6554629727200687
pearson: 0.8104260885013564

=== Experiment 2613 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0057692421173950155
rmse: 0.07595552723400066
mae: 0.03184314216409466
r2: 0.7398674121643003
pearson: 0.8603967265268938

=== Experiment 2759 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061002195906034165
rmse: 0.07810390253120145
mae: 0.0312798404154137
r2: 0.7249437835716598
pearson: 0.8538437159801873

=== Experiment 2838 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00602852364038427
rmse: 0.07764356792667548
mae: 0.029011069826693055
r2: 0.7281765224112404
pearson: 0.8592105646120092

=== Experiment 2736 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007550376418417446
rmse: 0.08689290200250792
mae: 0.04270497390864147
r2: 0.6595568504683558
pearson: 0.8167752677761686

=== Experiment 2942 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005977796255821888
rmse: 0.07731620952828643
mae: 0.03152604843212603
r2: 0.7304637978543287
pearson: 0.8558494595171685

=== Experiment 2941 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006416756778897736
rmse: 0.08010466140554953
mae: 0.03338851144718946
r2: 0.7106712610701398
pearson: 0.8453118722057628

=== Experiment 2665 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006538194024941188
rmse: 0.08085909982767053
mae: 0.0349911167120278
r2: 0.7051957090946601
pearson: 0.8513107161986375

=== Experiment 2694 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006448973780426945
rmse: 0.08030550280290227
mae: 0.03355685440801065
r2: 0.7092186106509746
pearson: 0.8468397339824842

=== Experiment 2731 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006229420208920999
rmse: 0.07892667615528351
mae: 0.041982862726034506
r2: 0.7191181845572608
pearson: 0.8568057994750999

=== Experiment 2779 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006441383943827572
rmse: 0.08025823287256935
mae: 0.03740109809211469
r2: 0.7095608330426977
pearson: 0.8448536035901738

=== Experiment 2790 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005671481737845556
rmse: 0.07530924072015038
mae: 0.030477622501570635
r2: 0.744275384650552
pearson: 0.8627688792433333

=== Experiment 2723 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006684018838658054
rmse: 0.08175584895686702
mae: 0.03010486010649124
r2: 0.6986205324265753
pearson: 0.8505047517766122

=== Experiment 2588 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006376515523576824
rmse: 0.07985308712615201
mae: 0.030147986002767774
r2: 0.712485721561029
pearson: 0.8444296818063097

=== Experiment 2946 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065172877629210615
rmse: 0.08072972044371925
mae: 0.03805673494757499
r2: 0.7061383632475979
pearson: 0.8521054292098096

=== Experiment 2879 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006143972594664487
rmse: 0.0783834969535328
mae: 0.0323993813570198
r2: 0.722970979875716
pearson: 0.8515131405616729

=== Experiment 2673 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059353706536160004
rmse: 0.07704135677424172
mae: 0.03471520388584722
r2: 0.7323767495848568
pearson: 0.8574780714011057

=== Experiment 2762 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009685428183009421
rmse: 0.09841457302152676
mae: 0.0655986893654844
r2: 0.5632883061110485
pearson: 0.8019949619054201

=== Experiment 2733 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006435788655124976
rmse: 0.08022336726369055
mae: 0.034179071685445726
r2: 0.7098131221476229
pearson: 0.8488445943705956

=== Experiment 2992 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0067935525013277515
rmse: 0.08242300953816083
mae: 0.03702107340655125
r2: 0.6936817077862503
pearson: 0.8332735846468686

=== Experiment 2766 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006741243500944208
rmse: 0.08210507597550963
mae: 0.03792082999854309
r2: 0.6960402975905926
pearson: 0.8386434435812724

=== Experiment 2797 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006132463736866601
rmse: 0.07831004876046625
mae: 0.03701625751368113
r2: 0.723489909208387
pearson: 0.8522583804027201

=== Experiment 2954 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006613801655709618
rmse: 0.08132528300417785
mae: 0.036839245075410275
r2: 0.7017865943007746
pearson: 0.8506201596262641

=== Experiment 2817 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006201598062010246
rmse: 0.07875022579021755
mae: 0.03220048398166599
r2: 0.7203726729160034
pearson: 0.8491761793869501

=== Experiment 2827 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007048385806009094
rmse: 0.08395466518311591
mae: 0.045268032328947445
r2: 0.6821913862388834
pearson: 0.8359976460419772

=== Experiment 2995 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007082666333289097
rmse: 0.08415857848899955
mae: 0.04531975780699264
r2: 0.6806456923518419
pearson: 0.8321588731804205

=== Experiment 2896 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00934304918886627
rmse: 0.09665944955805547
mae: 0.043504626318732253
r2: 0.5787260242644436
pearson: 0.7895439451155177

=== Experiment 2938 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006230794775878587
rmse: 0.07893538354805522
mae: 0.029217606841418494
r2: 0.7190562059381365
pearson: 0.8482742386445269

=== Experiment 2685 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005900918580571259
rmse: 0.07681743669617765
mae: 0.03014532210902044
r2: 0.733930178394927
pearson: 0.85939149045557

=== Experiment 2849 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00771413184843467
rmse: 0.08783013064111125
mae: 0.031021006482700175
r2: 0.6521731902031558
pearson: 0.8331034980426973

=== Experiment 2866 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006327502584613467
rmse: 0.07954560066159201
mae: 0.03265999960348654
r2: 0.7146956934066424
pearson: 0.8530106990453529

=== Experiment 2709 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009404048890665722
rmse: 0.09697447545960598
mae: 0.03946032603923437
r2: 0.5759755745583283
pearson: 0.7982362478995667

=== Experiment 2828 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006563040884768035
rmse: 0.08101259707457868
mae: 0.03144721281728824
r2: 0.7040753751210067
pearson: 0.8445709959264144

=== Experiment 2654 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006424698720171241
rmse: 0.08015421835543804
mae: 0.03167180146141408
r2: 0.7103131624336331
pearson: 0.8616581284550509

=== Experiment 2655 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006273820134289547
rmse: 0.07920744999234319
mae: 0.029548054107377555
r2: 0.7171162114642947
pearson: 0.8471399418717294

=== Experiment 2725 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062859734112320625
rmse: 0.0792841308915729
mae: 0.0309503261631729
r2: 0.7165682255560228
pearson: 0.8476303708647612

=== Experiment 2904 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005479998559429708
rmse: 0.07402701236325634
mae: 0.027782495288118943
r2: 0.7529092768871306
pearson: 0.8682699450003244

=== Experiment 2837 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0071796968718341745
rmse: 0.0847330919525198
mae: 0.03155126791607458
r2: 0.6762706280752504
pearson: 0.8415253095508638

=== Experiment 2864 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006138310594626798
rmse: 0.07834737133195215
mae: 0.0285991274516004
r2: 0.7232262769653787
pearson: 0.8533705315169281

=== Experiment 2729 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005905295292954234
rmse: 0.07684591916916755
mae: 0.029266207426300556
r2: 0.7337328343599172
pearson: 0.8576869541510926

=== Experiment 2805 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006157145648875152
rmse: 0.07846748147401668
mae: 0.029890464272792946
r2: 0.7223770126592617
pearson: 0.8504187668883947

=== Experiment 2801 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006334887174899643
rmse: 0.07959200446589873
mae: 0.038878446690750704
r2: 0.7143627254808455
pearson: 0.847633161066109

=== Experiment 2693 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007172534954035807
rmse: 0.08469081977425776
mae: 0.043687905238277605
r2: 0.6765935557965777
pearson: 0.8380566038613164

=== Experiment 2865 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008113524581667243
rmse: 0.09007510522706727
mae: 0.04226369336148542
r2: 0.634164747647884
pearson: 0.8078584941457909

=== Experiment 2951 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0063271416014693774
rmse: 0.07954333159649134
mae: 0.03571740896730593
r2: 0.7147119699777287
pearson: 0.8466506382587687

=== Experiment 2603 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062874239372053544
rmse: 0.07929327800769341
mae: 0.031115434421755312
r2: 0.7165028219783125
pearson: 0.848733669630216

=== Experiment 2751 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006044689250497864
rmse: 0.07774759964460552
mae: 0.03577990482049535
r2: 0.7274476221662474
pearson: 0.8552428426770251

=== Experiment 2535 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006204576113818228
rmse: 0.07876913173203211
mae: 0.04016247598114073
r2: 0.7202383938707223
pearson: 0.853142908286085

=== Experiment 2905 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005518084710436887
rmse: 0.07428381190028475
mae: 0.029066064683397633
r2: 0.7511919891012138
pearson: 0.86821874141452

=== Experiment 2885 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0065723888527136655
rmse: 0.0810702710783285
mae: 0.03819590554985013
r2: 0.7036538793606937
pearson: 0.8407587551896738

=== Experiment 2758 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006248833263805653
rmse: 0.079049562072194
mae: 0.0346956865522999
r2: 0.7182428584568508
pearson: 0.8475561087613684

=== Experiment 2863 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006327001577375861
rmse: 0.07954245141668605
mae: 0.034772162841363995
r2: 0.7147182836024759
pearson: 0.845499614779195

=== Experiment 2784 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009861282571868087
rmse: 0.09930399071471441
mae: 0.04019706085836916
r2: 0.5553591091168469
pearson: 0.7919460414086692

=== Experiment 2667 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006459698078925712
rmse: 0.08037224694461212
mae: 0.03320714654524458
r2: 0.7087350567517898
pearson: 0.8430910393541609

=== Experiment 2612 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00713349354048093
rmse: 0.0844600114875728
mae: 0.04066938120306031
r2: 0.6783539159503268
pearson: 0.8487075724687035

=== Experiment 2867 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006490196058369204
rmse: 0.08056175307408103
mae: 0.02982781247762488
r2: 0.70735991628187
pearson: 0.8467993900834468

=== Experiment 2526 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006745804770920475
rmse: 0.08213284830639003
mae: 0.044093055195188954
r2: 0.6958346319349311
pearson: 0.8499032238576246

=== Experiment 2832 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007456810709699495
rmse: 0.08635282687729161
mae: 0.032537367978627406
r2: 0.6637756870930303
pearson: 0.8224404850679962

=== Experiment 2913 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007415052885916212
rmse: 0.08611070134377151
mae: 0.03528616549431813
r2: 0.6656585289883947
pearson: 0.8324044869637788

=== Experiment 2982 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005420146661814714
rmse: 0.07362164533487903
mae: 0.033404407433467016
r2: 0.755607972607756
pearson: 0.8706069708319294

=== Experiment 2947 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006165904927025315
rmse: 0.07852327633909142
mae: 0.033533818226142945
r2: 0.7219820606627239
pearson: 0.850996071905561

=== Experiment 2883 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006979811853426068
rmse: 0.08354526828867131
mae: 0.031080464008850655
r2: 0.6852833555791473
pearson: 0.8695755800930606

=== Experiment 2666 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006633349548043102
rmse: 0.08144537769599391
mae: 0.03313922111172052
r2: 0.7009051884391433
pearson: 0.8416602607285177

=== Experiment 2999 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005781267355558779
rmse: 0.07603464575809359
mae: 0.03380887711109136
r2: 0.7393251994682084
pearson: 0.8610240962726591

=== Experiment 2670 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005613695931247689
rmse: 0.07492460164223558
mae: 0.034262801097946334
r2: 0.7468809212365712
pearson: 0.8675576350824628

=== Experiment 2580 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007433431626755354
rmse: 0.08621735107711993
mae: 0.040693674646689285
r2: 0.6648298396530603
pearson: 0.8463390778161838

=== Experiment 2668 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005694510928609214
rmse: 0.07546198333339255
mae: 0.030736003817222194
r2: 0.7432370085749406
pearson: 0.8677156037793466

=== Experiment 2987 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007788207610807844
rmse: 0.08825082215372185
mae: 0.039066783612697255
r2: 0.6488331466809858
pearson: 0.8139783164921706

=== Experiment 2662 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006569040282274306
rmse: 0.08104961617598387
mae: 0.04215488083851865
r2: 0.7038048649279858
pearson: 0.8454509226023347

=== Experiment 2516 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006665133676554974
rmse: 0.0816402699441579
mae: 0.039588169017622056
r2: 0.6994720560738077
pearson: 0.8391321481658472

=== Experiment 2894 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006236554256136862
rmse: 0.07897185736790582
mae: 0.03677894493517208
r2: 0.7187965135082319
pearson: 0.8492231308681373

=== Experiment 2644 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007244897838579357
rmse: 0.08511696563305907
mae: 0.03280658697098493
r2: 0.67333074518185
pearson: 0.8444971522833638

=== Experiment 2823 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007060083730383829
rmse: 0.08402430440285613
mae: 0.0357708338618797
r2: 0.6816639319774769
pearson: 0.831998734086007

=== Experiment 2537 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006245697073235609
rmse: 0.07902972272022475
mae: 0.0331644305231408
r2: 0.718384267909311
pearson: 0.8504943178046399

=== Experiment 2856 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005983333784034393
rmse: 0.07735201215246047
mae: 0.03210912272114174
r2: 0.7302141131444794
pearson: 0.8550330058001839

=== Experiment 2989 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006318927253433465
rmse: 0.07949168040388545
mae: 0.03278740555785498
r2: 0.7150823513152569
pearson: 0.8461280819664025

=== Experiment 2585 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006075817326883141
rmse: 0.07794752931865859
mae: 0.03509978687590023
r2: 0.7260440709025495
pearson: 0.8537379048048087

=== Experiment 2522 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068173206268977125
rmse: 0.08256706744760718
mae: 0.031951977916558794
r2: 0.6926100134654529
pearson: 0.837490036523265

=== Experiment 2813 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005624321241231162
rmse: 0.07499547480502515
mae: 0.028354179747932078
r2: 0.746401830685973
pearson: 0.8652788415610637

=== Experiment 2990 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006873277018944689
rmse: 0.08290522914113856
mae: 0.03518876121596286
r2: 0.6900869643763463
pearson: 0.831893871262128

=== Experiment 2554 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011303778567509163
rmse: 0.10631922952838382
mae: 0.042511888549354795
r2: 0.4903175995644311
pearson: 0.7153848495481202

=== Experiment 2519 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.01194675821464261
rmse: 0.10930122695854155
mae: 0.048361297683260626
r2: 0.4613259302721687
pearson: 0.7094965818617948

=== Experiment 2638 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006815086361774176
rmse: 0.08255353633718047
mae: 0.03627514349840006
r2: 0.6927107554964977
pearson: 0.8411219140506456

=== Experiment 2965 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006622668444034879
rmse: 0.08137977908568492
mae: 0.031982204140076124
r2: 0.7013867947177606
pearson: 0.845444316351716

=== Experiment 2791 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006561758709463032
rmse: 0.0810046832563589
mae: 0.03163648179781665
r2: 0.704133187841181
pearson: 0.8392770930480525

=== Experiment 2953 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0062691281291104226
rmse: 0.07917782599383759
mae: 0.029837108857380088
r2: 0.717327771912712
pearson: 0.855088474873283

=== Experiment 2567 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005741787042111023
rmse: 0.07577458044826789
mae: 0.030124847330671483
r2: 0.7411053494249529
pearson: 0.8610937084096997

=== Experiment 2661 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.004846899820745189
rmse: 0.06961967983799687
mae: 0.025484916060999938
r2: 0.7814554203663514
pearson: 0.885243268751754

=== Experiment 2897 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006018115551759135
rmse: 0.07757651417638675
mae: 0.03472975923844348
r2: 0.7286458185463975
pearson: 0.8602605178374115

=== Experiment 2789 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009124048457946511
rmse: 0.09551988514412332
mae: 0.03888306532681895
r2: 0.5886006708320227
pearson: 0.7873328821400036

=== Experiment 2656 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006041224051971092
rmse: 0.07772531152701218
mae: 0.0375800229657443
r2: 0.7276038664426692
pearson: 0.8593131856758305

=== Experiment 2610 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00625791987062818
rmse: 0.07910701530602819
mae: 0.0341129238338658
r2: 0.7178331473545445
pearson: 0.8472796323636216

=== Experiment 2979 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007123613998982726
rmse: 0.08440150471989659
mae: 0.03127858849631938
r2: 0.6787993801281622
pearson: 0.8387529716065492

=== Experiment 2862 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005955272859251525
rmse: 0.07717041440378253
mae: 0.03000151956041293
r2: 0.7314793678923814
pearson: 0.8569254134676749

=== Experiment 2901 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005537441789498829
rmse: 0.07441398920565158
mae: 0.030080306094924243
r2: 0.750319186926013
pearson: 0.8665406333978637

=== Experiment 2930 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006252153014476798
rmse: 0.07907055719088363
mae: 0.030777534109633662
r2: 0.7180931723602229
pearson: 0.847432389216078

=== Experiment 2696 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005989503982582034
rmse: 0.0773918857670624
mae: 0.03575336973481231
r2: 0.7299359016076783
pearson: 0.8601058988360504

=== Experiment 2706 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005705600217286867
rmse: 0.07553542359242363
mae: 0.03284348597205424
r2: 0.7427369974292344
pearson: 0.8630534487973597

=== Experiment 2963 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006893203667529562
rmse: 0.0830253194364801
mae: 0.03326565893899712
r2: 0.689188480562043
pearson: 0.8486530599885266

=== Experiment 2772 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007319013147167156
rmse: 0.08555123112595842
mae: 0.034224392207319376
r2: 0.6699889185382681
pearson: 0.8313774287590734

=== Experiment 2521 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007236027395320662
rmse: 0.08506484229880558
mae: 0.037361927112296134
r2: 0.6737307095642039
pearson: 0.8330123923464977

=== Experiment 2688 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.010443823881381488
rmse: 0.10219502865297063
mae: 0.046778981009291454
r2: 0.5290925778669255
pearson: 0.7358525429514381

=== Experiment 2840 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005700538554005913
rmse: 0.07550191092949841
mae: 0.029617901335958693
r2: 0.7429652255987643
pearson: 0.8634385300116443

=== Experiment 2787 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.01273088004347733
rmse: 0.11283120155115485
mae: 0.046076168684510255
r2: 0.4259702221284206
pearson: 0.7540592556935736

=== Experiment 2985 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006823655763575162
rmse: 0.08260542211970812
mae: 0.036290916580572034
r2: 0.6923243649409736
pearson: 0.8332182893030917

=== Experiment 2586 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006196856852218401
rmse: 0.07872011720150321
mae: 0.03156050548437579
r2: 0.7205864519787517
pearson: 0.84965622915899

=== Experiment 2599 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007666894882312498
rmse: 0.08756080677056657
mae: 0.037274617777770684
r2: 0.6543030842150256
pearson: 0.8186710781913373

=== Experiment 2620 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.011924614719332555
rmse: 0.10919988424596683
mae: 0.044370123682333826
r2: 0.4623243707296013
pearson: 0.7591886006558174

=== Experiment 2715 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00571833398309724
rmse: 0.07561966664233082
mae: 0.027491875554724852
r2: 0.7421628375334037
pearson: 0.8667100210715418

=== Experiment 2502 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006489002842318554
rmse: 0.08055434713482913
mae: 0.03535921899013486
r2: 0.7074137178684194
pearson: 0.8444607750950145

=== Experiment 2686 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006434749440323741
rmse: 0.08021688999408878
mae: 0.02913720428395293
r2: 0.7098599798856171
pearson: 0.8442870523301107

=== Experiment 2651 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011464833168747763
rmse: 0.10707396120788547
mae: 0.04220581934148288
r2: 0.4830557184801336
pearson: 0.7093375785179075

=== Experiment 2861 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006048399738193922
rmse: 0.07777145837769742
mae: 0.02903275979602384
r2: 0.7272803179091427
pearson: 0.8543802452115689

=== Experiment 2931 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008222032333457776
rmse: 0.09067542298471938
mae: 0.03629628292962186
r2: 0.6292721808775628
pearson: 0.8303807420970701

=== Experiment 2804 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009112384477193865
rmse: 0.09545881036967654
mae: 0.037968884295543834
r2: 0.5891265945904489
pearson: 0.7934266116889972

=== Experiment 2980 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005646871331615113
rmse: 0.07514566741745736
mae: 0.029480055865447597
r2: 0.7453850570356144
pearson: 0.8646201698925263

=== Experiment 2730 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005469950391817603
rmse: 0.07395911297343691
mae: 0.028191094556686617
r2: 0.7533623443422237
pearson: 0.872505188671327

=== Experiment 2742 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0076309213885866565
rmse: 0.08735514517523657
mae: 0.03093902011659381
r2: 0.6559251132139783
pearson: 0.8556291783901712

=== Experiment 2912 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006387036138343804
rmse: 0.07991893479234945
mae: 0.03470219672861995
r2: 0.7120113516716624
pearson: 0.8512601736056866

=== Experiment 2974 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005984539055229673
rmse: 0.07735980258008465
mae: 0.03037461240242054
r2: 0.7301597679967647
pearson: 0.8580693794660929

=== Experiment 2703 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00614061673893603
rmse: 0.0783620873824583
mae: 0.029606201622084917
r2: 0.723122293933487
pearson: 0.8553481023884021

=== Experiment 2818 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005873291118393408
rmse: 0.07663740025857746
mae: 0.02879127312095864
r2: 0.7351758885047505
pearson: 0.8579764879370368

=== Experiment 2977 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006929617763064451
rmse: 0.08324432571091228
mae: 0.03237687321649372
r2: 0.6875465850214441
pearson: 0.8428508754563011

=== Experiment 2683 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005659063851674151
rmse: 0.07522674957536149
mae: 0.03139629738512332
r2: 0.7448353016724909
pearson: 0.8635006773443794

=== Experiment 2785 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006286141702592928
rmse: 0.07928519220253506
mae: 0.03013543143121228
r2: 0.7165606373726323
pearson: 0.8506496162183224

=== Experiment 2939 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.004994753383797742
rmse: 0.0706735692023386
mae: 0.026534999528978646
r2: 0.7747887682836414
pearson: 0.8804119019284045

=== Experiment 2755 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006033745590848256
rmse: 0.07767718835570875
mae: 0.03354784004201481
r2: 0.7279410669631754
pearson: 0.8542182612148304

=== Experiment 2652 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005722469863868956
rmse: 0.07564700829424094
mae: 0.028825999783844633
r2: 0.7419763524897472
pearson: 0.8642223406220764

=== Experiment 575 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006914827923343377
rmse: 0.08315544433976273
mae: 0.031489604655540966
r2: 0.688213452384961
pearson: 0.8587481559893281

=== Experiment 523 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006480226198987447
rmse: 0.08049985216748815
mae: 0.038398285618269454
r2: 0.7078094528533229
pearson: 0.8436036865170352

=== Experiment 612 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008431081481326505
rmse: 0.09182092071704849
mae: 0.056687490294599435
r2: 0.6198462468097279
pearson: 0.8112726710453806

=== Experiment 527 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006618645198469003
rmse: 0.08135505637923805
mae: 0.03242876850350734
r2: 0.7015682010895601
pearson: 0.839543020565262

=== Experiment 551 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0070607292209025705
rmse: 0.08402814540915782
mae: 0.04897314674530094
r2: 0.681634827094089
pearson: 0.8392988299790984

=== Experiment 553 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006436201428691428
rmse: 0.08022593987415434
mae: 0.034947170947067285
r2: 0.7097945103691872
pearson: 0.8427172822824043

=== Experiment 501 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005683147278338777
rmse: 0.07538665185786392
mae: 0.030331899018645424
r2: 0.7437493905641097
pearson: 0.8628368134641314

=== Experiment 592 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006579426367878595
rmse: 0.0811136632626008
mae: 0.034864137830918435
r2: 0.7033365608993543
pearson: 0.8405320518746539

=== Experiment 572 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006786495306119802
rmse: 0.08238018758245093
mae: 0.037079507962474126
r2: 0.6939999136120676
pearson: 0.8376480661950271

=== Experiment 554 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007451815362602294
rmse: 0.0863238979808158
mae: 0.048998712988529605
r2: 0.6640009250950227
pearson: 0.8269119611499001

=== Experiment 621 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006346785135202342
rmse: 0.07966671284295808
mae: 0.03588572008999202
r2: 0.7138262516874896
pearson: 0.8515839803471268

=== Experiment 638 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007231694335258624
rmse: 0.08503936932538143
mae: 0.04488007956214747
r2: 0.6739260853352758
pearson: 0.8304955574104145

=== Experiment 547 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006573678711107049
rmse: 0.08107822587542877
mae: 0.0333395158290791
r2: 0.7035957202134462
pearson: 0.8416217019241333

=== Experiment 633 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006498643962943744
rmse: 0.08061416725950683
mae: 0.04121528662668005
r2: 0.7069790039828112
pearson: 0.8476146191526778

=== Experiment 548 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0060125735885979015
rmse: 0.07754078661322635
mae: 0.03546875714120362
r2: 0.728895703226133
pearson: 0.857985029405856

=== Experiment 568 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0063813645403242025
rmse: 0.07988344346811924
mae: 0.03434314329342982
r2: 0.7122670815301049
pearson: 0.8446656093096846

=== Experiment 651 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00761551874279377
rmse: 0.0872669395750405
mae: 0.05108329106093232
r2: 0.6566196117335568
pearson: 0.8238698004990398

=== Experiment 619 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006980463580796579
rmse: 0.08354916864216291
mae: 0.043513698363941866
r2: 0.6852539694788581
pearson: 0.8340136056663227

=== Experiment 532 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006303194216977717
rmse: 0.0793926584576793
mae: 0.03096936513325771
r2: 0.7157917470044075
pearson: 0.848553687136322

=== Experiment 611 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006811138014258665
rmse: 0.08252961901195634
mae: 0.036821035790642595
r2: 0.6928887847481717
pearson: 0.8350758103147021

=== Experiment 563 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006873478545649719
rmse: 0.08290644453629475
mae: 0.04333287564022845
r2: 0.6900778776259152
pearson: 0.8391898337727437

=== Experiment 542 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006878798555780693
rmse: 0.08293852274896565
mae: 0.03685504903410279
r2: 0.6898380007106251
pearson: 0.8376813717157825

=== Experiment 511 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007325172328920493
rmse: 0.08558722059350037
mae: 0.050569867833566005
r2: 0.6697112037438735
pearson: 0.832289731165814

=== Experiment 500 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062775872431917205
rmse: 0.0792312264400326
mae: 0.037763212825521135
r2: 0.7169463541819272
pearson: 0.8489378497280498

=== Experiment 589 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006961240599739426
rmse: 0.0834340494027434
mae: 0.0342140655162815
r2: 0.6861207252340443
pearson: 0.8391854344160609

=== Experiment 639 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007588085293453933
rmse: 0.08710961653832447
mae: 0.0332274264524018
r2: 0.6578565738899057
pearson: 0.8237413356907705

=== Experiment 640 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007114754634958278
rmse: 0.08434900494349816
mae: 0.04013922436203884
r2: 0.6791988449527202
pearson: 0.8255880344471919

=== Experiment 698 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00658965859871814
rmse: 0.08117671216992063
mae: 0.03604065528087361
r2: 0.7028751941143483
pearson: 0.8408395631427629

=== Experiment 634 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00601730160060162
rmse: 0.07757126788058591
mae: 0.029831401123829673
r2: 0.7286825192458426
pearson: 0.8546754304185695

=== Experiment 604 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0058558860225527256
rmse: 0.07652376116313629
mae: 0.031228955877584407
r2: 0.7359606766156381
pearson: 0.8578883364007728

=== Experiment 584 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007489356334903851
rmse: 0.08654106733166544
mae: 0.040234930212711
r2: 0.6623082191769909
pearson: 0.8257920320954965

=== Experiment 681 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006672947826757765
rmse: 0.0816881131301107
mae: 0.03722896015789441
r2: 0.6991197194804343
pearson: 0.8377708361411511

=== Experiment 711 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005921078108688293
rmse: 0.07694854195297202
mae: 0.03451554782858429
r2: 0.7330211941450169
pearson: 0.8566313586743889

=== Experiment 629 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009326685581224596
rmse: 0.09657476679352943
mae: 0.04143450460735482
r2: 0.5794638521308324
pearson: 0.8082120870934028

=== Experiment 519 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008249159178792626
rmse: 0.0908248819365741
mae: 0.03854039358134088
r2: 0.6280490433608519
pearson: 0.7953088748340895

=== Experiment 665 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007674114749880163
rmse: 0.08760202480468224
mae: 0.04654834948191788
r2: 0.6539775435641062
pearson: 0.8209043350079175

=== Experiment 565 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006303982896062538
rmse: 0.07939762525455367
mae: 0.03002425727755221
r2: 0.7157561858115973
pearson: 0.8500118425518203

=== Experiment 545 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006203382013561278
rmse: 0.07876155161981814
mae: 0.02896696742346415
r2: 0.7202922353257446
pearson: 0.8493747584813957

=== Experiment 658 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007180372095559544
rmse: 0.08473707627455378
mae: 0.04646590037312199
r2: 0.6762401825346622
pearson: 0.8355979401193535

=== Experiment 567 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067631065090689255
rmse: 0.08223810861801799
mae: 0.0311703559522513
r2: 0.6950545041768948
pearson: 0.8448190844986948

=== Experiment 631 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0063814297086786645
rmse: 0.07988385136358077
mae: 0.031217869067905912
r2: 0.712264143117685
pearson: 0.8516278829148423

=== Experiment 524 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007336752629557961
rmse: 0.08565484591987753
mae: 0.03557258135321368
r2: 0.6691890530850102
pearson: 0.8307992682546974

=== Experiment 573 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006898239709006299
rmse: 0.08305564224666677
mae: 0.030149467991881146
r2: 0.6889614076683884
pearson: 0.8407923540092298

=== Experiment 664 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006543797938782137
rmse: 0.08089374474446177
mae: 0.03134872219636082
r2: 0.7049430310860427
pearson: 0.841109059984395

=== Experiment 716 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007036593369297168
rmse: 0.08388440480385594
mae: 0.035972730396919304
r2: 0.6827231020199758
pearson: 0.8405855894434207

=== Experiment 510 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005727521155464672
rmse: 0.07568038818257126
mae: 0.02935981096484845
r2: 0.74174859197494
pearson: 0.8684734312847653

=== Experiment 505 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006428773320443619
rmse: 0.08017963158086734
mae: 0.033810488334778066
r2: 0.7101294405007204
pearson: 0.8480809555009732

=== Experiment 606 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006313991118673522
rmse: 0.07946062621621806
mae: 0.032904986357999974
r2: 0.7153049194590236
pearson: 0.8509691110362555

=== Experiment 593 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007127253564089515
rmse: 0.08442306298689663
mae: 0.04148249225982495
r2: 0.6786352737393921
pearson: 0.8278253629022119

=== Experiment 678 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007589699366158805
rmse: 0.08711888065258187
mae: 0.04409661370955287
r2: 0.6577837960620507
pearson: 0.8201878951423898

=== Experiment 642 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006804365109722642
rmse: 0.08248857563155423
mae: 0.03341697347831755
r2: 0.6931941720327176
pearson: 0.8413668094605734

=== Experiment 738 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007089455594227517
rmse: 0.08419890494672432
mae: 0.03885574130536258
r2: 0.6803395675643108
pearson: 0.8346835081600779

=== Experiment 566 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005885918639778831
rmse: 0.0767197408740334
mae: 0.031205931230947743
r2: 0.7346065191232787
pearson: 0.8573875842164416

=== Experiment 526 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007629657899725372
rmse: 0.087347912967199
mae: 0.032611488662188465
r2: 0.6559820833706298
pearson: 0.850367375650009

=== Experiment 588 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006202631484006064
rmse: 0.07875678690758063
mae: 0.035511171942756714
r2: 0.720326076373056
pearson: 0.851245393551015

=== Experiment 557 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007115453740314745
rmse: 0.08435314896501935
mae: 0.0350335677288404
r2: 0.6791673225998969
pearson: 0.8472002306924236

=== Experiment 672 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007850931441667553
rmse: 0.08860548200685753
mae: 0.048825196381389074
r2: 0.6460049567543904
pearson: 0.8303905434653123

=== Experiment 598 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006643351547321102
rmse: 0.08150675767886428
mae: 0.030795155784624814
r2: 0.7004542026939148
pearson: 0.8442714002359034

=== Experiment 608 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006518236088965462
rmse: 0.0807355936930265
mae: 0.03607887174165572
r2: 0.7060956036436461
pearson: 0.8403976052463613

=== Experiment 751 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007248722516180586
rmse: 0.08513942985585812
mae: 0.03738991881409024
r2: 0.6731582921521801
pearson: 0.8224440889604322

=== Experiment 749 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006493800923429394
rmse: 0.0805841232714571
mae: 0.031645211473349096
r2: 0.7071973745029285
pearson: 0.8422293060994811

=== Experiment 689 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005995433007374849
rmse: 0.07743018150162667
mae: 0.03255200201549136
r2: 0.7296685644893337
pearson: 0.8588909140303241

=== Experiment 763 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0064963254315493065
rmse: 0.0805997855552315
mae: 0.03455035924733119
r2: 0.7070835455429227
pearson: 0.8453942540897934

=== Experiment 659 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006405531366332772
rmse: 0.08003456357307617
mae: 0.031148243004847708
r2: 0.7111774099820121
pearson: 0.847364135399432

=== Experiment 607 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0068468382549990246
rmse: 0.08274562378155732
mae: 0.031181481301063112
r2: 0.6912790766060664
pearson: 0.8418582696132463

=== Experiment 687 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006641946551240676
rmse: 0.08149813832990711
mae: 0.030419702972702883
r2: 0.7005175533487952
pearson: 0.8431547752129789

=== Experiment 686 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006248114789981526
rmse: 0.07904501748991853
mae: 0.030623869609705204
r2: 0.7182752541253561
pearson: 0.8608863323020503

=== Experiment 625 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.010557168817113704
rmse: 0.10274808425033385
mae: 0.04711733831550025
r2: 0.5239819046016814
pearson: 0.7289936926629211

=== Experiment 626 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006580126844118529
rmse: 0.0811179810160394
mae: 0.02869407234259346
r2: 0.7033049767339987
pearson: 0.8405701612775458

=== Experiment 536 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007084757187306436
rmse: 0.08417099968104474
mae: 0.0412001480323445
r2: 0.6805514166644553
pearson: 0.8419580968679694

=== Experiment 868 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008967680205982582
rmse: 0.09469783633210731
mae: 0.05938681533895298
r2: 0.5956512464901454
pearson: 0.7986531441178613

=== Experiment 776 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0068270054138939084
rmse: 0.08262569463486469
mae: 0.04138022780946256
r2: 0.6921733306823952
pearson: 0.838194371667937

=== Experiment 535 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007493212300736157
rmse: 0.08656334270773142
mae: 0.04139793935883856
r2: 0.6621343553747527
pearson: 0.8392342614960479

=== Experiment 570 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007879802882027071
rmse: 0.08876825379620279
mae: 0.04099069507002321
r2: 0.6447031562158726
pearson: 0.8078840504558441

=== Experiment 622 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00524055289992268
rmse: 0.07239166319351062
mae: 0.029014558448474274
r2: 0.7637057762862084
pearson: 0.874535206041615

=== Experiment 595 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006377285844084347
rmse: 0.07985791034133279
mae: 0.036869584063048265
r2: 0.7124509881483886
pearson: 0.8468376806195859

=== Experiment 858 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005980643916848083
rmse: 0.07733462301484428
mae: 0.03432923939410074
r2: 0.7303353980720091
pearson: 0.855306918525602

=== Experiment 856 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006879739796250994
rmse: 0.0829441968811019
mae: 0.035832928287501765
r2: 0.6897955605920908
pearson: 0.8328095394863394

=== Experiment 508 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006931525127544291
rmse: 0.08325578134606804
mae: 0.03897895675797594
r2: 0.6874605827965434
pearson: 0.8342015132897834

=== Experiment 892 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007166007600005579
rmse: 0.08465227462983839
mae: 0.034839106568465464
r2: 0.6768878713168923
pearson: 0.8299569454383657

=== Experiment 756 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00646405218499558
rmse: 0.0803993295058832
mae: 0.033817906769291244
r2: 0.7085387320254882
pearson: 0.8455337388025879

=== Experiment 560 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007939946226942649
rmse: 0.08910637590510934
mae: 0.032932636331189556
r2: 0.6419913192647395
pearson: 0.8343908213185328

=== Experiment 859 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006603065060212169
rmse: 0.0812592459982996
mae: 0.033002059615466824
r2: 0.7022707026662174
pearson: 0.8434359356720035

=== Experiment 767 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006288126291810367
rmse: 0.07929770672478724
mae: 0.034268429671007745
r2: 0.7164711531183023
pearson: 0.8489677950191814

=== Experiment 780 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006551434072751678
rmse: 0.08094092952735148
mae: 0.03642059416325653
r2: 0.7045987211662759
pearson: 0.8423185288708197

=== Experiment 685 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007767800678348886
rmse: 0.08813512738034072
mae: 0.042733605497549995
r2: 0.6497532862837823
pearson: 0.8232102572027825

=== Experiment 860 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00664862971840071
rmse: 0.08153912998309898
mae: 0.03293919315715485
r2: 0.700216212283036
pearson: 0.8372924767178503

=== Experiment 862 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007139912052100884
rmse: 0.08449800028462735
mae: 0.0384668578115422
r2: 0.6780645080863861
pearson: 0.8395424405276128

=== Experiment 733 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007644366096641154
rmse: 0.08743206560891235
mae: 0.05012801704697961
r2: 0.6553188972452696
pearson: 0.8271856781244595

=== Experiment 730 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007834758060909336
rmse: 0.0885141687014533
mae: 0.050449723383912175
r2: 0.6467342073743291
pearson: 0.8184340843767246

=== Experiment 590 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006203149089613461
rmse: 0.07876007294063066
mae: 0.03226810848697392
r2: 0.7203027377640339
pearson: 0.8505763255704465

=== Experiment 509 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006837414023070227
rmse: 0.08268865716088408
mae: 0.03258722563229643
r2: 0.6917040110758141
pearson: 0.8394024215509588

=== Experiment 938 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007034351032289433
rmse: 0.08387103810189446
mae: 0.03820901240337278
r2: 0.6828242080087266
pearson: 0.8305170123074128

=== Experiment 741 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006370957053939943
rmse: 0.07981827518770336
mae: 0.03012200726859975
r2: 0.7127363505104862
pearson: 0.8443902322569911

=== Experiment 812 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006168136136127611
rmse: 0.07853748236433106
mae: 0.02917732532767992
r2: 0.7218814564263316
pearson: 0.8570309901569545

=== Experiment 837 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007439257471276829
rmse: 0.08625113026086573
mae: 0.040705570843280046
r2: 0.6645671548877374
pearson: 0.8162584695323345

=== Experiment 504 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006269279507369096
rmse: 0.07917878192653065
mae: 0.04034363190399647
r2: 0.7173209463336552
pearson: 0.8526393613731066

=== Experiment 735 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007257383373470949
rmse: 0.08519027745858648
mae: 0.038981702936507885
r2: 0.6727677779088923
pearson: 0.8233538448924886

=== Experiment 662 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006565708656869099
rmse: 0.08102906057007633
mae: 0.030216346118501314
r2: 0.7039550864511461
pearson: 0.8481619501110447

=== Experiment 793 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006587899794626
rmse: 0.08116587826559878
mae: 0.035242420281441096
r2: 0.7029544978167542
pearson: 0.844887659882738

=== Experiment 724 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0061033283977060026
rmse: 0.07812380173612907
mae: 0.02944562077214863
r2: 0.724803608827695
pearson: 0.8527547179609334

=== Experiment 690 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005936825273297234
rmse: 0.07705079670773843
mae: 0.029169618999110854
r2: 0.7323111614236568
pearson: 0.8661836652209677

=== Experiment 737 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006388999303180645
rmse: 0.07993121607470166
mae: 0.04027335623877671
r2: 0.7119228334332239
pearson: 0.8472224420157128

=== Experiment 903 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006689891693512096
rmse: 0.08179175810258693
mae: 0.04004071421551042
r2: 0.6983557279860476
pearson: 0.8374576097166354

=== Experiment 888 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006890353764951122
rmse: 0.08300815480994095
mae: 0.04270377638411649
r2: 0.6893169814149669
pearson: 0.8347936833106382

=== Experiment 720 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006805279758829182
rmse: 0.08249411954090535
mae: 0.030225645840457133
r2: 0.6931529309070716
pearson: 0.836063531963906

=== Experiment 728 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006605799525173842
rmse: 0.08127606981869781
mae: 0.03691210907857322
r2: 0.7021474068446258
pearson: 0.8460041274487254

=== Experiment 706 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007657014015226851
rmse: 0.08750436569238618
mae: 0.0350946396935302
r2: 0.6547486081630152
pearson: 0.8329694743310235

=== Experiment 939 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007152889221962747
rmse: 0.08457475522851217
mae: 0.04296706833193205
r2: 0.677479373209004
pearson: 0.8259183550987308

=== Experiment 772 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008770750250025678
rmse: 0.0936522837416455
mae: 0.04221645877307589
r2: 0.6045307315287403
pearson: 0.7797052177855917

=== Experiment 941 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0061989068926511445
rmse: 0.07873313719553632
mae: 0.035091312270419915
r2: 0.7204940165579311
pearson: 0.8492337051943895

=== Experiment 522 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00631465322736986
rmse: 0.07946479237605708
mae: 0.029887192002523048
r2: 0.715275065269322
pearson: 0.8466141256540135

=== Experiment 652 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007355374847866405
rmse: 0.08576348201808509
mae: 0.039276235315745485
r2: 0.6683493854577475
pearson: 0.8239591496840563

=== Experiment 899 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0064802732193786385
rmse: 0.0805001442196139
mae: 0.031176107486249972
r2: 0.7078073327245793
pearson: 0.8509591399234128

=== Experiment 543 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006126696512275116
rmse: 0.0782732170814201
mae: 0.03221430750289064
r2: 0.7237499508268674
pearson: 0.8634140207547909

=== Experiment 701 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007798771432561133
rmse: 0.08831065299589361
mae: 0.04582111098163804
r2: 0.6483568286076232
pearson: 0.8102613210125728

=== Experiment 840 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007312245454046981
rmse: 0.08551166852568708
mae: 0.03916037206740905
r2: 0.6702940708423675
pearson: 0.8221098518651259

=== Experiment 537 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005939080753924882
rmse: 0.0770654316404241
mae: 0.02853762022711419
r2: 0.7322094627948692
pearson: 0.858598224540994

=== Experiment 648 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006487650590489616
rmse: 0.08054595328438056
mae: 0.028806072856627736
r2: 0.7074746903082145
pearson: 0.8418604749798397

=== Experiment 891 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007141647765759917
rmse: 0.0845082703985824
mae: 0.03743068083601071
r2: 0.6779862455214463
pearson: 0.828299308563708

=== Experiment 825 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006176869551794031
rmse: 0.07859306300045846
mae: 0.030058214730823492
r2: 0.7214876705578028
pearson: 0.8500885696611475

=== Experiment 513 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005460544570616274
rmse: 0.07389549763426913
mae: 0.02990990247072512
r2: 0.7537864486803731
pearson: 0.8688566206785894

=== Experiment 855 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005985558141694668
rmse: 0.07736638896636361
mae: 0.030374051327793816
r2: 0.7301138178365922
pearson: 0.8574196909626347

=== Experiment 842 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006822539242363524
rmse: 0.08259866368388488
mae: 0.03528512296203847
r2: 0.6923747083909877
pearson: 0.8364820455678648

=== Experiment 599 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0062521334434454
rmse: 0.0790704334340302
mae: 0.03724882093024613
r2: 0.7180940548094148
pearson: 0.8484702181091553

=== Experiment 804 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006243535637820195
rmse: 0.07901604671090673
mae: 0.031828735303783474
r2: 0.7184817260808716
pearson: 0.856313612175166

=== Experiment 969 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00619468985770427
rmse: 0.07870635207976717
mae: 0.030322476829409136
r2: 0.7206841608076267
pearson: 0.8591641676824169

=== Experiment 653 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005256614653545373
rmse: 0.07250251480842146
mae: 0.026222318848013338
r2: 0.7629815588846796
pearson: 0.8748124418394229

=== Experiment 643 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006176692504856015
rmse: 0.07859193664019239
mae: 0.029094088527206918
r2: 0.7214956535263137
pearson: 0.8517924744196289

=== Experiment 778 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006987044467096613
rmse: 0.08358854267838753
mae: 0.033472666370302266
r2: 0.6849572402120592
pearson: 0.8353436902775078

=== Experiment 866 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006204251923674364
rmse: 0.0787670738549704
mae: 0.032997673905146734
r2: 0.7202530114616221
pearson: 0.8517248614366187

=== Experiment 839 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007274285934420708
rmse: 0.0852894245168808
mae: 0.04125623543104529
r2: 0.6720056488750538
pearson: 0.8208657701255887

=== Experiment 819 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005797248645671645
rmse: 0.07613966538980615
mae: 0.0329663128207465
r2: 0.7386046101309229
pearson: 0.859709242937273

=== Experiment 540 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006328755653406858
rmse: 0.07955347668962594
mae: 0.032603926276912165
r2: 0.7146391930862661
pearson: 0.8455671374171773

=== Experiment 745 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006476486296033438
rmse: 0.08047661956141944
mae: 0.03336666618395923
r2: 0.707978083431462
pearson: 0.8414281144542867

=== Experiment 877 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005897306822300019
rmse: 0.07679392438402936
mae: 0.030375081626973034
r2: 0.7340930309857209
pearson: 0.861641305980665

=== Experiment 782 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005731950772358175
rmse: 0.07570964781557352
mae: 0.03023424682209268
r2: 0.7415488624988353
pearson: 0.861944069485636

=== Experiment 541 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0057202099229324435
rmse: 0.07563206940797298
mae: 0.029379576595306427
r2: 0.7420782522318987
pearson: 0.8619021922537878

=== Experiment 729 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00692818235842353
rmse: 0.08323570362785149
mae: 0.04722174284701785
r2: 0.6876113067849333
pearson: 0.839936814695316

=== Experiment 717 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008002208381473462
rmse: 0.08945506347587856
mae: 0.051914619557505454
r2: 0.639183946120615
pearson: 0.8138166391106226

=== Experiment 783 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008203189068882937
rmse: 0.0905714583568297
mae: 0.05042132746119989
r2: 0.6301218153836871
pearson: 0.8072756537956478

=== Experiment 530 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005660939767244266
rmse: 0.07523921694996742
mae: 0.027729850808743038
r2: 0.744750717465088
pearson: 0.8700717486915335

=== Experiment 863 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006762042127279679
rmse: 0.08223163702176724
mae: 0.03199769343316722
r2: 0.6951024966833017
pearson: 0.8347522396444003

=== Experiment 587 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007161905784708108
rmse: 0.08462804372492672
mae: 0.04015619455091098
r2: 0.6770728203633083
pearson: 0.8333165035824392

=== Experiment 754 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007111543570409621
rmse: 0.08432996840038315
mae: 0.03289392836500608
r2: 0.6793436304399215
pearson: 0.8325409821169677

=== Experiment 761 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006516549013682872
rmse: 0.08072514486628607
mae: 0.03467327990366564
r2: 0.7061716731255995
pearson: 0.8577766019927991

=== Experiment 795 ===
num_layers: 5
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006248612387506462
rmse: 0.07904816498506757
mae: 0.036781263726132676
r2: 0.7182528176719662
pearson: 0.8491549699134974

=== Experiment 870 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00668126330911823
rmse: 0.08173899503369386
mae: 0.029440565052677985
r2: 0.6987447780407245
pearson: 0.8387692205218666

=== Experiment 688 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005432085395227686
rmse: 0.07370268241541611
mae: 0.028568975728849654
r2: 0.7550696603728035
pearson: 0.8696683915558149

=== Experiment 954 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006499698418148789
rmse: 0.08062070713004686
mae: 0.0298423925057321
r2: 0.7069314590617171
pearson: 0.8609744220731664

=== Experiment 829 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006347776222720169
rmse: 0.07967293281108817
mae: 0.029483965741492535
r2: 0.7137815639875217
pearson: 0.8453198909335119

=== Experiment 726 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006392967724641576
rmse: 0.07995603619891106
mae: 0.03249524876242618
r2: 0.7117438990561866
pearson: 0.8441258597299578

=== Experiment 925 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0061670184780065335
rmse: 0.07853036659793798
mae: 0.03416700495898008
r2: 0.7219318511390921
pearson: 0.85563478179532

=== Experiment 904 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006991451188404119
rmse: 0.08361489812470095
mae: 0.03310195307620707
r2: 0.6847585430878502
pearson: 0.8363551709898094

=== Experiment 787 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00719995808079054
rmse: 0.08485256673071558
mae: 0.039770617521793034
r2: 0.6753570590810479
pearson: 0.8234564929086384

=== Experiment 666 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00740354894944332
rmse: 0.08604387804744344
mae: 0.043569436907859194
r2: 0.666177236420688
pearson: 0.8447441677349765

=== Experiment 602 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008607883068188011
rmse: 0.09277867787475746
mae: 0.05643179876916618
r2: 0.6118743410744718
pearson: 0.815226013839908

=== Experiment 614 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006144875691493051
rmse: 0.07838925750058519
mae: 0.033079103724286
r2: 0.7229302596371991
pearson: 0.8596142068489776

=== Experiment 944 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006896458268572222
rmse: 0.08304491717481705
mae: 0.04561053235790898
r2: 0.689041732033489
pearson: 0.8378355328629497

=== Experiment 832 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006294183808806913
rmse: 0.0793358923111533
mae: 0.02870198450482723
r2: 0.7161980223430452
pearson: 0.8572753633494911

=== Experiment 853 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006221797366818409
rmse: 0.0788783707160487
mae: 0.02949645294266086
r2: 0.7194618951525973
pearson: 0.8495006255333483

=== Experiment 920 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005825316430548183
rmse: 0.07632376058966292
mae: 0.030308897224576654
r2: 0.7373390460644167
pearson: 0.8598232831252928

=== Experiment 895 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005959543813995928
rmse: 0.07719808167303076
mae: 0.03192144014703726
r2: 0.7312867924227473
pearson: 0.8569229419265667

=== Experiment 833 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007637684331925118
rmse: 0.08739384607582572
mae: 0.045674512201861174
r2: 0.6556201750754462
pearson: 0.8206389944138953

=== Experiment 843 ===
num_layers: 6
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005466943882711641
rmse: 0.07393878469863865
mae: 0.028978793264813847
r2: 0.7534979065145457
pearson: 0.8698229407188603

=== Experiment 880 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006420758484782367
rmse: 0.08012963549637779
mae: 0.035034516941041434
r2: 0.7104908259130872
pearson: 0.8441135743378413

=== Experiment 710 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00839544859853297
rmse: 0.09162668060413937
mae: 0.037884604296421646
r2: 0.6214529178116575
pearson: 0.8043809971780221

=== Experiment 907 ===
num_layers: 4
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005978502421673025
rmse: 0.07732077613211746
mae: 0.028637152453391723
r2: 0.7304319571469082
pearson: 0.8576295289080574

=== Experiment 889 ===
num_layers: 4
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006443576979443845
rmse: 0.08027189408157655
mae: 0.03194124514218477
r2: 0.7094619500319888
pearson: 0.8466354230190262

=== Experiment 821 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006947758415002426
rmse: 0.0833532147850485
mae: 0.03437208462747001
r2: 0.6867286310098724
pearson: 0.8382361877690042

=== Experiment 581 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007621466014988791
rmse: 0.08730100809835355
mae: 0.053572909315877
r2: 0.656351451847878
pearson: 0.8310875214114126

=== Experiment 727 ===
num_layers: 6
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006558832030426437
rmse: 0.08098661636607889
mae: 0.03086493294377999
r2: 0.704265150510811
pearson: 0.8570764384844235

=== Experiment 958 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057887825000436065
rmse: 0.076084048919886
mae: 0.029844564203347346
r2: 0.7389863449110555
pearson: 0.8605332938268084

=== Experiment 894 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007207335006439888
rmse: 0.08489602467983932
mae: 0.04035408351583976
r2: 0.675024436750352
pearson: 0.8376935933136648

=== Experiment 576 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007265575154315455
rmse: 0.08523834321662672
mae: 0.03145005465834728
r2: 0.6723984141160921
pearson: 0.839338662009508

=== Experiment 970 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007709844042985163
rmse: 0.0878057175984865
mae: 0.03818992131564912
r2: 0.6523665254636664
pearson: 0.8294901189587625

=== Experiment 849 ===
num_layers: 5
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006487678560985095
rmse: 0.08054612691486224
mae: 0.03425155205278229
r2: 0.7074734291308842
pearson: 0.847718904419074

=== Experiment 945 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007076970639286551
rmse: 0.08412473262534954
mae: 0.0350758719493886
r2: 0.6809025086875502
pearson: 0.8375403012868502

=== Experiment 851 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005993786109135797
rmse: 0.07741954604062075
mae: 0.02843386967833878
r2: 0.7297428224060772
pearson: 0.8559557071651109

=== Experiment 617 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009865074614224943
rmse: 0.09932308198110318
mae: 0.03624077961984865
r2: 0.5551881275959816
pearson: 0.8166923891531416

=== Experiment 813 ===
num_layers: 4
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009451559844885351
rmse: 0.09721913312144555
mae: 0.057805605866814234
r2: 0.5738333265437354
pearson: 0.7958381717340092

=== Experiment 942 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007763757659325785
rmse: 0.08811218791589383
mae: 0.039803230395560385
r2: 0.6499355842320649
pearson: 0.8071360781796671

=== Experiment 881 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059799354356570395
rmse: 0.0773300422582132
mae: 0.02934006356790133
r2: 0.7303673431770867
pearson: 0.8556906841093095

=== Experiment 582 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.009327363269334489
rmse: 0.09657827534872679
mae: 0.04660426976776295
r2: 0.5794332954722303
pearson: 0.768768066171671

=== Experiment 844 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006480912949225747
rmse: 0.08050411759174649
mae: 0.03400582634080793
r2: 0.7077784875873399
pearson: 0.8423920623035863

=== Experiment 586 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005678746560416638
rmse: 0.07535745855863664
mae: 0.02990501618024237
r2: 0.7439478169983149
pearson: 0.8649996608561922

=== Experiment 847 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0076393025261547185
rmse: 0.08740310364143095
mae: 0.04597637961387352
r2: 0.6555472114098553
pearson: 0.8300982879704321

=== Experiment 600 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006845151416653848
rmse: 0.08273543023792074
mae: 0.037591598436264854
r2: 0.6913551354046159
pearson: 0.8316691256937886

=== Experiment 713 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007232277040715698
rmse: 0.08504279534866958
mae: 0.04395415405182151
r2: 0.6738998114026846
pearson: 0.8362104956462517

=== Experiment 765 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00664433499124736
rmse: 0.08151279035370683
mae: 0.037193781157784656
r2: 0.7004098596401266
pearson: 0.8388138947323553

=== Experiment 827 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005497156225792729
rmse: 0.07414280967020827
mae: 0.02852769974919332
r2: 0.7521356452624886
pearson: 0.8673904214384287

=== Experiment 667 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007166694549650231
rmse: 0.08465633201155263
mae: 0.03194982428939347
r2: 0.6768568970597648
pearson: 0.8376063718362838

=== Experiment 823 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005776540149945127
rmse: 0.07600355353498366
mae: 0.029213668644169875
r2: 0.7395383470887256
pearson: 0.8610567886016218

=== Experiment 771 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00672406021659309
rmse: 0.08200036717352606
mae: 0.030516635070816672
r2: 0.6968150843190428
pearson: 0.8656381442134156

=== Experiment 908 ===
num_layers: 6
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006357931311303571
rmse: 0.07973663719585602
mae: 0.033320844081519564
r2: 0.7133236755128924
pearson: 0.8471955572931961

=== Experiment 932 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006181863224150209
rmse: 0.07862482574956976
mae: 0.03392380651965757
r2: 0.7212625080691462
pearson: 0.8563890874952714

=== Experiment 769 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007661606352171258
rmse: 0.08753060237523365
mae: 0.03961653548755611
r2: 0.6545415417114342
pearson: 0.8142969204460216

=== Experiment 846 ===
num_layers: 4
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007116676274147302
rmse: 0.08436039517538607
mae: 0.04515259212534928
r2: 0.6791121990874651
pearson: 0.8364875376967835

=== Experiment 739 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006253176419333597
rmse: 0.07907702839215443
mae: 0.03141881960240887
r2: 0.7180470274856645
pearson: 0.850069235553789

=== Experiment 692 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006741824659842579
rmse: 0.08210861501598099
mae: 0.03526510785172426
r2: 0.6960140933916527
pearson: 0.8344855566604722

=== Experiment 683 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006891782882215944
rmse: 0.08301676265800748
mae: 0.0312577543266711
r2: 0.6892525431464986
pearson: 0.8399428765809295

=== Experiment 897 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006451885168212581
rmse: 0.08032362770824399
mae: 0.030754401962910405
r2: 0.7090873374571198
pearson: 0.8431417053594442

=== Experiment 949 ===
num_layers: 5
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006366552461728133
rmse: 0.07979067904040003
mae: 0.0359065308860775
r2: 0.7129349516347074
pearson: 0.8452493039423805

=== Experiment 909 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007874519714385197
rmse: 0.08873849060236036
mae: 0.051376586214641015
r2: 0.6449413719195447
pearson: 0.818396761216587

=== Experiment 806 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006917229080094986
rmse: 0.08316988084694474
mae: 0.038818188778359956
r2: 0.688105185283861
pearson: 0.8308827247877918

=== Experiment 746 ===
num_layers: 5
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006345538396813918
rmse: 0.07965888774527245
mae: 0.03114944726360735
r2: 0.713882466572692
pearson: 0.8519145395263625

=== Experiment 966 ===
num_layers: 5
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005721573877999379
rmse: 0.07564108591234911
mae: 0.02815613554603122
r2: 0.742016752098253
pearson: 0.8678432675956057

=== Experiment 976 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0067843643837553645
rmse: 0.08236725310313174
mae: 0.03581918588823517
r2: 0.6940959959635894
pearson: 0.8341663901168243

=== Experiment 552 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006673014211074588
rmse: 0.08168851945698727
mae: 0.036683426609479486
r2: 0.6991167262408069
pearson: 0.8377011936363725

=== Experiment 848 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006086094012314905
rmse: 0.07801342200105636
mae: 0.02896850291259641
r2: 0.7255806996795464
pearson: 0.8525730384601264

=== Experiment 929 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006577311478719716
rmse: 0.08110062563704251
mae: 0.031690704081868235
r2: 0.7034319203206941
pearson: 0.8459892782068986

=== Experiment 708 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005602640016943133
rmse: 0.07485078501220367
mae: 0.029012473379000342
r2: 0.7473794275464838
pearson: 0.8658896712973396

=== Experiment 960 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006053587022593815
rmse: 0.07780480076829331
mae: 0.03125498249546966
r2: 0.7270464255386498
pearson: 0.8534459493994462

=== Experiment 512 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008366029571478483
rmse: 0.09146600227121815
mae: 0.039332366399101565
r2: 0.6227794087932401
pearson: 0.8320665250121277

=== Experiment 972 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006153956905490762
rmse: 0.07844715995809384
mae: 0.031270321149300936
r2: 0.7225207916949905
pearson: 0.8564322052396462

=== Experiment 798 ===
num_layers: 8
units: 64
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0067500595892898045
rmse: 0.08215874627384356
mae: 0.029698656472675257
r2: 0.6956427840473469
pearson: 0.8352142236595719

=== Experiment 984 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062908375933491386
rmse: 0.07931480059452421
mae: 0.03197309718131064
r2: 0.7163489017252542
pearson: 0.8539084468353111

=== Experiment 852 ===
num_layers: 4
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005655731978842973
rmse: 0.07520460078241871
mae: 0.030519579292364585
r2: 0.7449855343519796
pearson: 0.8647147822050042

=== Experiment 802 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007177851372989374
rmse: 0.08472220118120972
mae: 0.03179013176923071
r2: 0.6763538408058962
pearson: 0.845823358028376

=== Experiment 975 ===
num_layers: 4
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0067410653777291605
rmse: 0.08210399124116416
mae: 0.03653510037202315
r2: 0.6960483290879611
pearson: 0.839127829495515

=== Experiment 784 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005818294996245448
rmse: 0.07627774902450549
mae: 0.03179593513846106
r2: 0.7376556394467575
pearson: 0.8620287334077551

=== Experiment 948 ===
num_layers: 5
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007147780953468171
rmse: 0.08454455011098096
mae: 0.040484418255935975
r2: 0.6777097027871035
pearson: 0.8389108490941668

=== Experiment 830 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0071437175307787
rmse: 0.08452051544316741
mae: 0.03777322377974931
r2: 0.6778929207277227
pearson: 0.8284481880484235

=== Experiment 996 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008105617976530085
rmse: 0.09003120557079132
mae: 0.04867797846835687
r2: 0.6345212529935553
pearson: 0.8038303830597203

=== Experiment 796 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008527668656838407
rmse: 0.09234537701930945
mae: 0.03582628056875793
r2: 0.6154911735771633
pearson: 0.8373749071768634

=== Experiment 529 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006244574056631859
rmse: 0.07902261737396363
mae: 0.03931560583937466
r2: 0.7184349042336966
pearson: 0.8521537613270398

=== Experiment 964 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006760661966193631
rmse: 0.0822232446829583
mae: 0.03418922099132352
r2: 0.6951647275392162
pearson: 0.8337676737759729

=== Experiment 886 ===
num_layers: 4
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0069980186530456
rmse: 0.08365416100258014
mae: 0.03955555643416143
r2: 0.684462418997728
pearson: 0.844458689027359

=== Experiment 876 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007812687971919455
rmse: 0.08838941097167384
mae: 0.04058989836268536
r2: 0.647729337973094
pearson: 0.8216735314625842

=== Experiment 921 ===
num_layers: 6
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00686016561356629
rmse: 0.08282611673600478
mae: 0.04049008734090819
r2: 0.6906781518740875
pearson: 0.8384710938009327

=== Experiment 989 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005904945863562845
rmse: 0.07684364556398171
mae: 0.028521214591509452
r2: 0.7337485899773794
pearson: 0.8571200796171661

=== Experiment 601 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006286687798488394
rmse: 0.07928863599841023
mae: 0.03292629771436897
r2: 0.7165360141490609
pearson: 0.8559936998010694

=== Experiment 562 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005583739291359414
rmse: 0.07472442232201874
mae: 0.02785387603020741
r2: 0.7482316529442081
pearson: 0.8705689708478961

=== Experiment 764 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006670260389161926
rmse: 0.08167166209378823
mae: 0.03508044781334616
r2: 0.6992408948587989
pearson: 0.8370862968952981

=== Experiment 718 ===
num_layers: 7
units: 64
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008693661366903416
rmse: 0.09323980569962281
mae: 0.041569294251645746
r2: 0.6080066353393108
pearson: 0.7971307051377012

=== Experiment 594 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006946807497039302
rmse: 0.08334751044295985
mae: 0.036554681380090125
r2: 0.6867715074822984
pearson: 0.8314269883263503

=== Experiment 785 ===
num_layers: 6
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00748380083319178
rmse: 0.08650896388925126
mae: 0.03342842383861393
r2: 0.662558714304025
pearson: 0.8490336868506386

=== Experiment 983 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006159165368072587
rmse: 0.07848035020355469
mae: 0.030813994353397947
r2: 0.7222859444095957
pearson: 0.869843114381302

=== Experiment 740 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006303195567936363
rmse: 0.07939266696576179
mae: 0.029793124862631973
r2: 0.7157916860902769
pearson: 0.8472111199415264

=== Experiment 951 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005457667428366895
rmse: 0.07387602742681075
mae: 0.028487592121847635
r2: 0.7539161777580704
pearson: 0.8683857492042067

=== Experiment 979 ===
num_layers: 4
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006884257370337428
rmse: 0.08297142502294044
mae: 0.0386474481990179
r2: 0.6895918651648699
pearson: 0.8313140024446914

=== Experiment 822 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007314267519456098
rmse: 0.08552349103875553
mae: 0.03596641429476832
r2: 0.6702028968030493
pearson: 0.8383907703853103

=== Experiment 736 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007488179221119376
rmse: 0.08653426616733614
mae: 0.03270051260476581
r2: 0.6623612947194457
pearson: 0.8269136063308663

=== Experiment 766 ===
num_layers: 8
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00585516587066653
rmse: 0.07651905560490492
mae: 0.028133319106626885
r2: 0.7359931479472237
pearson: 0.8595598898853694

=== Experiment 861 ===
num_layers: 6
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006092057940242981
rmse: 0.07805163637133421
mae: 0.031544639683982266
r2: 0.7253117887941949
pearson: 0.8524621244525276

=== Experiment 982 ===
num_layers: 4
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007971617945001161
rmse: 0.08928391761678674
mae: 0.04934571095765463
r2: 0.6405632554372704
pearson: 0.8109142758597193

=== Experiment 719 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070456948161766115
rmse: 0.08393863720704912
mae: 0.04017002265954498
r2: 0.6823127217860349
pearson: 0.8462677173056979

=== Experiment 902 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008883464513307583
rmse: 0.09425213267246309
mae: 0.04592044236578305
r2: 0.5994484950067002
pearson: 0.800326707894924

=== Experiment 675 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007339750766450846
rmse: 0.08567234540066501
mae: 0.04914158766484283
r2: 0.6690538684120908
pearson: 0.8328284853310232

=== Experiment 744 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0061698980885001925
rmse: 0.07854869883390936
mae: 0.02974483049292251
r2: 0.7218020107693464
pearson: 0.8555117375579788

=== Experiment 928 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005767613415272249
rmse: 0.07594480505783295
mae: 0.03364411267601323
r2: 0.7399408496261692
pearson: 0.8621187540197411

=== Experiment 654 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005800170126610083
rmse: 0.0761588479863639
mae: 0.035163402075547835
r2: 0.7384728818412508
pearson: 0.8625094125256166

=== Experiment 935 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068132019950419885
rmse: 0.08254212254989562
mae: 0.030188403379265358
r2: 0.692795720763079
pearson: 0.8421198889799405

=== Experiment 531 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005822197954876792
rmse: 0.07630332859631218
mae: 0.03557561637947099
r2: 0.7374796567598936
pearson: 0.8624253692247118

=== Experiment 628 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007739618452644321
rmse: 0.08797510132216002
mae: 0.0489796546165647
r2: 0.6510240104368548
pearson: 0.824890405833954

=== Experiment 618 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0059644015216513335
rmse: 0.07722953788319165
mae: 0.03666815631091979
r2: 0.7310677605226057
pearson: 0.857008478809505

=== Experiment 790 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007531479910546533
rmse: 0.0867840994108168
mae: 0.04002440583422552
r2: 0.6604088856912661
pearson: 0.8180131914549951

=== Experiment 578 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006857963838367101
rmse: 0.0828128241178086
mae: 0.03855864359530312
r2: 0.6907774289487434
pearson: 0.8474776116271115

=== Experiment 999 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007066925282552344
rmse: 0.08406500629008687
mae: 0.035885717651173606
r2: 0.6813554494013958
pearson: 0.8259588532908156

=== Experiment 549 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006035990337917987
rmse: 0.07769163621599166
mae: 0.03189058438362393
r2: 0.72783985230579
pearson: 0.8541151959365587

=== Experiment 799 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0073850574570036905
rmse: 0.08593635701496596
mae: 0.038793035409926784
r2: 0.6670110096760625
pearson: 0.817266648544163

=== Experiment 990 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007214963555680725
rmse: 0.0849409415751952
mae: 0.036611046810579716
r2: 0.674680468822664
pearson: 0.8303459826867426

=== Experiment 585 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00667278442163022
rmse: 0.08168711294708743
mae: 0.030269895414717946
r2: 0.6991270873457152
pearson: 0.8488499505526643

=== Experiment 933 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007172042961889066
rmse: 0.08468791508762667
mae: 0.035685286643871626
r2: 0.676615739505932
pearson: 0.8353371354211253

=== Experiment 670 ===
num_layers: 7
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006410164479418216
rmse: 0.08006350279258469
mae: 0.03151057242410268
r2: 0.7109685049521784
pearson: 0.8557711916494023

=== Experiment 677 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007887605628682397
rmse: 0.08881219301808957
mae: 0.0401404877266533
r2: 0.6443513338034352
pearson: 0.8137915209640799

=== Experiment 705 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005407567437134207
rmse: 0.07353616414482202
mae: 0.027288354766081775
r2: 0.75617516431206
pearson: 0.8722294398489138

=== Experiment 577 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007456880155773106
rmse: 0.08635322898289968
mae: 0.038697933770555595
r2: 0.6637725558001368
pearson: 0.8307569105879409

=== Experiment 731 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006042935872937431
rmse: 0.07773632273871353
mae: 0.034229268923038444
r2: 0.7275266811887269
pearson: 0.8532319141954602

=== Experiment 963 ===
num_layers: 6
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0066447425095603994
rmse: 0.08151529003543077
mae: 0.031800476637999675
r2: 0.7003914848187541
pearson: 0.8583305342746628

=== Experiment 905 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007818044616783178
rmse: 0.08841970717426731
mae: 0.041715278249458886
r2: 0.6474878092138283
pearson: 0.8213302807224209

=== Experiment 800 ===
num_layers: 7
units: 32
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008273112848811649
rmse: 0.09095665368081463
mae: 0.03230975383268266
r2: 0.6269689829225106
pearson: 0.810700752361614

=== Experiment 927 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005764200993256638
rmse: 0.0759223352726761
mae: 0.029885295324881208
r2: 0.7400947142329314
pearson: 0.8615615695744467

=== Experiment 956 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006166859418605409
rmse: 0.07852935386596155
mae: 0.029617274536013382
r2: 0.7219390230574814
pearson: 0.8517241934521216

=== Experiment 520 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006784434693288178
rmse: 0.08236767990715885
mae: 0.03691546936996067
r2: 0.6940928257377006
pearson: 0.8335379654745002

=== Experiment 596 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006134953095881398
rmse: 0.07832594139799022
mae: 0.03770432973679201
r2: 0.7233776651060604
pearson: 0.8565764712580648

=== Experiment 957 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006380847782353787
rmse: 0.07988020895286758
mae: 0.02993796240555829
r2: 0.7122903819195484
pearson: 0.8550062363561572

=== Experiment 712 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0063391831308736435
rmse: 0.0796189872509921
mae: 0.03213777152137473
r2: 0.7141690227167726
pearson: 0.8479025600758492

=== Experiment 923 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007124665821867394
rmse: 0.08440773555704117
mae: 0.034777980642130814
r2: 0.6787519538972351
pearson: 0.8361824708293846

=== Experiment 645 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006220024646328105
rmse: 0.07886713286488932
mae: 0.030870129652074356
r2: 0.7195418263392712
pearson: 0.8527570909607989

=== Experiment 978 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0069400666158984155
rmse: 0.08330706222102911
mae: 0.03944525473832584
r2: 0.6870754508460524
pearson: 0.8300601172971813

=== Experiment 743 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006865866355535574
rmse: 0.08286052350507794
mae: 0.03344224248693891
r2: 0.6904211079277809
pearson: 0.8328826353975447

=== Experiment 811 ===
num_layers: 5
units: 128
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006884651765149478
rmse: 0.0829738016795029
mae: 0.04381001926215553
r2: 0.6895740820763812
pearson: 0.8370923162797905

=== Experiment 814 ===
num_layers: 5
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006751393086753479
rmse: 0.08216686124438172
mae: 0.04180296835897213
r2: 0.6955826572336269
pearson: 0.8404510683674109

=== Experiment 515 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006220483759811391
rmse: 0.07887004348807848
mae: 0.02873924155239393
r2: 0.7195211251143812
pearson: 0.8585505000840046

=== Experiment 820 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0066306316944387675
rmse: 0.08142869085548882
mae: 0.032213457537361705
r2: 0.7010277352619438
pearson: 0.8518516957696979

=== Experiment 874 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006752890874396996
rmse: 0.08217597504378635
mae: 0.03500128987722561
r2: 0.6955151226480074
pearson: 0.8421072664841366

=== Experiment 968 ===
num_layers: 7
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007805101864578889
rmse: 0.08834648756220526
mae: 0.04203450863657879
r2: 0.6480713922141788
pearson: 0.8074770438310732

=== Experiment 828 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006137618412188146
rmse: 0.07834295381326993
mae: 0.033076092429755594
r2: 0.7232574871668845
pearson: 0.8534609100638753

=== Experiment 817 ===
num_layers: 8
units: 128
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007590997308964803
rmse: 0.08712632959653932
mae: 0.03282141116641021
r2: 0.6577252723921965
pearson: 0.842764145891116

=== Experiment 922 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006312389606969923
rmse: 0.07945054818545888
mae: 0.03884597636473861
r2: 0.7153771309168596
pearson: 0.8498629973013271

=== Experiment 555 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006331405016138875
rmse: 0.07957012640519603
mae: 0.03753770377686366
r2: 0.7145197344867531
pearson: 0.8480644006390652

=== Experiment 993 ===
num_layers: 7
units: 32
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006241222017882632
rmse: 0.07900140516397561
mae: 0.02834959404388605
r2: 0.7185860461855542
pearson: 0.8511823545665697

=== Experiment 750 ===
num_layers: 7
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006746545305379553
rmse: 0.0821373563330325
mae: 0.02972023000188832
r2: 0.6958012415621055
pearson: 0.8448775569661969

=== Experiment 841 ===
num_layers: 7
units: 64
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006516618606432272
rmse: 0.08072557591267016
mae: 0.02918468419813951
r2: 0.7061685352191591
pearson: 0.8507376173050181

=== Experiment 869 ===
num_layers: 8
units: 128
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006063628177309211
rmse: 0.07786930189303877
mae: 0.02949790779047364
r2: 0.726593674291983
pearson: 0.8540509751771521

=== Experiment 534 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006848358381163245
rmse: 0.08275480880990085
mae: 0.0330279819271168
r2: 0.6912105347863804
pearson: 0.8380640847051222

=== Experiment 878 ===
num_layers: 6
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006690360698307547
rmse: 0.08179462511869313
mae: 0.03766636703438751
r2: 0.6983345807662455
pearson: 0.8384478397937571

=== Experiment 757 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0059195257929656655
rmse: 0.07693845457874537
mae: 0.030809244783766745
r2: 0.7330911873777237
pearson: 0.8586560902326293

=== Experiment 816 ===
num_layers: 8
units: 64
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006821175958918232
rmse: 0.08259041081698426
mae: 0.03169904592455499
r2: 0.692436178241509
pearson: 0.8392926615537247

=== Experiment 668 ===
num_layers: 6
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006130826731167148
rmse: 0.07829959598342222
mae: 0.03348115843290885
r2: 0.723563721074874
pearson: 0.851017800999393

=== Experiment 732 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008432507998326797
rmse: 0.0918286883186665
mae: 0.03674556902299658
r2: 0.6197819257860429
pearson: 0.8461589498420476

=== Experiment 694 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0064307474328695125
rmse: 0.08019194119654115
mae: 0.032469654057146816
r2: 0.7100404286403103
pearson: 0.8531190606791498

=== Experiment 637 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0066539821546095345
rmse: 0.08157194465384245
mae: 0.03651949384746021
r2: 0.6999748732901674
pearson: 0.8377684249246496

=== Experiment 641 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006796727988148282
rmse: 0.08244227063920717
mae: 0.04632160899530873
r2: 0.6935385264831513
pearson: 0.8414275131713692

=== Experiment 834 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006166183373796303
rmse: 0.07852504933966169
mae: 0.033928679646063754
r2: 0.7219695056203739
pearson: 0.8512420546292127

=== Experiment 967 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006508891565421695
rmse: 0.080677701785696
mae: 0.037938719341092
r2: 0.7065169440973952
pearson: 0.8457596292151147

=== Experiment 943 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006909335680273349
rmse: 0.0831224138260755
mae: 0.03163667132217703
r2: 0.6884610952076675
pearson: 0.8428110040977392

=== Experiment 696 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006613423767752383
rmse: 0.08132295965932612
mae: 0.04289369718132595
r2: 0.7018036331024451
pearson: 0.8443654018660469

=== Experiment 597 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006308474133673702
rmse: 0.07942590341742234
mae: 0.034889863204570104
r2: 0.7155536778844547
pearson: 0.8500278184216107

=== Experiment 912 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005788397475004169
rmse: 0.07608151861657447
mae: 0.03376534200613761
r2: 0.7390037055206213
pearson: 0.8604042556863983

=== Experiment 950 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005877764501674003
rmse: 0.07666658008333228
mae: 0.032577100888726125
r2: 0.7349741856215148
pearson: 0.8574151052023681

=== Experiment 955 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005972228968587094
rmse: 0.07728019777787253
mae: 0.028351106323086104
r2: 0.7307148243853951
pearson: 0.857392269872072

=== Experiment 931 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006637996279979442
rmse: 0.08147389937875468
mae: 0.035146845412002706
r2: 0.7006956693413169
pearson: 0.8389299733189138

=== Experiment 919 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006472038389583296
rmse: 0.08044898004066488
mae: 0.035260290374381296
r2: 0.7081786375756256
pearson: 0.8429064917352804

=== Experiment 906 ===
num_layers: 8
units: 64
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065240378572422085
rmse: 0.08077151637329961
mae: 0.027716092304225996
r2: 0.7058340044656011
pearson: 0.8464625024716684

=== Experiment 752 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006089741530940115
rmse: 0.07803679600637199
mae: 0.030450296036160892
r2: 0.7254162346701314
pearson: 0.8531396504171461

=== Experiment 680 ===
num_layers: 4
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0060227093532944755
rmse: 0.07760611672603182
mae: 0.03322487375238194
r2: 0.7284386860570555
pearson: 0.8546293582287586

=== Experiment 961 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007416883925983294
rmse: 0.0861213325836479
mae: 0.0385579672673394
r2: 0.6655759681976725
pearson: 0.8301485776061805

=== Experiment 580 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006887474231484432
rmse: 0.08299080811441971
mae: 0.03951680500982964
r2: 0.6894468183116003
pearson: 0.8519610076762059

=== Experiment 992 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007243965570711294
rmse: 0.08511148906411692
mae: 0.029692050457498082
r2: 0.6733727807296985
pearson: 0.8308736407509133

=== Experiment 525 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006374715258745352
rmse: 0.07984181397454189
mae: 0.02986087304085938
r2: 0.7125668947099227
pearson: 0.8621741155240183

=== Experiment 959 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005758492684621603
rmse: 0.07588473288232359
mae: 0.029258217559601014
r2: 0.7403520993568664
pearson: 0.8682035163242369

=== Experiment 977 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005650023589198914
rmse: 0.07516663880471784
mae: 0.027399832063307008
r2: 0.7452429231285755
pearson: 0.8638930344477557

=== Experiment 986 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00619061293982601
rmse: 0.07868044826909676
mae: 0.031200827682321117
r2: 0.7208679872403685
pearson: 0.8534145582790067

=== Experiment 609 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006822240118038825
rmse: 0.08259685295481194
mae: 0.03216620883546338
r2: 0.6923881957751339
pearson: 0.834690455251323

=== Experiment 809 ===
num_layers: 5
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005523769176454199
rmse: 0.0743220638602979
mae: 0.029009270156758696
r2: 0.7509356790304181
pearson: 0.8721832808112066

=== Experiment 789 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007051796539451004
rmse: 0.08397497567401258
mae: 0.04097880185590015
r2: 0.6820375977691663
pearson: 0.8503170414787523

=== Experiment 571 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007497492480877823
rmse: 0.08658806199978045
mae: 0.04912853703566099
r2: 0.6619413639360163
pearson: 0.8267438048901605

=== Experiment 896 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009618976148984056
rmse: 0.09807637915922496
mae: 0.041855368465462145
r2: 0.5662845990774752
pearson: 0.7534358523905947

=== Experiment 857 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00590661048528992
rmse: 0.07685447602638326
mae: 0.033906058486437775
r2: 0.7336735329163615
pearson: 0.8578964491334902

=== Experiment 917 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006488845274454568
rmse: 0.08055336910678887
mae: 0.032714522127075846
r2: 0.7074208225340555
pearson: 0.8423464205790507

=== Experiment 616 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007310871416617163
rmse: 0.08550363393807986
mae: 0.0377245550303193
r2: 0.6703560255853176
pearson: 0.8243956663521188

=== Experiment 807 ===
num_layers: 5
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0077752289472006976
rmse: 0.08817725867365518
mae: 0.04868610789435966
r2: 0.6494183489106851
pearson: 0.8169423778502252

=== Experiment 974 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00659320108049871
rmse: 0.08119852880747723
mae: 0.03307268021341538
r2: 0.7027154651700279
pearson: 0.8408311750460681

=== Experiment 803 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007354215451446369
rmse: 0.08575672248544931
mae: 0.03852705670741757
r2: 0.6684016621320374
pearson: 0.846540748345914

=== Experiment 661 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005876997296754996
rmse: 0.07666157640405652
mae: 0.029384713460549796
r2: 0.7350087785536402
pearson: 0.8582551275849352

=== Experiment 838 ===
num_layers: 8
units: 32
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006363292037537292
rmse: 0.07977024531451117
mae: 0.029970900653495235
r2: 0.71308196272645
pearson: 0.8515230143659185

=== Experiment 734 ===
num_layers: 4
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006788823361466322
rmse: 0.08239431631773106
mae: 0.040788273524429604
r2: 0.6938949426211485
pearson: 0.8353581737031777

=== Experiment 890 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006445420912273876
rmse: 0.08028337880454382
mae: 0.03007611057192942
r2: 0.7093788079122596
pearson: 0.8510744713251721

=== Experiment 550 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008083142806325255
rmse: 0.08990630014812785
mae: 0.034503759886673736
r2: 0.6355346485260123
pearson: 0.8087846320411715

=== Experiment 884 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0076343678605245375
rmse: 0.08737486973108766
mae: 0.04178006623328551
r2: 0.6557697133112077
pearson: 0.8352636666821635

=== Experiment 656 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006797322221136739
rmse: 0.08244587449434167
mae: 0.03468302535363602
r2: 0.6935117327792437
pearson: 0.8363821803319613

=== Experiment 650 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007091762457556334
rmse: 0.0842126027240361
mae: 0.04829319675945877
r2: 0.6802355521121428
pearson: 0.8398937165566365

=== Experiment 971 ===
num_layers: 4
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006483233818362484
rmse: 0.08051853090042368
mae: 0.04112556033669809
r2: 0.7076738406194569
pearson: 0.846677387364073

=== Experiment 879 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007360865657219344
rmse: 0.08579548739426418
mae: 0.03323525158479544
r2: 0.6681018072807136
pearson: 0.8199497309803352

=== Experiment 703 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007032143361095603
rmse: 0.08385787596341565
mae: 0.037651146575217065
r2: 0.6829237509311858
pearson: 0.8314159643873378

=== Experiment 915 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00683416247267765
rmse: 0.08266899341759067
mae: 0.031581737666351806
r2: 0.6918506220518987
pearson: 0.8527884188667666

=== Experiment 882 ===
num_layers: 7
units: 128
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005609372449335917
rmse: 0.07489574386663048
mae: 0.02849652824587232
r2: 0.7470758651330631
pearson: 0.8668427198038303

=== Experiment 805 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006460061121225643
rmse: 0.08037450541823347
mae: 0.03722780280990368
r2: 0.7087186873342731
pearson: 0.8426676831071637

=== Experiment 901 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00754048918171919
rmse: 0.08683599012920386
mae: 0.03267743373585549
r2: 0.6600026616193746
pearson: 0.842874201732488

=== Experiment 867 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005965861172470018
rmse: 0.0772389873863583
mae: 0.03890887350873536
r2: 0.7310019455096326
pearson: 0.8599802137643932

=== Experiment 561 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005532193722609719
rmse: 0.07437871821031684
mae: 0.03178699903504606
r2: 0.7505558199521921
pearson: 0.8701148135437063

=== Experiment 985 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0060687737908440105
rmse: 0.07790233495116825
mae: 0.033744591269644415
r2: 0.7263616608424566
pearson: 0.8530526467976713

=== Experiment 900 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006810145243023427
rmse: 0.08252360415677097
mae: 0.03605892197077611
r2: 0.6929335483662165
pearson: 0.8351421955047389

=== Experiment 779 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006174087311489025
rmse: 0.07857536071497874
mae: 0.030091531713698248
r2: 0.721613120548598
pearson: 0.849711292348206

=== Experiment 770 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00624872775792818
rmse: 0.07904889472932673
mae: 0.030879729156930675
r2: 0.7182476156704294
pearson: 0.8475067403989704

=== Experiment 854 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00791569195616855
rmse: 0.08897017453151673
mae: 0.04058360644352449
r2: 0.6430849336588227
pearson: 0.8034991598547396

=== Experiment 845 ===
num_layers: 5
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007827813830008945
rmse: 0.08847493334277763
mae: 0.040251964309267536
r2: 0.647047319689238
pearson: 0.8405084376761445

=== Experiment 697 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0070318343499462185
rmse: 0.08385603347372339
mae: 0.036233598977148625
r2: 0.6829376841079055
pearson: 0.8271615529945735

=== Experiment 673 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005988494352594593
rmse: 0.077385362650792
mae: 0.030871773168082275
r2: 0.72998142537944
pearson: 0.8558232294921613

=== Experiment 747 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005727528869846535
rmse: 0.0756804391494033
mae: 0.029670825900609314
r2: 0.7417482441368571
pearson: 0.8614506551176737

=== Experiment 516 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006729002708427433
rmse: 0.08203049864792626
mae: 0.03993312569281868
r2: 0.6965922295375893
pearson: 0.8358515718509807

=== Experiment 885 ===
num_layers: 8
units: 32
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0064982744860836135
rmse: 0.08061187558966491
mae: 0.029983265369162192
r2: 0.7069956635318153
pearson: 0.8498573637236899

=== Experiment 774 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.010314467049534473
rmse: 0.10156016467855136
mae: 0.04503105348673372
r2: 0.5349252204806116
pearson: 0.7452272797445209

=== Experiment 953 ===
num_layers: 6
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006044292459660552
rmse: 0.07774504781438206
mae: 0.03139792650910157
r2: 0.7274655132904608
pearson: 0.8534110447870993

=== Experiment 973 ===
num_layers: 7
units: 128
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005529130036362973
rmse: 0.07435812017771141
mae: 0.02801396616167648
r2: 0.7506939602166265
pearson: 0.8666760075885274

=== Experiment 762 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006531630046061388
rmse: 0.08081850064225016
mae: 0.033476256223083006
r2: 0.7054916760133221
pearson: 0.8524750811764059

=== Experiment 564 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006808944838878148
rmse: 0.08251633074051554
mae: 0.03554188351167198
r2: 0.6929876740607888
pearson: 0.8440585773921889

=== Experiment 559 ===
num_layers: 5
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006503087765060557
rmse: 0.08064172471531444
mae: 0.03118769668015737
r2: 0.706778634901231
pearson: 0.8553117767569954

=== Experiment 965 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0058969159917163835
rmse: 0.07679137967061396
mae: 0.029387788429599535
r2: 0.7341106533647193
pearson: 0.8614357508462215

=== Experiment 995 ===
num_layers: 6
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006294692175870334
rmse: 0.07933909613721557
mae: 0.038540995692013934
r2: 0.716175100295905
pearson: 0.8487920271088619

=== Experiment 615 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007101022939060121
rmse: 0.08426756753971316
mae: 0.034950339128069975
r2: 0.6798180010769865
pearson: 0.8338080291236517

=== Experiment 775 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061470980431780516
rmse: 0.07840343132273007
mae: 0.03248954089835944
r2: 0.7228300547778543
pearson: 0.8515898961848506

=== Experiment 883 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007047636213015171
rmse: 0.08395020079198841
mae: 0.0369574545554066
r2: 0.6822251850570582
pearson: 0.8268139513503258

=== Experiment 748 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006458102045982471
rmse: 0.08036231732586158
mae: 0.03445399357374322
r2: 0.7088070211747328
pearson: 0.8443061378666176

=== Experiment 644 ===
num_layers: 7
units: 256
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00620220881319134
rmse: 0.0787541034689072
mae: 0.03238779397976519
r2: 0.7203451344140601
pearson: 0.8540410641518431

=== Experiment 655 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006261230261316368
rmse: 0.07912793603599406
mae: 0.034685025817849185
r2: 0.7176838832954284
pearson: 0.8485270994374986

=== Experiment 635 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007543604734176799
rmse: 0.08685392756909038
mae: 0.031368313618327114
r2: 0.6598621827303203
pearson: 0.8614441926275082

=== Experiment 937 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00604149269073491
rmse: 0.07772703963702021
mae: 0.029217810650667232
r2: 0.7275917536390455
pearson: 0.8572643123546957

=== Experiment 758 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006647850828299223
rmse: 0.08153435366947617
mae: 0.035679343978265894
r2: 0.7002513320948962
pearson: 0.8391502016596757

=== Experiment 721 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00608131437663318
rmse: 0.07798278256534054
mae: 0.031865033331032924
r2: 0.7257962113487568
pearson: 0.8562445093227783

=== Experiment 962 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00605586431348364
rmse: 0.0778194340347168
mae: 0.03462776887761269
r2: 0.7269437434947401
pearson: 0.8562149630729172

=== Experiment 558 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0061688011428668536
rmse: 0.07854171594042782
mae: 0.029181296153969048
r2: 0.721851471565151
pearson: 0.8523219574472685

=== Experiment 792 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0066054814259721515
rmse: 0.0812741128894813
mae: 0.031532166187959766
r2: 0.7021617497976238
pearson: 0.8580052715508282

=== Experiment 676 ===
num_layers: 6
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005928999256836549
rmse: 0.07699999517426316
mae: 0.029864244440891377
r2: 0.7326640330613758
pearson: 0.856673740192307

=== Experiment 981 ===
num_layers: 7
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006399666014297515
rmse: 0.07999791256212574
mae: 0.04027924156778278
r2: 0.711441876123743
pearson: 0.8474645101424733

=== Experiment 574 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.012998411754895455
rmse: 0.11401057738164233
mae: 0.04602978066015301
r2: 0.4139073350102883
pearson: 0.7103063220750524

=== Experiment 865 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006503211723136372
rmse: 0.08064249328447362
mae: 0.03731637821377217
r2: 0.7067730456861506
pearson: 0.8428645628165617

=== Experiment 506 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005904122140420209
rmse: 0.0768382856421212
mae: 0.029830273179884723
r2: 0.7337857312913325
pearson: 0.8590353747072322

=== Experiment 707 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008031533809910157
rmse: 0.08961882508664212
mae: 0.04346585544166397
r2: 0.6378616754595127
pearson: 0.8127353876498857

=== Experiment 691 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0071695307049099745
rmse: 0.08467308134767491
mae: 0.0382529361867657
r2: 0.6767290160673902
pearson: 0.8249188554816873

=== Experiment 502 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005798197535127166
rmse: 0.0761458963774619
mae: 0.028777098392424923
r2: 0.7385618251230117
pearson: 0.8608133195657461

=== Experiment 704 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00785757863250993
rmse: 0.08864298411329534
mae: 0.032211463563623886
r2: 0.6457052378449601
pearson: 0.8179655743673789

=== Experiment 723 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006458673068576104
rmse: 0.08036587004802538
mae: 0.038391669389756966
r2: 0.7087812740173123
pearson: 0.8439198701051208

=== Experiment 786 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006330986012864473
rmse: 0.07956749344339353
mae: 0.03270780825660694
r2: 0.7145386271599787
pearson: 0.8498573438160401

=== Experiment 980 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005963552276977335
rmse: 0.07722403950181145
mae: 0.02924009497359247
r2: 0.7311060525911752
pearson: 0.8556855661348141

=== Experiment 709 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00605076501551256
rmse: 0.07778666348103999
mae: 0.0326190483051635
r2: 0.7271736685958834
pearson: 0.8558152269523609

=== Experiment 934 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.009733107301126927
rmse: 0.09865651170159488
mae: 0.04685928807472451
r2: 0.5611384756603148
pearson: 0.7495663503785808

=== Experiment 647 ===
num_layers: 6
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006210348298718448
rmse: 0.07880576310599656
mae: 0.03142397258738151
r2: 0.7199781285941043
pearson: 0.8500166606830831

=== Experiment 693 ===
num_layers: 8
units: 128
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005900865512886599
rmse: 0.07681709128108535
mae: 0.02942908838816961
r2: 0.7339325711934717
pearson: 0.8682865058450895

=== Experiment 910 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.010908306607904344
rmse: 0.1044428389498502
mae: 0.04233946546213462
r2: 0.5081492561624918
pearson: 0.7725606783263538

=== Experiment 887 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005688790684993044
rmse: 0.07542407231774909
mae: 0.028797849847631073
r2: 0.743494931841922
pearson: 0.8623557933581588

=== Experiment 636 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008077746838731137
rmse: 0.0898762862980616
mae: 0.0406760776703404
r2: 0.635777950329884
pearson: 0.8247548079790329

=== Experiment 924 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0054567508880462145
rmse: 0.07386982393404098
mae: 0.030476356234098074
r2: 0.7539575041577297
pearson: 0.8687537837525491

=== Experiment 818 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0064592767552649095
rmse: 0.08036962582508961
mae: 0.035408752066218394
r2: 0.7087540540502166
pearson: 0.8428681030616355

=== Experiment 660 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005894593675902115
rmse: 0.076776257240778
mae: 0.032808307056614104
r2: 0.7342153655626553
pearson: 0.8631800316884634

=== Experiment 753 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007378119551413216
rmse: 0.08589598099686165
mae: 0.032909720930994216
r2: 0.6673238367855313
pearson: 0.8462385958480143

=== Experiment 663 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.009472892492023322
rmse: 0.09732878552629393
mae: 0.04059421116007453
r2: 0.5728714468735003
pearson: 0.7571467200255655

=== Experiment 507 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006761086910600239
rmse: 0.08222582873161108
mae: 0.03719069615693077
r2: 0.6951455669829547
pearson: 0.8619562112462684

=== Experiment 808 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062476212712503825
rmse: 0.07904189567090596
mae: 0.03252043318315752
r2: 0.7182975066677291
pearson: 0.8493841781433198

=== Experiment 546 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006154162866532096
rmse: 0.07844847268450862
mae: 0.03252769740718382
r2: 0.722511505002287
pearson: 0.8502761454043404

=== Experiment 583 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009072279456311684
rmse: 0.095248514194772
mae: 0.03771193379521601
r2: 0.5909349123304569
pearson: 0.7707946857724787

=== Experiment 679 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007849090037994614
rmse: 0.08859509037184066
mae: 0.036016042092026856
r2: 0.6460879848355395
pearson: 0.8454634045980968

=== Experiment 997 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006957638606694548
rmse: 0.08341246073995509
mae: 0.03500988344813766
r2: 0.6862831375150793
pearson: 0.8307670123065477

=== Experiment 699 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.010289750779987507
rmse: 0.10143840880055004
mae: 0.04261511169300646
r2: 0.5360396661960223
pearson: 0.7564699634585254

=== Experiment 914 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0059424757405866615
rmse: 0.07708745514405481
mae: 0.03146479597407015
r2: 0.7320563843405374
pearson: 0.8570318318708886

=== Experiment 682 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008936709613002199
rmse: 0.0945341716682502
mae: 0.0387133259443357
r2: 0.5970476968964303
pearson: 0.8049931093168237

=== Experiment 725 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009782437528827428
rmse: 0.09890620571444153
mae: 0.03791650244537181
r2: 0.5589141974051925
pearson: 0.7810149780524087

=== Experiment 671 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00576763182209769
rmse: 0.07594492624328297
mae: 0.03052722858427875
r2: 0.7399400196705116
pearson: 0.8601986286665236

=== Experiment 836 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0070538192259764505
rmse: 0.08398701819910295
mae: 0.030958170171768332
r2: 0.6819463957239862
pearson: 0.838911827328269

=== Experiment 788 ===
num_layers: 7
units: 256
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006378852012330654
rmse: 0.07986771570747879
mae: 0.03852553392627739
r2: 0.712380370311486
pearson: 0.8470432454977586

=== Experiment 742 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007269724214452364
rmse: 0.08526267773447163
mae: 0.03591523063175356
r2: 0.67221133482066
pearson: 0.8208399427960154

=== Experiment 815 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005934447719051394
rmse: 0.07703536667694516
mae: 0.030656857251969524
r2: 0.7324183642981602
pearson: 0.8559075514032002

=== Experiment 579 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00616300929839393
rmse: 0.07850483614653259
mae: 0.03356104865673566
r2: 0.7221126232832495
pearson: 0.8510524598878052

=== Experiment 544 ===
num_layers: 8
units: 256
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005502251720101854
rmse: 0.07417716441130555
mae: 0.02864615731380409
r2: 0.7519058916667887
pearson: 0.8703825781459924

=== Experiment 998 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006128393652957486
rmse: 0.07828405746355695
mae: 0.030099594179666966
r2: 0.7236734275004681
pearson: 0.8583426281794289

=== Experiment 850 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006276140855238051
rmse: 0.07922209827591069
mae: 0.03157401412852146
r2: 0.7170115711781541
pearson: 0.8520872772615646

=== Experiment 893 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0061538640749987045
rmse: 0.07844656828057366
mae: 0.029838196113712613
r2: 0.7225249773810195
pearson: 0.8714465924833443

=== Experiment 657 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0067562878465007685
rmse: 0.08219664133345576
mae: 0.03257864321710289
r2: 0.6953619546709702
pearson: 0.8423214024063025

=== Experiment 930 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005596965618934878
rmse: 0.07481287067700904
mae: 0.029501693041635937
r2: 0.7476352836551824
pearson: 0.8657180298124597

=== Experiment 760 ===
num_layers: 8
units: 256
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006420613856362883
rmse: 0.08012873302606802
mae: 0.027744771358179588
r2: 0.7104973471448659
pearson: 0.8487903758492769

=== Experiment 936 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006237107390279322
rmse: 0.07897535938683231
mae: 0.032936340594267706
r2: 0.7187715729332014
pearson: 0.8490777460117006

=== Experiment 940 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007013829074119361
rmse: 0.08374860640105816
mae: 0.036542795454634576
r2: 0.6837495340702106
pearson: 0.830840765944217

=== Experiment 591 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005913392833539154
rmse: 0.07689858798143925
mae: 0.03145196251166047
r2: 0.7333677198189433
pearson: 0.8593906588249204

=== Experiment 794 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009082134235507333
rmse: 0.09530023208527529
mae: 0.033518814224551025
r2: 0.5904905646739452
pearson: 0.7780660940588329

=== Experiment 918 ===
num_layers: 8
units: 256
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006374009572262959
rmse: 0.07983739457336367
mae: 0.04159895440814765
r2: 0.7125987138028174
pearson: 0.8500380868807242

=== Experiment 875 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005918097594256649
rmse: 0.07692917258268575
mae: 0.0292037559359415
r2: 0.733155584228915
pearson: 0.8597681033325677

=== Experiment 674 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005861094819480114
rmse: 0.07655778745157225
mae: 0.029710757132241493
r2: 0.7357258142547491
pearson: 0.8586474166268081

=== Experiment 605 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006850034993513128
rmse: 0.0827649381895083
mae: 0.032378473907700565
r2: 0.6911349370734576
pearson: 0.8407890590318661

=== Experiment 835 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006185151679061146
rmse: 0.07864573528845124
mae: 0.03016019817866012
r2: 0.7211142330845717
pearson: 0.8559104709318339

=== Experiment 620 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008672832384595446
rmse: 0.09312804295482348
mae: 0.038648196817867034
r2: 0.6089458049840408
pearson: 0.791344799597264

=== Experiment 913 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007245152204359344
rmse: 0.08511845983310168
mae: 0.032776483733618404
r2: 0.6733192759407854
pearson: 0.8401883973460271

=== Experiment 991 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006269066209021647
rmse: 0.07917743497374519
mae: 0.03085462185070798
r2: 0.7173305638622618
pearson: 0.8499947079855391

=== Experiment 610 ===
num_layers: 4
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006057838756872874
rmse: 0.07783211905680631
mae: 0.0299292530140818
r2: 0.7268547167113386
pearson: 0.8526149105067358

=== Experiment 988 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007441107892282044
rmse: 0.08626185653162147
mae: 0.045261188795934326
r2: 0.6644837202190448
pearson: 0.8220087539023316

=== Experiment 533 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00684702809239223
rmse: 0.08274677088800644
mae: 0.03840500969773583
r2: 0.6912705169215605
pearson: 0.8325425114085522

=== Experiment 714 ===
num_layers: 5
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00638084878770799
rmse: 0.079880215245754
mae: 0.030323057543089678
r2: 0.71229033658857
pearson: 0.8456012523515608

=== Experiment 603 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010203922964085798
rmse: 0.1010144690828289
mae: 0.03901259253911196
r2: 0.5399096046393195
pearson: 0.7363336755273017

=== Experiment 649 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007000507609780372
rmse: 0.08366903614707398
mae: 0.030588627050628286
r2: 0.6843501930340308
pearson: 0.8305825678264804

=== Experiment 872 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006219644307293207
rmse: 0.0788647215635306
mae: 0.037038744086030743
r2: 0.7195589756589547
pearson: 0.8506408833142285

=== Experiment 669 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006205701146909291
rmse: 0.07877627274065009
mae: 0.03721684635214776
r2: 0.7201876666238114
pearson: 0.8510208148041829

=== Experiment 987 ===
num_layers: 4
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006503359201771777
rmse: 0.08064340767708032
mae: 0.036889723239463205
r2: 0.7067663959393906
pearson: 0.8473367395292529

=== Experiment 824 ===
num_layers: 5
units: 512
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00583042875020599
rmse: 0.07635724425492312
mae: 0.034985859952262305
r2: 0.7371085338211502
pearson: 0.8594919262968507

=== Experiment 569 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.012056542387306744
rmse: 0.1098022877143584
mae: 0.04478902126889195
r2: 0.45637581024645224
pearson: 0.693428312545132

=== Experiment 623 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006164945087796503
rmse: 0.07851716428779444
mae: 0.028390080015704967
r2: 0.7220253393910934
pearson: 0.852754231112425

=== Experiment 947 ===
num_layers: 8
units: 512
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008159112810548565
rmse: 0.09032780751545211
mae: 0.0389898608374953
r2: 0.6321091944725392
pearson: 0.8010759006788807

=== Experiment 759 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006243189990940787
rmse: 0.07901385948642672
mae: 0.030007761700794326
r2: 0.7184973111465334
pearson: 0.8480688382732395

=== Experiment 773 ===
num_layers: 5
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005956511249542433
rmse: 0.07717843772416252
mae: 0.02935609550951766
r2: 0.7314235294192217
pearson: 0.8554667774960586

=== Experiment 613 ===
num_layers: 4
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006293963773361642
rmse: 0.07933450556574763
mae: 0.03340004157303083
r2: 0.7162079436444275
pearson: 0.8500236187472989

=== Experiment 781 ===
num_layers: 6
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006244318403068769
rmse: 0.07902099975999272
mae: 0.031155286329217227
r2: 0.7184464315403347
pearson: 0.8498404898449904

=== Experiment 926 ===
num_layers: 8
units: 512
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006077429839617603
rmse: 0.07795787221068572
mae: 0.029029614232943337
r2: 0.7259713634130737
pearson: 0.8672934617302364

=== Experiment 826 ===
num_layers: 6
units: 512
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005377320214172646
rmse: 0.07333021351511698
mae: 0.02801041661461357
r2: 0.7575389982825758
pearson: 0.87039170256003

=== Experiment 801 ===
num_layers: 7
units: 512
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005874177390077541
rmse: 0.07664318228047125
mae: 0.03922632333992581
r2: 0.735135926904591
pearson: 0.8669928176835641

=== Experiment 517 ===
num_layers: 7
units: 512
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005641069656313778
rmse: 0.07510705463745584
mae: 0.028245060394507224
r2: 0.7456466520214391
pearson: 0.8660309078755055

=== Experiment 994 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005568535711819872
rmse: 0.07462262198435453
mae: 0.02955572865927965
r2: 0.7489171756540389
pearson: 0.8658678069998028

=== Experiment 518 ===
num_layers: 7
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0055631957096961985
rmse: 0.07458683335345588
mae: 0.029207489317990012
r2: 0.7491579539994808
pearson: 0.8728542324935048

=== Experiment 946 ===
num_layers: 8
units: 512
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005955145874009266
rmse: 0.0771695916408093
mae: 0.030087512774634236
r2: 0.7314850936010644
pearson: 0.8576865763813225

=== Experiment 0 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005807712816714718
rmse: 0.07620835135806782
mae: 0.029793269529277428
r2: 0.7381327852642247
pearson: 0.8635641493722068

=== Experiment 1 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0058719238669042535
rmse: 0.07662847947665577
mae: 0.028669220433819892
r2: 0.7352375372726229
pearson: 0.8613940944135547

=== Experiment 0 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005807712816714718
rmse: 0.07620835135806782
mae: 0.029793269529277428
r2: 0.7381327852642247
pearson: 0.8635641493722068

=== Experiment 1 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005501132754845199
rmse: 0.07416962150938347
mae: 0.03022512544510874
r2: 0.7519563453177169
pearson: 0.8679950205878277

=== Experiment 31 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007059125987511592
rmse: 0.08401860500812658
mae: 0.04086916506380403
r2: 0.6817071161820512
pearson: 0.8288451753112509

=== Experiment 135 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006756246798727251
rmse: 0.08219639164055348
mae: 0.03716649331464835
r2: 0.6953638054970116
pearson: 0.835159767664585

=== Experiment 77 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007141433273650209
rmse: 0.08450700132918106
mae: 0.03831547391085268
r2: 0.6779959168762678
pearson: 0.8238735461536928

=== Experiment 62 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006770729285477813
rmse: 0.0822844413329629
mae: 0.034441924874486295
r2: 0.6947107965436585
pearson: 0.8345153803604389

=== Experiment 160 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007549124406142004
rmse: 0.08688569736235074
mae: 0.043284125062541555
r2: 0.6596133031508027
pearson: 0.8207593442614551

=== Experiment 23 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007734000724976521
rmse: 0.08794316758552947
mae: 0.03693453233325893
r2: 0.6512773113048449
pearson: 0.8151901972465909

=== Experiment 3 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007215725224275088
rmse: 0.08494542497553996
mae: 0.036060526385815826
r2: 0.6746461255209801
pearson: 0.8283972444108598

=== Experiment 84 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007206292226730035
rmse: 0.08488988294685083
mae: 0.035818219029551034
r2: 0.6750714552285069
pearson: 0.823053798449995

=== Experiment 136 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007568953395469585
rmse: 0.08699973215745888
mae: 0.032809279749947344
r2: 0.6587192227494276
pearson: 0.815678939917712

=== Experiment 117 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007319637983193859
rmse: 0.08555488287172076
mae: 0.0333491070684028
r2: 0.6699607449568354
pearson: 0.8204832969749276

=== Experiment 396 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008554630529870749
rmse: 0.09249124569315059
mae: 0.04167988238620896
r2: 0.6142754745889627
pearson: 0.7898990820046232

=== Experiment 37 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0070771933950801525
rmse: 0.08412605657630787
mae: 0.043750345679564165
r2: 0.6808924647268589
pearson: 0.8306485848514906

=== Experiment 89 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007333260162659736
rmse: 0.08563445663201079
mae: 0.03673935634653819
r2: 0.6693465268802988
pearson: 0.8360871844152244

=== Experiment 101 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008043004800297349
rmse: 0.08968280102838753
mae: 0.04169582267609988
r2: 0.637344453551723
pearson: 0.803433075903636

=== Experiment 85 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006128015458042066
rmse: 0.0782816418966929
mae: 0.03371615764739907
r2: 0.7236904801427482
pearson: 0.8510039641734872

=== Experiment 79 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007919307962367743
rmse: 0.08899049366290616
mae: 0.04544318196780765
r2: 0.6429218895308273
pearson: 0.8074306589995156

=== Experiment 21 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007246743256117606
rmse: 0.0851278054228911
mae: 0.04179060869291317
r2: 0.6732475361172809
pearson: 0.8256067068982326

=== Experiment 132 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006333476426240098
rmse: 0.07958314159569285
mae: 0.029967971577697695
r2: 0.7144263355169864
pearson: 0.8473124681920683

=== Experiment 175 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006605255436629475
rmse: 0.08127272258654483
mae: 0.03751742316504118
r2: 0.7021719395576196
pearson: 0.8423250492119913

=== Experiment 74 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005678786800528693
rmse: 0.07535772555304926
mae: 0.030152847825727885
r2: 0.7439460025893734
pearson: 0.8637940107311798

=== Experiment 419 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006649662320677196
rmse: 0.08154546168044667
mae: 0.037421004900397396
r2: 0.7001696527008723
pearson: 0.8399451926340957

=== Experiment 51 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007687680477693771
rmse: 0.08767941878054263
mae: 0.04735989540374576
r2: 0.6533658708677241
pearson: 0.816591814675921

=== Experiment 92 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008323189836628992
rmse: 0.09123151778102231
mae: 0.044906179673938625
r2: 0.6247110335823944
pearson: 0.8135528332139761

=== Experiment 29 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007618049002286713
rmse: 0.0872814356108257
mae: 0.048835195140230284
r2: 0.6565055234466723
pearson: 0.8186954390328018

=== Experiment 142 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007047792745396256
rmse: 0.08395113307988318
mae: 0.036433434668632456
r2: 0.6822181270808904
pearson: 0.8260222323163801

=== Experiment 64 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006480912949225747
rmse: 0.08050411759174649
mae: 0.03400582634080793
r2: 0.7077784875873399
pearson: 0.8423920623035863

=== Experiment 40 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007605671024584167
rmse: 0.08721049836220503
mae: 0.044117597590519184
r2: 0.6570636410133157
pearson: 0.825196346872979

=== Experiment 318 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007592081974876891
rmse: 0.08713255404770877
mae: 0.03922129785970613
r2: 0.6576763652836185
pearson: 0.8168790912846692

=== Experiment 612 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008210042840627601
rmse: 0.09060928672397549
mae: 0.04778274598798888
r2: 0.6298127818322959
pearson: 0.8035235995038009

=== Experiment 17 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006156377786222506
rmse: 0.07846258844967138
mae: 0.0318246990820669
r2: 0.7224116352483072
pearson: 0.8499899110018514

=== Experiment 326 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007157895026178322
rmse: 0.08460434401482185
mae: 0.03978843532389386
r2: 0.6772536637001466
pearson: 0.829217844883318

=== Experiment 171 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008095106132675561
rmse: 0.0899728077403143
mae: 0.047324438278258135
r2: 0.6349952274063395
pearson: 0.8032408354392692

=== Experiment 181 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006947095503666406
rmse: 0.08334923817088195
mae: 0.03757507477024532
r2: 0.6867585213902438
pearson: 0.8310314037394279

=== Experiment 119 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008351887178626638
rmse: 0.0913886600111121
mae: 0.03773283463757292
r2: 0.6234170830623867
pearson: 0.7904700341447075

=== Experiment 35 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006898116751538574
rmse: 0.0830549020319606
mae: 0.041779284071313705
r2: 0.6889669517664905
pearson: 0.8351453197831654

=== Experiment 387 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007131899155022047
rmse: 0.08445057225988493
mae: 0.03915139466957009
r2: 0.6784258060889252
pearson: 0.827642180207703

=== Experiment 78 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007959277216724079
rmse: 0.08921478138024035
mae: 0.050060683309361864
r2: 0.6411196934437142
pearson: 0.8279325936687418

=== Experiment 104 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006300112369074653
rmse: 0.07937324718741606
mae: 0.040998180952498825
r2: 0.7159307061699317
pearson: 0.850139006619224

=== Experiment 122 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006922560108899657
rmse: 0.0832019237091286
mae: 0.039671048170557534
r2: 0.6878648115414234
pearson: 0.8352341597396793

=== Experiment 443 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0075407211021220125
rmse: 0.08683732551225891
mae: 0.048101225610355326
r2: 0.6599922044304871
pearson: 0.8235474244587022

=== Experiment 99 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006673134985440907
rmse: 0.08168925869072939
mae: 0.037479543558330185
r2: 0.6991112805777864
pearson: 0.8525001550466177

=== Experiment 110 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006906065142787569
rmse: 0.08310273847947232
mae: 0.042136254379079474
r2: 0.6886085623034277
pearson: 0.839599226962145

=== Experiment 133 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066918677142269265
rmse: 0.08180383679404607
mae: 0.032692626909584396
r2: 0.6982666300817291
pearson: 0.8405270278113702

=== Experiment 115 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005708411763867521
rmse: 0.07555403208212995
mae: 0.03331849582516964
r2: 0.7426102260313691
pearson: 0.8636857192636281

=== Experiment 127 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00704014217689409
rmse: 0.08390555510151929
mae: 0.036990737678178294
r2: 0.6825630878473803
pearson: 0.8267307713338127

=== Experiment 120 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006210988734303454
rmse: 0.07880982638163501
mae: 0.030699921295596126
r2: 0.7199492516354536
pearson: 0.8514514251522725

=== Experiment 157 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006641926363161831
rmse: 0.08149801447373936
mae: 0.037817465812534826
r2: 0.7005184636203847
pearson: 0.840508559624855

=== Experiment 833 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008051481138702413
rmse: 0.0897300459082821
mae: 0.04006968088763594
r2: 0.6369622591837605
pearson: 0.7991765418827456

=== Experiment 793 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009250114488347668
rmse: 0.09617751550309286
mae: 0.04072370476716686
r2: 0.5829164090071377
pearson: 0.8195717844857199

=== Experiment 45 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007324984590627571
rmse: 0.08558612382055616
mae: 0.03781261528119519
r2: 0.6697196687808715
pearson: 0.821782042035906

=== Experiment 696 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007219221380273447
rmse: 0.08496600131978348
mae: 0.04013219368643331
r2: 0.67448848538579
pearson: 0.8261339462123561

=== Experiment 13 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005847633341012024
rmse: 0.07646981980501866
mae: 0.029213989324368184
r2: 0.7363327863940256
pearson: 0.859965996788045

=== Experiment 907 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00764152897415861
rmse: 0.08741583937799036
mae: 0.042004941024089165
r2: 0.6554468218492944
pearson: 0.8131198030851813

=== Experiment 93 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006874248978146383
rmse: 0.08291109080301852
mae: 0.04213781194024048
r2: 0.6900431391637338
pearson: 0.8380680189486746

=== Experiment 83 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005415867025200669
rmse: 0.07359257452488444
mae: 0.029062148556035405
r2: 0.7558009395390712
pearson: 0.8704977222585509

=== Experiment 543 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006916224976215447
rmse: 0.08316384416448921
mae: 0.04172437211416731
r2: 0.6881504598858494
pearson: 0.8341288109064202

=== Experiment 783 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009943359253219389
rmse: 0.09971639410457735
mae: 0.053645203937980966
r2: 0.5516583076794267
pearson: 0.8141314166774021

=== Experiment 38 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00622840017821734
rmse: 0.07892021400260733
mae: 0.0335564318525585
r2: 0.7191641772927391
pearson: 0.8511265034852945

=== Experiment 94 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006561307775126383
rmse: 0.08100189982417932
mae: 0.03979242778122505
r2: 0.7041535202719489
pearson: 0.8472095777679433

=== Experiment 418 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006978429162802516
rmse: 0.08353699278045934
mae: 0.04543509770580427
r2: 0.6853457004907948
pearson: 0.83637157938937

=== Experiment 294 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007621811711725432
rmse: 0.0873029879885301
mae: 0.03536308789165835
r2: 0.6563358645341755
pearson: 0.8111336547232479

=== Experiment 116 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00789783746498988
rmse: 0.08886977813064394
mae: 0.04150830622832226
r2: 0.6438899848077064
pearson: 0.8068738743043469

=== Experiment 507 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007750498678017463
rmse: 0.08803691656354999
mae: 0.04180130316817431
r2: 0.6505334258635332
pearson: 0.8190192961048085

=== Experiment 129 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007284207323870057
rmse: 0.08534756776774635
mae: 0.041553123289384056
r2: 0.6715582977915201
pearson: 0.8335132335260975

=== Experiment 449 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007280840330105951
rmse: 0.0853278402990838
mae: 0.038164481024280106
r2: 0.6717101140584163
pearson: 0.8229983648277115

=== Experiment 913 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006793487259028347
rmse: 0.08242261375998912
mae: 0.036936555565280894
r2: 0.6936846495328151
pearson: 0.8339881261979073

=== Experiment 475 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00682350149319014
rmse: 0.08260448833562338
mae: 0.03228624017532041
r2: 0.6923313209247346
pearson: 0.8471640623221328

=== Experiment 118 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0077691757587294045
rmse: 0.088142928013139
mae: 0.04030266725798845
r2: 0.6496912845146439
pearson: 0.8154361750740667

=== Experiment 666 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007297576199533613
rmse: 0.0854258520562342
mae: 0.04911059556364024
r2: 0.6709555010719425
pearson: 0.8386046310104912

=== Experiment 686 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007012619111624573
rmse: 0.08374138231259723
mae: 0.04475734685604038
r2: 0.6838040907465553
pearson: 0.8344112946071655

=== Experiment 643 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066966632711736584
rmse: 0.08183314286506206
mae: 0.038508866227453184
r2: 0.698050400529687
pearson: 0.8384774199210558

=== Experiment 352 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006948936986951916
rmse: 0.08336028423027309
mae: 0.03271845479838698
r2: 0.686675489719399
pearson: 0.8442773206996435

=== Experiment 711 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007256978934776293
rmse: 0.08518790368811932
mae: 0.04046288616713205
r2: 0.6727860138716273
pearson: 0.8244464799352905

=== Experiment 683 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006144210613244398
rmse: 0.0783850152340637
mae: 0.03902136199431226
r2: 0.7229602477227077
pearson: 0.8550607894873516

=== Experiment 36 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006430037484516072
rmse: 0.08018751451763592
mae: 0.032938987127526034
r2: 0.7100724398990945
pearson: 0.858936225797339

=== Experiment 601 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0060347310144974005
rmse: 0.0776835311665053
mae: 0.03472405306749365
r2: 0.7278966346445535
pearson: 0.8540785939633093

=== Experiment 43 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007150851630851162
rmse: 0.08456270827528622
mae: 0.03492284917438943
r2: 0.677571247295413
pearson: 0.8401740590287199

=== Experiment 42 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006966413483240082
rmse: 0.0834650434807296
mae: 0.03790660766152741
r2: 0.6858874821937597
pearson: 0.8309178976188468

=== Experiment 273 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005912821513116625
rmse: 0.07689487312634455
mae: 0.0360470797502354
r2: 0.7333934804053377
pearson: 0.8577049862947129

=== Experiment 6 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006218890140222066
rmse: 0.07885994002167429
mae: 0.03269917528792131
r2: 0.7195929807202581
pearson: 0.8488850414853618

=== Experiment 107 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008123528499446874
rmse: 0.09013061910054138
mae: 0.05528137503492113
r2: 0.6337136753982613
pearson: 0.8251285514989815

=== Experiment 24 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007466046591901349
rmse: 0.08640628791876982
mae: 0.0380612767763501
r2: 0.6633592452295716
pearson: 0.8179109578485411

=== Experiment 308 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.012578788032897775
rmse: 0.1121551961921416
mae: 0.049886735046678135
r2: 0.4328279839446395
pearson: 0.6931003034721999

=== Experiment 537 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012162521785507443
rmse: 0.11028382377079353
mae: 0.045590800758889986
r2: 0.45159724582668304
pearson: 0.6855045145216466

=== Experiment 538 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006970823310565221
rmse: 0.0834914565124194
mae: 0.03721873137407225
r2: 0.6856886450205826
pearson: 0.8444475311756566

=== Experiment 5 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006223091561148759
rmse: 0.07888657402339613
mae: 0.029072401594304745
r2: 0.7194035404998602
pearson: 0.8575666628461942

=== Experiment 20 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006723212178795274
rmse: 0.08199519607144844
mae: 0.03818314523192783
r2: 0.6968533219700963
pearson: 0.8364550564398442

=== Experiment 809 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006064046634573767
rmse: 0.07787198876729531
mae: 0.03169523325545485
r2: 0.7265748062381012
pearson: 0.8539370866788719

=== Experiment 870 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00784179294925628
rmse: 0.08855389855481395
mae: 0.038697129229798295
r2: 0.6464170073550568
pearson: 0.8062965924384851

=== Experiment 34 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006433766708118815
rmse: 0.0802107642908283
mae: 0.03306898289853545
r2: 0.7099042908482038
pearson: 0.8467598706132989

=== Experiment 262 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008107283980591785
rmse: 0.09004045746547373
mae: 0.03670643877134516
r2: 0.6344461336036777
pearson: 0.7968855174983124

=== Experiment 2 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005723944019904894
rmse: 0.07565675131741313
mae: 0.030926078408609933
r2: 0.7419098834429125
pearson: 0.8642455766046991

=== Experiment 147 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0068094124246823404
rmse: 0.08251916398438813
mae: 0.030768283972547703
r2: 0.6929665908226781
pearson: 0.8329409931450364

=== Experiment 569 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0099571562716566
rmse: 0.09978555141730991
mae: 0.044183144804170967
r2: 0.5510362061906249
pearson: 0.7436787801042115

=== Experiment 207 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00630942461870676
rmse: 0.07943188666213814
mae: 0.03283667112084613
r2: 0.7155108209326545
pearson: 0.8514869476698066

=== Experiment 178 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005978258807595686
rmse: 0.07731920076925063
mae: 0.03297563008392044
r2: 0.7304429415984328
pearson: 0.8565679301461162

=== Experiment 450 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00601643217084577
rmse: 0.07756566360733189
mae: 0.030813429088518803
r2: 0.7287217214508686
pearson: 0.8592915143228982

=== Experiment 280 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006244352452510942
rmse: 0.07902121520522791
mae: 0.033315637639061674
r2: 0.7184448962659735
pearson: 0.8477067301490046

=== Experiment 346 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006634605412269282
rmse: 0.08145308718685426
mae: 0.03996445362137087
r2: 0.7008485620739329
pearson: 0.8399163404364642

=== Experiment 95 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005593759404141971
rmse: 0.07479143937739112
mae: 0.03140355259199452
r2: 0.7477798504690316
pearson: 0.8666626371693613

=== Experiment 972 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008449518455827093
rmse: 0.09192126226193313
mae: 0.04114680622041616
r2: 0.6190149317442293
pearson: 0.7878659533304436

=== Experiment 855 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059907888109313225
rmse: 0.07740018611690364
mae: 0.03187690374087872
r2: 0.7298779692629052
pearson: 0.8594964378309892

=== Experiment 1000 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006503493006900839
rmse: 0.08064423728265299
mae: 0.04074626457699597
r2: 0.7067603627250133
pearson: 0.844791750933957

=== Experiment 831 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005988328644587375
rmse: 0.07738429197574515
mae: 0.03795938412343097
r2: 0.7299888970805516
pearson: 0.859986123377088

=== Experiment 69 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006073624455644364
rmse: 0.07793346171988233
mae: 0.027902526931093987
r2: 0.7261429465015494
pearson: 0.8528371315717276

=== Experiment 371 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008464857512455853
rmse: 0.09200466027574827
mae: 0.03838433114336989
r2: 0.6183233004320738
pearson: 0.7904460045790307

=== Experiment 981 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009897586792938897
rmse: 0.09948661614980628
mae: 0.0423588788362352
r2: 0.5537221677674737
pearson: 0.7548843677296583

=== Experiment 455 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006032590939620283
rmse: 0.0776697556299766
mae: 0.03002322775200745
r2: 0.7279931296788438
pearson: 0.8540819078186065

=== Experiment 877 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005809288226038116
rmse: 0.07621868685590244
mae: 0.03157337689386826
r2: 0.7380617507512233
pearson: 0.8609037200646502

=== Experiment 968 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005727469775347264
rmse: 0.07568004872717289
mae: 0.03007421716345416
r2: 0.7417509086818198
pearson: 0.8626578572069324

=== Experiment 164 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005935459467611693
rmse: 0.07704193317675571
mae: 0.029023481894111167
r2: 0.7323727450008809
pearson: 0.8570609430432925

=== Experiment 188 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.01071172489796365
rmse: 0.10349746324409913
mae: 0.042873200288372236
r2: 0.5170130389414902
pearson: 0.7450278337977867

=== Experiment 15 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0059902858620041275
rmse: 0.07739693703244417
mae: 0.028126184285531944
r2: 0.7299006470086675
pearson: 0.8588158054424877

=== Experiment 296 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005671482363155485
rmse: 0.07530924487176514
mae: 0.03233213693117097
r2: 0.7442753564556026
pearson: 0.8628900149221138

=== Experiment 826 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007184674055583858
rmse: 0.08476245663962234
mae: 0.04473001000267912
r2: 0.6760462090506033
pearson: 0.827005923302443

=== Experiment 408 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008546116605244191
rmse: 0.09244520866569662
mae: 0.04290117558329572
r2: 0.6146593637031095
pearson: 0.7845732783028034

=== Experiment 541 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00835307478463173
rmse: 0.09139515733687277
mae: 0.036381742586983755
r2: 0.6233635344303226
pearson: 0.7983434700868359

=== Experiment 921 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005912015947322085
rmse: 0.07688963484971226
mae: 0.031072074341048677
r2: 0.7334298030124569
pearson: 0.8597006219815418

=== Experiment 533 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005511522574218253
rmse: 0.07423962940517856
mae: 0.029254128944501342
r2: 0.7514878729350952
pearson: 0.8671357599657016

=== Experiment 943 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005661801974445168
rmse: 0.07524494650436778
mae: 0.03451451176366524
r2: 0.7447118409218858
pearson: 0.8673790269260794

=== Experiment 390 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006446229014838489
rmse: 0.08028841146042491
mae: 0.03275286595569121
r2: 0.7093423709232942
pearson: 0.8429642337214521

=== Experiment 762 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005808555524483746
rmse: 0.07621388013009013
mae: 0.03222050088154332
r2: 0.7380947879418231
pearson: 0.859350940094792

=== Experiment 838 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.010111515184132844
rmse: 0.1005560300734513
mae: 0.04659967080643011
r2: 0.544076230765624
pearson: 0.7410652963585749

=== Experiment 319 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00850723388892073
rmse: 0.09223466750046172
mae: 0.03152762225878887
r2: 0.6164125682685421
pearson: 0.8016910563270306

=== Experiment 540 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0064012079832725215
rmse: 0.08000754953923112
mae: 0.03216385693760897
r2: 0.7113723494213323
pearson: 0.846620919901205

=== Experiment 438 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005692600870880492
rmse: 0.07544932651044998
mae: 0.02985043102128125
r2: 0.7433231322372481
pearson: 0.862723785165494

=== Experiment 983 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005819899306640368
rmse: 0.07628826454075599
mae: 0.030443626626513128
r2: 0.7375833017971615
pearson: 0.865465910028879

=== Experiment 974 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006313830205949728
rmse: 0.07945961367858347
mae: 0.035768786358431955
r2: 0.7153121749429162
pearson: 0.8482859630276877

=== Experiment 517 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005315050360740459
rmse: 0.07290439191667714
mae: 0.029069999042008576
r2: 0.7603467185667746
pearson: 0.8720498969975067

=== Experiment 869 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006784830004525434
rmse: 0.08237007954667419
mae: 0.036262327909368106
r2: 0.6940750013280024
pearson: 0.8338024680917734

=== Experiment 277 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00669017918482771
rmse: 0.08179351554266212
mae: 0.031240510978574138
r2: 0.6983427651291613
pearson: 0.8366664805755117

=== Experiment 905 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006814174926159186
rmse: 0.08254801588263153
mae: 0.03249931886652769
r2: 0.6927518517272242
pearson: 0.8412648699448887

=== Experiment 915 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006747133724323869
rmse: 0.08214093817533294
mae: 0.035850784246556716
r2: 0.6957747100108891
pearson: 0.83500884635111

=== Experiment 1009 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00689546583363907
rmse: 0.08303894166979171
mae: 0.03528026302457408
r2: 0.6890864804878208
pearson: 0.8314358287819822

=== Experiment 1006 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007292362556726475
rmse: 0.08539533100074309
mae: 0.04159451460927655
r2: 0.6711905819314166
pearson: 0.827419121942486

=== Experiment 1014 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00724038458509641
rmse: 0.085090449435271
mae: 0.04677145202586232
r2: 0.6735342457949614
pearson: 0.8301288105153767

=== Experiment 1013 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0060802343185716356
rmse: 0.07797585728013277
mae: 0.03349639287854388
r2: 0.725844910691382
pearson: 0.8524439968448845

=== Experiment 1016 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007508984779478326
rmse: 0.0866543985004704
mae: 0.03654445300349142
r2: 0.6614231812502671
pearson: 0.8321565564317845

=== Experiment 1020 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007126145601272886
rmse: 0.08441650076420419
mae: 0.04278798793263096
r2: 0.6786852312951422
pearson: 0.8277760264716784

=== Experiment 1025 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006584868592429877
rmse: 0.08114720323233499
mae: 0.03488523028017967
r2: 0.7030911733896432
pearson: 0.8438859027972238

=== Experiment 1003 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008482062832210565
rmse: 0.09209811524787337
mae: 0.04222957074480651
r2: 0.6175475201369742
pearson: 0.7866030126951175

=== Experiment 1028 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007097741721054195
rmse: 0.08424809624587487
mae: 0.046095457837136096
r2: 0.6799659497526993
pearson: 0.8315313137299605

=== Experiment 1022 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008315367520962359
rmse: 0.09118863701669391
mae: 0.05066637535793899
r2: 0.6250637383529385
pearson: 0.8190999736943873

=== Experiment 1030 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006322489772203182
rmse: 0.07951408536984615
mae: 0.036609252998293984
r2: 0.7149217189119148
pearson: 0.8467125361639629

=== Experiment 1032 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006433451081512889
rmse: 0.08020879678384965
mae: 0.03377442435506541
r2: 0.709918522312947
pearson: 0.8425922769363882

=== Experiment 1005 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006642772713374952
rmse: 0.0815032067674331
mae: 0.03273594220206757
r2: 0.7004803020617822
pearson: 0.8468606418572447

=== Experiment 1024 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0058821476948682895
rmse: 0.07669516083083919
mae: 0.031013857698513603
r2: 0.7347765493695755
pearson: 0.867979640589391

=== Experiment 1010 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006549178797650738
rmse: 0.08092699671710757
mae: 0.04106308751336514
r2: 0.7047004105279545
pearson: 0.8426114230424795

=== Experiment 1039 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0070994946800778305
rmse: 0.08425849915633336
mae: 0.045276433992631315
r2: 0.6798869096018598
pearson: 0.8317113663951413

=== Experiment 1047 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006995885617520477
rmse: 0.0836414109010631
mae: 0.0345798173780017
r2: 0.684558596630735
pearson: 0.8279229500071207

=== Experiment 1042 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007231109615526443
rmse: 0.0850359313203921
mae: 0.04236915847011757
r2: 0.673952450090645
pearson: 0.8375214082609103

=== Experiment 1058 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0076141484044619035
rmse: 0.0872590878044339
mae: 0.040158679534598826
r2: 0.6566813996858087
pearson: 0.8107520767650506

=== Experiment 1060 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006609736595789892
rmse: 0.08130028656646845
mae: 0.03938590025650294
r2: 0.7019698860633854
pearson: 0.8402246738621288

=== Experiment 1040 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007902360478401772
rmse: 0.08889522190985166
mae: 0.038143098938604986
r2: 0.6436860441236947
pearson: 0.8111919901415701

=== Experiment 1017 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006919208656806634
rmse: 0.08318178079848156
mae: 0.03311074106162924
r2: 0.6880159270412111
pearson: 0.8372970364568247

=== Experiment 1019 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.012120144155318863
rmse: 0.11009152626482595
mae: 0.04819591713416047
r2: 0.4535080345200644
pearson: 0.7001757472207389

=== Experiment 1018 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006170529289151182
rmse: 0.07855271662489581
mae: 0.03418205587920399
r2: 0.7217735502098059
pearson: 0.8541129249182365

=== Experiment 1002 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00641004295010704
rmse: 0.08006274383324019
mae: 0.038369720237369
r2: 0.7109739846553302
pearson: 0.8531954429671849

=== Experiment 1054 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006505985457735869
rmse: 0.08065968917455527
mae: 0.034946659052387745
r2: 0.7066479792138733
pearson: 0.8453799177414099

=== Experiment 1063 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006815395395575358
rmse: 0.08255540803348596
mae: 0.0413525773584684
r2: 0.6926968212984186
pearson: 0.8369310955862452

=== Experiment 1089 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0069769435138914105
rmse: 0.08352810014534875
mae: 0.03752073711181303
r2: 0.6854126877463124
pearson: 0.8283835624072265

=== Experiment 1055 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061425745137915645
rmse: 0.0783745782367699
mae: 0.03474368571975811
r2: 0.7230340187269336
pearson: 0.8572551781782706

=== Experiment 1044 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007129322589841725
rmse: 0.08443531601078855
mae: 0.034835332085775585
r2: 0.6785419822788727
pearson: 0.8449071715266531

=== Experiment 1067 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007954131163801233
rmse: 0.08918593590808604
mae: 0.036072747479617674
r2: 0.6413517267050541
pearson: 0.8284263840357378

=== Experiment 1011 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00627637725669449
rmse: 0.07922359027899764
mae: 0.03093017920698492
r2: 0.7170009119405314
pearson: 0.8501548139272546

=== Experiment 1065 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006932037630989629
rmse: 0.08325885917420217
mae: 0.03634962451908215
r2: 0.687437474241763
pearson: 0.8298469088584343

=== Experiment 1015 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005728881777188497
rmse: 0.07568937691108639
mae: 0.0319304956759778
r2: 0.741687242140268
pearson: 0.8626643467071798

=== Experiment 1083 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00781638188714894
rmse: 0.08841030419102142
mae: 0.0398103729667057
r2: 0.6475627809612129
pearson: 0.8077185595491408

=== Experiment 1139 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006912897510683444
rmse: 0.08314383627595881
mae: 0.03761544834139136
r2: 0.6883004938421577
pearson: 0.8303634213743406

=== Experiment 1035 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006702524143376571
rmse: 0.0818689449264895
mae: 0.041238312457726106
r2: 0.6977861363816249
pearson: 0.8476210753450083

=== Experiment 1049 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00565232870216355
rmse: 0.0751819705924469
mae: 0.029108468144902398
r2: 0.7451389865995579
pearson: 0.864253876770423

=== Experiment 1107 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006988796685113187
rmse: 0.08359902323061667
mae: 0.0379522363266991
r2: 0.684878233472902
pearson: 0.8357007986931168

=== Experiment 1105 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007127887450714341
rmse: 0.08442681713007036
mae: 0.039604397244505936
r2: 0.6786066920704736
pearson: 0.8310497038649892

=== Experiment 1131 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006603584207540991
rmse: 0.08126244032479575
mae: 0.0341347491924165
r2: 0.7022472945416551
pearson: 0.8380802349582038

=== Experiment 1007 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005919618187501795
rmse: 0.07693905502085267
mae: 0.030505566491686262
r2: 0.7330870213487557
pearson: 0.8563737042982729

=== Experiment 1123 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006684408542658274
rmse: 0.08175823226231273
mae: 0.03507547034255521
r2: 0.6986029608447271
pearson: 0.8360302683170985

=== Experiment 1087 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007392273188926143
rmse: 0.08597832976352904
mae: 0.04722888044986814
r2: 0.6666856554995657
pearson: 0.8241030841932254

=== Experiment 1077 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007681943921485966
rmse: 0.08764669943292769
mae: 0.04231635394327308
r2: 0.6536245296622827
pearson: 0.829320022196343

=== Experiment 1104 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007222582626531217
rmse: 0.08498577896643189
mae: 0.037234080838433284
r2: 0.674336928271425
pearson: 0.8304167363791584

=== Experiment 1086 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007308692116882619
rmse: 0.08549088908698177
mae: 0.03548723975597056
r2: 0.6704542892511651
pearson: 0.8263234882723479

=== Experiment 1124 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076651304646011015
rmse: 0.08755073080563693
mae: 0.03911026404169202
r2: 0.6543826410330529
pearson: 0.8383202869997801

=== Experiment 1101 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007164681044238698
rmse: 0.08464443894455617
mae: 0.043018368807189795
r2: 0.6769476851325592
pearson: 0.8288966877777826

=== Experiment 1151 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007227444156500498
rmse: 0.08501437617544752
mae: 0.03913304198742924
r2: 0.6741177240248329
pearson: 0.821404552501484

=== Experiment 1097 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006916220441035606
rmse: 0.0831638168979491
mae: 0.037172121883786845
r2: 0.6881506643751123
pearson: 0.8306554374787644

=== Experiment 1061 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00587869215916681
rmse: 0.07667262979164606
mae: 0.03323063281544444
r2: 0.7349323579534575
pearson: 0.8583515857808564

=== Experiment 1062 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0074402016856392125
rmse: 0.08625660372191345
mae: 0.03086547501708279
r2: 0.664524580677718
pearson: 0.8287952082960358

=== Experiment 1142 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00692705380243226
rmse: 0.083228924073499
mae: 0.042296505309121823
r2: 0.6876621928778641
pearson: 0.8332610990341073

=== Experiment 1145 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006787585344628975
rmse: 0.08238680321889529
mae: 0.03704708988426473
r2: 0.6939507642554517
pearson: 0.834718974650113

=== Experiment 1121 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007966785811638387
rmse: 0.08925685302338632
mae: 0.035405092207842145
r2: 0.6407811342038132
pearson: 0.8268513419321163

=== Experiment 1034 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005869407189442431
rmse: 0.07661205642353187
mae: 0.03145910365733955
r2: 0.735351013151702
pearson: 0.8576529646234351

=== Experiment 1038 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006978939845198915
rmse: 0.08354004934879387
mae: 0.03580912647714712
r2: 0.6853226740463094
pearson: 0.834740719025183

=== Experiment 1136 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008114181304709187
rmse: 0.09007875057253617
mae: 0.053029343386183085
r2: 0.6341351362949685
pearson: 0.8179330467010902

=== Experiment 1173 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006852752370129575
rmse: 0.0827813527923383
mae: 0.03969290286363247
r2: 0.6910124117578309
pearson: 0.8339983261049391

=== Experiment 1026 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057527798912222275
rmse: 0.07584708228549222
mae: 0.0304171775804653
r2: 0.7406096866968515
pearson: 0.8632655348030106

=== Experiment 1170 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008328520819247837
rmse: 0.09126072988557475
mae: 0.03851985363042055
r2: 0.6244706619224556
pearson: 0.8045589293793126

=== Experiment 1133 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006605044856823984
rmse: 0.08127142706279977
mae: 0.03274815628446354
r2: 0.7021814345083659
pearson: 0.8417435151739869

=== Experiment 1147 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007536909596226348
rmse: 0.08681537649648446
mae: 0.04786053786828359
r2: 0.6601640635537505
pearson: 0.8309067930568322

=== Experiment 1082 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006966974586911879
rmse: 0.08346840472245698
mae: 0.039947303700351405
r2: 0.6858621822761591
pearson: 0.8361969089327322

=== Experiment 1091 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006338182766720573
rmse: 0.07961270480721386
mae: 0.03288919847396332
r2: 0.7142141286961405
pearson: 0.8451123424834999

=== Experiment 1150 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007200115129293121
rmse: 0.08485349214553943
mae: 0.03148321778241428
r2: 0.6753499778331888
pearson: 0.8277279283876676

=== Experiment 1134 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006189767055163395
rmse: 0.07867507264161498
mae: 0.03393970213547482
r2: 0.7209061278075035
pearson: 0.851568031358887

=== Experiment 1120 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006955932854740265
rmse: 0.08340223531021375
mae: 0.03933722820903136
r2: 0.6863600491199392
pearson: 0.8339893548434699

=== Experiment 1174 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007255963073518516
rmse: 0.08518194100581716
mae: 0.03680454567683841
r2: 0.6728318186086257
pearson: 0.8215382937230836

=== Experiment 1090 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006981254000659402
rmse: 0.08355389877593626
mae: 0.03665493684364977
r2: 0.685218329795134
pearson: 0.8284889586706912

=== Experiment 1052 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006757703288557455
rmse: 0.0822052509792255
mae: 0.03693839390054589
r2: 0.6952981330116179
pearson: 0.8341066581894896

=== Experiment 1127 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005844119135816199
rmse: 0.07644683862538855
mae: 0.029624236187957637
r2: 0.7364912403595854
pearson: 0.8584721169897322

=== Experiment 1126 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006330121704734963
rmse: 0.07956206196884896
mae: 0.036561308448437684
r2: 0.7145775984331271
pearson: 0.8461100750883531

=== Experiment 1159 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007567471686582362
rmse: 0.08699121614612801
mae: 0.035206440036454685
r2: 0.6587860323510026
pearson: 0.813612607326287

=== Experiment 1158 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006561683881981248
rmse: 0.08100422138371091
mae: 0.038205878062901814
r2: 0.7041365617794004
pearson: 0.8401464844906135

=== Experiment 1027 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006411486954537163
rmse: 0.08007176128034879
mae: 0.029922457032688515
r2: 0.7109088751311309
pearson: 0.8462131925282703

=== Experiment 1157 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0071150396428719025
rmse: 0.08435069438286742
mae: 0.042358685281058274
r2: 0.6791859940713363
pearson: 0.8362638635138835

=== Experiment 1148 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006867402554000194
rmse: 0.08286979277155333
mae: 0.036749096240525914
r2: 0.6903518414151437
pearson: 0.843390178186227

=== Experiment 1167 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070384933213315124
rmse: 0.08389572886226994
mae: 0.0333924927061233
r2: 0.68263743401898
pearson: 0.842441170450479

=== Experiment 1073 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008994120572044092
rmse: 0.09483733743649751
mae: 0.04390590971809803
r2: 0.594459062021727
pearson: 0.7760810620816656

=== Experiment 1165 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00846846402465954
rmse: 0.09202425780553485
mae: 0.04539822251280856
r2: 0.6181606843841569
pearson: 0.7975309873405394

=== Experiment 1179 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007526625260866449
rmse: 0.0867561252066184
mae: 0.046374000314549274
r2: 0.6606277797086462
pearson: 0.8329852113012112

=== Experiment 1162 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008792868341239355
rmse: 0.09377029562307754
mae: 0.03499456209206411
r2: 0.6035334365308318
pearson: 0.8039396736664252

=== Experiment 1249 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006819392794360986
rmse: 0.08257961488382581
mae: 0.033763853890646296
r2: 0.6925165803465646
pearson: 0.8333441455936582

=== Experiment 1187 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006699685579631629
rmse: 0.08185160706810606
mae: 0.038754264134936635
r2: 0.6979141259715369
pearson: 0.8365998530044667

=== Experiment 1046 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005955376563411961
rmse: 0.07717108631742825
mae: 0.029922426393780125
r2: 0.7314746919174311
pearson: 0.8553316145617622

=== Experiment 1197 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007175671500728091
rmse: 0.08470933538122047
mae: 0.034013803506346245
r2: 0.6764521302867224
pearson: 0.8235314013676516

=== Experiment 1208 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006459550600468581
rmse: 0.08037132946809192
mae: 0.03655833928772739
r2: 0.7087417064905117
pearson: 0.8424399276173201

=== Experiment 1144 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006124995097694139
rmse: 0.07826234789280308
mae: 0.03063665095712112
r2: 0.7238266668614737
pearson: 0.8527797674491221

=== Experiment 1242 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007460908302865592
rmse: 0.08637654949617744
mae: 0.043742216491434616
r2: 0.6635909284206598
pearson: 0.8191372339804859

=== Experiment 1202 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007853060099196296
rmse: 0.08861749318952944
mae: 0.04584339700895749
r2: 0.6459089765233132
pearson: 0.8256178910910001

=== Experiment 1258 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0066488267249248285
rmse: 0.08154033802312098
mae: 0.035943772158098866
r2: 0.7002073293455726
pearson: 0.8374132065345881

=== Experiment 1257 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007506511018448392
rmse: 0.08664012360591594
mae: 0.03904071004168507
r2: 0.6615347220463215
pearson: 0.8161280234918483

=== Experiment 1149 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007364925233177337
rmse: 0.0858191425800639
mae: 0.04694270237862586
r2: 0.6679187627875236
pearson: 0.8244872562640244

=== Experiment 1233 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006791060857981404
rmse: 0.08240789317766474
mae: 0.03636383233738229
r2: 0.6937940548880812
pearson: 0.8362813058065003

=== Experiment 1085 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0066257136431325985
rmse: 0.08139848673736262
mae: 0.035800532855149406
r2: 0.701249488030739
pearson: 0.8464833531789766

=== Experiment 1210 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006799439369657601
rmse: 0.08245871312152284
mae: 0.03624774653257392
r2: 0.6934162714842662
pearson: 0.837055513996433

=== Experiment 1185 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0074114347136581315
rmse: 0.08608968993821578
mae: 0.040846782272229806
r2: 0.6658216707830323
pearson: 0.8170610513917261

=== Experiment 1152 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0069197217636811845
rmse: 0.0831848649916629
mae: 0.038401003454586365
r2: 0.6879927912780731
pearson: 0.830152342062068

=== Experiment 1059 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005913564433723294
rmse: 0.07689970372974979
mae: 0.030894054105052495
r2: 0.7333599824421675
pearson: 0.8575094113778196

=== Experiment 1226 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006647424220915408
rmse: 0.08153173750703102
mae: 0.04006926019239323
r2: 0.7002705676340681
pearson: 0.8387097228387029

=== Experiment 1241 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007906090748043314
rmse: 0.08891620070630163
mae: 0.03640755825965482
r2: 0.6435178479073748
pearson: 0.8301893902172355

=== Experiment 1099 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005909849417951032
rmse: 0.07687554499287164
mae: 0.03014979308061828
r2: 0.7335274908682353
pearson: 0.8564673156888456

=== Experiment 1214 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006560049904713581
rmse: 0.08099413500194678
mae: 0.03702649104119408
r2: 0.7042102370952312
pearson: 0.849039339317438

=== Experiment 1066 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006192659404369081
rmse: 0.07869345210606205
mae: 0.03579884702217553
r2: 0.7207757130548396
pearson: 0.8498103305950185

=== Experiment 1230 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0069218753097499836
rmse: 0.083197808322996
mae: 0.033035925161692244
r2: 0.6878956888336791
pearson: 0.8342890712595847

=== Experiment 1286 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006790798037417411
rmse: 0.08240629853001172
mae: 0.03582413899335885
r2: 0.6938059053516353
pearson: 0.8373001760634253

=== Experiment 1287 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007213270708637336
rmse: 0.08493097614320311
mae: 0.03783940860470811
r2: 0.6747567985507714
pearson: 0.8216187007979473

=== Experiment 1168 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006273508356236746
rmse: 0.07920548185723478
mae: 0.034873349623016935
r2: 0.7171302693994714
pearson: 0.8470248814883025

=== Experiment 1041 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006100839610380681
rmse: 0.07810787162879732
mae: 0.03315471691363461
r2: 0.7249158271527945
pearson: 0.8533567978284227

=== Experiment 1100 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007990749112664192
rmse: 0.08939099010898242
mae: 0.042568547445646615
r2: 0.6397006395076151
pearson: 0.8001667288799911

=== Experiment 1201 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007357524475961139
rmse: 0.08577601340678605
mae: 0.03484104614791972
r2: 0.6682524596730797
pearson: 0.8307531691511784

=== Experiment 1050 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006578153657252771
rmse: 0.08110581765356152
mae: 0.03734457502582492
r2: 0.7033939468613097
pearson: 0.8387169722488004

=== Experiment 1276 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006590242713403758
rmse: 0.0811803098873351
mae: 0.035364417752040925
r2: 0.7028488566402624
pearson: 0.8414894500023891

=== Experiment 1199 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0068367833572092005
rmse: 0.08268484357613069
mae: 0.03895454263255272
r2: 0.6917324475219107
pearson: 0.8414361635742688

=== Experiment 1169 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006817446162329482
rmse: 0.08256782764690786
mae: 0.035603867155726346
r2: 0.6926043531280868
pearson: 0.8328682539628925

=== Experiment 1096 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00582650140016039
rmse: 0.07633152297812738
mae: 0.032054200041473836
r2: 0.7372856163061474
pearson: 0.861384716664286

=== Experiment 1218 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008506180605711396
rmse: 0.09222895752263166
mae: 0.053669763596946
r2: 0.6164600603448656
pearson: 0.8079959447867068

=== Experiment 1272 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006809038635309141
rmse: 0.08251689908927226
mae: 0.033271332192365125
r2: 0.6929834448209984
pearson: 0.8326027842659235

=== Experiment 1195 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006823530213975827
rmse: 0.08260466218062892
mae: 0.04343957048994499
r2: 0.692330025917149
pearson: 0.8394579930788642

=== Experiment 1053 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006230730388286109
rmse: 0.07893497569700081
mae: 0.03247702351162519
r2: 0.7190591091463421
pearson: 0.8500061383391879

=== Experiment 1289 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00843369750944146
rmse: 0.09183516488492553
mae: 0.039923498157682716
r2: 0.6197282912534267
pearson: 0.8113435743280195

=== Experiment 1263 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008689096461050117
rmse: 0.09321532310221381
mae: 0.05409257895282327
r2: 0.6082124649350671
pearson: 0.7940162793272665

=== Experiment 1268 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006022189133253989
rmse: 0.0776027649845931
mae: 0.0347519583663826
r2: 0.7284621425497149
pearson: 0.855282475967397

=== Experiment 1279 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007146930161823692
rmse: 0.08453951834393009
mae: 0.03426417466268626
r2: 0.677748064607891
pearson: 0.827628509509696

=== Experiment 1143 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0060804309513963406
rmse: 0.07797711812702711
mae: 0.03645079202189268
r2: 0.7258360446038608
pearson: 0.8523115520921679

=== Experiment 1314 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006916315194604599
rmse: 0.0831643865762539
mae: 0.03963634778184056
r2: 0.6881463919783906
pearson: 0.831543894570225

=== Experiment 1320 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00803862337595758
rmse: 0.08965837036193319
mae: 0.036993992258396696
r2: 0.637542010046791
pearson: 0.8283634223797876

=== Experiment 1219 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007214920867169756
rmse: 0.08494069029134244
mae: 0.04106514463347111
r2: 0.6746823936288349
pearson: 0.8271791884557489

=== Experiment 1307 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006826222365087511
rmse: 0.082620955968129
mae: 0.0367527525075762
r2: 0.6922086380084294
pearson: 0.8324709849593255

=== Experiment 1334 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007471653998380387
rmse: 0.08643872973604128
mae: 0.04226758360044109
r2: 0.66310640973944
pearson: 0.8168096673557861

=== Experiment 1295 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0069184669300299305
rmse: 0.08317732221002266
mae: 0.03706944047102311
r2: 0.6880493711751137
pearson: 0.829932446404778

=== Experiment 1304 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007439209251607722
rmse: 0.08625085072976221
mae: 0.041550505784335175
r2: 0.6645693290913941
pearson: 0.8203046243750683

=== Experiment 1106 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005718322671044766
rmse: 0.07561959184658938
mae: 0.029894008401218863
r2: 0.7421633475888709
pearson: 0.8695757857667317

=== Experiment 1111 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008366081793412448
rmse: 0.0914662877426019
mae: 0.042316913162754856
r2: 0.6227770541292222
pearson: 0.7999894053542342

=== Experiment 1043 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006616093019014229
rmse: 0.08133936942842765
mae: 0.036151766118794365
r2: 0.7016832777378759
pearson: 0.8383140717228331

=== Experiment 1303 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00636719914965736
rmse: 0.0797947313402167
mae: 0.03262787544683486
r2: 0.7129057927606075
pearson: 0.8556188395114346

=== Experiment 1301 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006265050436770572
rmse: 0.07915207158862346
mae: 0.032730385211512156
r2: 0.7175116332655862
pearson: 0.8473340274591676

=== Experiment 1352 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007161614601780862
rmse: 0.08462632333843213
mae: 0.036598449698974446
r2: 0.6770859496733386
pearson: 0.831474639897153

=== Experiment 1282 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007119917834049797
rmse: 0.08437960555756228
mae: 0.04599733687313214
r2: 0.6789660385781813
pearson: 0.831857932072921

=== Experiment 1261 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007132288425065393
rmse: 0.0844528769496066
mae: 0.04359805244933975
r2: 0.6784082540740015
pearson: 0.828951930699856

=== Experiment 1330 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007056512750407785
rmse: 0.08400305203031486
mae: 0.04191252251851943
r2: 0.6818249458928882
pearson: 0.8292408947083035

=== Experiment 1125 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.016581401986084498
rmse: 0.12876879274919253
mae: 0.053584717908917986
r2: 0.25235188248057383
pearson: 0.5785215203689933

=== Experiment 1244 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006854335998809234
rmse: 0.08279091736905222
mae: 0.04058965024822402
r2: 0.6909410066376735
pearson: 0.8427674410233588

=== Experiment 1057 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00611254986777451
rmse: 0.0781827977740277
mae: 0.0314899377555836
r2: 0.7243878168009852
pearson: 0.8552796762685164

=== Experiment 1031 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006107883046629698
rmse: 0.07815294649998615
mae: 0.029939397864457467
r2: 0.7245982417123797
pearson: 0.8552539888995567

=== Experiment 1154 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006633109786067026
rmse: 0.08144390576382635
mae: 0.03208054488036409
r2: 0.7009159992011171
pearson: 0.8411492385690943

=== Experiment 1321 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008355213229791849
rmse: 0.09140685548574488
mae: 0.04223943139460825
r2: 0.6232671128792544
pearson: 0.7972926433863902

=== Experiment 1308 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007097647508643083
rmse: 0.08424753710728333
mae: 0.04811304691295621
r2: 0.6799701977488504
pearson: 0.836533982969715

=== Experiment 1260 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007965355915415208
rmse: 0.08924884265588662
mae: 0.03814595046853927
r2: 0.640845607595171
pearson: 0.8149022332232925

=== Experiment 1369 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007017493293951132
rmse: 0.08377047984792216
mae: 0.03326679792685183
r2: 0.6835843160107425
pearson: 0.8318358400544219

=== Experiment 1328 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007318634431789047
rmse: 0.08554901771375897
mae: 0.04931165813807823
r2: 0.6700059946479817
pearson: 0.8301016601162563

=== Experiment 1309 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0067843643837553645
rmse: 0.08236725310313174
mae: 0.03581918588823517
r2: 0.6940959959635894
pearson: 0.8341663901168243

=== Experiment 1205 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006498730051097441
rmse: 0.08061470120950298
mae: 0.03460322232990084
r2: 0.7069751223058512
pearson: 0.8422321365644184

=== Experiment 1265 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072548238830518665
rmse: 0.08517525393594003
mae: 0.036302948698108684
r2: 0.6728831842053711
pearson: 0.8210687085902871

=== Experiment 1324 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006660664650278852
rmse: 0.08161289512251635
mae: 0.04076581181004127
r2: 0.6996735625016293
pearson: 0.8474657943706634

=== Experiment 1037 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0070339211518684564
rmse: 0.08386847531622628
mae: 0.03890260367348273
r2: 0.6828435911277037
pearson: 0.8288782444563102

=== Experiment 1247 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006210711965000218
rmse: 0.07880807043063684
mae: 0.03735361429236001
r2: 0.7199617310415211
pearson: 0.850034427905121

=== Experiment 1070 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006397448378948487
rmse: 0.07998405077856764
mae: 0.034117489527133005
r2: 0.7115418683255748
pearson: 0.8454032489929647

=== Experiment 1227 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006766588004411775
rmse: 0.08225927306031688
mae: 0.035654646660170576
r2: 0.6948975250842091
pearson: 0.8494484633644274

=== Experiment 1215 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008139200789154758
rmse: 0.09021751930282032
mae: 0.039170661120640674
r2: 0.6330070187532351
pearson: 0.8047897273171167

=== Experiment 1339 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0082044844247223
rmse: 0.09057860909023885
mae: 0.03473753811506994
r2: 0.63006340835902
pearson: 0.8274161912081325

=== Experiment 1353 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0071434756815047405
rmse: 0.08451908471762304
mae: 0.03968578478531564
r2: 0.6779038256050396
pearson: 0.846099649485207

=== Experiment 1189 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065506170314757065
rmse: 0.08093588222460855
mae: 0.03636504125539621
r2: 0.7046355611978017
pearson: 0.8401410071834271

=== Experiment 1326 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007799960472620352
rmse: 0.08831738488327398
mae: 0.04083270676410779
r2: 0.6483032153146897
pearson: 0.8114086740367416

=== Experiment 1293 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011407465458898926
rmse: 0.10680573701304123
mae: 0.045380710840924564
r2: 0.48564240326775754
pearson: 0.7057079996594096

=== Experiment 1355 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00702295925971239
rmse: 0.08380309815103729
mae: 0.04287959808523897
r2: 0.6833378580203229
pearson: 0.8406391326501991

=== Experiment 1306 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006928837073980129
rmse: 0.08323963643589591
mae: 0.04130329490338928
r2: 0.6875817859486487
pearson: 0.8355553234773829

=== Experiment 1023 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007074647790057609
rmse: 0.08411092550945809
mae: 0.03569659996032018
r2: 0.681007244936919
pearson: 0.8300730031343911

=== Experiment 1178 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006255694192581133
rmse: 0.07909294654127594
mae: 0.037002174190570405
r2: 0.7179335021980899
pearson: 0.8484592425734987

=== Experiment 1273 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005980067992476714
rmse: 0.07733089933834156
mae: 0.031900367123079997
r2: 0.7303613662484304
pearson: 0.8566796045917235

=== Experiment 1193 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006013606775197319
rmse: 0.07754744854085993
mae: 0.03146847021314787
r2: 0.7288491172971054
pearson: 0.8554414608414067

=== Experiment 1373 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007242372074964848
rmse: 0.08510212732338039
mae: 0.04152075196957095
r2: 0.6734446307515547
pearson: 0.823037278962321

=== Experiment 1342 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0069341543142837974
rmse: 0.08327156966386426
mae: 0.039261607979348676
r2: 0.6873420339236527
pearson: 0.8314730555027543

=== Experiment 1113 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006603878853589479
rmse: 0.08126425323344502
mae: 0.03067821594497453
r2: 0.7022340090810124
pearson: 0.8425026154962737

=== Experiment 1164 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006064482550550116
rmse: 0.07787478764369195
mae: 0.0363568166445168
r2: 0.7265551509785889
pearson: 0.8547017758032722

=== Experiment 1264 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006920176706068666
rmse: 0.08318759947293025
mae: 0.03664739304364733
r2: 0.6879722781260571
pearson: 0.8362156510435738

=== Experiment 1333 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006580910564258771
rmse: 0.0811228116146055
mae: 0.042647107696156546
r2: 0.7032696391378168
pearson: 0.8447919086359795

=== Experiment 1224 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006252985572351075
rmse: 0.07907582166725222
mae: 0.03252847377829508
r2: 0.7180556326921085
pearson: 0.8593599519642914

=== Experiment 1029 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010569904570047849
rmse: 0.10281004119271546
mae: 0.05270408076693334
r2: 0.5234076551073142
pearson: 0.724813331195442

=== Experiment 1376 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007016108325652274
rmse: 0.08376221299400032
mae: 0.043965933575278796
r2: 0.6836467636217668
pearson: 0.8332199994112065

=== Experiment 1245 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006136085433351891
rmse: 0.0783331694325711
mae: 0.0319978546828419
r2: 0.7233266085078958
pearson: 0.8540783317021536

=== Experiment 1048 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008840246723664229
rmse: 0.094022586242159
mae: 0.04055891858990023
r2: 0.601397166120115
pearson: 0.7763859648534209

=== Experiment 1375 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006576476881460113
rmse: 0.08109548003101105
mae: 0.03675544795519834
r2: 0.7034695519437955
pearson: 0.8499370859278865

=== Experiment 1198 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006639405183306428
rmse: 0.08148254526772239
mae: 0.029886722469923976
r2: 0.7006321425104087
pearson: 0.8413935475416158

=== Experiment 1275 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006260006600720933
rmse: 0.07912020349266635
mae: 0.031854405431237826
r2: 0.7177390576131024
pearson: 0.8549744189259857

=== Experiment 1417 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006477846336228377
rmse: 0.08048506902667338
mae: 0.03447149216517912
r2: 0.7079167598176715
pearson: 0.8422271193167972

=== Experiment 1188 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006716391909375408
rmse: 0.08195359607348178
mae: 0.0418038126057512
r2: 0.6971608449164078
pearson: 0.838395377237918

=== Experiment 1311 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007577871762996959
rmse: 0.08705097221167009
mae: 0.04097794686381801
r2: 0.658317097482892
pearson: 0.8155549607357913

=== Experiment 1137 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00673674562301325
rmse: 0.08207768041930309
mae: 0.03227968200666021
r2: 0.6962431049268321
pearson: 0.8344866710690655

=== Experiment 1394 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00649048737771148
rmse: 0.08056356110371164
mae: 0.03337767684850705
r2: 0.7073467808209464
pearson: 0.8415754269032544

=== Experiment 1180 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006229514707250058
rmse: 0.078927274799337
mae: 0.03377371450479413
r2: 0.7191139236691955
pearson: 0.8483710201531951

=== Experiment 1235 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007057535369784524
rmse: 0.08400913860875212
mae: 0.03340274363850731
r2: 0.6817788364352801
pearson: 0.8262618453341296

=== Experiment 1449 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007199883897642258
rmse: 0.08485212959992376
mae: 0.03976083516148687
r2: 0.6753604039665534
pearson: 0.8240290127293295

=== Experiment 1470 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008779267336347468
rmse: 0.09369774456382324
mae: 0.04214177767010849
r2: 0.604146699855147
pearson: 0.8013481148068242

=== Experiment 1416 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00784378485226884
rmse: 0.08856514468044886
mae: 0.042447302625717144
r2: 0.6463271933249237
pearson: 0.8110607974076158

=== Experiment 1332 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006986180592200504
rmse: 0.0835833750945755
mae: 0.03584112924771011
r2: 0.6849961919509043
pearson: 0.8278079124079911

=== Experiment 1012 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006073194937649489
rmse: 0.07793070599994259
mae: 0.028673561394501612
r2: 0.726162313278893
pearson: 0.8527911551727995

=== Experiment 1475 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007239114541631318
rmse: 0.08508298620541782
mae: 0.043627526113995146
r2: 0.6735915114958135
pearson: 0.8229754949282952

=== Experiment 1237 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006316198401099695
rmse: 0.07947451416082828
mae: 0.0315034313985072
r2: 0.7152053940659266
pearson: 0.8464508039245292

=== Experiment 1371 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007319710683698802
rmse: 0.08555530774708721
mae: 0.04612204068084353
r2: 0.6699574669230657
pearson: 0.827137188738116

=== Experiment 1331 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00790119941381983
rmse: 0.08888869114696105
mae: 0.04782390150607514
r2: 0.6437383960146708
pearson: 0.8160266511160915

=== Experiment 1430 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007850179852838472
rmse: 0.08860124069581911
mae: 0.038994289902695704
r2: 0.6460388455638941
pearson: 0.8123399544207168

=== Experiment 1400 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007381400324167884
rmse: 0.08591507623326586
mae: 0.034630614760026886
r2: 0.6671759081860018
pearson: 0.8424990765856376

=== Experiment 1271 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00651603751946516
rmse: 0.08072197668209792
mae: 0.030827327181093763
r2: 0.7061947361747503
pearson: 0.8461557441986519

=== Experiment 1033 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006185400253927769
rmse: 0.07864731561806651
mae: 0.038694032869521555
r2: 0.7211030249532419
pearson: 0.8523267621804327

=== Experiment 1238 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006504481704299305
rmse: 0.08065036704379779
mae: 0.03160998718271986
r2: 0.7067157827944754
pearson: 0.8414214793256262

=== Experiment 1457 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006921029093194314
rmse: 0.08319272259755846
mae: 0.03823681226329324
r2: 0.6879338443657258
pearson: 0.8312993578537393

=== Experiment 1072 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0060714513418725505
rmse: 0.07791951836268336
mae: 0.029497734800608095
r2: 0.72624093124506
pearson: 0.858100018274883

=== Experiment 1459 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007845474391079752
rmse: 0.08857468256268126
mae: 0.039432753140463836
r2: 0.6462510127635629
pearson: 0.8240907910852019

=== Experiment 1323 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00558439886657479
rmse: 0.07472883557620037
mae: 0.030316226441066622
r2: 0.7482019129880481
pearson: 0.8650167913743088

=== Experiment 1403 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0067218738940327425
rmse: 0.08198703491426398
mae: 0.034783734210441605
r2: 0.6969136646410146
pearson: 0.8406686044396201

=== Experiment 1045 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006163135723077706
rmse: 0.07850564134556004
mae: 0.038069562516714416
r2: 0.7221069228499016
pearson: 0.8524082048050245

=== Experiment 1240 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006354565438838009
rmse: 0.07971552821651506
mae: 0.03457718499835836
r2: 0.7134754412209308
pearson: 0.8508095572124836

=== Experiment 1382 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007611636784485779
rmse: 0.08724469487874767
mae: 0.04543343938471826
r2: 0.6567946475250837
pearson: 0.8286213967878588

=== Experiment 1422 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007065148432314101
rmse: 0.08405443731483841
mae: 0.0407443372540335
r2: 0.6814355667965867
pearson: 0.8300518689432667

=== Experiment 1225 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006749435969856698
rmse: 0.08215495097592536
mae: 0.03719566316870878
r2: 0.6956709027731098
pearson: 0.8459511831518972

=== Experiment 1001 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0055856747325595925
rmse: 0.07473737172632974
mae: 0.03168148827947573
r2: 0.748144384752345
pearson: 0.87015440117159

=== Experiment 1074 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010682680231335812
rmse: 0.103357052160633
mae: 0.04288397424000013
r2: 0.5183226501762042
pearson: 0.7203266694360477

=== Experiment 1493 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007309074201516922
rmse: 0.08549312370896808
mae: 0.03800987923290834
r2: 0.6704370612231728
pearson: 0.8220412487928251

=== Experiment 1501 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006953306393830697
rmse: 0.08338648807709015
mae: 0.03264003249287831
r2: 0.6864784750863577
pearson: 0.8421704872722264

=== Experiment 1364 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006743444748615141
rmse: 0.08211847994583887
mae: 0.03968187422818952
r2: 0.6959410443019536
pearson: 0.8362481797761253

=== Experiment 1450 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008953005563507675
rmse: 0.09462032320547037
mae: 0.03739612122106721
r2: 0.5963129196605348
pearson: 0.7890034540658232

=== Experiment 1478 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007196300224787686
rmse: 0.08483100980648342
mae: 0.03229362896697307
r2: 0.675521990198271
pearson: 0.8225876529459986

=== Experiment 1370 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006806608969884047
rmse: 0.08250217554636997
mae: 0.043544132423659655
r2: 0.6930929973656383
pearson: 0.8384074858993276

=== Experiment 1402 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0069748686638810954
rmse: 0.08351567914997217
mae: 0.04104262872577604
r2: 0.6855062418200661
pearson: 0.8309975400029592

=== Experiment 1255 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006111367400182904
rmse: 0.07817523521028193
mae: 0.029076903858725505
r2: 0.7244411337442471
pearson: 0.8517563899251207

=== Experiment 1112 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.015323326204512893
rmse: 0.12378742345049797
mae: 0.05559988636937068
r2: 0.3090779657501409
pearson: 0.5562193143258655

=== Experiment 1004 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.019954159242399724
rmse: 0.14125919170942372
mae: 0.06294442045697818
r2: 0.10027574225731217
pearson: 0.3196279709214851

=== Experiment 1184 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005891640001503866
rmse: 0.07675701923279633
mae: 0.036334526174514666
r2: 0.7343485454412608
pearson: 0.8611488970859499

=== Experiment 1365 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006153338031586555
rmse: 0.07844321533177076
mae: 0.03864885899946539
r2: 0.7225486964469439
pearson: 0.8557287325120478

=== Experiment 1393 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006923450666665338
rmse: 0.08320727532292677
mae: 0.03720160206341342
r2: 0.6878246566837334
pearson: 0.8309203077113416

=== Experiment 1114 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007639531567806098
rmse: 0.08740441389201176
mae: 0.03714481692380684
r2: 0.6555368840226035
pearson: 0.8243104861551134

=== Experiment 1431 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007370746876189814
rmse: 0.08585305397124679
mae: 0.03552398651301812
r2: 0.667656267466392
pearson: 0.8367087341173811

=== Experiment 1093 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006873831618628904
rmse: 0.08290857385475223
mae: 0.042924960848445416
r2: 0.690061957720683
pearson: 0.8403257248879227

=== Experiment 1213 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00701826189701629
rmse: 0.08377506727551037
mae: 0.042046441974265256
r2: 0.6835496600368226
pearson: 0.8316805258530013

=== Experiment 1315 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00616736099789832
rmse: 0.0785325473794039
mae: 0.03903342547663303
r2: 0.7219164070679259
pearson: 0.8538386369034895

=== Experiment 1278 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006037976625770868
rmse: 0.07770441831563292
mae: 0.027335359811473336
r2: 0.7277502914607357
pearson: 0.866571334808868

=== Experiment 1221 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006315052943141864
rmse: 0.0794673073857537
mae: 0.03616359324022353
r2: 0.7152570422610908
pearson: 0.8465460824443723

=== Experiment 1517 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0071179287032806775
rmse: 0.0843678179359919
mae: 0.03399960928450831
r2: 0.6790557276090776
pearson: 0.8331103633247864

=== Experiment 1414 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007376626637164068
rmse: 0.0858872903121531
mae: 0.035473253256879486
r2: 0.6673911516319436
pearson: 0.8200896694561514

=== Experiment 1095 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005866505227324064
rmse: 0.07659311475141917
mae: 0.030892105688499445
r2: 0.7354818613463681
pearson: 0.8577563261399914

=== Experiment 1267 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006623075245053636
rmse: 0.08138227844594692
mae: 0.04118130579163917
r2: 0.7013684522388715
pearson: 0.8462099719707827

=== Experiment 1102 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009969943452567966
rmse: 0.09984960416830888
mae: 0.04633650706554406
r2: 0.5504596378314
pearson: 0.7425174390124152

=== Experiment 1524 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006584789189429037
rmse: 0.08114671397801046
mae: 0.035620719047708714
r2: 0.7030947536360032
pearson: 0.8409698575454995

=== Experiment 1064 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005043830699324295
rmse: 0.07101993170458766
mae: 0.02740494818980941
r2: 0.7725758937271276
pearson: 0.8791996792851015

=== Experiment 1356 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006641012536714
rmse: 0.08149240784707493
mae: 0.03449677702297694
r2: 0.7005596676527117
pearson: 0.8385713324197753

=== Experiment 1456 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008249159178792626
rmse: 0.0908248819365741
mae: 0.03854039358134088
r2: 0.6280490433608519
pearson: 0.7953088748340895

=== Experiment 1182 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006391516045138279
rmse: 0.07994695769782788
mae: 0.03483428094116233
r2: 0.7118093546460572
pearson: 0.8489574472044132

=== Experiment 1561 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007408176764084618
rmse: 0.08607076602473467
mae: 0.037744847605477506
r2: 0.6659685702953687
pearson: 0.8174484405752597

=== Experiment 1562 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007562997117752876
rmse: 0.08696549383377798
mae: 0.039839595909475806
r2: 0.6589877886901159
pearson: 0.81613200564416

=== Experiment 1317 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006690242050709912
rmse: 0.08179389983800694
mae: 0.037204371813956
r2: 0.6983399305342022
pearson: 0.8447789905281001

=== Experiment 1217 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063349299656257916
rmse: 0.07959227327841435
mae: 0.03120938146027572
r2: 0.7143607960658371
pearson: 0.8472853430326001

=== Experiment 1438 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00631593359363025
rmse: 0.07947284815350618
mae: 0.0343601630691404
r2: 0.7152173341181733
pearson: 0.848168855094225

=== Experiment 1410 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006278841068527164
rmse: 0.07923913848930442
mae: 0.03684043965296526
r2: 0.7168898197494027
pearson: 0.8481095211767681

=== Experiment 1531 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007185987245892848
rmse: 0.08477020258258705
mae: 0.039614112894938124
r2: 0.6759869978775501
pearson: 0.8247359348062876

=== Experiment 1236 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006268814112964543
rmse: 0.07917584298865749
mae: 0.03590142174464493
r2: 0.7173419307625205
pearson: 0.8470925841655015

=== Experiment 1177 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00983095913806383
rmse: 0.09915119332647404
mae: 0.037693050808239384
r2: 0.5567263794007168
pearson: 0.7553506934436048

=== Experiment 1138 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005497740415539794
rmse: 0.0741467491906408
mae: 0.02713189023545517
r2: 0.752109304403915
pearson: 0.8684587087665208

=== Experiment 1398 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006476241221385627
rmse: 0.08047509690199588
mae: 0.03745297322766393
r2: 0.7079891337394661
pearson: 0.8436457216779404

=== Experiment 1480 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007506641239807694
rmse: 0.0866408751098908
mae: 0.037450537226673596
r2: 0.6615288504225452
pearson: 0.819447103662847

=== Experiment 1506 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006528999507791667
rmse: 0.08080222464630332
mae: 0.03346629697156968
r2: 0.7056102858261153
pearson: 0.8402629120182062

=== Experiment 1499 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00774377164325994
rmse: 0.08799870250895714
mae: 0.05101297473849509
r2: 0.6508367448999556
pearson: 0.8225073940351838

=== Experiment 1534 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007708437368148515
rmse: 0.08779770707796711
mae: 0.03914561543841524
r2: 0.652429951812926
pearson: 0.808184665447781

=== Experiment 1436 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006840243358656255
rmse: 0.08270576375716662
mae: 0.04120424099148666
r2: 0.691576437579526
pearson: 0.8352956405907609

=== Experiment 1071 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006500474378883355
rmse: 0.08062551940225474
mae: 0.03201009308483265
r2: 0.7068964713337218
pearson: 0.8408919115785232

=== Experiment 1252 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007348738757525416
rmse: 0.08572478496634107
mae: 0.043240679881135426
r2: 0.668648603850456
pearson: 0.8206639581615925

=== Experiment 1250 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00528529576664879
rmse: 0.07270003966057233
mae: 0.029851461294147584
r2: 0.7616883401183697
pearson: 0.8742753408226448

=== Experiment 1525 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007704428543281465
rmse: 0.08777487421398829
mae: 0.04432283505492694
r2: 0.6526107079617625
pearson: 0.8116217541549463

=== Experiment 1504 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0074104574676872766
rmse: 0.08608401400775452
mae: 0.041290895359068554
r2: 0.6658657343737393
pearson: 0.8205713974820912

=== Experiment 1588 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0077393352281254785
rmse: 0.08797349162176911
mae: 0.038641304880648125
r2: 0.6510367809057529
pearson: 0.8088675581254896

=== Experiment 1297 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006923353966191118
rmse: 0.08320669423905216
mae: 0.042335423514587536
r2: 0.6878290168655548
pearson: 0.8377288932718346

=== Experiment 1175 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006109358539622396
rmse: 0.07816238570835972
mae: 0.030257126369685982
r2: 0.724531712382753
pearson: 0.8520607866571925

=== Experiment 1467 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006395606063836872
rmse: 0.079972533183818
mae: 0.03074168177590868
r2: 0.7116249375030925
pearson: 0.8539950501895435

=== Experiment 1482 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006760578162475496
rmse: 0.08222273507051135
mae: 0.03647555449973868
r2: 0.6951685062119821
pearson: 0.8364532736181316

=== Experiment 1585 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0075800060580519715
rmse: 0.08706323022982763
mae: 0.03666244742060614
r2: 0.6582208630582367
pearson: 0.8151384588423378

=== Experiment 1541 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005731332399029144
rmse: 0.0757055638578113
mae: 0.030242266377503456
r2: 0.7415767446800745
pearson: 0.8623665535413672

=== Experiment 1118 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005443913753170213
rmse: 0.07378288252142372
mae: 0.02868923704355155
r2: 0.7545363249192334
pearson: 0.8687731923458877

=== Experiment 1483 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006737527375932923
rmse: 0.0820824425558409
mae: 0.0368356963735169
r2: 0.6962078560317597
pearson: 0.8364368711743978

=== Experiment 1526 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006725736044681223
rmse: 0.08201058495512163
mae: 0.030750870558271682
r2: 0.6967395219681372
pearson: 0.8362821333537542

=== Experiment 1008 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005859636580724365
rmse: 0.07654826308104165
mae: 0.02603934275494884
r2: 0.735791565598426
pearson: 0.8586632867296496

=== Experiment 1274 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00742176801855073
rmse: 0.08614968379832122
mae: 0.04443825362395443
r2: 0.6653557466134603
pearson: 0.8311750547997758

=== Experiment 1540 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006041379118002855
rmse: 0.07772630904656966
mae: 0.03332926419778029
r2: 0.7275968745835466
pearson: 0.853163752586805

=== Experiment 1340 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006645053912505709
rmse: 0.08151720010222203
mae: 0.033735445639068634
r2: 0.700377443797009
pearson: 0.8383767620950684

=== Experiment 1098 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005842068239396929
rmse: 0.07643342357501022
mae: 0.03370319443449149
r2: 0.7365837143764616
pearson: 0.8607493579487342

=== Experiment 1609 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007327448371396314
rmse: 0.08560051618650623
mae: 0.03499007456539722
r2: 0.6696085779904015
pearson: 0.8200030208586165

=== Experiment 1338 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006625063073457828
rmse: 0.08139449043674779
mae: 0.03308764759122804
r2: 0.7012788219310369
pearson: 0.8374841654159015

=== Experiment 1545 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064768681617755055
rmse: 0.08047899205243257
mae: 0.036706983940603156
r2: 0.7079608652732239
pearson: 0.8421625135605355

=== Experiment 1361 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006405581942831061
rmse: 0.08003487953905511
mae: 0.03629042090782435
r2: 0.7111751295099646
pearson: 0.8486972742351395

=== Experiment 1277 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007325795874958452
rmse: 0.08559086326798236
mae: 0.03363711010509809
r2: 0.6696830883274656
pearson: 0.8237308917572993

=== Experiment 1469 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006678715255353307
rmse: 0.08172340702242722
mae: 0.03991203610629346
r2: 0.6988596686635008
pearson: 0.8397436327061423

=== Experiment 1539 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00674868350442684
rmse: 0.08215037129816785
mae: 0.03771143329796169
r2: 0.6957048311081566
pearson: 0.8348804555482988

=== Experiment 1312 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006305091707426332
rmse: 0.07940460759569518
mae: 0.0332925697775639
r2: 0.715706189995229
pearson: 0.8533210112781459

=== Experiment 1472 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006603200603230915
rmse: 0.08126008000999578
mae: 0.042004774558530016
r2: 0.7022645910911594
pearson: 0.8429583631455928

=== Experiment 1572 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076880157026044645
rmse: 0.08768133041078052
mae: 0.0418157212861697
r2: 0.6533507557240448
pearson: 0.8291189783778083

=== Experiment 1490 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006207740488704128
rmse: 0.07878921556091371
mae: 0.03715439203894626
r2: 0.7200957135998649
pearson: 0.8510910033010313

=== Experiment 1259 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006306243893250288
rmse: 0.07941186242149398
mae: 0.03593956549239991
r2: 0.7156542384435425
pearson: 0.8461909364848493

=== Experiment 1606 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008479982104030065
rmse: 0.09208681829681198
mae: 0.03821831013204579
r2: 0.6176413392548343
pearson: 0.7915490339202845

=== Experiment 1500 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006249954385509991
rmse: 0.07905665301231764
mae: 0.034605893297080616
r2: 0.7181923075726453
pearson: 0.8500917863194241

=== Experiment 1171 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006141829061461474
rmse: 0.07836982239013607
mae: 0.03446760669970709
r2: 0.7230676308443987
pearson: 0.851267651465383

=== Experiment 1527 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006861534061870955
rmse: 0.08283437729512401
mae: 0.03869192618332846
r2: 0.6906164491423302
pearson: 0.8318628497588111

=== Experiment 1092 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006093970316037382
rmse: 0.07806388611923815
mae: 0.030051386808685045
r2: 0.7252255606113263
pearson: 0.8523043257625705

=== Experiment 1254 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006788071206571135
rmse: 0.08238975182977028
mae: 0.03502678269661801
r2: 0.6939288569543238
pearson: 0.833673265873725

=== Experiment 1069 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006308034602566119
rmse: 0.0794231364437726
mae: 0.033970137745765266
r2: 0.7155734961486431
pearson: 0.8607979416730189

=== Experiment 1620 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007625548442999785
rmse: 0.08732438630187896
mae: 0.04146932779990921
r2: 0.6561673769656762
pearson: 0.8148985719635499

=== Experiment 1544 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005895679280889633
rmse: 0.0767833268417671
mae: 0.03110224681656937
r2: 0.7341664161115795
pearson: 0.8572407389969945

=== Experiment 1460 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006622869232079429
rmse: 0.08138101272458723
mae: 0.03558668584408027
r2: 0.7013777412732054
pearson: 0.8385234117847421

=== Experiment 1283 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006728035981476459
rmse: 0.08202460595136352
mae: 0.03524971577069253
r2: 0.6966358188303196
pearson: 0.8367856148207206

=== Experiment 1508 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0069234302229493555
rmse: 0.08320715247470829
mae: 0.03933350090258756
r2: 0.6878255784818891
pearson: 0.8342566701873838

=== Experiment 1397 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006952069177893521
rmse: 0.08337906918341989
mae: 0.044173315733778
r2: 0.6865342606084246
pearson: 0.8349593820564174

=== Experiment 1608 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007297823697205487
rmse: 0.0854273006550335
mae: 0.03767166137469513
r2: 0.6709443415108498
pearson: 0.8194650387888166

=== Experiment 1515 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006884631911022111
rmse: 0.08297368203847598
mae: 0.03579215353758707
r2: 0.6895749772902455
pearson: 0.8353860550703123

=== Experiment 1192 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006740536951009673
rmse: 0.08210077314501778
mae: 0.039091160673345185
r2: 0.6960721556161644
pearson: 0.8371402779803674

=== Experiment 1310 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00682424408637154
rmse: 0.08260898308520412
mae: 0.03163077474372313
r2: 0.6922978377250255
pearson: 0.8472943554177185

=== Experiment 1564 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007108867810745389
rmse: 0.08431410208704941
mae: 0.04821806324171595
r2: 0.6794642792654899
pearson: 0.8362293301077823

=== Experiment 1589 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006577913549327018
rmse: 0.08110433742610205
mae: 0.03491595308930152
r2: 0.7034047732220023
pearson: 0.8391533793864429

=== Experiment 1191 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005945706746429417
rmse: 0.07710840905134418
mae: 0.031845843855098005
r2: 0.731910699709162
pearson: 0.8570705644516203

=== Experiment 1604 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006779020886712481
rmse: 0.08233480969014552
mae: 0.03835254689846872
r2: 0.6943369318934002
pearson: 0.8385546353809566

=== Experiment 1156 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006892465403668023
rmse: 0.08302087330104413
mae: 0.03879514218730201
r2: 0.6892217685546251
pearson: 0.8405146808621914

=== Experiment 1633 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007336942860579514
rmse: 0.0856559563636967
mae: 0.03974348566437306
r2: 0.6691804756519755
pearson: 0.8193895056465977

=== Experiment 1581 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006334911881310486
rmse: 0.07959215967236023
mae: 0.03870997851034642
r2: 0.7143616114796547
pearson: 0.8476498992382895

=== Experiment 1617 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006919471047952888
rmse: 0.08318335799877814
mae: 0.03418061190215252
r2: 0.6880040959399134
pearson: 0.8336923040475244

=== Experiment 1349 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005645969493279936
rmse: 0.07513966657684831
mae: 0.027382639922657787
r2: 0.7454257205292183
pearson: 0.8676018177948287

=== Experiment 1405 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006669818608773524
rmse: 0.08166895743655311
mae: 0.035933043838135774
r2: 0.6992608145420697
pearson: 0.8366478159928007

=== Experiment 1550 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006705656741904298
rmse: 0.08188807447915904
mae: 0.04065976375502678
r2: 0.6976448888927718
pearson: 0.8386759830290142

=== Experiment 1119 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007333454681516829
rmse: 0.08563559237558195
mae: 0.049104028388743844
r2: 0.6693377561106468
pearson: 0.8374164848322557

=== Experiment 1487 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006773939545285868
rmse: 0.08230394611004911
mae: 0.04278837660742033
r2: 0.6945660473418569
pearson: 0.840499516977915

=== Experiment 1590 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007232077280477975
rmse: 0.08504162087165304
mae: 0.03332094119724998
r2: 0.6739088185038822
pearson: 0.8274851605851565

=== Experiment 1336 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008365619835046821
rmse: 0.09146376241466793
mae: 0.04517207542860256
r2: 0.622797883628605
pearson: 0.8133456320409234

=== Experiment 1409 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006188946123109602
rmse: 0.07866985523762964
mae: 0.03336324337108064
r2: 0.7209431432724893
pearson: 0.8496337935529942

=== Experiment 1172 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006077883687619514
rmse: 0.07796078301055932
mae: 0.03150961423416046
r2: 0.7259508996064212
pearson: 0.8523391964592112

=== Experiment 1439 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0071996079502591485
rmse: 0.08485050353568414
mae: 0.03680109986097003
r2: 0.6753728463126041
pearson: 0.8241722228563731

=== Experiment 1678 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007555459830584262
rmse: 0.08692214810152969
mae: 0.03742588246292645
r2: 0.6593276416511348
pearson: 0.815951065397365

=== Experiment 1576 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006882329793722299
rmse: 0.08295980830331215
mae: 0.04073641542613514
r2: 0.6896787787460548
pearson: 0.8335036234363372

=== Experiment 1516 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006675576140280802
rmse: 0.08170419903701891
mae: 0.03938970686130568
r2: 0.6990012099804899
pearson: 0.8381135875746849

=== Experiment 1648 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006706039288632951
rmse: 0.0818904102360768
mae: 0.034717668631228216
r2: 0.6976276400291483
pearson: 0.8363722165589845

=== Experiment 1632 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007493669370059857
rmse: 0.08656598275338794
mae: 0.04717703432996257
r2: 0.6621137463201188
pearson: 0.8258624777636542

=== Experiment 1705 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006688620382191085
rmse: 0.08178398609869224
mae: 0.037345503987403085
r2: 0.6984130508539651
pearson: 0.8366490940823351

=== Experiment 1600 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0066825507914301625
rmse: 0.08174687022406522
mae: 0.04220969296919413
r2: 0.6986867260299439
pearson: 0.842488795841973

=== Experiment 1444 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00631762895026368
rmse: 0.07948351370104167
mae: 0.036901886096229373
r2: 0.7151408912337553
pearson: 0.8542445703567897

=== Experiment 1674 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007187709537386229
rmse: 0.0847803605641438
mae: 0.0399036041424229
r2: 0.675909340512152
pearson: 0.8235892841536041

=== Experiment 1160 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006310411579939771
rmse: 0.07943809904535588
mae: 0.029871340001880485
r2: 0.7154663192850529
pearson: 0.8470944938497181

=== Experiment 1466 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00695241436537945
rmse: 0.0833811391465687
mae: 0.03301534986538904
r2: 0.6865186962566128
pearson: 0.8400348942503685

=== Experiment 1510 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008929738130839108
rmse: 0.0944972916587513
mae: 0.04449597054047626
r2: 0.5973620379587803
pearson: 0.772972702255441

=== Experiment 1635 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006755993680156682
rmse: 0.08219485190787001
mae: 0.04132466255599383
r2: 0.6953752185019524
pearson: 0.837359623178436

=== Experiment 1354 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007072430849643754
rmse: 0.08409774580595936
mae: 0.035690786570717434
r2: 0.6811072058044296
pearson: 0.8407231769596036

=== Experiment 1629 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007498503169369813
rmse: 0.08659389799154334
mae: 0.03863337039891442
r2: 0.6618957924367506
pearson: 0.8179689958358013

=== Experiment 1512 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005769433859624958
rmse: 0.07595678942415193
mae: 0.028211523983984566
r2: 0.7398587665915445
pearson: 0.860515260686867

=== Experiment 1711 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006453654300114882
rmse: 0.08033463947834012
mae: 0.039055936342279655
r2: 0.7090075680782997
pearson: 0.8453589690165748

=== Experiment 1625 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007170666020350613
rmse: 0.08467978519310623
mae: 0.037467425501027725
r2: 0.6766778251938625
pearson: 0.8252247285642376

=== Experiment 1122 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005886267474717673
rmse: 0.07672201427698358
mae: 0.03517790918433744
r2: 0.7345907903094204
pearson: 0.8595600200956777

=== Experiment 1621 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0062539325670498075
rmse: 0.07908180933090624
mae: 0.034967476812919014
r2: 0.7180129331179469
pearson: 0.8489726541534368

=== Experiment 1056 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006003369776890799
rmse: 0.07748141568718785
mae: 0.033574239322425536
r2: 0.729310699045099
pearson: 0.8558876982440226

=== Experiment 1584 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0070269663142256775
rmse: 0.0838270022977422
mae: 0.041733437933075525
r2: 0.6831571816959583
pearson: 0.837042526736967

=== Experiment 1556 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006574218987653065
rmse: 0.08108155762966733
mae: 0.04093513888106691
r2: 0.7035713593817801
pearson: 0.8461053312222974

=== Experiment 1619 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006092458410647563
rmse: 0.07805420174883325
mae: 0.029866569191763923
r2: 0.7252937317599126
pearson: 0.8520894506843906

=== Experiment 1682 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007486266327671968
rmse: 0.08652321265228174
mae: 0.037598553917859524
r2: 0.6624475462430603
pearson: 0.8309633427631037

=== Experiment 1316 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.011277950871845894
rmse: 0.10619769711178248
mae: 0.04406789639051003
r2: 0.49148215899424963
pearson: 0.7029759985129382

=== Experiment 1676 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007105271238708778
rmse: 0.08429277097538541
mae: 0.04448381419287256
r2: 0.6796264471156483
pearson: 0.8335698025599554

=== Experiment 1281 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0072745224322378725
rmse: 0.08529081094841268
mae: 0.038111071617241206
r2: 0.6719949852925686
pearson: 0.8215199473283749

=== Experiment 1650 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062780386804506694
rmse: 0.07923407524828362
mae: 0.031010404142242393
r2: 0.716925999074614
pearson: 0.8468459857400301

=== Experiment 1296 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007844552775413042
rmse: 0.08856947993193277
mae: 0.041369828834549496
r2: 0.6462925680083407
pearson: 0.8089922403927378

=== Experiment 1327 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006195772110094823
rmse: 0.07871322703392881
mae: 0.038004674387168924
r2: 0.7206353625236723
pearson: 0.85699883003891

=== Experiment 1021 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0064458008331235925
rmse: 0.08028574489362102
mae: 0.03842724781445537
r2: 0.7093616774483618
pearson: 0.8485419825650437

=== Experiment 1223 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005661867221355671
rmse: 0.07524538006652416
mae: 0.027532663706225112
r2: 0.7447088989674087
pearson: 0.8720444759984525

=== Experiment 1618 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007161973738252923
rmse: 0.08462844520758327
mae: 0.035626887558418335
r2: 0.6770697563678814
pearson: 0.8283095320656824

=== Experiment 1488 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005734402710742724
rmse: 0.07572583912207724
mae: 0.031984265013758804
r2: 0.7414383056762567
pearson: 0.8614145842931119

=== Experiment 1644 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006205247444216086
rmse: 0.07877339299672248
mae: 0.03561232852469372
r2: 0.720208123878558
pearson: 0.85019449422954

=== Experiment 1426 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00634202780272571
rmse: 0.07963684952787692
mae: 0.03796825767103409
r2: 0.7140407577149988
pearson: 0.8469623071598578

=== Experiment 1378 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006787257742719897
rmse: 0.08238481500082341
mae: 0.031056417240392643
r2: 0.6939655356813433
pearson: 0.8505503251272326

=== Experiment 1269 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005780408791977975
rmse: 0.07602899967760969
mae: 0.028130836840350577
r2: 0.7393639117221842
pearson: 0.8654074144964626

=== Experiment 1396 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009298366298445404
rmse: 0.09642803688992846
mae: 0.03480479968974702
r2: 0.5807407561270764
pearson: 0.782524995470374

=== Experiment 1637 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064336808744860745
rmse: 0.08021022923845857
mae: 0.03500938492162893
r2: 0.7099081610489257
pearson: 0.8427664067781511

=== Experiment 1404 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062306961548128925
rmse: 0.07893475885066663
mae: 0.030554579097727526
r2: 0.7190606527185821
pearson: 0.8515728573669518

=== Experiment 1346 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007773811153715198
rmse: 0.08816921885621534
mae: 0.03426138638237503
r2: 0.6494822765949276
pearson: 0.8377646974138141

=== Experiment 1132 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006721802863151184
rmse: 0.08198660172949715
mae: 0.0325539357895494
r2: 0.696916867392201
pearson: 0.8484380460829962

=== Experiment 1253 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005911159974511158
rmse: 0.07688406840504187
mae: 0.0329187847325306
r2: 0.7334683984497594
pearson: 0.8564617062459983

=== Experiment 1723 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006694214242439193
rmse: 0.08181817794621922
mae: 0.03550514087292238
r2: 0.6981608261574241
pearson: 0.8358041786336746

=== Experiment 1362 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006902623169215182
rmse: 0.08308202675197074
mae: 0.037101470283522016
r2: 0.6887637593768783
pearson: 0.8314063650916236

=== Experiment 1712 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006858299846501496
rmse: 0.0828148528133782
mae: 0.038645948664045145
r2: 0.6907622784898522
pearson: 0.8328468774385248

=== Experiment 1390 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009367079180099377
rmse: 0.09678367207385437
mae: 0.040482914272194165
r2: 0.5776425225361508
pearson: 0.763407111672053

=== Experiment 1497 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007010212628237846
rmse: 0.0837270125362051
mae: 0.04122603778381538
r2: 0.6839125980232885
pearson: 0.8423666619728131

=== Experiment 1507 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00604209652887323
rmse: 0.07773092389051625
mae: 0.028832758230600802
r2: 0.7275645268431582
pearson: 0.8575976038504318

=== Experiment 1595 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007591241086805319
rmse: 0.08712772857595519
mae: 0.048514588926968166
r2: 0.6577142805566626
pearson: 0.8212755022390252

=== Experiment 1386 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00555977443115199
rmse: 0.07456389495695614
mae: 0.03017616525933449
r2: 0.7493122179432193
pearson: 0.8671874239704584

=== Experiment 1645 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006155891224476534
rmse: 0.07845948779132154
mae: 0.034658162474736964
r2: 0.722433574103281
pearson: 0.8511162803112634

=== Experiment 1654 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007017785663089473
rmse: 0.08377222489041027
mae: 0.04896000783819858
r2: 0.6835711332149763
pearson: 0.8399505765380628

=== Experiment 1655 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006895418188473127
rmse: 0.08303865478482372
mae: 0.0424932770488741
r2: 0.689088628787383
pearson: 0.8349408100209008

=== Experiment 1300 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006451351883233515
rmse: 0.08032030803746656
mae: 0.027933469277049965
r2: 0.7091113830421094
pearson: 0.8553342318845719

=== Experiment 1455 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006680045684520228
rmse: 0.08173154644640114
mae: 0.036663231648457806
r2: 0.6987996801979308
pearson: 0.8369414669844345

=== Experiment 1248 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0074318825004208965
rmse: 0.0862083667657664
mae: 0.0376281603715455
r2: 0.6648996890776584
pearson: 0.8393744848952877

=== Experiment 1624 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007575717004983389
rmse: 0.08703859491618295
mae: 0.0389009681220472
r2: 0.6584142545733424
pearson: 0.8124103722955212

=== Experiment 1731 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007377686827870773
rmse: 0.08589346207873318
mae: 0.03912980513435836
r2: 0.6673433480996039
pearson: 0.8219675285638747

=== Experiment 1718 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006902132134359136
rmse: 0.08307907157858191
mae: 0.03629784012375847
r2: 0.6887858999224166
pearson: 0.8422868815678279

=== Experiment 1549 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009139406451160117
rmse: 0.09560024294508941
mae: 0.04489950921427745
r2: 0.587908185677591
pearson: 0.7723486968572031

=== Experiment 1662 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0068263073871140025
rmse: 0.08262147049716558
mae: 0.041736990603945444
r2: 0.6922048044026761
pearson: 0.8378843815953883

=== Experiment 1298 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005839568538103359
rmse: 0.07641706967754887
mae: 0.02988790590086507
r2: 0.736696424807586
pearson: 0.8632005470869618

=== Experiment 1406 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0063554250077922135
rmse: 0.07972091951170793
mae: 0.029607703596573984
r2: 0.7134366836351109
pearson: 0.8552899300459166

=== Experiment 1155 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006135468644399119
rmse: 0.07832923237463213
mae: 0.04025383032280287
r2: 0.7233544192503067
pearson: 0.8549180424710087

=== Experiment 1724 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009191431523186267
rmse: 0.09587195378830175
mae: 0.04265790904138723
r2: 0.5855623980779172
pearson: 0.821447302120749

=== Experiment 1302 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0061022670355201074
rmse: 0.07811700861861076
mae: 0.03615663519925423
r2: 0.7248514651815157
pearson: 0.8684034088139642

=== Experiment 1715 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007189773579242123
rmse: 0.0847925325676862
mae: 0.04164674954398423
r2: 0.675816273773324
pearson: 0.82526286406695

=== Experiment 1407 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.011721869414721663
rmse: 0.10826758247380266
mae: 0.05392380352497815
r2: 0.4714660672794737
pearson: 0.6934910829136135

=== Experiment 1418 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005556213546686059
rmse: 0.07454001305799496
mae: 0.031528905014945825
r2: 0.7494727766565231
pearson: 0.8671196640463333

=== Experiment 1128 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006724902447710989
rmse: 0.08200550254532307
mae: 0.03272755927282269
r2: 0.696777108488631
pearson: 0.8374532798590276

=== Experiment 1657 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0056888469694979455
rmse: 0.07542444543712566
mae: 0.03368299289397972
r2: 0.7434923939983689
pearson: 0.8633732400390063

=== Experiment 1570 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006379363986045277
rmse: 0.0798709207787495
mae: 0.03395689277932775
r2: 0.7123572856420304
pearson: 0.8445496372155135

=== Experiment 1209 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006597094718598741
rmse: 0.0812225013072039
mae: 0.0346484157578834
r2: 0.7025399027418188
pearson: 0.8416518200950993

=== Experiment 1432 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006436290225827617
rmse: 0.08022649329135367
mae: 0.03186626867706273
r2: 0.7097905065453991
pearson: 0.854556612926785

=== Experiment 1753 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0074320959643543955
rmse: 0.086209604826576
mae: 0.04394912355389292
r2: 0.6648900640828497
pearson: 0.8203184355315981

=== Experiment 1194 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010435527377021477
rmse: 0.1021544290621874
mae: 0.045093080329667135
r2: 0.5294666635969477
pearson: 0.7286911887650694

=== Experiment 1708 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006599674550704135
rmse: 0.08123838101971342
mae: 0.03737268995065382
r2: 0.7024235792476574
pearson: 0.8429729366488958

=== Experiment 1756 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007852973235569065
rmse: 0.08861700308388376
mae: 0.0386151715843379
r2: 0.6459128931660334
pearson: 0.8052719694703461

=== Experiment 1739 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006933966180434471
rmse: 0.08327044001585719
mae: 0.04215645454682035
r2: 0.6873505167961147
pearson: 0.8323382705543013

=== Experiment 1652 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006126251471790029
rmse: 0.0782703741641116
mae: 0.03168219053547841
r2: 0.7237700175064625
pearson: 0.8510157659539711

=== Experiment 1141 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.01141429763537969
rmse: 0.10683771635232424
mae: 0.04832338163749624
r2: 0.48533434343731063
pearson: 0.7434223901718348

=== Experiment 1736 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006852695574405962
rmse: 0.08278100974502522
mae: 0.035834079327762405
r2: 0.6910149726520111
pearson: 0.8320045288900224

=== Experiment 1367 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005648572224956517
rmse: 0.07515698387346659
mae: 0.029863451493827985
r2: 0.7453083645034709
pearson: 0.8649721333608649

=== Experiment 1374 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005931330914717842
rmse: 0.07701513432253326
mae: 0.03223865140859195
r2: 0.7325588996337495
pearson: 0.8589479679648734

=== Experiment 1345 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0064396666125625165
rmse: 0.08024753337369639
mae: 0.029410589422666358
r2: 0.7096382667535837
pearson: 0.8550686465696998

=== Experiment 1725 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007336228123709573
rmse: 0.0856517841244978
mae: 0.03418347697888681
r2: 0.6692127028228612
pearson: 0.8336606288934593

=== Experiment 1471 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006396042562690076
rmse: 0.07997526219206834
mae: 0.03611638224992975
r2: 0.7116052559619194
pearson: 0.8562931474735591

=== Experiment 1553 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007554219728744224
rmse: 0.08691501440340572
mae: 0.04646621707271396
r2: 0.6593835572972915
pearson: 0.8326198657090033

=== Experiment 1754 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007211052342387017
rmse: 0.08491791532054362
mae: 0.04553538000434893
r2: 0.6748568237086208
pearson: 0.8275085900816033

=== Experiment 1380 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009152333335024833
rmse: 0.09566782810864284
mae: 0.04058199714912777
r2: 0.5873253181736876
pearson: 0.7686823251968073

=== Experiment 1807 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007657764193205124
rmse: 0.08750865210483547
mae: 0.04602257786737709
r2: 0.6547147829681537
pearson: 0.8152216203687312

=== Experiment 1075 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005720594819994301
rmse: 0.07563461390127077
mae: 0.027207521131739073
r2: 0.7420608973927856
pearson: 0.8783610459394015

=== Experiment 1578 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006751657297938407
rmse: 0.08216846900081812
mae: 0.0355343065889429
r2: 0.6955707440675868
pearson: 0.8350327405678605

=== Experiment 1551 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006258271519056851
rmse: 0.07910923788696773
mae: 0.035997544213810496
r2: 0.7178172916816681
pearson: 0.85176647805495

=== Experiment 1322 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0054909815123186795
rmse: 0.0741011572940577
mae: 0.029523061493137732
r2: 0.7524140603753341
pearson: 0.8841386489857798

=== Experiment 1782 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00860734541929673
rmse: 0.09277578034862725
mae: 0.042618639746974406
r2: 0.6118985834263425
pearson: 0.7919296495813481

=== Experiment 1348 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005815857852460695
rmse: 0.07626177189431606
mae: 0.031381220193336445
r2: 0.737765529187343
pearson: 0.8589921877493111

=== Experiment 1612 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0056807830059905995
rmse: 0.07537096925203098
mae: 0.0321632483227118
r2: 0.7438559945636932
pearson: 0.8671710326535721

=== Experiment 1593 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007319997832489832
rmse: 0.08555698587777524
mae: 0.03732880514197553
r2: 0.6699445195104619
pearson: 0.8401896947173909

=== Experiment 1781 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006758317698817042
rmse: 0.08220898794424514
mae: 0.03546439599467586
r2: 0.6952704295234358
pearson: 0.8340322686663718

=== Experiment 1379 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0056225889315737075
rmse: 0.0749839244876774
mae: 0.03090650681215936
r2: 0.7464799397659798
pearson: 0.866982763606921

=== Experiment 1790 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00745782827938439
rmse: 0.08635871860666061
mae: 0.04120311268268355
r2: 0.6637298053238052
pearson: 0.8156196277862584

=== Experiment 1808 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007339828981902064
rmse: 0.08567280187960508
mae: 0.03734312814217339
r2: 0.6690503417118208
pearson: 0.8184487219503942

=== Experiment 1146 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.020667268919041734
rmse: 0.14376115232927753
mae: 0.06373190102742128
r2: 0.06812194080109513
pearson: 0.27870687083859735

=== Experiment 1751 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007513616257299641
rmse: 0.08668111822824877
mae: 0.034221108897893725
r2: 0.6612143499537734
pearson: 0.8165219530126582

=== Experiment 1368 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00663998558170211
rmse: 0.08148610667900455
mae: 0.03578188909213963
r2: 0.7006059726022004
pearson: 0.8394139732804758

=== Experiment 1814 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007010894555240577
rmse: 0.08373108476092124
mae: 0.0410866939581597
r2: 0.6838818502348744
pearson: 0.8294681533198787

=== Experiment 1587 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005859830841054029
rmse: 0.07654953194536221
mae: 0.02975451854831834
r2: 0.7357828064856619
pearson: 0.8642957253551128

=== Experiment 1803 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0064549062884836144
rmse: 0.08034243143248537
mae: 0.034438720606927375
r2: 0.7089511164737958
pearson: 0.8433725698553233

=== Experiment 1256 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0060127989367186555
rmse: 0.07754223969372213
mae: 0.03673562411783544
r2: 0.7288855423785515
pearson: 0.8567154986111862

=== Experiment 1116 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0067022939950110725
rmse: 0.08186753932427109
mae: 0.036465791123954115
r2: 0.6977965136701291
pearson: 0.8506058600758376

=== Experiment 1211 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0054730427485112886
rmse: 0.0739800158726077
mae: 0.029481159571234847
r2: 0.7532229113399557
pearson: 0.8690172820124632

=== Experiment 1771 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006910651393427239
rmse: 0.08313032775965243
mae: 0.032337512030940374
r2: 0.6884017702806483
pearson: 0.8324204663039048

=== Experiment 1109 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005881206657998621
rmse: 0.0766890256685963
mae: 0.028331116457171148
r2: 0.7348189803078466
pearson: 0.8577751649507785

=== Experiment 1434 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0075617535016378095
rmse: 0.08695834348489977
mae: 0.039684244453977655
r2: 0.6590438627933868
pearson: 0.8482008361982883

=== Experiment 1770 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008097100621198578
rmse: 0.0899838908983079
mae: 0.03904120963301166
r2: 0.6349052967966782
pearson: 0.8274338683723462

=== Experiment 1760 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00758513527263557
rmse: 0.08709268208429208
mae: 0.04093942404140088
r2: 0.6579895890302023
pearson: 0.81231903887004

=== Experiment 1559 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006359254806392121
rmse: 0.07974493592945023
mae: 0.03169934342315085
r2: 0.7132639997018644
pearson: 0.8493198555436329

=== Experiment 1335 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006850191974913612
rmse: 0.0827658865409754
mae: 0.03562614873763619
r2: 0.6911278588512025
pearson: 0.8318685063570042

=== Experiment 1358 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00618553929320429
rmse: 0.07864819955475326
mae: 0.029261335208132858
r2: 0.7210967557334593
pearson: 0.8507864325589761

=== Experiment 1835 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007437470210091554
rmse: 0.08624076883986805
mae: 0.04205780604290807
r2: 0.6646477417079476
pearson: 0.8178557812396151

=== Experiment 1239 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006298569731865861
rmse: 0.0793635289781513
mae: 0.036813897820171165
r2: 0.7160002630027096
pearson: 0.8507548962569408

=== Experiment 1555 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00702436962922913
rmse: 0.08381151251009093
mae: 0.035478573961196753
r2: 0.6832742650795642
pearson: 0.8424355174944791

=== Experiment 1484 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006977659631598851
rmse: 0.08353238672274875
mae: 0.04095335802614032
r2: 0.6853803983140713
pearson: 0.8338300170471309

=== Experiment 1665 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006824433538686889
rmse: 0.08261012975832255
mae: 0.03723709597671685
r2: 0.6922892954035109
pearson: 0.8358862876458456

=== Experiment 1788 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007177826018114574
rmse: 0.08472205154571373
mae: 0.038162959889178394
r2: 0.6763549840460414
pearson: 0.823736327653596

=== Experiment 1703 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007061837264841529
rmse: 0.08403473844096576
mae: 0.04277466383140741
r2: 0.6815848658805688
pearson: 0.8344776241050963

=== Experiment 1695 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00545401725561873
rmse: 0.07385131857738716
mae: 0.030398255920965473
r2: 0.7540807624407213
pearson: 0.868652604089945

=== Experiment 1523 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006603703204278297
rmse: 0.08126317249700689
mae: 0.0306446300117926
r2: 0.7022419290311444
pearson: 0.8505404828615881

=== Experiment 1592 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00691150481033517
rmse: 0.08313546060698268
mae: 0.036222117196650486
r2: 0.6883632900878871
pearson: 0.8317236652041039

=== Experiment 1761 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006652554884422155
rmse: 0.08156319564866346
mae: 0.03392069114216343
r2: 0.7000392282747268
pearson: 0.8368202567476174

=== Experiment 1262 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006173546466784983
rmse: 0.07857191907281497
mae: 0.03139549520970496
r2: 0.7216375069982592
pearson: 0.8654613328787303

=== Experiment 1791 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006994849322672096
rmse: 0.08363521580454071
mae: 0.046509528429299736
r2: 0.6846053227093405
pearson: 0.8378673470403745

=== Experiment 1325 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012524158129660625
rmse: 0.1119113851654988
mae: 0.04384920260060212
r2: 0.43529122223714356
pearson: 0.6628589508172404

=== Experiment 1176 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005908050645649152
rmse: 0.07686384485341045
mae: 0.033025573728732806
r2: 0.7336085967196269
pearson: 0.8605985849468754

=== Experiment 1646 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.012056045657129677
rmse: 0.10980002576106108
mae: 0.05116962083172288
r2: 0.4563982075915086
pearson: 0.6914702628914203

=== Experiment 1825 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006570406020696745
rmse: 0.08105804106130832
mae: 0.03423034031663533
r2: 0.7037432843836254
pearson: 0.8396907925752892

=== Experiment 1785 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006388556938030719
rmse: 0.07992844886541162
mae: 0.03454974795801837
r2: 0.7119427794831344
pearson: 0.8466562250766226

=== Experiment 1836 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00754340797520047
rmse: 0.08685279486119299
mae: 0.03706054599258151
r2: 0.6598710545059627
pearson: 0.8324509600803218

=== Experiment 1651 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008837821991082732
rmse: 0.09400969094238494
mae: 0.04442302122680955
r2: 0.6015064962450083
pearson: 0.7836585978222912

=== Experiment 1844 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007360844772356932
rmse: 0.085795365681119
mae: 0.036511110653978864
r2: 0.6681027489699676
pearson: 0.8184196484662809

=== Experiment 1234 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009663938060383685
rmse: 0.09830533078314566
mae: 0.04381458508839957
r2: 0.5642572862817166
pearson: 0.7648236516463899

=== Experiment 1767 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00803213922268822
rmse: 0.08962220273285086
mae: 0.03997843339606527
r2: 0.6378343776638125
pearson: 0.8039551065714685

=== Experiment 1351 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008146230201314166
rmse: 0.09025646902751162
mae: 0.04136621232294566
r2: 0.6326900656528482
pearson: 0.8098493076230101

=== Experiment 1186 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0057722836357127156
rmse: 0.07597554630085075
mae: 0.029397546080114335
r2: 0.7397302714420299
pearson: 0.870419087606963

=== Experiment 1717 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006427038629560891
rmse: 0.08016881332264368
mae: 0.037646744065056685
r2: 0.7102076569491267
pearson: 0.8459086213933523

=== Experiment 1706 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006504698355402017
rmse: 0.08065171018274825
mae: 0.035297351138648264
r2: 0.7067060140916097
pearson: 0.8420578465169444

=== Experiment 1485 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006031717996840404
rmse: 0.07766413584686567
mae: 0.03498672738938938
r2: 0.728032490284577
pearson: 0.8536694522164633

=== Experiment 1893 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007522877560723627
rmse: 0.08673452346513254
mae: 0.04355243557320635
r2: 0.6607967618585971
pearson: 0.8168918002801377

=== Experiment 1784 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006138371393028778
rmse: 0.07834775933636379
mae: 0.0364967786526414
r2: 0.7232235355921928
pearson: 0.8543492657451478

=== Experiment 1838 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007349184929819233
rmse: 0.08572738727979078
mae: 0.03845432140388742
r2: 0.6686284861381031
pearson: 0.8240612091554723

=== Experiment 1392 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010567428565176685
rmse: 0.10279799883838539
mae: 0.04506739098049674
r2: 0.5235192970771818
pearson: 0.7269188288217494

=== Experiment 1076 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005730436213546446
rmse: 0.07569964473857488
mae: 0.027542777921548887
r2: 0.7416171532890479
pearson: 0.8658302605570067

=== Experiment 1181 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006099024794212451
rmse: 0.07809625339420868
mae: 0.02841958109127766
r2: 0.7249976564150584
pearson: 0.8522642574367776

=== Experiment 1806 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007227683554164925
rmse: 0.08501578414720955
mae: 0.03560460552275208
r2: 0.6741069296895109
pearson: 0.8217801494023402

=== Experiment 1395 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0066673948378578535
rmse: 0.08165411709067616
mae: 0.043505363015424986
r2: 0.699370101305874
pearson: 0.8497692693600504

=== Experiment 1318 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005715090125803096
rmse: 0.0755982150966747
mae: 0.031764564540943
r2: 0.7423091016310658
pearson: 0.8656737280330365

=== Experiment 1777 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010756743209886227
rmse: 0.10371472031436149
mae: 0.04479746696725175
r2: 0.5149831830709757
pearson: 0.7714566373488628

=== Experiment 1713 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007622008282286285
rmse: 0.08730411377642112
mae: 0.03374979611416646
r2: 0.6563270012541041
pearson: 0.8125718484604698

=== Experiment 1663 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007084231716065118
rmse: 0.08416787817252563
mae: 0.031502145589659095
r2: 0.6805751099314488
pearson: 0.8490387282592612

=== Experiment 1832 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007352196754196113
rmse: 0.08574495177091251
mae: 0.037988894565349665
r2: 0.6684926843025549
pearson: 0.8211306107991894

=== Experiment 1424 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00633206807023542
rmse: 0.07957429277244894
mae: 0.033010417903764695
r2: 0.7144898376693725
pearson: 0.8488957399408953

=== Experiment 1661 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006634486114209201
rmse: 0.08145235487209194
mae: 0.034399561725404315
r2: 0.7008539411709548
pearson: 0.8375325938996379

=== Experiment 1285 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006231408235543453
rmse: 0.07893926928686035
mae: 0.02881043154513309
r2: 0.7190285453118519
pearson: 0.8510643020779411

=== Experiment 1847 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008649165717429066
rmse: 0.09300089094965201
mae: 0.05203285275636569
r2: 0.610012924590076
pearson: 0.8143677819090392

=== Experiment 1607 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006152800653554502
rmse: 0.07843978998922997
mae: 0.03499634760336823
r2: 0.7225729265858911
pearson: 0.8508914765149831

=== Experiment 1716 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006711235656002711
rmse: 0.08192213166173541
mae: 0.0316011567301595
r2: 0.6973933381115422
pearson: 0.836479114014649

=== Experiment 1575 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005977010145957494
rmse: 0.07731112562857621
mae: 0.033117826231012094
r2: 0.7304992432021209
pearson: 0.8551807491022086

=== Experiment 1752 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005740010805835381
rmse: 0.07576285901307699
mae: 0.03520571149566568
r2: 0.7411854391368402
pearson: 0.8664899685876234

=== Experiment 1563 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007132567326337357
rmse: 0.08445452815768588
mae: 0.033207055596632215
r2: 0.6783956785384024
pearson: 0.8431887607795799

=== Experiment 1891 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007251799083572345
rmse: 0.08515749575681723
mae: 0.03795643038456821
r2: 0.6730195710825866
pearson: 0.8297117712243243

=== Experiment 1909 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0072698833693736736
rmse: 0.085263611050516
mae: 0.037504363032518695
r2: 0.6722041585953075
pearson: 0.8217899849493245

=== Experiment 1704 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00694669594213377
rmse: 0.08334684122469052
mae: 0.03319264976116893
r2: 0.6867765374438899
pearson: 0.8496143804513945

=== Experiment 1698 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007183138500179489
rmse: 0.08475339816302052
mae: 0.035818906470157494
r2: 0.6761154465679365
pearson: 0.8288847149491796

=== Experiment 1697 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005955367754247665
rmse: 0.07717102924185776
mae: 0.03886158150323498
r2: 0.731475089118772
pearson: 0.8583141967662663

=== Experiment 1051 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005678254522432321
rmse: 0.07535419379458798
mae: 0.0319865595375854
r2: 0.7439700027744651
pearson: 0.8652091077513524

=== Experiment 1232 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005768519064461695
rmse: 0.07595076737243472
mae: 0.03007802949843247
r2: 0.7399000143028239
pearson: 0.8647197919916065

=== Experiment 1684 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006095806597611862
rmse: 0.0780756466358868
mae: 0.02883048856012221
r2: 0.7251427634833423
pearson: 0.8701864894004964

=== Experiment 1598 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006085768821327687
rmse: 0.0780113377742472
mae: 0.03866457588073474
r2: 0.7255953623980326
pearson: 0.8543100859873054

=== Experiment 1626 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005942125893688934
rmse: 0.07708518595481842
mae: 0.028284810896496076
r2: 0.7320721587831767
pearson: 0.8558878929706575

=== Experiment 1924 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008249289364018624
rmse: 0.09082559861635169
mae: 0.038948038730426106
r2: 0.6280431733663105
pearson: 0.8270516884213988

=== Experiment 1163 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006595599535946098
rmse: 0.08121329654647752
mae: 0.02825632697851999
r2: 0.7026073198695462
pearson: 0.8410745408552425

=== Experiment 1895 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007304317695703891
rmse: 0.08546530112100402
mae: 0.04845117407070015
r2: 0.6706515299767841
pearson: 0.8324773853634133

=== Experiment 1477 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007495753704261588
rmse: 0.08657802090751202
mae: 0.03615596409755076
r2: 0.6620197646083483
pearson: 0.8263711537812841

=== Experiment 1833 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006889360725300418
rmse: 0.08300217301553266
mae: 0.03354820957236702
r2: 0.6893617571357471
pearson: 0.8306698064802928

=== Experiment 1792 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007584873045772403
rmse: 0.08709117662411275
mae: 0.03966917295907865
r2: 0.6580014127240474
pearson: 0.8212979334455798

=== Experiment 1509 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007167609888445701
rmse: 0.08466173804290637
mae: 0.03299506422106502
r2: 0.6768156248363484
pearson: 0.8382945608838598

=== Experiment 1631 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006636321506415931
rmse: 0.08146362075439521
mae: 0.032029536506058554
r2: 0.7007711841441713
pearson: 0.8465401043188863

=== Experiment 1360 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005908417871573558
rmse: 0.07686623362422253
mae: 0.02911351121255493
r2: 0.7335920386643238
pearson: 0.8569753003359545

=== Experiment 1826 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006926718689694101
rmse: 0.08322691085036199
mae: 0.03682184701359577
r2: 0.6876773029637333
pearson: 0.8347645279350203

=== Experiment 1886 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006902142336973114
rmse: 0.0830791329815924
mae: 0.03838647645801858
r2: 0.6887854398910427
pearson: 0.8363653568820858

=== Experiment 1728 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0071109452409603684
rmse: 0.08432642077641128
mae: 0.03319794397244521
r2: 0.6793706088514435
pearson: 0.8594321716424691

=== Experiment 1419 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01017353167361119
rmse: 0.10086392652287135
mae: 0.04853169622545742
r2: 0.5412799345505896
pearson: 0.7477320225687106

=== Experiment 1387 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006595341337576232
rmse: 0.08121170689978281
mae: 0.03374964092628118
r2: 0.7026189619204015
pearson: 0.8490073392133538

=== Experiment 1873 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0063332174838050675
rmse: 0.07958151471167829
mae: 0.0356067016639153
r2: 0.7144380111174127
pearson: 0.8460799971467282

=== Experiment 1804 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006922245256746531
rmse: 0.08320003159101882
mae: 0.036815664916189955
r2: 0.6878790080864309
pearson: 0.8320241375247093

=== Experiment 1889 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006753553535009885
rmse: 0.08218000690563299
mae: 0.03516007729703476
r2: 0.6954852435726315
pearson: 0.834899203668708

=== Experiment 1921 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006908917104611714
rmse: 0.08311989596126618
mae: 0.03400292697992344
r2: 0.6884799686000214
pearson: 0.8393151039352801

=== Experiment 1423 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006369235421459336
rmse: 0.07980748975791267
mae: 0.03471488314516167
r2: 0.7128139781612883
pearson: 0.8443763387444969

=== Experiment 1820 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007116031943875609
rmse: 0.08435657617444896
mae: 0.04335329798490422
r2: 0.6791412516558244
pearson: 0.8295054738453522

=== Experiment 1660 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011363192495919699
rmse: 0.10659827623334112
mae: 0.04729499905246631
r2: 0.4876386516824699
pearson: 0.698918469426083

=== Experiment 1822 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00619889947353488
rmse: 0.07873309007993322
mae: 0.03229775654861431
r2: 0.7204943510826182
pearson: 0.8517667755600261

=== Experiment 1946 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006744342731425097
rmse: 0.08212394736874926
mae: 0.0374157679785194
r2: 0.695900554652285
pearson: 0.834568978333452

=== Experiment 1938 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007766627378774728
rmse: 0.08812847087505109
mae: 0.03547352366783526
r2: 0.6498061898451721
pearson: 0.8188158575266122

=== Experiment 1372 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006680434470083899
rmse: 0.0817339248420379
mae: 0.033805550857421825
r2: 0.6987821500279827
pearson: 0.8392158583845386

=== Experiment 1865 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006796166073573597
rmse: 0.08243886264119367
mae: 0.04143838769187093
r2: 0.6935638629640062
pearson: 0.8383126333954574

=== Experiment 1894 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00630512231578987
rmse: 0.07940480033215794
mae: 0.03823363185528627
r2: 0.7157048098775888
pearson: 0.8524520218485461

=== Experiment 1861 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007418487432345504
rmse: 0.08613064165757447
mae: 0.04082823067820245
r2: 0.6655036668015577
pearson: 0.8183513361637165

=== Experiment 1520 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006183602447353434
rmse: 0.07863588523920509
mae: 0.031552228493467206
r2: 0.7211840872604034
pearson: 0.8493075495268345

=== Experiment 1824 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008414609377987197
rmse: 0.09173117996617725
mae: 0.03960051051782877
r2: 0.6205889666993701
pearson: 0.8261563125021723

=== Experiment 1401 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066788484748749755
rmse: 0.08172422208179761
mae: 0.04197455665963376
r2: 0.6988536618539032
pearson: 0.839659545972179

=== Experiment 1088 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005255028623215673
rmse: 0.07249157622245272
mae: 0.025402414804424536
r2: 0.7630530722941803
pearson: 0.8741065362717787

=== Experiment 1780 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007209542775209858
rmse: 0.08490902646485743
mae: 0.03809381753427181
r2: 0.6749248894282274
pearson: 0.834153895127726

=== Experiment 1796 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007315637266483271
rmse: 0.08553149868021295
mae: 0.04213721673699929
r2: 0.6701411355124659
pearson: 0.8210300774445202

=== Experiment 1963 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007357644878919905
rmse: 0.08577671524906923
mae: 0.036452899778737335
r2: 0.6682470307566621
pearson: 0.8185625737299465

=== Experiment 1859 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00811767967879631
rmse: 0.09009816690031107
mae: 0.04113375221564416
r2: 0.6339773961471324
pearson: 0.8188328952382149

=== Experiment 1228 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006587273203169837
rmse: 0.08116201823001838
mae: 0.035494954319291304
r2: 0.7029827505497284
pearson: 0.8424275879707189

=== Experiment 1947 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006868866568575685
rmse: 0.0828786255229639
mae: 0.03548571573929283
r2: 0.6902858296422971
pearson: 0.8313055805627134

=== Experiment 1305 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005909475702811229
rmse: 0.07687311430410003
mae: 0.02953813921957821
r2: 0.7335443415194043
pearson: 0.8574012179555447

=== Experiment 1926 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007457028841204302
rmse: 0.0863540898927451
mae: 0.039402573353187455
r2: 0.6637658516394858
pearson: 0.8150158112634518

=== Experiment 1900 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00750960863890657
rmse: 0.08665799812427338
mae: 0.04533325061594547
r2: 0.6613950517032292
pearson: 0.8261196140581106

=== Experiment 1897 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007610076438239846
rmse: 0.0872357520643907
mae: 0.046294929497997725
r2: 0.6568650028505518
pearson: 0.8155472087391753

=== Experiment 1870 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00588568935174644
rmse: 0.07671824653722503
mae: 0.02906571835288125
r2: 0.7346168576197419
pearson: 0.8572074270161436

=== Experiment 1872 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006283407464172
rmse: 0.079267947268565
mae: 0.03152288334966405
r2: 0.7166839229796014
pearson: 0.846680028272731

=== Experiment 1495 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006351864489139002
rmse: 0.0796985852392563
mae: 0.030211550354744692
r2: 0.7135972258540803
pearson: 0.845430952361095

=== Experiment 1860 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009920797838422209
rmse: 0.09960320194864324
mae: 0.04140135790566793
r2: 0.5526755919425934
pearson: 0.7629760189273129

=== Experiment 1388 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007119505663977753
rmse: 0.08437716316621313
mae: 0.03752139317158058
r2: 0.6789846231453198
pearson: 0.8274281403682278

=== Experiment 1750 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006907765323938273
rmse: 0.08311296724301373
mae: 0.0340596487034374
r2: 0.6885319018836494
pearson: 0.8470735225107312

=== Experiment 1978 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006973596497462607
rmse: 0.0835080624698155
mae: 0.039137525790729835
r2: 0.6855636032439525
pearson: 0.8290731740938572

=== Experiment 1667 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007207486265622763
rmse: 0.08489691552478666
mae: 0.041904490577879955
r2: 0.6750176165403704
pearson: 0.8307607913567033

=== Experiment 1890 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006135144250501773
rmse: 0.07832716163950902
mae: 0.03438606618169125
r2: 0.7233690460283613
pearson: 0.8507413577782905

=== Experiment 1542 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005550923972829556
rmse: 0.07450452317027172
mae: 0.031578695269061666
r2: 0.7497112812135933
pearson: 0.8659118376078508

=== Experiment 1866 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006413182199973288
rmse: 0.08008234636905495
mae: 0.030626101200388927
r2: 0.7108324372605507
pearson: 0.8437465685232015

=== Experiment 1597 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006372194426638135
rmse: 0.07982602599802983
mae: 0.030346250786060194
r2: 0.7126805579201341
pearson: 0.8557346006903729

=== Experiment 1964 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007040026444962922
rmse: 0.08390486544273176
mae: 0.03490359745886406
r2: 0.6825683061492192
pearson: 0.826469880851687

=== Experiment 1451 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008258879879603429
rmse: 0.09087837960485116
mae: 0.03819340227295134
r2: 0.6276107412397004
pearson: 0.8177124586416612

=== Experiment 1980 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00717403898771081
rmse: 0.0846996988643455
mae: 0.040190563686239994
r2: 0.6765257395801476
pearson: 0.827137976810556

=== Experiment 1448 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006052896835148204
rmse: 0.07780036526359117
mae: 0.030614043389333195
r2: 0.7270775457867977
pearson: 0.8549991574740463

=== Experiment 1560 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00638845545192535
rmse: 0.07992781400692346
mae: 0.03417722195736659
r2: 0.711947355446958
pearson: 0.8612841076769222

=== Experiment 1941 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007369552218332137
rmse: 0.08584609611585221
mae: 0.041827823325087145
r2: 0.6677101340633855
pearson: 0.8191292476568717

=== Experiment 1568 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00813448660891409
rmse: 0.09019138877361901
mae: 0.046342154015029065
r2: 0.6332195790654171
pearson: 0.8349692856336989

=== Experiment 1594 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007954562493093092
rmse: 0.08918835402166077
mae: 0.03836679095932712
r2: 0.6413322782571267
pearson: 0.8104203633102318

=== Experiment 1943 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0069300297312184875
rmse: 0.08324680012600176
mae: 0.038854604414753996
r2: 0.6875280095586995
pearson: 0.8297511171318593

=== Experiment 1691 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005775970021994214
rmse: 0.07599980277602182
mae: 0.032409678441625914
r2: 0.7395640539070998
pearson: 0.8602740366569165

=== Experiment 1280 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006372310995119633
rmse: 0.07982675613551908
mae: 0.02967523163809651
r2: 0.7126753018986091
pearson: 0.8444839866170748

=== Experiment 1962 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066542920825815345
rmse: 0.08157384435333138
mae: 0.03712242593344695
r2: 0.6999608987743197
pearson: 0.8381904209271229

=== Experiment 1203 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006760796674668274
rmse: 0.08222406384184787
mae: 0.03986949918745346
r2: 0.6951586535933844
pearson: 0.8368698058759352

=== Experiment 1479 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005489196845289631
rmse: 0.07408911421585246
mae: 0.027227252806314984
r2: 0.7524945302261818
pearson: 0.8781273531802833

=== Experiment 1851 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0075287356918036395
rmse: 0.08676828736239779
mae: 0.04034996338784542
r2: 0.6605326213065601
pearson: 0.8218024827550414

=== Experiment 1794 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006633399409329344
rmse: 0.08144568379803403
mae: 0.04262381575542096
r2: 0.7009029402156923
pearson: 0.8443485325412734

=== Experiment 1898 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007984357993390059
rmse: 0.08935523484043931
mae: 0.03657336338712154
r2: 0.6399888122627391
pearson: 0.8347944451620948

=== Experiment 1977 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006277037809463309
rmse: 0.07922775908394297
mae: 0.0385380777464774
r2: 0.7169711279069169
pearson: 0.8492957757243021

=== Experiment 1854 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006739935116993186
rmse: 0.08209710784792108
mae: 0.04053321083213856
r2: 0.6960992920470803
pearson: 0.8371533210735741

=== Experiment 1757 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006492330119888637
rmse: 0.08057499686558255
mae: 0.034175200131311846
r2: 0.7072636923872311
pearson: 0.8411347278836058

=== Experiment 1902 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067383819231368975
rmse: 0.08208764781096421
mae: 0.03797587876708233
r2: 0.6961693248744475
pearson: 0.8358433910427798

=== Experiment 1776 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007104668216413822
rmse: 0.08428919394806088
mae: 0.04064439592003532
r2: 0.6796536371255175
pearson: 0.8257903483550013

=== Experiment 1876 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006485255428495023
rmse: 0.08053108361679373
mae: 0.02964734969612244
r2: 0.7075826871083595
pearson: 0.8433217655060169

=== Experiment 1819 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006065775037342162
rmse: 0.07788308569479102
mae: 0.030475407094749205
r2: 0.7264968733180024
pearson: 0.8588660685729196

=== Experiment 1939 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007200878396251564
rmse: 0.08485798958407843
mae: 0.04159883970083518
r2: 0.675315562461972
pearson: 0.825520896490687

=== Experiment 1929 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006739476510379141
rmse: 0.08209431472628992
mae: 0.03452892668102691
r2: 0.6961199704174594
pearson: 0.8367681355478667

=== Experiment 1190 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006925072323740556
rmse: 0.08321701943557314
mae: 0.03674787734106115
r2: 0.6877515368799576
pearson: 0.8303887044083316

=== Experiment 1960 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006922400250117774
rmse: 0.08320096303599961
mae: 0.037687726836923306
r2: 0.6878720195035392
pearson: 0.8309326841543568

=== Experiment 1877 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006813399978906174
rmse: 0.08254332183106138
mae: 0.03943272761891104
r2: 0.6927867937577823
pearson: 0.8414100811287627

=== Experiment 1602 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007443754657676105
rmse: 0.08627719662620074
mae: 0.03386962723478341
r2: 0.6643643787323572
pearson: 0.8184179412193169

=== Experiment 1933 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007199377610256899
rmse: 0.08484914619639315
mae: 0.034602259627493574
r2: 0.6753832322419251
pearson: 0.8263586261832255

=== Experiment 1932 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007602274936270534
rmse: 0.08719102554890919
mae: 0.04458775184890706
r2: 0.6572167691406445
pearson: 0.8290078661931102

=== Experiment 1081 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006676958297094984
rmse: 0.0817126568965603
mae: 0.031513109834172034
r2: 0.6989388891380721
pearson: 0.8437587011149515

=== Experiment 1722 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0060884311696303
rmse: 0.07802839976335732
mae: 0.03452010287925282
r2: 0.7254753182848568
pearson: 0.8562030298381235

=== Experiment 1918 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006396803115496789
rmse: 0.07998001697609715
mae: 0.03932672609250705
r2: 0.7115709629706096
pearson: 0.846624365501248

=== Experiment 1848 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0074811598334275885
rmse: 0.08649369822956808
mae: 0.04629837695431082
r2: 0.6626777958209995
pearson: 0.8193664402531

=== Experiment 1350 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006286996782592011
rmse: 0.0792905844510684
mae: 0.03200218043021327
r2: 0.716522082191823
pearson: 0.8478413527740248

=== Experiment 1937 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006528818617415978
rmse: 0.08080110529823202
mae: 0.03594706115718457
r2: 0.7056184420935393
pearson: 0.840616741385032

=== Experiment 1730 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00898223516319706
rmse: 0.09477465464562274
mae: 0.04699318221156206
r2: 0.5949949698754711
pearson: 0.7865124814879644

=== Experiment 1094 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005939211829255663
rmse: 0.07706628205159285
mae: 0.03378245018705762
r2: 0.732203552665895
pearson: 0.8574200543346747

=== Experiment 1538 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008692477658757402
rmse: 0.09323345782902939
mae: 0.04337349101772455
r2: 0.6080600082186247
pearson: 0.7962089493881123

=== Experiment 1677 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006671645599544298
rmse: 0.08168014201471677
mae: 0.04057377506903857
r2: 0.6991784363323356
pearson: 0.8396982416500761

=== Experiment 1532 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005551696982099931
rmse: 0.07450971065639653
mae: 0.028703397234263947
r2: 0.7496764265658193
pearson: 0.8663277026637852

=== Experiment 1546 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00870990684661324
rmse: 0.09332688169339656
mae: 0.03567220583180639
r2: 0.6072741338093772
pearson: 0.784303073724684

=== Experiment 1420 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006799848115899353
rmse: 0.08246119157457861
mae: 0.04618506449659083
r2: 0.6933978412961286
pearson: 0.8448065269355916

=== Experiment 1816 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006758886587798447
rmse: 0.08221244788837301
mae: 0.03274885211394194
r2: 0.6952447785696514
pearson: 0.8348682824351502

=== Experiment 1357 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009299446833217297
rmse: 0.0964336395311164
mae: 0.04337421556412901
r2: 0.5806920352897916
pearson: 0.7753973875852238

=== Experiment 1522 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006731664680374483
rmse: 0.08204672254498947
mae: 0.0314256810493101
r2: 0.6964722023941201
pearson: 0.8434167430796135

=== Experiment 1910 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006036909828703538
rmse: 0.0776975535567468
mae: 0.03026063794029632
r2: 0.7277983928709681
pearson: 0.8567822922164241

=== Experiment 1343 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007543784190590199
rmse: 0.08685496065620087
mae: 0.03759135956164446
r2: 0.6598540911196249
pearson: 0.8227924020109282

=== Experiment 1952 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007169990101588784
rmse: 0.08467579407120304
mae: 0.031393552086741
r2: 0.6767083020733388
pearson: 0.8270952455931171

=== Experiment 1733 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005834185510878618
rmse: 0.0763818401904446
mae: 0.02968803968628698
r2: 0.7369391431358983
pearson: 0.8594147651229647

=== Experiment 1759 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.015511228445996632
rmse: 0.12454408234033695
mae: 0.06018654093780534
r2: 0.3006055363837402
pearson: 0.5700349298544429

=== Experiment 1762 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006605320083077322
rmse: 0.0812731202986407
mae: 0.0362070162734555
r2: 0.7021690246777393
pearson: 0.8442732395495782

=== Experiment 1447 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00634262168115438
rmse: 0.07964057810660581
mae: 0.036664217735791226
r2: 0.7140139799980361
pearson: 0.8456630328028313

=== Experiment 1974 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007606600604499724
rmse: 0.08721582771779285
mae: 0.053101713794349484
r2: 0.6570217266640626
pearson: 0.8289283019378045

=== Experiment 1981 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006318803864592793
rmse: 0.07949090428843285
mae: 0.03273256130341869
r2: 0.7150879148637751
pearson: 0.8480636682220478

=== Experiment 1068 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0066330747377249715
rmse: 0.08144369059494401
mae: 0.034742307115649386
r2: 0.7009175795154343
pearson: 0.8386362252343319

=== Experiment 1685 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007300424008364088
rmse: 0.08544251873841319
mae: 0.030610757420058206
r2: 0.6708270946251929
pearson: 0.8500426281723611

=== Experiment 1925 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0073214764447352
rmse: 0.08556562653738474
mae: 0.048839808065296614
r2: 0.6698778495351052
pearson: 0.830114709064662

=== Experiment 1078 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072386730388293435
rmse: 0.08508039162362467
mae: 0.03507626100533044
r2: 0.6736114186628344
pearson: 0.8262165116856982

=== Experiment 1852 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006260595254289048
rmse: 0.07912392340050542
mae: 0.04075224144534768
r2: 0.7177125154828023
pearson: 0.8519232872215682

=== Experiment 1498 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007298485629997756
rmse: 0.08543117481340026
mae: 0.038272838430456146
r2: 0.6709144952525854
pearson: 0.820414303466099

=== Experiment 1513 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006362085768428417
rmse: 0.07976268405983099
mae: 0.035292762707626685
r2: 0.7131363528696495
pearson: 0.8540406063928389

=== Experiment 1623 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006566784849488871
rmse: 0.08103570107976404
mae: 0.03691797096547234
r2: 0.7039065613995892
pearson: 0.8509702027503019

=== Experiment 1591 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006152614597186373
rmse: 0.07843860399819959
mae: 0.03241275463698765
r2: 0.7225813157856402
pearson: 0.8575165139388347

=== Experiment 1243 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005682018652417329
rmse: 0.07537916590422933
mae: 0.029546487200700005
r2: 0.7438002798101615
pearson: 0.8626132451916472

=== Experiment 1084 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005745620897846165
rmse: 0.07579987399624201
mae: 0.03188673672427218
r2: 0.7409324825572641
pearson: 0.8610202135311926

=== Experiment 1543 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00547000321118558
rmse: 0.07395947005749554
mae: 0.029212049454193055
r2: 0.7533599627401691
pearson: 0.8761939816146529

=== Experiment 1204 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005577029760589543
rmse: 0.07467951366063884
mae: 0.02847878514776917
r2: 0.7485341827335312
pearson: 0.8731174571764245

=== Experiment 1968 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008220707045285132
rmse: 0.09066811482150235
mae: 0.04156671450289249
r2: 0.6293319375379631
pearson: 0.8013864499218004

=== Experiment 1694 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00809993490916308
rmse: 0.08999963838351285
mae: 0.05218994246988602
r2: 0.6347774999998244
pearson: 0.8124617482210764

=== Experiment 1746 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.01074098252735338
rmse: 0.10363871152881717
mae: 0.04303252720369892
r2: 0.5156938253095751
pearson: 0.7514660157177663

=== Experiment 1429 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060202067118882326
rmse: 0.07758999105482764
mae: 0.02981324939971139
r2: 0.7285515290565665
pearson: 0.8575696944764349

=== Experiment 1828 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006525983278649114
rmse: 0.08078355821978328
mae: 0.03380550411741938
r2: 0.7057462862706091
pearson: 0.8478634348423423

=== Experiment 1533 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005405906791073961
rmse: 0.07352487192150668
mae: 0.026970804576561428
r2: 0.7562500421120102
pearson: 0.8713341986066693

=== Experiment 1930 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006905957082783407
rmse: 0.08310208831782392
mae: 0.03557014250361196
r2: 0.6886134346814546
pearson: 0.8303138046115792

=== Experiment 1389 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00631808650590111
rmse: 0.07948639195422767
mae: 0.034652970810235446
r2: 0.7151202602514494
pearson: 0.8529099421243297

=== Experiment 1686 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005900841212054179
rmse: 0.07681693310757844
mae: 0.0349691465970954
r2: 0.7339336669073098
pearson: 0.8571991430227276

=== Experiment 1385 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007406568823462074
rmse: 0.08606142471201644
mae: 0.03989752302984254
r2: 0.6660410716303304
pearson: 0.8173849401667185

=== Experiment 1454 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0056464709440938315
rmse: 0.07514300329434426
mae: 0.02909719229334054
r2: 0.7454031103327251
pearson: 0.8645105782128396

=== Experiment 1381 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005869860181362235
rmse: 0.07661501276748725
mae: 0.03214154448285639
r2: 0.7353305879454151
pearson: 0.8589117092377575

=== Experiment 1140 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005809409705778039
rmse: 0.07621948376745961
mae: 0.0357240783866372
r2: 0.7380562732832174
pearson: 0.8610523139388063

=== Experiment 1871 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007552591355109393
rmse: 0.08690564627864747
mae: 0.04809709416405459
r2: 0.6594569799477841
pearson: 0.8392171981806996

=== Experiment 1957 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007078323839581629
rmse: 0.08413277506169417
mae: 0.038518775139243545
r2: 0.6808414934818341
pearson: 0.8267423192448474

=== Experiment 1940 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00793941691247096
rmse: 0.08910340572879893
mae: 0.0395510163095214
r2: 0.642015185821299
pearson: 0.8081571263840447

=== Experiment 1103 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006277139153437497
rmse: 0.07922839865501194
mae: 0.03655924813585138
r2: 0.7169665583517257
pearson: 0.8483679190833956

=== Experiment 1473 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0069397467861351935
rmse: 0.08330514261517828
mae: 0.03681786397003714
r2: 0.6870898718293083
pearson: 0.8311950671635615

=== Experiment 1415 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007180316945107804
rmse: 0.084736750852908
mae: 0.03433223995068662
r2: 0.6762426692442576
pearson: 0.8573746836406766

=== Experiment 1882 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006382441025630391
rmse: 0.07989018103390673
mae: 0.03798408491776738
r2: 0.7122185432814464
pearson: 0.8480488108771255

=== Experiment 1583 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005818371996123693
rmse: 0.07627825375638651
mae: 0.03252180881914918
r2: 0.7376521675561376
pearson: 0.8611323158078878

=== Experiment 1535 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008305365972597486
rmse: 0.09113378063373365
mae: 0.03771672842753434
r2: 0.6255147037666926
pearson: 0.8060683341112966

=== Experiment 1972 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007297275521020493
rmse: 0.0854240921580118
mae: 0.04342119744691484
r2: 0.6709690585337604
pearson: 0.8359954172068971

=== Experiment 1129 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006193627892832922
rmse: 0.07869960541726319
mae: 0.03070836056419963
r2: 0.7207320443362693
pearson: 0.8495180498532662

=== Experiment 1699 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006738958164717559
rmse: 0.08209115765243878
mae: 0.03439515616472096
r2: 0.6961433423951988
pearson: 0.8391341136408507

=== Experiment 1875 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007013263973467525
rmse: 0.08374523254172457
mae: 0.04736317756541961
r2: 0.683775014209882
pearson: 0.8364071100998516

=== Experiment 1216 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006197255320390176
rmse: 0.07872264807785734
mae: 0.031685058407757136
r2: 0.7205684852242658
pearson: 0.8488690413421109

=== Experiment 1987 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006536414374846636
rmse: 0.0808480944416542
mae: 0.03835327588564856
r2: 0.7052759527341395
pearson: 0.8410437639440865

=== Experiment 1856 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006243862017698469
rmse: 0.07901811195984418
mae: 0.032849078742947714
r2: 0.7184670097558109
pearson: 0.8508971703828939

=== Experiment 1599 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0064925920033625005
rmse: 0.08057662194062556
mae: 0.03618985616373164
r2: 0.7072518841766584
pearson: 0.8666558835036511

=== Experiment 1689 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005787556337226699
rmse: 0.07607599054384175
mae: 0.034305928547247774
r2: 0.7390416320527944
pearson: 0.8608919277695578

=== Experiment 1696 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005953208691628147
rmse: 0.07715703915799353
mae: 0.0318303074969301
r2: 0.7315724403020099
pearson: 0.8571114245855174

=== Experiment 1903 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.012442821639655863
rmse: 0.11154739638223683
mae: 0.04442477183823508
r2: 0.4389586487725311
pearson: 0.7267549910025302

=== Experiment 1206 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005772239268347202
rmse: 0.07597525431577838
mae: 0.030008885642234518
r2: 0.7397322719470132
pearson: 0.8679215907456641

=== Experiment 1518 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0068697901627906
rmse: 0.08288419730437521
mae: 0.031675296410975634
r2: 0.6902441851856513
pearson: 0.8669198472093

=== Experiment 1229 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005867774621728294
rmse: 0.07660140091230899
mae: 0.030339066566265827
r2: 0.7354246249113845
pearson: 0.8598415804302766

=== Experiment 1880 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006418826192636598
rmse: 0.08011757730134254
mae: 0.03660275403734766
r2: 0.7105779521154733
pearson: 0.8439491051010803

=== Experiment 1571 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006134974477082724
rmse: 0.07832607788650421
mae: 0.0292584234573653
r2: 0.7233767010371033
pearson: 0.8531009761164854

=== Experiment 1292 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007454180129237297
rmse: 0.08633759395093946
mae: 0.04287131101802187
r2: 0.6638942988082639
pearson: 0.8164999483053731

=== Experiment 1288 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006598875536224075
rmse: 0.08123346315542675
mae: 0.035115575654855485
r2: 0.7024596064588902
pearson: 0.8476688735501274

=== Experiment 1502 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006407999130533995
rmse: 0.08004997895398847
mae: 0.03565233625348727
r2: 0.711066139580325
pearson: 0.8437153496816615

=== Experiment 1384 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0063506980942702905
rmse: 0.07969126736519058
mae: 0.03168596439623099
r2: 0.7136498180853408
pearson: 0.8468626879804609

=== Experiment 1934 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007761392222890182
rmse: 0.08809876402589416
mae: 0.04745635004133221
r2: 0.6500422407198383
pearson: 0.8307632300874386

=== Experiment 1778 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007399235042384188
rmse: 0.08601880632968693
mae: 0.03569443904093197
r2: 0.6663717485912884
pearson: 0.8168578309852723

=== Experiment 1688 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006179186130289086
rmse: 0.07860779942403352
mae: 0.029896943151626342
r2: 0.721383217053065
pearson: 0.8617890100902241

=== Experiment 1793 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006661946284018275
rmse: 0.08162074665192837
mae: 0.03438933974168992
r2: 0.6996157742004085
pearson: 0.8460474063508682

=== Experiment 1664 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006035291527943277
rmse: 0.07768713875502996
mae: 0.02949876431936211
r2: 0.7278713613399799
pearson: 0.8532823751139792

=== Experiment 1519 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006172448032873393
rmse: 0.07856492877151607
mae: 0.030485578335734405
r2: 0.7216870348998752
pearson: 0.850550865139716

=== Experiment 1183 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006624741173818751
rmse: 0.08139251300837658
mae: 0.03367870354283108
r2: 0.701293336244084
pearson: 0.8452395340697995

=== Experiment 1613 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006304768434376819
rmse: 0.07940257196323566
mae: 0.03659162981733261
r2: 0.7157207662347482
pearson: 0.8479985776923463

=== Experiment 1896 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00706057673786047
rmse: 0.08402723807111875
mae: 0.03786219133035592
r2: 0.6816417024873443
pearson: 0.8338879600255759

=== Experiment 1605 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00709360349356569
rmse: 0.08422353289648736
mae: 0.03872556448197098
r2: 0.6801525406087828
pearson: 0.8262021082725824

=== Experiment 1841 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008824527669864187
rmse: 0.09393895714699087
mae: 0.039624587177986816
r2: 0.6021059313374745
pearson: 0.7786323349136471

=== Experiment 1850 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0064559598098064275
rmse: 0.08034898760909454
mae: 0.030566633999520317
r2: 0.7089036136610403
pearson: 0.8454053567819383

=== Experiment 1992 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006840226506231355
rmse: 0.08270566187529942
mae: 0.042623781726394736
r2: 0.6915771974479474
pearson: 0.8395621671871095

=== Experiment 1984 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006369472864731279
rmse: 0.07980897734422662
mae: 0.03134391026689374
r2: 0.7128032719486632
pearson: 0.854814495537822

=== Experiment 1996 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006339228199349825
rmse: 0.07961927027642131
mae: 0.03175306844384214
r2: 0.7141669905990178
pearson: 0.8598435935933919

=== Experiment 1954 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007330451649657573
rmse: 0.08561805679678541
mae: 0.04742035258095519
r2: 0.6694731614954452
pearson: 0.8283257436498226

=== Experiment 1935 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007183277757901487
rmse: 0.0847542197055786
mae: 0.03745155526898287
r2: 0.6761091674985436
pearson: 0.8294946472224992

=== Experiment 1537 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005446402120800319
rmse: 0.0737997433654096
mae: 0.029618198524903456
r2: 0.7544241255180064
pearson: 0.8703699644480914

=== Experiment 1786 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006012102972155266
rmse: 0.07753775191579432
mae: 0.03264018452359236
r2: 0.7289169231143973
pearson: 0.8540078284541409

=== Experiment 1985 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0068961970110514225
rmse: 0.0830433441706885
mae: 0.04106640826866809
r2: 0.6890535120201137
pearson: 0.8331302252976618

=== Experiment 1714 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005702133740537995
rmse: 0.07551247407242061
mae: 0.03182982816157243
r2: 0.7428932993401279
pearson: 0.865725097368554

=== Experiment 1437 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006486379894376934
rmse: 0.08053806487852147
mae: 0.028406917089914926
r2: 0.7075319854366593
pearson: 0.8536113970594912

=== Experiment 1577 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00805262700949316
rmse: 0.08973643078200269
mae: 0.03716414237951978
r2: 0.6369105923741423
pearson: 0.8357766707405048

=== Experiment 1702 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.011366452665055763
rmse: 0.10661356698401833
mae: 0.04511697263436336
r2: 0.4874916520910345
pearson: 0.7316122416270079

=== Experiment 1377 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006724183285037235
rmse: 0.08200111758407463
mae: 0.03400060298958155
r2: 0.6968095352170629
pearson: 0.8362145055459091

=== Experiment 1683 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005906598118000242
rmse: 0.07685439556720385
mae: 0.03477890348033208
r2: 0.7336740905520098
pearson: 0.8571947769321189

=== Experiment 1857 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00525196901978562
rmse: 0.072470469984578
mae: 0.032221253037598034
r2: 0.763191028466207
pearson: 0.8739482449136744

=== Experiment 1709 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006314391390702813
rmse: 0.07946314485786989
mae: 0.0323894577644661
r2: 0.7152868713693961
pearson: 0.8465737729303268

=== Experiment 1768 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008313186785004227
rmse: 0.09117667895358016
mae: 0.039327591178779424
r2: 0.6251620667774753
pearson: 0.7928237091113259

=== Experiment 1821 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006436756779998303
rmse: 0.08022940096995804
mae: 0.03211142355022336
r2: 0.7097694698231889
pearson: 0.8435605284625906

=== Experiment 1496 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006476152205318675
rmse: 0.08047454383417575
mae: 0.031737496874304355
r2: 0.7079931474347461
pearson: 0.8438063569138259

=== Experiment 1914 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006894040045820682
rmse: 0.08303035617062401
mae: 0.036803917015335634
r2: 0.6891507686330147
pearson: 0.8319799286959249

=== Experiment 1383 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059509541773392184
rmse: 0.07714242786780319
mae: 0.032968648019024265
r2: 0.7316740953590102
pearson: 0.8570441787712333

=== Experiment 1566 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006285798783318635
rmse: 0.07928302960481919
mae: 0.03051800808659371
r2: 0.7165760994517798
pearson: 0.847559121281473

=== Experiment 1341 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007197586368297163
rmse: 0.08483859008904594
mae: 0.047980306480662
r2: 0.6754639985535038
pearson: 0.8350072582288811

=== Experiment 1764 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006702715022418257
rmse: 0.08187011067794069
mae: 0.03362141639731566
r2: 0.6977775297296476
pearson: 0.8354973017516264

=== Experiment 1961 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006646363198762689
rmse: 0.08152523044286775
mae: 0.044074038068213826
r2: 0.7003184086559426
pearson: 0.8431282067246079

=== Experiment 1879 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060229906362539954
rmse: 0.07760792895222753
mae: 0.03834106087985783
r2: 0.728426003132213
pearson: 0.8641783608245165

=== Experiment 1666 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008849052637841822
rmse: 0.09406940330331548
mae: 0.0428690023163394
r2: 0.6010001113256266
pearson: 0.7811552463026062

=== Experiment 1494 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006415506795389186
rmse: 0.08009685883596925
mae: 0.035114312917734515
r2: 0.7107276222763805
pearson: 0.8493704072981959

=== Experiment 1628 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005715998881520947
rmse: 0.07560422528880874
mae: 0.030686911968035097
r2: 0.7422681262357218
pearson: 0.8620725752667877

=== Experiment 1452 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006995537100787308
rmse: 0.08363932747689515
mae: 0.03133152485213788
r2: 0.6845743110968397
pearson: 0.8413219036214196

=== Experiment 1863 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005868739875544985
rmse: 0.07660770115037381
mae: 0.03185530826736752
r2: 0.7353811020416305
pearson: 0.8598162224297872

=== Experiment 1569 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007247883241845993
rmse: 0.08513450089033231
mae: 0.029952568831790885
r2: 0.6731961346625339
pearson: 0.8249527927893874

=== Experiment 1640 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006212108122925242
rmse: 0.0788169278957588
mae: 0.030461233749977848
r2: 0.719898778895178
pearson: 0.8618335190166287

=== Experiment 1391 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006820950332676175
rmse: 0.08258904487083123
mae: 0.03421274113662137
r2: 0.6924463516294577
pearson: 0.8345948719159691

=== Experiment 1729 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00652032659934759
rmse: 0.08074853930163436
mae: 0.030353591473365332
r2: 0.7060013434506198
pearson: 0.8402899758900798

=== Experiment 1647 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005784828507819287
rmse: 0.07605806011080803
mae: 0.03555964616659007
r2: 0.739164628680165
pearson: 0.8610847292591488

=== Experiment 1811 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006516820942593827
rmse: 0.080726829137492
mae: 0.03248494059611436
r2: 0.7061594119706894
pearson: 0.8417767238533098

=== Experiment 1408 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006563924453171343
rmse: 0.08101805016890584
mae: 0.03360298129520953
r2: 0.7040355354105898
pearson: 0.8391097938713676

=== Experiment 1443 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012642769686793946
rmse: 0.11244007153499123
mae: 0.05334493383420177
r2: 0.42994307933094056
pearson: 0.657676568621214

=== Experiment 1818 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006158278887592102
rmse: 0.07847470221410274
mae: 0.02892121304124042
r2: 0.7223259154242908
pearson: 0.8511199782653377

=== Experiment 1319 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00561894419977652
rmse: 0.07495961712666707
mae: 0.028838485609251285
r2: 0.7466442791185459
pearson: 0.8641219004683973

=== Experiment 1266 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005631772242637373
rmse: 0.0750451347033062
mae: 0.02968010078329289
r2: 0.7460658683120035
pearson: 0.8651772071324141

=== Experiment 1481 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006936565690364729
rmse: 0.08328604739309417
mae: 0.03292244589338392
r2: 0.6872333060374911
pearson: 0.835505839466729

=== Experiment 1080 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006222522711082994
rmse: 0.07888296844746015
mae: 0.033575523643593584
r2: 0.7194291896989555
pearson: 0.8508327945742489

=== Experiment 1441 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007562717752241126
rmse: 0.08696388763297744
mae: 0.04051920732597444
r2: 0.6590003851580823
pearson: 0.8267373486343436

=== Experiment 1596 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00595735918652747
rmse: 0.07718393088284291
mae: 0.031586895581275136
r2: 0.7313852963137719
pearson: 0.8559897146463016

=== Experiment 1622 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0077296302946639655
rmse: 0.0879183160363298
mae: 0.03931201541702152
r2: 0.6514743720840646
pearson: 0.8180121413847357

=== Experiment 1445 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010753333285395048
rmse: 0.10369828005032218
mae: 0.044068710384153775
r2: 0.5151369350654604
pearson: 0.7678326538973116

=== Experiment 1284 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006294842962005277
rmse: 0.0793400463952806
mae: 0.03529753629827878
r2: 0.7161683014154465
pearson: 0.8497005996938088

=== Experiment 1834 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0070542025786380705
rmse: 0.08398930038188239
mae: 0.03952981241874352
r2: 0.6819291105212002
pearson: 0.8279674318507741

=== Experiment 1908 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005954831610278732
rmse: 0.07716755542505369
mae: 0.03447073041210247
r2: 0.7314992636143561
pearson: 0.856131394369471

=== Experiment 1639 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006324458829033247
rmse: 0.07952646621743763
mae: 0.033900896084158666
r2: 0.7148329350060951
pearson: 0.8523041406810719

=== Experiment 1579 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0072575406007658345
rmse: 0.08519120025428585
mae: 0.03135954591170124
r2: 0.6727606885993671
pearson: 0.8357967984283109

=== Experiment 1969 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01265432091949857
rmse: 0.11249142598215461
mae: 0.0499655261233398
r2: 0.42942223933237533
pearson: 0.6560966176570442

=== Experiment 1476 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006075137391998806
rmse: 0.07794316770569956
mae: 0.028166117871043773
r2: 0.726074728867222
pearson: 0.8691672759969643

=== Experiment 1721 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007435807056642829
rmse: 0.0862311257994631
mae: 0.03444954160931278
r2: 0.6647227325649407
pearson: 0.8215920340129742

=== Experiment 1787 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010249184166822698
rmse: 0.10123825446353121
mae: 0.04427316545462075
r2: 0.5378687969288916
pearson: 0.7418804409001686

=== Experiment 1427 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00630872526110884
rmse: 0.07942748429296272
mae: 0.0312615531253919
r2: 0.7155423546589339
pearson: 0.8489260965392865

=== Experiment 1486 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005811903386037782
rmse: 0.07623584056097094
mae: 0.030457982557048024
r2: 0.7379438343378584
pearson: 0.8593163078158222

=== Experiment 1687 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007292870831890309
rmse: 0.08539830696149842
mae: 0.044370059145032725
r2: 0.6711676640279883
pearson: 0.8282334346121755

=== Experiment 1680 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007447991483585404
rmse: 0.08630174670066304
mae: 0.048289931919690686
r2: 0.6641733421168765
pearson: 0.8273961086431836

=== Experiment 1643 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005690987631045955
rmse: 0.07543863486997862
mae: 0.032152586382314094
r2: 0.7433958725113462
pearson: 0.8665849927364194

=== Experiment 1907 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008194014205384587
rmse: 0.09052079432585966
mae: 0.04357749759181683
r2: 0.6305355059406614
pearson: 0.79507338166913

=== Experiment 1130 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007289592186453333
rmse: 0.08537910860657502
mae: 0.03901084017082364
r2: 0.6713154967076993
pearson: 0.8222761478100828

=== Experiment 1428 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006463984488644206
rmse: 0.08039890850405997
mae: 0.03294992212656699
r2: 0.7085417844241763
pearson: 0.8430665046635998

=== Experiment 1999 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005573872493596799
rmse: 0.07465837189221848
mae: 0.030197805022014787
r2: 0.7486765425126145
pearson: 0.865278735020178

=== Experiment 1809 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00645173499669185
rmse: 0.08032269291235106
mae: 0.038273928696859624
r2: 0.7090941086248941
pearson: 0.8537506462479675

=== Experiment 1681 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006377152097798962
rmse: 0.07985707293533217
mae: 0.035561366920402133
r2: 0.7124570187095303
pearson: 0.8464374532676259

=== Experiment 1670 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006243931829157214
rmse: 0.07901855370200858
mae: 0.037665321425879965
r2: 0.7184638619878625
pearson: 0.8505866206830821

=== Experiment 1769 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009574923859629996
rmse: 0.09785153989401493
mae: 0.036631464842878915
r2: 0.5682708974155544
pearson: 0.7552301924457574

=== Experiment 1294 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006086131854079607
rmse: 0.07801366453435966
mae: 0.028916798486450745
r2: 0.7255789934110314
pearson: 0.8549543399852612

=== Experiment 1951 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006523732727423827
rmse: 0.0807696275057885
mae: 0.04157711963271347
r2: 0.705847762634814
pearson: 0.843758030625409

=== Experiment 1737 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0052351250078172375
rmse: 0.0723541637213591
mae: 0.027606643325385507
r2: 0.7639505175522447
pearson: 0.8741306238935395

=== Experiment 1344 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006256682763225508
rmse: 0.07909919571794335
mae: 0.030075762026121527
r2: 0.7178889279828384
pearson: 0.8600142791063452

=== Experiment 2000 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005882211448703433
rmse: 0.07669557646111953
mae: 0.03035196735060722
r2: 0.734773674737209
pearson: 0.8589891005419427

=== Experiment 1246 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006944185592333412
rmse: 0.08333178020619392
mae: 0.03564912299350475
r2: 0.6868897280114985
pearson: 0.8293493523307512

=== Experiment 1658 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005181906553909728
rmse: 0.07198546071193633
mae: 0.028381245546394714
r2: 0.7663501142157015
pearson: 0.8770273311648735

=== Experiment 1920 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006261428129373493
rmse: 0.07912918633079385
mae: 0.029944006227904628
r2: 0.7176749615118211
pearson: 0.8481599621981456

=== Experiment 1212 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005603883945781435
rmse: 0.07485909394176124
mae: 0.03295182258364151
r2: 0.7473233393426596
pearson: 0.8667629356979722

=== Experiment 1299 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0063656665846819165
rmse: 0.07978512759081054
mae: 0.030435830478373796
r2: 0.7129748954408169
pearson: 0.8504851691941131

=== Experiment 1603 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008266013344531527
rmse: 0.09091761844951465
mae: 0.04009050375827225
r2: 0.6272890964457706
pearson: 0.7930363557383455

=== Experiment 1313 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005822207294613503
rmse: 0.07630338979765908
mae: 0.03807684717065536
r2: 0.737479235635276
pearson: 0.8621780393638575

=== Experiment 1922 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005722216356725019
rmse: 0.07564533268302162
mae: 0.028676392621012268
r2: 0.7419877830152904
pearson: 0.8628711861558048

=== Experiment 1675 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007351374841714406
rmse: 0.08574015886219483
mae: 0.04137372231630488
r2: 0.6685297439746039
pearson: 0.8206173204906133

=== Experiment 1998 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00735002184120948
rmse: 0.08573226837783705
mae: 0.035731529167438394
r2: 0.6685907501718698
pearson: 0.8324903466609215

=== Experiment 1855 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006189694565730855
rmse: 0.07867461195157467
mae: 0.03196413880552007
r2: 0.7209093963241118
pearson: 0.8581989903025095

=== Experiment 1858 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072422568942140265
rmse: 0.08510145059994
mae: 0.03602599362194457
r2: 0.6734498242009144
pearson: 0.8424295779538807

=== Experiment 1638 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008067727472042676
rmse: 0.08982052923492867
mae: 0.05308862040482766
r2: 0.6362297191640061
pearson: 0.8231258371281464

=== Experiment 1772 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006285465149667816
rmse: 0.07928092550965721
mae: 0.028634854675892427
r2: 0.7165911428462517
pearson: 0.8466956145872814

=== Experiment 1988 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006384949626056023
rmse: 0.07990587979652075
mae: 0.03438204374618083
r2: 0.7121054315923743
pearson: 0.844623060372134

=== Experiment 1610 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006192890902270518
rmse: 0.07869492297645711
mae: 0.039440967393852146
r2: 0.7207652749163538
pearson: 0.8587414756507448

=== Experiment 1614 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010537238194752514
rmse: 0.1026510506266376
mae: 0.04659826573836813
r2: 0.5248805675918093
pearson: 0.7423336073556703

=== Experiment 1734 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008802284490857374
rmse: 0.09382049078350302
mae: 0.04120371105069983
r2: 0.6031088664889197
pearson: 0.8231229194862252

=== Experiment 1425 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006813311266604654
rmse: 0.082542784461179
mae: 0.030323909028187122
r2: 0.6927907937564124
pearson: 0.842858138912217

=== Experiment 1727 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007273924735382991
rmse: 0.08528730700041473
mae: 0.03270890096877453
r2: 0.6720219351806886
pearson: 0.8340724478429353

=== Experiment 1574 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007092124479130101
rmse: 0.08421475214669993
mae: 0.035509925174301585
r2: 0.6802192287187194
pearson: 0.8322321140431268

=== Experiment 1842 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006882150672120834
rmse: 0.08295872872772843
mae: 0.03307941464268024
r2: 0.6896868552602274
pearson: 0.8475885116238088

=== Experiment 1949 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006357953755321172
rmse: 0.07973677793415766
mae: 0.035407809197241985
r2: 0.7133226635220171
pearson: 0.8447460577128839

=== Experiment 1565 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006782858732284119
rmse: 0.08235811272900878
mae: 0.03421430510752232
r2: 0.6941638851257401
pearson: 0.8417188744784035

=== Experiment 1774 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009226701250201233
rmse: 0.09605571950800865
mae: 0.04311611542140819
r2: 0.5839721016102064
pearson: 0.7704349745786007

=== Experiment 1911 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006419427814404184
rmse: 0.08012133183119327
mae: 0.03834597466581902
r2: 0.7105508252547665
pearson: 0.850679924914165

=== Experiment 1161 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006982674003193987
rmse: 0.08356239586796196
mae: 0.03760445134503636
r2: 0.6851543025058404
pearson: 0.8290557453213532

=== Experiment 1982 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008863567673152493
rmse: 0.09414652236356101
mae: 0.04080755495681247
r2: 0.6003456347720242
pearson: 0.7798838251074065

=== Experiment 1853 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006272389486124996
rmse: 0.07919841845722045
mae: 0.03479096438260879
r2: 0.7171807187603578
pearson: 0.8477458260382055

=== Experiment 1557 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008056643611062016
rmse: 0.0897588079859688
mae: 0.03967162673985048
r2: 0.636729485577244
pearson: 0.8242439488929767

=== Experiment 1942 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.010528816618935975
rmse: 0.10261002201995659
mae: 0.044376737205018287
r2: 0.5252602927387583
pearson: 0.7333906506893048

=== Experiment 1869 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007166236802551619
rmse: 0.08465362840747949
mae: 0.03267417123366071
r2: 0.6768775366749713
pearson: 0.8256519805343044

=== Experiment 1433 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005299296873933828
rmse: 0.0727962696429826
mae: 0.02726840718065004
r2: 0.7610570363532458
pearson: 0.8769125284671578

=== Experiment 1738 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006781357632787091
rmse: 0.08234899897865845
mae: 0.03655818099540091
r2: 0.6942315690413765
pearson: 0.8361149859442731

=== Experiment 1976 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006129976860190899
rmse: 0.07829416874960037
mae: 0.03259570931588566
r2: 0.7236020413831361
pearson: 0.8526404979009139

=== Experiment 1668 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007031830527114982
rmse: 0.08385601067970609
mae: 0.0362239389285298
r2: 0.6829378564776833
pearson: 0.8385771109793453

=== Experiment 1813 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006345752719891477
rmse: 0.0796602329891865
mae: 0.03314440530694047
r2: 0.7138728028394556
pearson: 0.8462492692473906

=== Experiment 1567 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062420529979520765
rmse: 0.07900666426290935
mae: 0.03151778723190225
r2: 0.7185485776599653
pearson: 0.8478079612182419

=== Experiment 1548 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006160898198746271
rmse: 0.07849138932867905
mae: 0.039363681109772015
r2: 0.7222078118371964
pearson: 0.8528923334681001

=== Experiment 1291 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007430901209394699
rmse: 0.08620267518699579
mae: 0.038573168406867496
r2: 0.6649439350581305
pearson: 0.818145314782354

=== Experiment 1979 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007234815836848447
rmse: 0.08505772061869779
mae: 0.03362474318912972
r2: 0.6737853382024683
pearson: 0.8352787182662444

=== Experiment 1953 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006505812784648441
rmse: 0.08065861878713546
mae: 0.03153252183112995
r2: 0.7066557649673861
pearson: 0.8422060358839555

=== Experiment 1530 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006255343678521011
rmse: 0.0790907306738344
mae: 0.030320296930407612
r2: 0.7179493067227878
pearson: 0.8562150543321886

=== Experiment 1528 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006661008618935239
rmse: 0.0816150024133752
mae: 0.03514258610093668
r2: 0.6996580531063049
pearson: 0.842557711734499

=== Experiment 1672 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00950971447095781
rmse: 0.09751776489931366
mae: 0.0414874673790084
r2: 0.5712111600499366
pearson: 0.7593737763592106

=== Experiment 1810 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007979082524419335
rmse: 0.08932571032138135
mae: 0.041165517219536055
r2: 0.6402266808367175
pearson: 0.8051790185869969

=== Experiment 1862 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007186342333663321
rmse: 0.08477229697055118
mae: 0.02845024576197232
r2: 0.6759709871262656
pearson: 0.8329167576634783

=== Experiment 1740 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009873386478663971
rmse: 0.0993649157331901
mae: 0.04844658303937489
r2: 0.5548133492867571
pearson: 0.7455767544541843

=== Experiment 1270 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005838847401809379
rmse: 0.07641235110771935
mae: 0.029890729881916128
r2: 0.7367289405256854
pearson: 0.8607354171121879

=== Experiment 1363 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006847033346338484
rmse: 0.08274680263513802
mae: 0.03153930886736347
r2: 0.6912702800234364
pearson: 0.8337225422465464

=== Experiment 1453 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061687362736330695
rmse: 0.07854130297896178
mae: 0.030322526542417037
r2: 0.7218543964903512
pearson: 0.8605405722054122

=== Experiment 1251 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007221736846019125
rmse: 0.0849808028087469
mae: 0.03671415977509312
r2: 0.6743750641424586
pearson: 0.8289327336258103

=== Experiment 1521 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007447146409617024
rmse: 0.08629685051968597
mae: 0.042621992532924556
r2: 0.6642114461301654
pearson: 0.8226970560917718

=== Experiment 1627 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009030052096093916
rmse: 0.09502658625928807
mae: 0.043793939633380435
r2: 0.5928389254169935
pearson: 0.7816361344313975

=== Experiment 1110 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005763933362083061
rmse: 0.07592057271967237
mae: 0.028983615608183563
r2: 0.7401067816047545
pearson: 0.8611881872714329

=== Experiment 1995 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006004405437570502
rmse: 0.07748809868341397
mae: 0.03035236033714937
r2: 0.7292640015608802
pearson: 0.8557226869655273

=== Experiment 1912 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005765113881781726
rmse: 0.07592834702389963
mae: 0.030971458214767045
r2: 0.7400535524911281
pearson: 0.8612669939739441

=== Experiment 1993 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006086542962868973
rmse: 0.07801629934102856
mae: 0.03010942615016034
r2: 0.7255604566966616
pearson: 0.8544534811050672

=== Experiment 1461 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00632684917768116
rmse: 0.07954149343381202
mae: 0.031349803087136446
r2: 0.7147251552376337
pearson: 0.8567242743998112

=== Experiment 1827 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006914692915969778
rmse: 0.08315463255868417
mae: 0.03889271100362694
r2: 0.6882195398080153
pearson: 0.8299192516783057

=== Experiment 1616 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005822837936176331
rmse: 0.07630752214674731
mae: 0.027703627814856146
r2: 0.7374508002847746
pearson: 0.8593427706171142

=== Experiment 1906 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007274351361545398
rmse: 0.08528980807544004
mae: 0.035170208180336546
r2: 0.6720026987947982
pearson: 0.8568783535182541

=== Experiment 1905 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006013721813130423
rmse: 0.07754819026341249
mae: 0.02968640562896752
r2: 0.7288439302873339
pearson: 0.8542314481495851

=== Experiment 1945 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00686235417827371
rmse: 0.08283932748564361
mae: 0.03558426678680252
r2: 0.6905794704546908
pearson: 0.8322847352561717

=== Experiment 1789 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005358929923746425
rmse: 0.07320471244220843
mae: 0.033567336407429677
r2: 0.7583682083837082
pearson: 0.8730411076412151

=== Experiment 1931 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0109033493361592
rmse: 0.10441910426813285
mae: 0.047946077681691915
r2: 0.5083727773636182
pearson: 0.7147270484940256

=== Experiment 1867 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006311305043067203
rmse: 0.07944372248999415
mae: 0.03537015349456246
r2: 0.7154260334258802
pearson: 0.8466974361790292

=== Experiment 1115 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009532694799561625
rmse: 0.09763552017355992
mae: 0.04687621421001787
r2: 0.5701749871476067
pearson: 0.7660859197416663

=== Experiment 1200 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072049055681821204
rmse: 0.08488171515810763
mae: 0.033321452651077985
r2: 0.6751339790521171
pearson: 0.8512354949928845

=== Experiment 1742 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006208554051019413
rmse: 0.07879437829578588
mae: 0.030609587586253463
r2: 0.7200590304331449
pearson: 0.8552131648971781

=== Experiment 1337 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007610429054270995
rmse: 0.0872377730932593
mae: 0.03883033597720054
r2: 0.6568491035489055
pearson: 0.818671390340818

=== Experiment 1868 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007693162533639024
rmse: 0.08771067514070921
mae: 0.03919648190536944
r2: 0.65311868737799
pearson: 0.81091706884572

=== Experiment 1615 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008454391508282255
rmse: 0.0919477651075993
mae: 0.04024532659949345
r2: 0.6187952079538207
pearson: 0.8060123279377961

=== Experiment 1812 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006102698380572752
rmse: 0.07811976946057095
mae: 0.027071883780355167
r2: 0.7248320160229412
pearson: 0.8526930475125658

=== Experiment 1775 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.02009107514092698
rmse: 0.14174298974173988
mae: 0.054160311620639126
r2: 0.0941022646540135
pearson: 0.3926164688624552

=== Experiment 1547 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006148716461036417
rmse: 0.07841375173422335
mae: 0.037485674386151455
r2: 0.722757081028941
pearson: 0.8528402604817087

=== Experiment 1744 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005735347880348935
rmse: 0.07573207959873368
mae: 0.02886709917374454
r2: 0.7413956883947135
pearson: 0.8610777778057326

=== Experiment 1673 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0058479863281784865
rmse: 0.07647212778639344
mae: 0.02958636294154214
r2: 0.7363168703580512
pearson: 0.8605523108695922

=== Experiment 1845 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005561940599983004
rmse: 0.07457841913035569
mae: 0.02837217810775909
r2: 0.7492145463440327
pearson: 0.8665301123108525

=== Experiment 1990 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005775643131235301
rmse: 0.07599765214291361
mae: 0.034962256732547735
r2: 0.7395787932675436
pearson: 0.8617031521881395

=== Experiment 1153 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0053822093955529235
rmse: 0.07336354268676591
mae: 0.030215682318890904
r2: 0.7573185472460324
pearson: 0.8710953443509295

=== Experiment 1720 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005962229930857588
rmse: 0.07721547727533379
mae: 0.03674711017448297
r2: 0.7311656765957055
pearson: 0.8566333411918313

=== Experiment 1800 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00850712823200809
rmse: 0.09223409473729381
mae: 0.04231760911016553
r2: 0.61641733229223
pearson: 0.7895417163784636

=== Experiment 1966 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007637642190364572
rmse: 0.08739360497407446
mae: 0.03828625025633117
r2: 0.6556220752198632
pearson: 0.8138472502351225

=== Experiment 1036 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006721230475837528
rmse: 0.08198311091827101
mae: 0.03992984944662565
r2: 0.6969426760842468
pearson: 0.8393736265125246

=== Experiment 1755 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007019168333557656
rmse: 0.08378047704302988
mae: 0.037002053456642814
r2: 0.6835087892121257
pearson: 0.8277588646629876

=== Experiment 1765 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0072819050257808575
rmse: 0.08533407892384412
mae: 0.03669845963871071
r2: 0.6716621073990431
pearson: 0.8262913788766588

=== Experiment 1166 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005994674399093997
rmse: 0.07742528268656174
mae: 0.031327370583450094
r2: 0.7297027698028282
pearson: 0.8544248083582359

=== Experiment 1511 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008231199160351482
rmse: 0.09072595637606408
mae: 0.03268980951286114
r2: 0.6288588526875544
pearson: 0.8274743616303561

=== Experiment 1435 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00603337508948972
rmse: 0.07767480344030309
mae: 0.029784397073213377
r2: 0.7279577727063615
pearson: 0.8535416806951577

=== Experiment 1843 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0067276252362524986
rmse: 0.08202210212041934
mae: 0.029977160401733743
r2: 0.6966543391516851
pearson: 0.8523949596127062

=== Experiment 1959 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006765243498752389
rmse: 0.08225110028900762
mae: 0.03774728033864791
r2: 0.6949581482526286
pearson: 0.8390964608491155

=== Experiment 1464 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00545502540344463
rmse: 0.0738581437855341
mae: 0.027810323656152276
r2: 0.7540353054989712
pearson: 0.8710573630142137

=== Experiment 1928 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007033561747205883
rmse: 0.08386633262046149
mae: 0.03439393413142026
r2: 0.682859796525753
pearson: 0.8303722948690115

=== Experiment 1763 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00638245595851646
rmse: 0.0798902744926844
mae: 0.03467376482844031
r2: 0.7122178699641859
pearson: 0.8529125967622274

=== Experiment 1829 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059002520542251295
rmse: 0.07681309819441688
mae: 0.03318123118349016
r2: 0.7339602317745133
pearson: 0.8570296457581121

=== Experiment 1222 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006876714271135532
rmse: 0.08292595655845962
mae: 0.029797749425951973
r2: 0.6899319801879115
pearson: 0.837001688338485

=== Experiment 1874 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006034987944931626
rmse: 0.07768518484840997
mae: 0.030045478201534276
r2: 0.7278850497643581
pearson: 0.8580768298203079

=== Experiment 1915 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006056768263749483
rmse: 0.07782524181619664
mae: 0.03300743221224399
r2: 0.7269029847751123
pearson: 0.8531309920642155

=== Experiment 1554 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00616486718700738
rmse: 0.07851666821132555
mae: 0.03299813232836919
r2: 0.7220288519033883
pearson: 0.8544172263157223

=== Experiment 1412 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007450144915288875
rmse: 0.08631422197580695
mae: 0.034027137427243344
r2: 0.6640762448291648
pearson: 0.8173882504583656

=== Experiment 1700 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005675593674327368
rmse: 0.07533653611845562
mae: 0.028796122174695203
r2: 0.7440899792443888
pearson: 0.8658170991078642

=== Experiment 1991 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005809885308646144
rmse: 0.07622260365958476
mae: 0.03307275424592041
r2: 0.7380348285592234
pearson: 0.8603549034503181

=== Experiment 1601 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005963536024449107
rmse: 0.07722393427201897
mae: 0.03511211059462275
r2: 0.7311067854105195
pearson: 0.8563728247550593

=== Experiment 1446 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007889224777556892
rmse: 0.08882130812793117
mae: 0.03414826193068209
r2: 0.6442783270933261
pearson: 0.8525248746941881

=== Experiment 1758 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006527838146885329
rmse: 0.08079503788528927
mae: 0.03279506224582088
r2: 0.7056626510782282
pearson: 0.8420508972390824

=== Experiment 1783 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006655358564149618
rmse: 0.08158038100027247
mae: 0.03616797679319347
r2: 0.6999128115898092
pearson: 0.836957610555295

=== Experiment 1773 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006287605517770122
rmse: 0.07929442299285695
mae: 0.029255908047443036
r2: 0.716494634590567
pearson: 0.8576560263711165

=== Experiment 1399 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007332562584532683
rmse: 0.08563038353605969
mae: 0.037506971388172834
r2: 0.66937798037102
pearson: 0.829302366164428

=== Experiment 1948 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059087580557074535
rmse: 0.07686844642444293
mae: 0.029323266616638678
r2: 0.733576699911453
pearson: 0.8566359761792965

=== Experiment 1830 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008587058960298679
rmse: 0.09266638527696372
mae: 0.041681220861921
r2: 0.6128132909338111
pearson: 0.7932457297474789

=== Experiment 1837 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006488916042958137
rmse: 0.08055380837029455
mae: 0.0332131103446202
r2: 0.7074176316133773
pearson: 0.8458617809742669

=== Experiment 1779 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008291391895192658
rmse: 0.09105708042317553
mae: 0.036747015818645846
r2: 0.6261447887664144
pearson: 0.8298527138531511

=== Experiment 1552 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008342951812427685
rmse: 0.09133976030419438
mae: 0.04185914206093671
r2: 0.6238199747915419
pearson: 0.798983541916822

=== Experiment 1989 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005707876645438734
rmse: 0.0755504907028322
mae: 0.028595689951829675
r2: 0.7426343542857994
pearson: 0.8639695019876131

=== Experiment 1815 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007836708153562863
rmse: 0.08852518372510086
mae: 0.039654075360084376
r2: 0.646646278554883
pearson: 0.806754871157875

=== Experiment 1117 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005766956492000745
rmse: 0.07594047993001325
mae: 0.029342805331531944
r2: 0.7399704700073473
pearson: 0.8608725709125198

=== Experiment 1207 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006145829075381793
rmse: 0.07839533835236502
mae: 0.031995790774619406
r2: 0.7228872719772723
pearson: 0.8518036156860063

=== Experiment 1881 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006258128769671116
rmse: 0.07910833565226306
mae: 0.03230652819264467
r2: 0.7178237281886419
pearson: 0.8486021771475477

=== Experiment 1956 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00684793361017745
rmse: 0.08275224232718682
mae: 0.038055182589596614
r2: 0.6912296875231738
pearson: 0.8385008381423773

=== Experiment 1108 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005711394693772281
rmse: 0.07557376987931912
mae: 0.02857214411174157
r2: 0.7424757270348513
pearson: 0.8619551650228038

=== Experiment 1885 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006265951067964256
rmse: 0.07915776063004977
mae: 0.034994432770650674
r2: 0.7174710242014611
pearson: 0.8473066464382281

=== Experiment 1656 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0070763928821858165
rmse: 0.08412129862398593
mae: 0.03468511720104346
r2: 0.6809285595009318
pearson: 0.8407481518706121

=== Experiment 1745 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006025534511289152
rmse: 0.07762431649482752
mae: 0.02985666170822638
r2: 0.7283113009265227
pearson: 0.8561163184402728

=== Experiment 1573 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0056692585660023
rmse: 0.07529447898752138
mae: 0.0337750132173307
r2: 0.7443756264904717
pearson: 0.8632471973988578

=== Experiment 1634 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006800267327416084
rmse: 0.0824637334069716
mae: 0.03113202852399638
r2: 0.6933789392333485
pearson: 0.8338815815319942

=== Experiment 1797 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007451585747173557
rmse: 0.08632256800613358
mae: 0.036852943541253784
r2: 0.6640112783536432
pearson: 0.8153787146835295

=== Experiment 1802 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005641829588505779
rmse: 0.07511211346051833
mae: 0.03142789825912443
r2: 0.7456123870133735
pearson: 0.8642000298327936

=== Experiment 1692 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009025172511563356
rmse: 0.09500090795125779
mae: 0.038595679988683324
r2: 0.593058943735807
pearson: 0.7746513571233533

=== Experiment 1536 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007412735981635203
rmse: 0.08609724723610623
mae: 0.03427873026403221
r2: 0.665762997182678
pearson: 0.8399769050767778

=== Experiment 1462 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007334193491180549
rmse: 0.08563990595032522
mae: 0.03776541644262017
r2: 0.6693044435080846
pearson: 0.820897672570586

=== Experiment 1329 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006751241661701978
rmse: 0.08216593979077935
mae: 0.03938435404262386
r2: 0.6955894849225519
pearson: 0.8393537918438916

=== Experiment 1975 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005818245486810698
rmse: 0.07627742448988888
mae: 0.029449853828569395
r2: 0.7376578718053796
pearson: 0.8644662690312855

=== Experiment 1440 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006573262412087178
rmse: 0.08107565856708891
mae: 0.034147686823084535
r2: 0.7036144909530249
pearson: 0.8422107378436146

=== Experiment 1944 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006860130762107633
rmse: 0.08282590634643024
mae: 0.030362086200118973
r2: 0.6906797233110191
pearson: 0.843836184215405

=== Experiment 1347 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0057694189478568
rmse: 0.07595669126454101
mae: 0.02824357629054428
r2: 0.7398594389566076
pearson: 0.8604152116435808

=== Experiment 1079 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006450892153967962
rmse: 0.08031744613698796
mae: 0.034319548049389
r2: 0.7091321120323508
pearson: 0.846521651945155

=== Experiment 1986 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005939943493313192
rmse: 0.07707102888448546
mae: 0.02878990284957473
r2: 0.732170562255571
pearson: 0.8599341591043682

=== Experiment 1766 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00781843518040315
rmse: 0.08842191572457107
mae: 0.03364465128587379
r2: 0.647470198872104
pearson: 0.8062877937820621

=== Experiment 1630 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010008553974825363
rmse: 0.10004276073172592
mae: 0.04642815005317736
r2: 0.5487187063766048
pearson: 0.7464064504882149

=== Experiment 1965 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005617233780242147
rmse: 0.07494820731840186
mae: 0.03209313717158814
r2: 0.7467214011825376
pearson: 0.8673119448175246

=== Experiment 1899 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006165905018227069
rmse: 0.07852327691982212
mae: 0.030424310249467033
r2: 0.7219820565504769
pearson: 0.8538186019235369

=== Experiment 1636 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005405936000863032
rmse: 0.07352507056006836
mae: 0.028094775271938485
r2: 0.7562487250554772
pearson: 0.8712894410932592

=== Experiment 1983 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007055653980863992
rmse: 0.08399794033703441
mae: 0.03755769132064781
r2: 0.6818636674336443
pearson: 0.8265096033008551

=== Experiment 1707 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00695577853414257
rmse: 0.08340131014643937
mae: 0.03905399612792051
r2: 0.6863670073677675
pearson: 0.8285486871484078

=== Experiment 1927 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006547973531247018
rmse: 0.08091954974693703
mae: 0.0302790986577795
r2: 0.70475475545962
pearson: 0.8580501673640584

=== Experiment 1669 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008249822044144097
rmse: 0.09082853100289631
mae: 0.03994346637530362
r2: 0.6280191550539033
pearson: 0.7959906595775582

=== Experiment 1489 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005774257871010184
rmse: 0.07598853776070562
mae: 0.0312912131413039
r2: 0.7396412540413946
pearson: 0.8606022811397775

=== Experiment 1743 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.004805015643914688
rmse: 0.06931822014387479
mae: 0.029744194866480854
r2: 0.7833439594650027
pearson: 0.8857186551239976

=== Experiment 1839 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005756644213581978
rmse: 0.07587255243882321
mae: 0.03203431443427517
r2: 0.7404354461024689
pearson: 0.8660219198135869

=== Experiment 1642 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009370832972612288
rmse: 0.09680306282660837
mae: 0.04490018530281456
r2: 0.5774732656839141
pearson: 0.7697042199384102

=== Experiment 1671 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005852448628982093
rmse: 0.07650129821762565
mae: 0.028734022276930594
r2: 0.7361156671788256
pearson: 0.8596512987607852

=== Experiment 1904 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005789714844730969
rmse: 0.0760901757438565
mae: 0.03320222374125948
r2: 0.7389443058994586
pearson: 0.8615920423290634

=== Experiment 1290 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00562827921572273
rmse: 0.07502185825293006
mae: 0.028906248686367337
r2: 0.7462233673581861
pearson: 0.8647198565713482

=== Experiment 1465 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005295814105196739
rmse: 0.07277234437062433
mae: 0.029654427471046976
r2: 0.7612140728627176
pearson: 0.8739294088702638

=== Experiment 1458 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005916083323113196
rmse: 0.07691607974353085
mae: 0.02829015295597587
r2: 0.7332464068282247
pearson: 0.8573234186223703

=== Experiment 1690 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006132717193323512
rmse: 0.07831166703195323
mae: 0.03029218729139261
r2: 0.7234784809682996
pearson: 0.8508969990926186

=== Experiment 1474 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005716770605169042
rmse: 0.07560932882369108
mae: 0.027957689271870603
r2: 0.7422333295560909
pearson: 0.8631102324753933

=== Experiment 1888 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006168767332861666
rmse: 0.07854150070416063
mae: 0.032828710709615176
r2: 0.7218529960434039
pearson: 0.8502679478810417

=== Experiment 1735 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006056944698848386
rmse: 0.07782637534183631
mae: 0.033010795530917895
r2: 0.7268950293941576
pearson: 0.8527672889960812

=== Experiment 1817 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00560174076835004
rmse: 0.07484477782951887
mae: 0.028233421695442452
r2: 0.7474199742697567
pearson: 0.865347707053054

=== Experiment 1659 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007369462229413081
rmse: 0.08584557198489087
mae: 0.03807882864958478
r2: 0.6677141916241386
pearson: 0.832610404910622

=== Experiment 1463 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006074151953734728
rmse: 0.07793684593139966
mae: 0.0304549055897214
r2: 0.7261191618448246
pearson: 0.8530079468972682

=== Experiment 1710 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005749367466609564
rmse: 0.07582458352414186
mae: 0.03497404337969085
r2: 0.7407635514207138
pearson: 0.8616578755014435

=== Experiment 1359 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006912363753348255
rmse: 0.08314062637091602
mae: 0.030097327328777693
r2: 0.6883245607254769
pearson: 0.834336291635032

=== Experiment 1831 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006695619215717119
rmse: 0.08182676344398035
mae: 0.038667162595521186
r2: 0.6980974765306984
pearson: 0.8366813249744474

=== Experiment 1611 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006204481983243326
rmse: 0.07876853422048252
mae: 0.03129763228966753
r2: 0.7202426381769147
pearson: 0.8497050118658556

=== Experiment 1971 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006015106732619768
rmse: 0.07755711916142688
mae: 0.028711444534892668
r2: 0.7287814848771088
pearson: 0.8545275839643915

=== Experiment 1411 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00597670692653113
rmse: 0.07730916457012797
mae: 0.029030969287271204
r2: 0.7305129152325984
pearson: 0.8547511345078617

=== Experiment 1958 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005831304783487196
rmse: 0.0763629804518341
mae: 0.030312757040041532
r2: 0.7370690338660723
pearson: 0.8589186822109478

=== Experiment 1726 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007529245424641185
rmse: 0.08677122463490523
mae: 0.03753484504039323
r2: 0.6605096376772658
pearson: 0.8138929558010549

=== Experiment 1864 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00600334677642579
rmse: 0.07748126726135672
mae: 0.02865157824908107
r2: 0.7293117361259429
pearson: 0.85555070235693

=== Experiment 1805 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006412224518939487
rmse: 0.08007636679407656
mae: 0.03566587072573118
r2: 0.7108756186768572
pearson: 0.8591072301123953

=== Experiment 1747 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005919688114205651
rmse: 0.07693950944869386
mae: 0.030193670410912952
r2: 0.7330838683844558
pearson: 0.857783210591683

=== Experiment 1231 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005941691043248494
rmse: 0.07708236531949765
mae: 0.02796526580887599
r2: 0.7320917659981437
pearson: 0.8569040913179515

=== Experiment 1994 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066591095204731904
rmse: 0.08160336709029346
mae: 0.031938149779045036
r2: 0.6997436826201014
pearson: 0.8460306409483376

=== Experiment 1795 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00702956235288708
rmse: 0.08384248536921529
mae: 0.037154502648888
r2: 0.6830401274553197
pearson: 0.8345213503921498

=== Experiment 1887 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007383576742220555
rmse: 0.0859277414006708
mae: 0.03773319811019752
r2: 0.6670777744539218
pearson: 0.820422383858186

=== Experiment 1799 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006424969779381141
rmse: 0.08015590919814422
mae: 0.034099977792783065
r2: 0.7103009404931611
pearson: 0.8491109159532857

=== Experiment 1997 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006088742528875719
rmse: 0.0780303949040098
mae: 0.03615398812611814
r2: 0.7254612792335206
pearson: 0.8558738294728722

=== Experiment 1801 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007715773796609822
rmse: 0.08783947743816457
mae: 0.03846273984029428
r2: 0.6520991554826147
pearson: 0.8116202503089524

=== Experiment 1917 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00616320744722923
rmse: 0.07850609815313221
mae: 0.03836479599717142
r2: 0.7221036888394777
pearson: 0.8585290838340113

=== Experiment 1936 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006143706125974138
rmse: 0.07838179715963482
mae: 0.02755187012338004
r2: 0.7229829948316819
pearson: 0.8682385887690877

=== Experiment 1649 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005892554173041556
rmse: 0.07676297397210166
mae: 0.029284539340141517
r2: 0.7343073258489841
pearson: 0.8638718596129037

=== Experiment 1916 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009483677030725975
rmse: 0.09738417238302113
mae: 0.04690124725874987
r2: 0.5723851767700363
pearson: 0.7566712520327975

=== Experiment 1732 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062604847275368955
rmse: 0.07912322495662633
mae: 0.038126846499179204
r2: 0.7177174990854104
pearson: 0.849648636743187

=== Experiment 1967 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006400774355678885
rmse: 0.08000483957660864
mae: 0.029823537514638582
r2: 0.7113919014986754
pearson: 0.8542626171232016

=== Experiment 1641 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006006382642642776
rmse: 0.07750085575426104
mae: 0.029900847721231923
r2: 0.729174850254406
pearson: 0.8548189999653693

=== Experiment 1582 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0076607109134067975
rmse: 0.08752548722176186
mae: 0.04106014804059743
r2: 0.6545819166512108
pearson: 0.8105190912408379

=== Experiment 1679 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007112299285733186
rmse: 0.08433444898576847
mae: 0.04118859110771222
r2: 0.6793095555685977
pearson: 0.833752288148856

=== Experiment 1883 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0074219745754447826
rmse: 0.08615088261558777
mae: 0.038162894903642566
r2: 0.6653464330540202
pearson: 0.8162069161889055

=== Experiment 1798 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005779889444618304
rmse: 0.07602558414519617
mae: 0.028450083888720834
r2: 0.7393873288660493
pearson: 0.8631330662628913

=== Experiment 1558 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.004771223123812116
rmse: 0.06907404088231783
mae: 0.030463043097138015
r2: 0.7848676493232022
pearson: 0.8861932679959597

=== Experiment 1923 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057081586764222165
rmse: 0.07555235718640561
mae: 0.02743364352941248
r2: 0.7426216376328856
pearson: 0.8637662204434587

=== Experiment 1884 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007054589842374674
rmse: 0.08399160578518948
mae: 0.03513122938901972
r2: 0.6819116489697645
pearson: 0.8263451366350909

=== Experiment 1529 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00668796437845842
rmse: 0.08177997541243467
mae: 0.036493963727771014
r2: 0.6984426297735418
pearson: 0.8368329083911452

=== Experiment 1741 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006008276909384536
rmse: 0.07751307573167598
mae: 0.0293768628976895
r2: 0.7290894386007505
pearson: 0.8585065405663984

=== Experiment 1823 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005548734122648819
rmse: 0.07448982563175201
mae: 0.028572671734272033
r2: 0.7498100205944158
pearson: 0.8782245132555443

=== Experiment 1503 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0057652953219030165
rmse: 0.07592954182597848
mae: 0.02611611058669381
r2: 0.740045371435917
pearson: 0.8701980003986443

=== Experiment 1955 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005648124267558646
rmse: 0.0751540036695228
mae: 0.030183953280320103
r2: 0.7453285627053798
pearson: 0.8649219674469713

=== Experiment 1840 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005902580071601271
rmse: 0.076828250478592
mae: 0.028547872919820142
r2: 0.7338552624956618
pearson: 0.8592337333855017

=== Experiment 1492 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007779214686384978
rmse: 0.08819985649866431
mae: 0.03921369258722871
r2: 0.6492386336851238
pearson: 0.813522071777556

=== Experiment 1919 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006579136818050155
rmse: 0.08111187840292046
mae: 0.03980679396479736
r2: 0.7033496165736779
pearson: 0.8474848891703652

=== Experiment 1491 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006900855944379837
rmse: 0.08307139064912683
mae: 0.03344839072349754
r2: 0.6888434427668888
pearson: 0.8372403654448447

=== Experiment 1846 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006990376265962341
rmse: 0.08360847006112683
mae: 0.04005650448946759
r2: 0.6848070108676403
pearson: 0.8353547310246303

=== Experiment 1580 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007860779213890449
rmse: 0.08866103548848529
mae: 0.03371486294029716
r2: 0.6455609250391962
pearson: 0.8253625926010734

=== Experiment 1701 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057927944246951345
rmse: 0.07611040943717971
mae: 0.03243207998699634
r2: 0.7388054489943012
pearson: 0.8618275642801142

=== Experiment 1950 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005756933778630591
rmse: 0.0758744606480375
mae: 0.029211432675155885
r2: 0.740422389741874
pearson: 0.8622322651454383

=== Experiment 1220 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064871705170826575
rmse: 0.08054297310804126
mae: 0.03055420898612057
r2: 0.7074963366068379
pearson: 0.8434877221783672

=== Experiment 1514 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005651309134183187
rmse: 0.0751751896185383
mae: 0.031468573551771545
r2: 0.7451849584710463
pearson: 0.8713752344853484

=== Experiment 1849 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005608010414456071
rmse: 0.07488665044222549
mae: 0.02732173730634588
r2: 0.7471372786863182
pearson: 0.8656747007957507

=== Experiment 1913 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006093360116006769
rmse: 0.07805997768387311
mae: 0.028839293754277344
r2: 0.725253074262137
pearson: 0.8526056494405851

=== Experiment 1421 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007352863100296361
rmse: 0.08574883731162984
mae: 0.03821015895967842
r2: 0.668462639050176
pearson: 0.8281393377843459

=== Experiment 1749 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006950077228786683
rmse: 0.08336712318886075
mae: 0.038750646673028445
r2: 0.6866240767169282
pearson: 0.8289794124257365

=== Experiment 1653 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.004503179884531208
rmse: 0.06710573659927449
mae: 0.02433501860124148
r2: 0.7969536010075267
pearson: 0.8959013369481559

=== Experiment 1970 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006072016025642612
rmse: 0.0779231417849833
mae: 0.03210674871366525
r2: 0.7262154699024044
pearson: 0.8524821322846943

=== Experiment 1973 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006867138106298477
rmse: 0.08286819719469271
mae: 0.035840135669729385
r2: 0.6903637652456219
pearson: 0.8384312932199258

=== Experiment 1135 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005349920314584387
rmse: 0.07314314947132361
mae: 0.031026433489508665
r2: 0.7587744476953924
pearson: 0.8726300633700976

=== Experiment 1892 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006006790843762186
rmse: 0.07750348923604786
mae: 0.027231040104397718
r2: 0.7291564446455947
pearson: 0.8547882201012066

=== Experiment 1196 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00596920353515309
rmse: 0.07726062085663751
mae: 0.033238406549692255
r2: 0.7308512398473392
pearson: 0.8560403352348097

=== Experiment 1748 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006007735610777598
rmse: 0.07750958399306242
mae: 0.028291445207385493
r2: 0.7291138455166934
pearson: 0.8549210381963387

=== Experiment 1693 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005524475007018389
rmse: 0.07432681216773923
mae: 0.030836193409744833
r2: 0.7509038534409387
pearson: 0.8718273679882469

=== Experiment 1468 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.004955826160842645
rmse: 0.07039762894332909
mae: 0.02554732023537728
r2: 0.7765439796335056
pearson: 0.8831737546249083

=== Experiment 1719 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006474707672040423
rmse: 0.0804655682390948
mae: 0.03176781065654019
r2: 0.7080582808044746
pearson: 0.8442394778353896

=== Experiment 1505 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0051965716607308395
rmse: 0.07208725033409749
mae: 0.030052436227766374
r2: 0.7656888710037447
pearson: 0.878668385108035

=== Experiment 1366 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.004634156792351535
rmse: 0.06807464133105319
mae: 0.024262212340379282
r2: 0.7910479098812538
pearson: 0.8896232168930005

=== Experiment 1901 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00743415311146077
rmse: 0.08622153507947286
mae: 0.03834815482344633
r2: 0.664797308225243
pearson: 0.8272045277328696

=== Experiment 1586 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006420034527106788
rmse: 0.08012511795377769
mae: 0.028778816809641276
r2: 0.7105234688460409
pearson: 0.8504113390459954

=== Experiment 1878 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0066335118250261515
rmse: 0.08144637392190122
mae: 0.02834852804661074
r2: 0.7008978714414011
pearson: 0.8563905357776831

=== Experiment 1413 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00613057522296221
rmse: 0.07829798990371471
mae: 0.033328596282897166
r2: 0.7235750614691374
pearson: 0.8539654846747409

=== Experiment 1442 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005771313532125383
rmse: 0.07596916171793251
mae: 0.028092009920027753
r2: 0.7397740129858075
pearson: 0.8619930135190893

=== Experiment 1009 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00689546583363907
rmse: 0.08303894166979171
mae: 0.03528026302457408
r2: 0.6890864804878208
pearson: 0.8314358287819822

=== Experiment 1006 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007292362556726475
rmse: 0.08539533100074309
mae: 0.04159451460927655
r2: 0.6711905819314166
pearson: 0.827419121942486

=== Experiment 1014 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00724038458509641
rmse: 0.085090449435271
mae: 0.04677145202586232
r2: 0.6735342457949614
pearson: 0.8301288105153767

=== Experiment 1013 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0060802343185716356
rmse: 0.07797585728013277
mae: 0.03349639287854388
r2: 0.725844910691382
pearson: 0.8524439968448845

=== Experiment 1016 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007508984779478326
rmse: 0.0866543985004704
mae: 0.03654445300349142
r2: 0.6614231812502671
pearson: 0.8321565564317845

=== Experiment 1020 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007126145601272886
rmse: 0.08441650076420419
mae: 0.04278798793263096
r2: 0.6786852312951422
pearson: 0.8277760264716784

=== Experiment 1025 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006584868592429877
rmse: 0.08114720323233499
mae: 0.03488523028017967
r2: 0.7030911733896432
pearson: 0.8438859027972238

=== Experiment 1003 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008482062832210565
rmse: 0.09209811524787337
mae: 0.04222957074480651
r2: 0.6175475201369742
pearson: 0.7866030126951175

=== Experiment 1028 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007097741721054195
rmse: 0.08424809624587487
mae: 0.046095457837136096
r2: 0.6799659497526993
pearson: 0.8315313137299605

=== Experiment 1022 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008315367520962359
rmse: 0.09118863701669391
mae: 0.05066637535793899
r2: 0.6250637383529385
pearson: 0.8190999736943873

=== Experiment 1030 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006322489772203182
rmse: 0.07951408536984615
mae: 0.036609252998293984
r2: 0.7149217189119148
pearson: 0.8467125361639629

=== Experiment 1032 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006433451081512889
rmse: 0.08020879678384965
mae: 0.03377442435506541
r2: 0.709918522312947
pearson: 0.8425922769363882

=== Experiment 1005 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006642772713374952
rmse: 0.0815032067674331
mae: 0.03273594220206757
r2: 0.7004803020617822
pearson: 0.8468606418572447

=== Experiment 1024 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0058821476948682895
rmse: 0.07669516083083919
mae: 0.031013857698513603
r2: 0.7347765493695755
pearson: 0.867979640589391

=== Experiment 1010 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006549178797650738
rmse: 0.08092699671710757
mae: 0.04106308751336514
r2: 0.7047004105279545
pearson: 0.8426114230424795

=== Experiment 1039 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0070994946800778305
rmse: 0.08425849915633336
mae: 0.045276433992631315
r2: 0.6798869096018598
pearson: 0.8317113663951413

=== Experiment 1047 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006995885617520477
rmse: 0.0836414109010631
mae: 0.0345798173780017
r2: 0.684558596630735
pearson: 0.8279229500071207

=== Experiment 1042 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007231109615526443
rmse: 0.0850359313203921
mae: 0.04236915847011757
r2: 0.673952450090645
pearson: 0.8375214082609103

=== Experiment 1058 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0076141484044619035
rmse: 0.0872590878044339
mae: 0.040158679534598826
r2: 0.6566813996858087
pearson: 0.8107520767650506

=== Experiment 1060 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006609736595789892
rmse: 0.08130028656646845
mae: 0.03938590025650294
r2: 0.7019698860633854
pearson: 0.8402246738621288

=== Experiment 1040 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007902360478401772
rmse: 0.08889522190985166
mae: 0.038143098938604986
r2: 0.6436860441236947
pearson: 0.8111919901415701

=== Experiment 1017 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006919208656806634
rmse: 0.08318178079848156
mae: 0.03311074106162924
r2: 0.6880159270412111
pearson: 0.8372970364568247

=== Experiment 1019 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.012120144155318863
rmse: 0.11009152626482595
mae: 0.04819591713416047
r2: 0.4535080345200644
pearson: 0.7001757472207389

=== Experiment 1018 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006170529289151182
rmse: 0.07855271662489581
mae: 0.03418205587920399
r2: 0.7217735502098059
pearson: 0.8541129249182365

=== Experiment 1002 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00641004295010704
rmse: 0.08006274383324019
mae: 0.038369720237369
r2: 0.7109739846553302
pearson: 0.8531954429671849

=== Experiment 1054 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006505985457735869
rmse: 0.08065968917455527
mae: 0.034946659052387745
r2: 0.7066479792138733
pearson: 0.8453799177414099

=== Experiment 1063 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006815395395575358
rmse: 0.08255540803348596
mae: 0.0413525773584684
r2: 0.6926968212984186
pearson: 0.8369310955862452

=== Experiment 1089 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0069769435138914105
rmse: 0.08352810014534875
mae: 0.03752073711181303
r2: 0.6854126877463124
pearson: 0.8283835624072265

=== Experiment 1055 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061425745137915645
rmse: 0.0783745782367699
mae: 0.03474368571975811
r2: 0.7230340187269336
pearson: 0.8572551781782706

=== Experiment 1044 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007129322589841725
rmse: 0.08443531601078855
mae: 0.034835332085775585
r2: 0.6785419822788727
pearson: 0.8449071715266531

=== Experiment 1067 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007954131163801233
rmse: 0.08918593590808604
mae: 0.036072747479617674
r2: 0.6413517267050541
pearson: 0.8284263840357378

=== Experiment 1011 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00627637725669449
rmse: 0.07922359027899764
mae: 0.03093017920698492
r2: 0.7170009119405314
pearson: 0.8501548139272546

=== Experiment 1065 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006932037630989629
rmse: 0.08325885917420217
mae: 0.03634962451908215
r2: 0.687437474241763
pearson: 0.8298469088584343

=== Experiment 1015 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005728881777188497
rmse: 0.07568937691108639
mae: 0.0319304956759778
r2: 0.741687242140268
pearson: 0.8626643467071798

=== Experiment 1083 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00781638188714894
rmse: 0.08841030419102142
mae: 0.0398103729667057
r2: 0.6475627809612129
pearson: 0.8077185595491408

=== Experiment 1139 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006912897510683444
rmse: 0.08314383627595881
mae: 0.03761544834139136
r2: 0.6883004938421577
pearson: 0.8303634213743406

=== Experiment 1035 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006702524143376571
rmse: 0.0818689449264895
mae: 0.041238312457726106
r2: 0.6977861363816249
pearson: 0.8476210753450083

=== Experiment 1049 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00565232870216355
rmse: 0.0751819705924469
mae: 0.029108468144902398
r2: 0.7451389865995579
pearson: 0.864253876770423

=== Experiment 1107 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006988796685113187
rmse: 0.08359902323061667
mae: 0.0379522363266991
r2: 0.684878233472902
pearson: 0.8357007986931168

=== Experiment 1105 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007127887450714341
rmse: 0.08442681713007036
mae: 0.039604397244505936
r2: 0.6786066920704736
pearson: 0.8310497038649892

=== Experiment 1131 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006603584207540991
rmse: 0.08126244032479575
mae: 0.0341347491924165
r2: 0.7022472945416551
pearson: 0.8380802349582038

=== Experiment 1007 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005919618187501795
rmse: 0.07693905502085267
mae: 0.030505566491686262
r2: 0.7330870213487557
pearson: 0.8563737042982729

=== Experiment 1123 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006684408542658274
rmse: 0.08175823226231273
mae: 0.03507547034255521
r2: 0.6986029608447271
pearson: 0.8360302683170985

=== Experiment 1087 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007392273188926143
rmse: 0.08597832976352904
mae: 0.04722888044986814
r2: 0.6666856554995657
pearson: 0.8241030841932254

=== Experiment 1077 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007681943921485966
rmse: 0.08764669943292769
mae: 0.04231635394327308
r2: 0.6536245296622827
pearson: 0.829320022196343

=== Experiment 1104 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007222582626531217
rmse: 0.08498577896643189
mae: 0.037234080838433284
r2: 0.674336928271425
pearson: 0.8304167363791584

=== Experiment 1086 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007308692116882619
rmse: 0.08549088908698177
mae: 0.03548723975597056
r2: 0.6704542892511651
pearson: 0.8263234882723479

=== Experiment 1124 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076651304646011015
rmse: 0.08755073080563693
mae: 0.03911026404169202
r2: 0.6543826410330529
pearson: 0.8383202869997801

=== Experiment 1101 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007164681044238698
rmse: 0.08464443894455617
mae: 0.043018368807189795
r2: 0.6769476851325592
pearson: 0.8288966877777826

=== Experiment 1151 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007227444156500498
rmse: 0.08501437617544752
mae: 0.03913304198742924
r2: 0.6741177240248329
pearson: 0.821404552501484

=== Experiment 1097 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006916220441035606
rmse: 0.0831638168979491
mae: 0.037172121883786845
r2: 0.6881506643751123
pearson: 0.8306554374787644

=== Experiment 1061 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00587869215916681
rmse: 0.07667262979164606
mae: 0.03323063281544444
r2: 0.7349323579534575
pearson: 0.8583515857808564

=== Experiment 1062 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0074402016856392125
rmse: 0.08625660372191345
mae: 0.03086547501708279
r2: 0.664524580677718
pearson: 0.8287952082960358

=== Experiment 1142 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00692705380243226
rmse: 0.083228924073499
mae: 0.042296505309121823
r2: 0.6876621928778641
pearson: 0.8332610990341073

=== Experiment 1145 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006787585344628975
rmse: 0.08238680321889529
mae: 0.03704708988426473
r2: 0.6939507642554517
pearson: 0.834718974650113

=== Experiment 1121 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007966785811638387
rmse: 0.08925685302338632
mae: 0.035405092207842145
r2: 0.6407811342038132
pearson: 0.8268513419321163

=== Experiment 1034 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005869407189442431
rmse: 0.07661205642353187
mae: 0.03145910365733955
r2: 0.735351013151702
pearson: 0.8576529646234351

=== Experiment 1038 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006978939845198915
rmse: 0.08354004934879387
mae: 0.03580912647714712
r2: 0.6853226740463094
pearson: 0.834740719025183

=== Experiment 1136 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008114181304709187
rmse: 0.09007875057253617
mae: 0.053029343386183085
r2: 0.6341351362949685
pearson: 0.8179330467010902

=== Experiment 1173 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006852752370129575
rmse: 0.0827813527923383
mae: 0.03969290286363247
r2: 0.6910124117578309
pearson: 0.8339983261049391

=== Experiment 1026 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057527798912222275
rmse: 0.07584708228549222
mae: 0.0304171775804653
r2: 0.7406096866968515
pearson: 0.8632655348030106

=== Experiment 1170 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008328520819247837
rmse: 0.09126072988557475
mae: 0.03851985363042055
r2: 0.6244706619224556
pearson: 0.8045589293793126

=== Experiment 1133 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006605044856823984
rmse: 0.08127142706279977
mae: 0.03274815628446354
r2: 0.7021814345083659
pearson: 0.8417435151739869

=== Experiment 1147 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007536909596226348
rmse: 0.08681537649648446
mae: 0.04786053786828359
r2: 0.6601640635537505
pearson: 0.8309067930568322

=== Experiment 1082 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006966974586911879
rmse: 0.08346840472245698
mae: 0.039947303700351405
r2: 0.6858621822761591
pearson: 0.8361969089327322

=== Experiment 1091 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006338182766720573
rmse: 0.07961270480721386
mae: 0.03288919847396332
r2: 0.7142141286961405
pearson: 0.8451123424834999

=== Experiment 1150 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007200115129293121
rmse: 0.08485349214553943
mae: 0.03148321778241428
r2: 0.6753499778331888
pearson: 0.8277279283876676

=== Experiment 1134 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006189767055163395
rmse: 0.07867507264161498
mae: 0.03393970213547482
r2: 0.7209061278075035
pearson: 0.851568031358887

=== Experiment 1120 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006955932854740265
rmse: 0.08340223531021375
mae: 0.03933722820903136
r2: 0.6863600491199392
pearson: 0.8339893548434699

=== Experiment 1174 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007255963073518516
rmse: 0.08518194100581716
mae: 0.03680454567683841
r2: 0.6728318186086257
pearson: 0.8215382937230836

=== Experiment 1090 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006981254000659402
rmse: 0.08355389877593626
mae: 0.03665493684364977
r2: 0.685218329795134
pearson: 0.8284889586706912

=== Experiment 1052 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006757703288557455
rmse: 0.0822052509792255
mae: 0.03693839390054589
r2: 0.6952981330116179
pearson: 0.8341066581894896

=== Experiment 1127 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005844119135816199
rmse: 0.07644683862538855
mae: 0.029624236187957637
r2: 0.7364912403595854
pearson: 0.8584721169897322

=== Experiment 1126 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006330121704734963
rmse: 0.07956206196884896
mae: 0.036561308448437684
r2: 0.7145775984331271
pearson: 0.8461100750883531

=== Experiment 1159 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007567471686582362
rmse: 0.08699121614612801
mae: 0.035206440036454685
r2: 0.6587860323510026
pearson: 0.813612607326287

=== Experiment 1158 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006561683881981248
rmse: 0.08100422138371091
mae: 0.038205878062901814
r2: 0.7041365617794004
pearson: 0.8401464844906135

=== Experiment 1027 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006411486954537163
rmse: 0.08007176128034879
mae: 0.029922457032688515
r2: 0.7109088751311309
pearson: 0.8462131925282703

=== Experiment 1157 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0071150396428719025
rmse: 0.08435069438286742
mae: 0.042358685281058274
r2: 0.6791859940713363
pearson: 0.8362638635138835

=== Experiment 1148 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006867402554000194
rmse: 0.08286979277155333
mae: 0.036749096240525914
r2: 0.6903518414151437
pearson: 0.843390178186227

=== Experiment 1167 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070384933213315124
rmse: 0.08389572886226994
mae: 0.0333924927061233
r2: 0.68263743401898
pearson: 0.842441170450479

=== Experiment 1073 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008994120572044092
rmse: 0.09483733743649751
mae: 0.04390590971809803
r2: 0.594459062021727
pearson: 0.7760810620816656

=== Experiment 1165 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00846846402465954
rmse: 0.09202425780553485
mae: 0.04539822251280856
r2: 0.6181606843841569
pearson: 0.7975309873405394

=== Experiment 1179 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007526625260866449
rmse: 0.0867561252066184
mae: 0.046374000314549274
r2: 0.6606277797086462
pearson: 0.8329852113012112

=== Experiment 1162 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008792868341239355
rmse: 0.09377029562307754
mae: 0.03499456209206411
r2: 0.6035334365308318
pearson: 0.8039396736664252

=== Experiment 1249 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006819392794360986
rmse: 0.08257961488382581
mae: 0.033763853890646296
r2: 0.6925165803465646
pearson: 0.8333441455936582

=== Experiment 1187 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006699685579631629
rmse: 0.08185160706810606
mae: 0.038754264134936635
r2: 0.6979141259715369
pearson: 0.8365998530044667

=== Experiment 1046 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005955376563411961
rmse: 0.07717108631742825
mae: 0.029922426393780125
r2: 0.7314746919174311
pearson: 0.8553316145617622

=== Experiment 1197 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007175671500728091
rmse: 0.08470933538122047
mae: 0.034013803506346245
r2: 0.6764521302867224
pearson: 0.8235314013676516

=== Experiment 1208 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006459550600468581
rmse: 0.08037132946809192
mae: 0.03655833928772739
r2: 0.7087417064905117
pearson: 0.8424399276173201

=== Experiment 1144 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006124995097694139
rmse: 0.07826234789280308
mae: 0.03063665095712112
r2: 0.7238266668614737
pearson: 0.8527797674491221

=== Experiment 1242 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007460908302865592
rmse: 0.08637654949617744
mae: 0.043742216491434616
r2: 0.6635909284206598
pearson: 0.8191372339804859

=== Experiment 1202 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007853060099196296
rmse: 0.08861749318952944
mae: 0.04584339700895749
r2: 0.6459089765233132
pearson: 0.8256178910910001

=== Experiment 1258 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0066488267249248285
rmse: 0.08154033802312098
mae: 0.035943772158098866
r2: 0.7002073293455726
pearson: 0.8374132065345881

=== Experiment 1257 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007506511018448392
rmse: 0.08664012360591594
mae: 0.03904071004168507
r2: 0.6615347220463215
pearson: 0.8161280234918483

=== Experiment 1149 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007364925233177337
rmse: 0.0858191425800639
mae: 0.04694270237862586
r2: 0.6679187627875236
pearson: 0.8244872562640244

=== Experiment 1233 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006791060857981404
rmse: 0.08240789317766474
mae: 0.03636383233738229
r2: 0.6937940548880812
pearson: 0.8362813058065003

=== Experiment 1085 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0066257136431325985
rmse: 0.08139848673736262
mae: 0.035800532855149406
r2: 0.701249488030739
pearson: 0.8464833531789766

=== Experiment 1210 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006799439369657601
rmse: 0.08245871312152284
mae: 0.03624774653257392
r2: 0.6934162714842662
pearson: 0.837055513996433

=== Experiment 1185 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0074114347136581315
rmse: 0.08608968993821578
mae: 0.040846782272229806
r2: 0.6658216707830323
pearson: 0.8170610513917261

=== Experiment 1152 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0069197217636811845
rmse: 0.0831848649916629
mae: 0.038401003454586365
r2: 0.6879927912780731
pearson: 0.830152342062068

=== Experiment 1059 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005913564433723294
rmse: 0.07689970372974979
mae: 0.030894054105052495
r2: 0.7333599824421675
pearson: 0.8575094113778196

=== Experiment 1226 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006647424220915408
rmse: 0.08153173750703102
mae: 0.04006926019239323
r2: 0.7002705676340681
pearson: 0.8387097228387029

=== Experiment 1241 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007906090748043314
rmse: 0.08891620070630163
mae: 0.03640755825965482
r2: 0.6435178479073748
pearson: 0.8301893902172355

=== Experiment 1099 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005909849417951032
rmse: 0.07687554499287164
mae: 0.03014979308061828
r2: 0.7335274908682353
pearson: 0.8564673156888456

=== Experiment 1214 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006560049904713581
rmse: 0.08099413500194678
mae: 0.03702649104119408
r2: 0.7042102370952312
pearson: 0.849039339317438

=== Experiment 1066 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006192659404369081
rmse: 0.07869345210606205
mae: 0.03579884702217553
r2: 0.7207757130548396
pearson: 0.8498103305950185

=== Experiment 1230 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0069218753097499836
rmse: 0.083197808322996
mae: 0.033035925161692244
r2: 0.6878956888336791
pearson: 0.8342890712595847

=== Experiment 1286 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006790798037417411
rmse: 0.08240629853001172
mae: 0.03582413899335885
r2: 0.6938059053516353
pearson: 0.8373001760634253

=== Experiment 1287 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007213270708637336
rmse: 0.08493097614320311
mae: 0.03783940860470811
r2: 0.6747567985507714
pearson: 0.8216187007979473

=== Experiment 1168 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006273508356236746
rmse: 0.07920548185723478
mae: 0.034873349623016935
r2: 0.7171302693994714
pearson: 0.8470248814883025

=== Experiment 1041 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006100839610380681
rmse: 0.07810787162879732
mae: 0.03315471691363461
r2: 0.7249158271527945
pearson: 0.8533567978284227

=== Experiment 1100 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007990749112664192
rmse: 0.08939099010898242
mae: 0.042568547445646615
r2: 0.6397006395076151
pearson: 0.8001667288799911

=== Experiment 1201 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007357524475961139
rmse: 0.08577601340678605
mae: 0.03484104614791972
r2: 0.6682524596730797
pearson: 0.8307531691511784

=== Experiment 1050 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006578153657252771
rmse: 0.08110581765356152
mae: 0.03734457502582492
r2: 0.7033939468613097
pearson: 0.8387169722488004

=== Experiment 1276 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006590242713403758
rmse: 0.0811803098873351
mae: 0.035364417752040925
r2: 0.7028488566402624
pearson: 0.8414894500023891

=== Experiment 1199 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0068367833572092005
rmse: 0.08268484357613069
mae: 0.03895454263255272
r2: 0.6917324475219107
pearson: 0.8414361635742688

=== Experiment 1169 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006817446162329482
rmse: 0.08256782764690786
mae: 0.035603867155726346
r2: 0.6926043531280868
pearson: 0.8328682539628925

=== Experiment 1096 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00582650140016039
rmse: 0.07633152297812738
mae: 0.032054200041473836
r2: 0.7372856163061474
pearson: 0.861384716664286

=== Experiment 1218 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008506180605711396
rmse: 0.09222895752263166
mae: 0.053669763596946
r2: 0.6164600603448656
pearson: 0.8079959447867068

=== Experiment 1272 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006809038635309141
rmse: 0.08251689908927226
mae: 0.033271332192365125
r2: 0.6929834448209984
pearson: 0.8326027842659235

=== Experiment 1195 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006823530213975827
rmse: 0.08260466218062892
mae: 0.04343957048994499
r2: 0.692330025917149
pearson: 0.8394579930788642

=== Experiment 1053 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006230730388286109
rmse: 0.07893497569700081
mae: 0.03247702351162519
r2: 0.7190591091463421
pearson: 0.8500061383391879

=== Experiment 1289 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00843369750944146
rmse: 0.09183516488492553
mae: 0.039923498157682716
r2: 0.6197282912534267
pearson: 0.8113435743280195

=== Experiment 1263 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008689096461050117
rmse: 0.09321532310221381
mae: 0.05409257895282327
r2: 0.6082124649350671
pearson: 0.7940162793272665

=== Experiment 1268 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006022189133253989
rmse: 0.0776027649845931
mae: 0.0347519583663826
r2: 0.7284621425497149
pearson: 0.855282475967397

=== Experiment 1279 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007146930161823692
rmse: 0.08453951834393009
mae: 0.03426417466268626
r2: 0.677748064607891
pearson: 0.827628509509696

=== Experiment 1143 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0060804309513963406
rmse: 0.07797711812702711
mae: 0.03645079202189268
r2: 0.7258360446038608
pearson: 0.8523115520921679

=== Experiment 1314 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006916315194604599
rmse: 0.0831643865762539
mae: 0.03963634778184056
r2: 0.6881463919783906
pearson: 0.831543894570225

=== Experiment 1320 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00803862337595758
rmse: 0.08965837036193319
mae: 0.036993992258396696
r2: 0.637542010046791
pearson: 0.8283634223797876

=== Experiment 1219 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007214920867169756
rmse: 0.08494069029134244
mae: 0.04106514463347111
r2: 0.6746823936288349
pearson: 0.8271791884557489

=== Experiment 1307 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006826222365087511
rmse: 0.082620955968129
mae: 0.0367527525075762
r2: 0.6922086380084294
pearson: 0.8324709849593255

=== Experiment 1334 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007471653998380387
rmse: 0.08643872973604128
mae: 0.04226758360044109
r2: 0.66310640973944
pearson: 0.8168096673557861

=== Experiment 1295 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0069184669300299305
rmse: 0.08317732221002266
mae: 0.03706944047102311
r2: 0.6880493711751137
pearson: 0.829932446404778

=== Experiment 1304 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007439209251607722
rmse: 0.08625085072976221
mae: 0.041550505784335175
r2: 0.6645693290913941
pearson: 0.8203046243750683

=== Experiment 1106 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005718322671044766
rmse: 0.07561959184658938
mae: 0.029894008401218863
r2: 0.7421633475888709
pearson: 0.8695757857667317

=== Experiment 1111 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008366081793412448
rmse: 0.0914662877426019
mae: 0.042316913162754856
r2: 0.6227770541292222
pearson: 0.7999894053542342

=== Experiment 1043 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006616093019014229
rmse: 0.08133936942842765
mae: 0.036151766118794365
r2: 0.7016832777378759
pearson: 0.8383140717228331

=== Experiment 1303 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00636719914965736
rmse: 0.0797947313402167
mae: 0.03262787544683486
r2: 0.7129057927606075
pearson: 0.8556188395114346

=== Experiment 1301 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006265050436770572
rmse: 0.07915207158862346
mae: 0.032730385211512156
r2: 0.7175116332655862
pearson: 0.8473340274591676

=== Experiment 1352 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007161614601780862
rmse: 0.08462632333843213
mae: 0.036598449698974446
r2: 0.6770859496733386
pearson: 0.831474639897153

=== Experiment 1282 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007119917834049797
rmse: 0.08437960555756228
mae: 0.04599733687313214
r2: 0.6789660385781813
pearson: 0.831857932072921

=== Experiment 1261 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007132288425065393
rmse: 0.0844528769496066
mae: 0.04359805244933975
r2: 0.6784082540740015
pearson: 0.828951930699856

=== Experiment 1330 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007056512750407785
rmse: 0.08400305203031486
mae: 0.04191252251851943
r2: 0.6818249458928882
pearson: 0.8292408947083035

=== Experiment 1125 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.016581401986084498
rmse: 0.12876879274919253
mae: 0.053584717908917986
r2: 0.25235188248057383
pearson: 0.5785215203689933

=== Experiment 1244 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006854335998809234
rmse: 0.08279091736905222
mae: 0.04058965024822402
r2: 0.6909410066376735
pearson: 0.8427674410233588

=== Experiment 1057 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00611254986777451
rmse: 0.0781827977740277
mae: 0.0314899377555836
r2: 0.7243878168009852
pearson: 0.8552796762685164

=== Experiment 1031 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006107883046629698
rmse: 0.07815294649998615
mae: 0.029939397864457467
r2: 0.7245982417123797
pearson: 0.8552539888995567

=== Experiment 1154 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006633109786067026
rmse: 0.08144390576382635
mae: 0.03208054488036409
r2: 0.7009159992011171
pearson: 0.8411492385690943

=== Experiment 1321 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008355213229791849
rmse: 0.09140685548574488
mae: 0.04223943139460825
r2: 0.6232671128792544
pearson: 0.7972926433863902

=== Experiment 1308 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007097647508643083
rmse: 0.08424753710728333
mae: 0.04811304691295621
r2: 0.6799701977488504
pearson: 0.836533982969715

=== Experiment 1260 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007965355915415208
rmse: 0.08924884265588662
mae: 0.03814595046853927
r2: 0.640845607595171
pearson: 0.8149022332232925

=== Experiment 1369 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007017493293951132
rmse: 0.08377047984792216
mae: 0.03326679792685183
r2: 0.6835843160107425
pearson: 0.8318358400544219

=== Experiment 1328 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007318634431789047
rmse: 0.08554901771375897
mae: 0.04931165813807823
r2: 0.6700059946479817
pearson: 0.8301016601162563

=== Experiment 1309 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0067843643837553645
rmse: 0.08236725310313174
mae: 0.03581918588823517
r2: 0.6940959959635894
pearson: 0.8341663901168243

=== Experiment 1205 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006498730051097441
rmse: 0.08061470120950298
mae: 0.03460322232990084
r2: 0.7069751223058512
pearson: 0.8422321365644184

=== Experiment 1265 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072548238830518665
rmse: 0.08517525393594003
mae: 0.036302948698108684
r2: 0.6728831842053711
pearson: 0.8210687085902871

=== Experiment 1324 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006660664650278852
rmse: 0.08161289512251635
mae: 0.04076581181004127
r2: 0.6996735625016293
pearson: 0.8474657943706634

=== Experiment 1037 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0070339211518684564
rmse: 0.08386847531622628
mae: 0.03890260367348273
r2: 0.6828435911277037
pearson: 0.8288782444563102

=== Experiment 1247 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006210711965000218
rmse: 0.07880807043063684
mae: 0.03735361429236001
r2: 0.7199617310415211
pearson: 0.850034427905121

=== Experiment 1070 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006397448378948487
rmse: 0.07998405077856764
mae: 0.034117489527133005
r2: 0.7115418683255748
pearson: 0.8454032489929647

=== Experiment 1227 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006766588004411775
rmse: 0.08225927306031688
mae: 0.035654646660170576
r2: 0.6948975250842091
pearson: 0.8494484633644274

=== Experiment 1215 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008139200789154758
rmse: 0.09021751930282032
mae: 0.039170661120640674
r2: 0.6330070187532351
pearson: 0.8047897273171167

=== Experiment 1339 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0082044844247223
rmse: 0.09057860909023885
mae: 0.03473753811506994
r2: 0.63006340835902
pearson: 0.8274161912081325

=== Experiment 1353 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0071434756815047405
rmse: 0.08451908471762304
mae: 0.03968578478531564
r2: 0.6779038256050396
pearson: 0.846099649485207

=== Experiment 1189 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065506170314757065
rmse: 0.08093588222460855
mae: 0.03636504125539621
r2: 0.7046355611978017
pearson: 0.8401410071834271

=== Experiment 1326 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007799960472620352
rmse: 0.08831738488327398
mae: 0.04083270676410779
r2: 0.6483032153146897
pearson: 0.8114086740367416

=== Experiment 1293 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011407465458898926
rmse: 0.10680573701304123
mae: 0.045380710840924564
r2: 0.48564240326775754
pearson: 0.7057079996594096

=== Experiment 1355 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00702295925971239
rmse: 0.08380309815103729
mae: 0.04287959808523897
r2: 0.6833378580203229
pearson: 0.8406391326501991

=== Experiment 1306 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006928837073980129
rmse: 0.08323963643589591
mae: 0.04130329490338928
r2: 0.6875817859486487
pearson: 0.8355553234773829

=== Experiment 1023 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007074647790057609
rmse: 0.08411092550945809
mae: 0.03569659996032018
r2: 0.681007244936919
pearson: 0.8300730031343911

=== Experiment 1178 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006255694192581133
rmse: 0.07909294654127594
mae: 0.037002174190570405
r2: 0.7179335021980899
pearson: 0.8484592425734987

=== Experiment 1273 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005980067992476714
rmse: 0.07733089933834156
mae: 0.031900367123079997
r2: 0.7303613662484304
pearson: 0.8566796045917235

=== Experiment 1193 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006013606775197319
rmse: 0.07754744854085993
mae: 0.03146847021314787
r2: 0.7288491172971054
pearson: 0.8554414608414067

=== Experiment 1373 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007242372074964848
rmse: 0.08510212732338039
mae: 0.04152075196957095
r2: 0.6734446307515547
pearson: 0.823037278962321

=== Experiment 1342 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0069341543142837974
rmse: 0.08327156966386426
mae: 0.039261607979348676
r2: 0.6873420339236527
pearson: 0.8314730555027543

=== Experiment 1113 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006603878853589479
rmse: 0.08126425323344502
mae: 0.03067821594497453
r2: 0.7022340090810124
pearson: 0.8425026154962737

=== Experiment 1164 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006064482550550116
rmse: 0.07787478764369195
mae: 0.0363568166445168
r2: 0.7265551509785889
pearson: 0.8547017758032722

=== Experiment 1264 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006920176706068666
rmse: 0.08318759947293025
mae: 0.03664739304364733
r2: 0.6879722781260571
pearson: 0.8362156510435738

=== Experiment 1333 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006580910564258771
rmse: 0.0811228116146055
mae: 0.042647107696156546
r2: 0.7032696391378168
pearson: 0.8447919086359795

=== Experiment 1224 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006252985572351075
rmse: 0.07907582166725222
mae: 0.03252847377829508
r2: 0.7180556326921085
pearson: 0.8593599519642914

=== Experiment 1029 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010569904570047849
rmse: 0.10281004119271546
mae: 0.05270408076693334
r2: 0.5234076551073142
pearson: 0.724813331195442

=== Experiment 1376 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007016108325652274
rmse: 0.08376221299400032
mae: 0.043965933575278796
r2: 0.6836467636217668
pearson: 0.8332199994112065

=== Experiment 1245 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006136085433351891
rmse: 0.0783331694325711
mae: 0.0319978546828419
r2: 0.7233266085078958
pearson: 0.8540783317021536

=== Experiment 1048 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008840246723664229
rmse: 0.094022586242159
mae: 0.04055891858990023
r2: 0.601397166120115
pearson: 0.7763859648534209

=== Experiment 1375 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006576476881460113
rmse: 0.08109548003101105
mae: 0.03675544795519834
r2: 0.7034695519437955
pearson: 0.8499370859278865

=== Experiment 1198 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006639405183306428
rmse: 0.08148254526772239
mae: 0.029886722469923976
r2: 0.7006321425104087
pearson: 0.8413935475416158

=== Experiment 1275 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006260006600720933
rmse: 0.07912020349266635
mae: 0.031854405431237826
r2: 0.7177390576131024
pearson: 0.8549744189259857

=== Experiment 1417 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006477846336228377
rmse: 0.08048506902667338
mae: 0.03447149216517912
r2: 0.7079167598176715
pearson: 0.8422271193167972

=== Experiment 1188 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006716391909375408
rmse: 0.08195359607348178
mae: 0.0418038126057512
r2: 0.6971608449164078
pearson: 0.838395377237918

=== Experiment 1311 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007577871762996959
rmse: 0.08705097221167009
mae: 0.04097794686381801
r2: 0.658317097482892
pearson: 0.8155549607357913

=== Experiment 1137 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00673674562301325
rmse: 0.08207768041930309
mae: 0.03227968200666021
r2: 0.6962431049268321
pearson: 0.8344866710690655

=== Experiment 1394 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00649048737771148
rmse: 0.08056356110371164
mae: 0.03337767684850705
r2: 0.7073467808209464
pearson: 0.8415754269032544

=== Experiment 1180 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006229514707250058
rmse: 0.078927274799337
mae: 0.03377371450479413
r2: 0.7191139236691955
pearson: 0.8483710201531951

=== Experiment 1235 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007057535369784524
rmse: 0.08400913860875212
mae: 0.03340274363850731
r2: 0.6817788364352801
pearson: 0.8262618453341296

=== Experiment 1449 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007199883897642258
rmse: 0.08485212959992376
mae: 0.03976083516148687
r2: 0.6753604039665534
pearson: 0.8240290127293295

=== Experiment 1470 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008779267336347468
rmse: 0.09369774456382324
mae: 0.04214177767010849
r2: 0.604146699855147
pearson: 0.8013481148068242

=== Experiment 1416 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00784378485226884
rmse: 0.08856514468044886
mae: 0.042447302625717144
r2: 0.6463271933249237
pearson: 0.8110607974076158

=== Experiment 1332 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006986180592200504
rmse: 0.0835833750945755
mae: 0.03584112924771011
r2: 0.6849961919509043
pearson: 0.8278079124079911

=== Experiment 1012 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006073194937649489
rmse: 0.07793070599994259
mae: 0.028673561394501612
r2: 0.726162313278893
pearson: 0.8527911551727995

=== Experiment 1475 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007239114541631318
rmse: 0.08508298620541782
mae: 0.043627526113995146
r2: 0.6735915114958135
pearson: 0.8229754949282952

=== Experiment 1237 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006316198401099695
rmse: 0.07947451416082828
mae: 0.0315034313985072
r2: 0.7152053940659266
pearson: 0.8464508039245292

=== Experiment 1371 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007319710683698802
rmse: 0.08555530774708721
mae: 0.04612204068084353
r2: 0.6699574669230657
pearson: 0.827137188738116

=== Experiment 1331 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00790119941381983
rmse: 0.08888869114696105
mae: 0.04782390150607514
r2: 0.6437383960146708
pearson: 0.8160266511160915

=== Experiment 1430 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007850179852838472
rmse: 0.08860124069581911
mae: 0.038994289902695704
r2: 0.6460388455638941
pearson: 0.8123399544207168

=== Experiment 1400 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007381400324167884
rmse: 0.08591507623326586
mae: 0.034630614760026886
r2: 0.6671759081860018
pearson: 0.8424990765856376

=== Experiment 1271 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00651603751946516
rmse: 0.08072197668209792
mae: 0.030827327181093763
r2: 0.7061947361747503
pearson: 0.8461557441986519

=== Experiment 1033 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006185400253927769
rmse: 0.07864731561806651
mae: 0.038694032869521555
r2: 0.7211030249532419
pearson: 0.8523267621804327

=== Experiment 1238 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006504481704299305
rmse: 0.08065036704379779
mae: 0.03160998718271986
r2: 0.7067157827944754
pearson: 0.8414214793256262

=== Experiment 1457 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006921029093194314
rmse: 0.08319272259755846
mae: 0.03823681226329324
r2: 0.6879338443657258
pearson: 0.8312993578537393

=== Experiment 1072 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0060714513418725505
rmse: 0.07791951836268336
mae: 0.029497734800608095
r2: 0.72624093124506
pearson: 0.858100018274883

=== Experiment 1459 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007845474391079752
rmse: 0.08857468256268126
mae: 0.039432753140463836
r2: 0.6462510127635629
pearson: 0.8240907910852019

=== Experiment 1323 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00558439886657479
rmse: 0.07472883557620037
mae: 0.030316226441066622
r2: 0.7482019129880481
pearson: 0.8650167913743088

=== Experiment 1403 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0067218738940327425
rmse: 0.08198703491426398
mae: 0.034783734210441605
r2: 0.6969136646410146
pearson: 0.8406686044396201

=== Experiment 1045 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006163135723077706
rmse: 0.07850564134556004
mae: 0.038069562516714416
r2: 0.7221069228499016
pearson: 0.8524082048050245

=== Experiment 1240 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006354565438838009
rmse: 0.07971552821651506
mae: 0.03457718499835836
r2: 0.7134754412209308
pearson: 0.8508095572124836

=== Experiment 1382 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007611636784485779
rmse: 0.08724469487874767
mae: 0.04543343938471826
r2: 0.6567946475250837
pearson: 0.8286213967878588

=== Experiment 1422 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007065148432314101
rmse: 0.08405443731483841
mae: 0.0407443372540335
r2: 0.6814355667965867
pearson: 0.8300518689432667

=== Experiment 1225 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006749435969856698
rmse: 0.08215495097592536
mae: 0.03719566316870878
r2: 0.6956709027731098
pearson: 0.8459511831518972

=== Experiment 1001 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0055856747325595925
rmse: 0.07473737172632974
mae: 0.03168148827947573
r2: 0.748144384752345
pearson: 0.87015440117159

=== Experiment 1074 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010682680231335812
rmse: 0.103357052160633
mae: 0.04288397424000013
r2: 0.5183226501762042
pearson: 0.7203266694360477

=== Experiment 1493 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007309074201516922
rmse: 0.08549312370896808
mae: 0.03800987923290834
r2: 0.6704370612231728
pearson: 0.8220412487928251

=== Experiment 1501 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006953306393830697
rmse: 0.08338648807709015
mae: 0.03264003249287831
r2: 0.6864784750863577
pearson: 0.8421704872722264

=== Experiment 1364 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006743444748615141
rmse: 0.08211847994583887
mae: 0.03968187422818952
r2: 0.6959410443019536
pearson: 0.8362481797761253

=== Experiment 1450 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008953005563507675
rmse: 0.09462032320547037
mae: 0.03739612122106721
r2: 0.5963129196605348
pearson: 0.7890034540658232

=== Experiment 1478 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007196300224787686
rmse: 0.08483100980648342
mae: 0.03229362896697307
r2: 0.675521990198271
pearson: 0.8225876529459986

=== Experiment 1370 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006806608969884047
rmse: 0.08250217554636997
mae: 0.043544132423659655
r2: 0.6930929973656383
pearson: 0.8384074858993276

=== Experiment 1402 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0069748686638810954
rmse: 0.08351567914997217
mae: 0.04104262872577604
r2: 0.6855062418200661
pearson: 0.8309975400029592

=== Experiment 1255 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006111367400182904
rmse: 0.07817523521028193
mae: 0.029076903858725505
r2: 0.7244411337442471
pearson: 0.8517563899251207

=== Experiment 1112 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.015323326204512893
rmse: 0.12378742345049797
mae: 0.05559988636937068
r2: 0.3090779657501409
pearson: 0.5562193143258655

=== Experiment 1004 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.019954159242399724
rmse: 0.14125919170942372
mae: 0.06294442045697818
r2: 0.10027574225731217
pearson: 0.3196279709214851

=== Experiment 1184 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005891640001503866
rmse: 0.07675701923279633
mae: 0.036334526174514666
r2: 0.7343485454412608
pearson: 0.8611488970859499

=== Experiment 1365 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006153338031586555
rmse: 0.07844321533177076
mae: 0.03864885899946539
r2: 0.7225486964469439
pearson: 0.8557287325120478

=== Experiment 1393 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006923450666665338
rmse: 0.08320727532292677
mae: 0.03720160206341342
r2: 0.6878246566837334
pearson: 0.8309203077113416

=== Experiment 1114 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007639531567806098
rmse: 0.08740441389201176
mae: 0.03714481692380684
r2: 0.6555368840226035
pearson: 0.8243104861551134

=== Experiment 1431 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007370746876189814
rmse: 0.08585305397124679
mae: 0.03552398651301812
r2: 0.667656267466392
pearson: 0.8367087341173811

=== Experiment 1093 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006873831618628904
rmse: 0.08290857385475223
mae: 0.042924960848445416
r2: 0.690061957720683
pearson: 0.8403257248879227

=== Experiment 1213 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00701826189701629
rmse: 0.08377506727551037
mae: 0.042046441974265256
r2: 0.6835496600368226
pearson: 0.8316805258530013

=== Experiment 1315 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00616736099789832
rmse: 0.0785325473794039
mae: 0.03903342547663303
r2: 0.7219164070679259
pearson: 0.8538386369034895

=== Experiment 1278 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006037976625770868
rmse: 0.07770441831563292
mae: 0.027335359811473336
r2: 0.7277502914607357
pearson: 0.866571334808868

=== Experiment 1221 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006315052943141864
rmse: 0.0794673073857537
mae: 0.03616359324022353
r2: 0.7152570422610908
pearson: 0.8465460824443723

=== Experiment 1517 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0071179287032806775
rmse: 0.0843678179359919
mae: 0.03399960928450831
r2: 0.6790557276090776
pearson: 0.8331103633247864

=== Experiment 1414 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007376626637164068
rmse: 0.0858872903121531
mae: 0.035473253256879486
r2: 0.6673911516319436
pearson: 0.8200896694561514

=== Experiment 1095 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005866505227324064
rmse: 0.07659311475141917
mae: 0.030892105688499445
r2: 0.7354818613463681
pearson: 0.8577563261399914

=== Experiment 1267 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006623075245053636
rmse: 0.08138227844594692
mae: 0.04118130579163917
r2: 0.7013684522388715
pearson: 0.8462099719707827

=== Experiment 1102 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009969943452567966
rmse: 0.09984960416830888
mae: 0.04633650706554406
r2: 0.5504596378314
pearson: 0.7425174390124152

=== Experiment 1524 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006584789189429037
rmse: 0.08114671397801046
mae: 0.035620719047708714
r2: 0.7030947536360032
pearson: 0.8409698575454995

=== Experiment 1064 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005043830699324295
rmse: 0.07101993170458766
mae: 0.02740494818980941
r2: 0.7725758937271276
pearson: 0.8791996792851015

=== Experiment 1356 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006641012536714
rmse: 0.08149240784707493
mae: 0.03449677702297694
r2: 0.7005596676527117
pearson: 0.8385713324197753

=== Experiment 1456 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008249159178792626
rmse: 0.0908248819365741
mae: 0.03854039358134088
r2: 0.6280490433608519
pearson: 0.7953088748340895

=== Experiment 1182 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006391516045138279
rmse: 0.07994695769782788
mae: 0.03483428094116233
r2: 0.7118093546460572
pearson: 0.8489574472044132

=== Experiment 1561 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007408176764084618
rmse: 0.08607076602473467
mae: 0.037744847605477506
r2: 0.6659685702953687
pearson: 0.8174484405752597

=== Experiment 1562 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007562997117752876
rmse: 0.08696549383377798
mae: 0.039839595909475806
r2: 0.6589877886901159
pearson: 0.81613200564416

=== Experiment 1317 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006690242050709912
rmse: 0.08179389983800694
mae: 0.037204371813956
r2: 0.6983399305342022
pearson: 0.8447789905281001

=== Experiment 1217 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063349299656257916
rmse: 0.07959227327841435
mae: 0.03120938146027572
r2: 0.7143607960658371
pearson: 0.8472853430326001

=== Experiment 1438 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00631593359363025
rmse: 0.07947284815350618
mae: 0.0343601630691404
r2: 0.7152173341181733
pearson: 0.848168855094225

=== Experiment 1410 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006278841068527164
rmse: 0.07923913848930442
mae: 0.03684043965296526
r2: 0.7168898197494027
pearson: 0.8481095211767681

=== Experiment 1531 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007185987245892848
rmse: 0.08477020258258705
mae: 0.039614112894938124
r2: 0.6759869978775501
pearson: 0.8247359348062876

=== Experiment 1236 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006268814112964543
rmse: 0.07917584298865749
mae: 0.03590142174464493
r2: 0.7173419307625205
pearson: 0.8470925841655015

=== Experiment 1177 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00983095913806383
rmse: 0.09915119332647404
mae: 0.037693050808239384
r2: 0.5567263794007168
pearson: 0.7553506934436048

=== Experiment 1138 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005497740415539794
rmse: 0.0741467491906408
mae: 0.02713189023545517
r2: 0.752109304403915
pearson: 0.8684587087665208

=== Experiment 1398 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006476241221385627
rmse: 0.08047509690199588
mae: 0.03745297322766393
r2: 0.7079891337394661
pearson: 0.8436457216779404

=== Experiment 1480 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007506641239807694
rmse: 0.0866408751098908
mae: 0.037450537226673596
r2: 0.6615288504225452
pearson: 0.819447103662847

=== Experiment 1506 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006528999507791667
rmse: 0.08080222464630332
mae: 0.03346629697156968
r2: 0.7056102858261153
pearson: 0.8402629120182062

=== Experiment 1499 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00774377164325994
rmse: 0.08799870250895714
mae: 0.05101297473849509
r2: 0.6508367448999556
pearson: 0.8225073940351838

=== Experiment 1534 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007708437368148515
rmse: 0.08779770707796711
mae: 0.03914561543841524
r2: 0.652429951812926
pearson: 0.808184665447781

=== Experiment 1436 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006840243358656255
rmse: 0.08270576375716662
mae: 0.04120424099148666
r2: 0.691576437579526
pearson: 0.8352956405907609

=== Experiment 1071 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006500474378883355
rmse: 0.08062551940225474
mae: 0.03201009308483265
r2: 0.7068964713337218
pearson: 0.8408919115785232

=== Experiment 1252 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007348738757525416
rmse: 0.08572478496634107
mae: 0.043240679881135426
r2: 0.668648603850456
pearson: 0.8206639581615925

=== Experiment 1250 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00528529576664879
rmse: 0.07270003966057233
mae: 0.029851461294147584
r2: 0.7616883401183697
pearson: 0.8742753408226448

=== Experiment 1525 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007704428543281465
rmse: 0.08777487421398829
mae: 0.04432283505492694
r2: 0.6526107079617625
pearson: 0.8116217541549463

=== Experiment 1504 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0074104574676872766
rmse: 0.08608401400775452
mae: 0.041290895359068554
r2: 0.6658657343737393
pearson: 0.8205713974820912

=== Experiment 1588 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0077393352281254785
rmse: 0.08797349162176911
mae: 0.038641304880648125
r2: 0.6510367809057529
pearson: 0.8088675581254896

=== Experiment 1297 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006923353966191118
rmse: 0.08320669423905216
mae: 0.042335423514587536
r2: 0.6878290168655548
pearson: 0.8377288932718346

=== Experiment 1175 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006109358539622396
rmse: 0.07816238570835972
mae: 0.030257126369685982
r2: 0.724531712382753
pearson: 0.8520607866571925

=== Experiment 1467 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006395606063836872
rmse: 0.079972533183818
mae: 0.03074168177590868
r2: 0.7116249375030925
pearson: 0.8539950501895435

=== Experiment 1482 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006760578162475496
rmse: 0.08222273507051135
mae: 0.03647555449973868
r2: 0.6951685062119821
pearson: 0.8364532736181316

=== Experiment 1585 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0075800060580519715
rmse: 0.08706323022982763
mae: 0.03666244742060614
r2: 0.6582208630582367
pearson: 0.8151384588423378

=== Experiment 1541 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005731332399029144
rmse: 0.0757055638578113
mae: 0.030242266377503456
r2: 0.7415767446800745
pearson: 0.8623665535413672

=== Experiment 1118 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005443913753170213
rmse: 0.07378288252142372
mae: 0.02868923704355155
r2: 0.7545363249192334
pearson: 0.8687731923458877

=== Experiment 1483 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006737527375932923
rmse: 0.0820824425558409
mae: 0.0368356963735169
r2: 0.6962078560317597
pearson: 0.8364368711743978

=== Experiment 1526 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006725736044681223
rmse: 0.08201058495512163
mae: 0.030750870558271682
r2: 0.6967395219681372
pearson: 0.8362821333537542

=== Experiment 1008 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005859636580724365
rmse: 0.07654826308104165
mae: 0.02603934275494884
r2: 0.735791565598426
pearson: 0.8586632867296496

=== Experiment 1274 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00742176801855073
rmse: 0.08614968379832122
mae: 0.04443825362395443
r2: 0.6653557466134603
pearson: 0.8311750547997758

=== Experiment 1540 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006041379118002855
rmse: 0.07772630904656966
mae: 0.03332926419778029
r2: 0.7275968745835466
pearson: 0.853163752586805

=== Experiment 1340 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006645053912505709
rmse: 0.08151720010222203
mae: 0.033735445639068634
r2: 0.700377443797009
pearson: 0.8383767620950684

=== Experiment 1098 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005842068239396929
rmse: 0.07643342357501022
mae: 0.03370319443449149
r2: 0.7365837143764616
pearson: 0.8607493579487342

=== Experiment 1609 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007327448371396314
rmse: 0.08560051618650623
mae: 0.03499007456539722
r2: 0.6696085779904015
pearson: 0.8200030208586165

=== Experiment 1338 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006625063073457828
rmse: 0.08139449043674779
mae: 0.03308764759122804
r2: 0.7012788219310369
pearson: 0.8374841654159015

=== Experiment 1545 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064768681617755055
rmse: 0.08047899205243257
mae: 0.036706983940603156
r2: 0.7079608652732239
pearson: 0.8421625135605355

=== Experiment 1361 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006405581942831061
rmse: 0.08003487953905511
mae: 0.03629042090782435
r2: 0.7111751295099646
pearson: 0.8486972742351395

=== Experiment 1277 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007325795874958452
rmse: 0.08559086326798236
mae: 0.03363711010509809
r2: 0.6696830883274656
pearson: 0.8237308917572993

=== Experiment 1469 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006678715255353307
rmse: 0.08172340702242722
mae: 0.03991203610629346
r2: 0.6988596686635008
pearson: 0.8397436327061423

=== Experiment 1539 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00674868350442684
rmse: 0.08215037129816785
mae: 0.03771143329796169
r2: 0.6957048311081566
pearson: 0.8348804555482988

=== Experiment 1312 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006305091707426332
rmse: 0.07940460759569518
mae: 0.0332925697775639
r2: 0.715706189995229
pearson: 0.8533210112781459

=== Experiment 1472 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006603200603230915
rmse: 0.08126008000999578
mae: 0.042004774558530016
r2: 0.7022645910911594
pearson: 0.8429583631455928

=== Experiment 1572 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076880157026044645
rmse: 0.08768133041078052
mae: 0.0418157212861697
r2: 0.6533507557240448
pearson: 0.8291189783778083

=== Experiment 1490 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006207740488704128
rmse: 0.07878921556091371
mae: 0.03715439203894626
r2: 0.7200957135998649
pearson: 0.8510910033010313

=== Experiment 1259 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006306243893250288
rmse: 0.07941186242149398
mae: 0.03593956549239991
r2: 0.7156542384435425
pearson: 0.8461909364848493

=== Experiment 1606 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008479982104030065
rmse: 0.09208681829681198
mae: 0.03821831013204579
r2: 0.6176413392548343
pearson: 0.7915490339202845

=== Experiment 1500 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006249954385509991
rmse: 0.07905665301231764
mae: 0.034605893297080616
r2: 0.7181923075726453
pearson: 0.8500917863194241

=== Experiment 1171 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006141829061461474
rmse: 0.07836982239013607
mae: 0.03446760669970709
r2: 0.7230676308443987
pearson: 0.851267651465383

=== Experiment 1527 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006861534061870955
rmse: 0.08283437729512401
mae: 0.03869192618332846
r2: 0.6906164491423302
pearson: 0.8318628497588111

=== Experiment 1092 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006093970316037382
rmse: 0.07806388611923815
mae: 0.030051386808685045
r2: 0.7252255606113263
pearson: 0.8523043257625705

=== Experiment 1254 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006788071206571135
rmse: 0.08238975182977028
mae: 0.03502678269661801
r2: 0.6939288569543238
pearson: 0.833673265873725

=== Experiment 1069 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006308034602566119
rmse: 0.0794231364437726
mae: 0.033970137745765266
r2: 0.7155734961486431
pearson: 0.8607979416730189

=== Experiment 1620 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007625548442999785
rmse: 0.08732438630187896
mae: 0.04146932779990921
r2: 0.6561673769656762
pearson: 0.8148985719635499

=== Experiment 1544 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005895679280889633
rmse: 0.0767833268417671
mae: 0.03110224681656937
r2: 0.7341664161115795
pearson: 0.8572407389969945

=== Experiment 1460 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006622869232079429
rmse: 0.08138101272458723
mae: 0.03558668584408027
r2: 0.7013777412732054
pearson: 0.8385234117847421

=== Experiment 1283 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006728035981476459
rmse: 0.08202460595136352
mae: 0.03524971577069253
r2: 0.6966358188303196
pearson: 0.8367856148207206

=== Experiment 1508 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0069234302229493555
rmse: 0.08320715247470829
mae: 0.03933350090258756
r2: 0.6878255784818891
pearson: 0.8342566701873838

=== Experiment 1397 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006952069177893521
rmse: 0.08337906918341989
mae: 0.044173315733778
r2: 0.6865342606084246
pearson: 0.8349593820564174

=== Experiment 1608 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007297823697205487
rmse: 0.0854273006550335
mae: 0.03767166137469513
r2: 0.6709443415108498
pearson: 0.8194650387888166

=== Experiment 1515 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006884631911022111
rmse: 0.08297368203847598
mae: 0.03579215353758707
r2: 0.6895749772902455
pearson: 0.8353860550703123

=== Experiment 1192 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006740536951009673
rmse: 0.08210077314501778
mae: 0.039091160673345185
r2: 0.6960721556161644
pearson: 0.8371402779803674

=== Experiment 1310 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00682424408637154
rmse: 0.08260898308520412
mae: 0.03163077474372313
r2: 0.6922978377250255
pearson: 0.8472943554177185

=== Experiment 1564 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007108867810745389
rmse: 0.08431410208704941
mae: 0.04821806324171595
r2: 0.6794642792654899
pearson: 0.8362293301077823

=== Experiment 1589 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006577913549327018
rmse: 0.08110433742610205
mae: 0.03491595308930152
r2: 0.7034047732220023
pearson: 0.8391533793864429

=== Experiment 1191 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005945706746429417
rmse: 0.07710840905134418
mae: 0.031845843855098005
r2: 0.731910699709162
pearson: 0.8570705644516203

=== Experiment 1604 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006779020886712481
rmse: 0.08233480969014552
mae: 0.03835254689846872
r2: 0.6943369318934002
pearson: 0.8385546353809566

=== Experiment 1156 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006892465403668023
rmse: 0.08302087330104413
mae: 0.03879514218730201
r2: 0.6892217685546251
pearson: 0.8405146808621914

=== Experiment 1633 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007336942860579514
rmse: 0.0856559563636967
mae: 0.03974348566437306
r2: 0.6691804756519755
pearson: 0.8193895056465977

=== Experiment 1581 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006334911881310486
rmse: 0.07959215967236023
mae: 0.03870997851034642
r2: 0.7143616114796547
pearson: 0.8476498992382895

=== Experiment 1617 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006919471047952888
rmse: 0.08318335799877814
mae: 0.03418061190215252
r2: 0.6880040959399134
pearson: 0.8336923040475244

=== Experiment 1349 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005645969493279936
rmse: 0.07513966657684831
mae: 0.027382639922657787
r2: 0.7454257205292183
pearson: 0.8676018177948287

=== Experiment 1405 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006669818608773524
rmse: 0.08166895743655311
mae: 0.035933043838135774
r2: 0.6992608145420697
pearson: 0.8366478159928007

=== Experiment 1550 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006705656741904298
rmse: 0.08188807447915904
mae: 0.04065976375502678
r2: 0.6976448888927718
pearson: 0.8386759830290142

=== Experiment 1119 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007333454681516829
rmse: 0.08563559237558195
mae: 0.049104028388743844
r2: 0.6693377561106468
pearson: 0.8374164848322557

=== Experiment 1487 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006773939545285868
rmse: 0.08230394611004911
mae: 0.04278837660742033
r2: 0.6945660473418569
pearson: 0.840499516977915

=== Experiment 1590 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007232077280477975
rmse: 0.08504162087165304
mae: 0.03332094119724998
r2: 0.6739088185038822
pearson: 0.8274851605851565

=== Experiment 1336 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008365619835046821
rmse: 0.09146376241466793
mae: 0.04517207542860256
r2: 0.622797883628605
pearson: 0.8133456320409234

=== Experiment 1409 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006188946123109602
rmse: 0.07866985523762964
mae: 0.03336324337108064
r2: 0.7209431432724893
pearson: 0.8496337935529942

=== Experiment 1172 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006077883687619514
rmse: 0.07796078301055932
mae: 0.03150961423416046
r2: 0.7259508996064212
pearson: 0.8523391964592112

=== Experiment 1439 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0071996079502591485
rmse: 0.08485050353568414
mae: 0.03680109986097003
r2: 0.6753728463126041
pearson: 0.8241722228563731

=== Experiment 1678 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007555459830584262
rmse: 0.08692214810152969
mae: 0.03742588246292645
r2: 0.6593276416511348
pearson: 0.815951065397365

=== Experiment 1576 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006882329793722299
rmse: 0.08295980830331215
mae: 0.04073641542613514
r2: 0.6896787787460548
pearson: 0.8335036234363372

=== Experiment 1516 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006675576140280802
rmse: 0.08170419903701891
mae: 0.03938970686130568
r2: 0.6990012099804899
pearson: 0.8381135875746849

=== Experiment 1648 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006706039288632951
rmse: 0.0818904102360768
mae: 0.034717668631228216
r2: 0.6976276400291483
pearson: 0.8363722165589845

=== Experiment 1632 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007493669370059857
rmse: 0.08656598275338794
mae: 0.04717703432996257
r2: 0.6621137463201188
pearson: 0.8258624777636542

=== Experiment 1705 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006688620382191085
rmse: 0.08178398609869224
mae: 0.037345503987403085
r2: 0.6984130508539651
pearson: 0.8366490940823351

=== Experiment 1600 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0066825507914301625
rmse: 0.08174687022406522
mae: 0.04220969296919413
r2: 0.6986867260299439
pearson: 0.842488795841973

=== Experiment 1444 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00631762895026368
rmse: 0.07948351370104167
mae: 0.036901886096229373
r2: 0.7151408912337553
pearson: 0.8542445703567897

=== Experiment 1674 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007187709537386229
rmse: 0.0847803605641438
mae: 0.0399036041424229
r2: 0.675909340512152
pearson: 0.8235892841536041

=== Experiment 1160 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006310411579939771
rmse: 0.07943809904535588
mae: 0.029871340001880485
r2: 0.7154663192850529
pearson: 0.8470944938497181

=== Experiment 1466 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00695241436537945
rmse: 0.0833811391465687
mae: 0.03301534986538904
r2: 0.6865186962566128
pearson: 0.8400348942503685

=== Experiment 1510 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008929738130839108
rmse: 0.0944972916587513
mae: 0.04449597054047626
r2: 0.5973620379587803
pearson: 0.772972702255441

=== Experiment 1635 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006755993680156682
rmse: 0.08219485190787001
mae: 0.04132466255599383
r2: 0.6953752185019524
pearson: 0.837359623178436

=== Experiment 1354 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007072430849643754
rmse: 0.08409774580595936
mae: 0.035690786570717434
r2: 0.6811072058044296
pearson: 0.8407231769596036

=== Experiment 1629 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007498503169369813
rmse: 0.08659389799154334
mae: 0.03863337039891442
r2: 0.6618957924367506
pearson: 0.8179689958358013

=== Experiment 1512 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005769433859624958
rmse: 0.07595678942415193
mae: 0.028211523983984566
r2: 0.7398587665915445
pearson: 0.860515260686867

=== Experiment 1711 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006453654300114882
rmse: 0.08033463947834012
mae: 0.039055936342279655
r2: 0.7090075680782997
pearson: 0.8453589690165748

=== Experiment 1625 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007170666020350613
rmse: 0.08467978519310623
mae: 0.037467425501027725
r2: 0.6766778251938625
pearson: 0.8252247285642376

=== Experiment 1122 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005886267474717673
rmse: 0.07672201427698358
mae: 0.03517790918433744
r2: 0.7345907903094204
pearson: 0.8595600200956777

=== Experiment 1621 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0062539325670498075
rmse: 0.07908180933090624
mae: 0.034967476812919014
r2: 0.7180129331179469
pearson: 0.8489726541534368

=== Experiment 1056 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006003369776890799
rmse: 0.07748141568718785
mae: 0.033574239322425536
r2: 0.729310699045099
pearson: 0.8558876982440226

=== Experiment 1584 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0070269663142256775
rmse: 0.0838270022977422
mae: 0.041733437933075525
r2: 0.6831571816959583
pearson: 0.837042526736967

=== Experiment 1556 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006574218987653065
rmse: 0.08108155762966733
mae: 0.04093513888106691
r2: 0.7035713593817801
pearson: 0.8461053312222974

=== Experiment 1619 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006092458410647563
rmse: 0.07805420174883325
mae: 0.029866569191763923
r2: 0.7252937317599126
pearson: 0.8520894506843906

=== Experiment 1682 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007486266327671968
rmse: 0.08652321265228174
mae: 0.037598553917859524
r2: 0.6624475462430603
pearson: 0.8309633427631037

=== Experiment 1316 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.011277950871845894
rmse: 0.10619769711178248
mae: 0.04406789639051003
r2: 0.49148215899424963
pearson: 0.7029759985129382

=== Experiment 1676 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007105271238708778
rmse: 0.08429277097538541
mae: 0.04448381419287256
r2: 0.6796264471156483
pearson: 0.8335698025599554

=== Experiment 1281 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0072745224322378725
rmse: 0.08529081094841268
mae: 0.038111071617241206
r2: 0.6719949852925686
pearson: 0.8215199473283749

=== Experiment 1650 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062780386804506694
rmse: 0.07923407524828362
mae: 0.031010404142242393
r2: 0.716925999074614
pearson: 0.8468459857400301

=== Experiment 1296 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007844552775413042
rmse: 0.08856947993193277
mae: 0.041369828834549496
r2: 0.6462925680083407
pearson: 0.8089922403927378

=== Experiment 1327 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006195772110094823
rmse: 0.07871322703392881
mae: 0.038004674387168924
r2: 0.7206353625236723
pearson: 0.85699883003891

=== Experiment 1021 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0064458008331235925
rmse: 0.08028574489362102
mae: 0.03842724781445537
r2: 0.7093616774483618
pearson: 0.8485419825650437

=== Experiment 1223 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005661867221355671
rmse: 0.07524538006652416
mae: 0.027532663706225112
r2: 0.7447088989674087
pearson: 0.8720444759984525

=== Experiment 1618 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007161973738252923
rmse: 0.08462844520758327
mae: 0.035626887558418335
r2: 0.6770697563678814
pearson: 0.8283095320656824

=== Experiment 1488 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005734402710742724
rmse: 0.07572583912207724
mae: 0.031984265013758804
r2: 0.7414383056762567
pearson: 0.8614145842931119

=== Experiment 1644 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006205247444216086
rmse: 0.07877339299672248
mae: 0.03561232852469372
r2: 0.720208123878558
pearson: 0.85019449422954

=== Experiment 1426 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00634202780272571
rmse: 0.07963684952787692
mae: 0.03796825767103409
r2: 0.7140407577149988
pearson: 0.8469623071598578

=== Experiment 1378 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006787257742719897
rmse: 0.08238481500082341
mae: 0.031056417240392643
r2: 0.6939655356813433
pearson: 0.8505503251272326

=== Experiment 1269 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005780408791977975
rmse: 0.07602899967760969
mae: 0.028130836840350577
r2: 0.7393639117221842
pearson: 0.8654074144964626

=== Experiment 1396 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009298366298445404
rmse: 0.09642803688992846
mae: 0.03480479968974702
r2: 0.5807407561270764
pearson: 0.782524995470374

=== Experiment 1637 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064336808744860745
rmse: 0.08021022923845857
mae: 0.03500938492162893
r2: 0.7099081610489257
pearson: 0.8427664067781511

=== Experiment 1404 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062306961548128925
rmse: 0.07893475885066663
mae: 0.030554579097727526
r2: 0.7190606527185821
pearson: 0.8515728573669518

=== Experiment 1346 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007773811153715198
rmse: 0.08816921885621534
mae: 0.03426138638237503
r2: 0.6494822765949276
pearson: 0.8377646974138141

=== Experiment 1132 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006721802863151184
rmse: 0.08198660172949715
mae: 0.0325539357895494
r2: 0.696916867392201
pearson: 0.8484380460829962

=== Experiment 1253 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005911159974511158
rmse: 0.07688406840504187
mae: 0.0329187847325306
r2: 0.7334683984497594
pearson: 0.8564617062459983

=== Experiment 1723 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006694214242439193
rmse: 0.08181817794621922
mae: 0.03550514087292238
r2: 0.6981608261574241
pearson: 0.8358041786336746

=== Experiment 1362 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006902623169215182
rmse: 0.08308202675197074
mae: 0.037101470283522016
r2: 0.6887637593768783
pearson: 0.8314063650916236

=== Experiment 1712 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006858299846501496
rmse: 0.0828148528133782
mae: 0.038645948664045145
r2: 0.6907622784898522
pearson: 0.8328468774385248

=== Experiment 1390 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009367079180099377
rmse: 0.09678367207385437
mae: 0.040482914272194165
r2: 0.5776425225361508
pearson: 0.763407111672053

=== Experiment 1497 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007010212628237846
rmse: 0.0837270125362051
mae: 0.04122603778381538
r2: 0.6839125980232885
pearson: 0.8423666619728131

=== Experiment 1507 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00604209652887323
rmse: 0.07773092389051625
mae: 0.028832758230600802
r2: 0.7275645268431582
pearson: 0.8575976038504318

=== Experiment 1595 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007591241086805319
rmse: 0.08712772857595519
mae: 0.048514588926968166
r2: 0.6577142805566626
pearson: 0.8212755022390252

=== Experiment 1386 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00555977443115199
rmse: 0.07456389495695614
mae: 0.03017616525933449
r2: 0.7493122179432193
pearson: 0.8671874239704584

=== Experiment 1645 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006155891224476534
rmse: 0.07845948779132154
mae: 0.034658162474736964
r2: 0.722433574103281
pearson: 0.8511162803112634

=== Experiment 1654 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007017785663089473
rmse: 0.08377222489041027
mae: 0.04896000783819858
r2: 0.6835711332149763
pearson: 0.8399505765380628

=== Experiment 1655 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006895418188473127
rmse: 0.08303865478482372
mae: 0.0424932770488741
r2: 0.689088628787383
pearson: 0.8349408100209008

=== Experiment 1300 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006451351883233515
rmse: 0.08032030803746656
mae: 0.027933469277049965
r2: 0.7091113830421094
pearson: 0.8553342318845719

=== Experiment 1455 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006680045684520228
rmse: 0.08173154644640114
mae: 0.036663231648457806
r2: 0.6987996801979308
pearson: 0.8369414669844345

=== Experiment 1248 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0074318825004208965
rmse: 0.0862083667657664
mae: 0.0376281603715455
r2: 0.6648996890776584
pearson: 0.8393744848952877

=== Experiment 1624 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007575717004983389
rmse: 0.08703859491618295
mae: 0.0389009681220472
r2: 0.6584142545733424
pearson: 0.8124103722955212

=== Experiment 1731 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007377686827870773
rmse: 0.08589346207873318
mae: 0.03912980513435836
r2: 0.6673433480996039
pearson: 0.8219675285638747

=== Experiment 1718 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006902132134359136
rmse: 0.08307907157858191
mae: 0.03629784012375847
r2: 0.6887858999224166
pearson: 0.8422868815678279

=== Experiment 1549 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009139406451160117
rmse: 0.09560024294508941
mae: 0.04489950921427745
r2: 0.587908185677591
pearson: 0.7723486968572031

=== Experiment 1662 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0068263073871140025
rmse: 0.08262147049716558
mae: 0.041736990603945444
r2: 0.6922048044026761
pearson: 0.8378843815953883

=== Experiment 1298 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005839568538103359
rmse: 0.07641706967754887
mae: 0.02988790590086507
r2: 0.736696424807586
pearson: 0.8632005470869618

=== Experiment 1406 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0063554250077922135
rmse: 0.07972091951170793
mae: 0.029607703596573984
r2: 0.7134366836351109
pearson: 0.8552899300459166

=== Experiment 1155 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006135468644399119
rmse: 0.07832923237463213
mae: 0.04025383032280287
r2: 0.7233544192503067
pearson: 0.8549180424710087

=== Experiment 1724 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009191431523186267
rmse: 0.09587195378830175
mae: 0.04265790904138723
r2: 0.5855623980779172
pearson: 0.821447302120749

=== Experiment 1302 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0061022670355201074
rmse: 0.07811700861861076
mae: 0.03615663519925423
r2: 0.7248514651815157
pearson: 0.8684034088139642

=== Experiment 1715 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007189773579242123
rmse: 0.0847925325676862
mae: 0.04164674954398423
r2: 0.675816273773324
pearson: 0.82526286406695

=== Experiment 1407 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.011721869414721663
rmse: 0.10826758247380266
mae: 0.05392380352497815
r2: 0.4714660672794737
pearson: 0.6934910829136135

=== Experiment 1418 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005556213546686059
rmse: 0.07454001305799496
mae: 0.031528905014945825
r2: 0.7494727766565231
pearson: 0.8671196640463333

=== Experiment 1128 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006724902447710989
rmse: 0.08200550254532307
mae: 0.03272755927282269
r2: 0.696777108488631
pearson: 0.8374532798590276

=== Experiment 1657 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0056888469694979455
rmse: 0.07542444543712566
mae: 0.03368299289397972
r2: 0.7434923939983689
pearson: 0.8633732400390063

=== Experiment 1570 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006379363986045277
rmse: 0.0798709207787495
mae: 0.03395689277932775
r2: 0.7123572856420304
pearson: 0.8445496372155135

=== Experiment 1209 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006597094718598741
rmse: 0.0812225013072039
mae: 0.0346484157578834
r2: 0.7025399027418188
pearson: 0.8416518200950993

=== Experiment 1432 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006436290225827617
rmse: 0.08022649329135367
mae: 0.03186626867706273
r2: 0.7097905065453991
pearson: 0.854556612926785

=== Experiment 1753 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0074320959643543955
rmse: 0.086209604826576
mae: 0.04394912355389292
r2: 0.6648900640828497
pearson: 0.8203184355315981

=== Experiment 1194 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010435527377021477
rmse: 0.1021544290621874
mae: 0.045093080329667135
r2: 0.5294666635969477
pearson: 0.7286911887650694

=== Experiment 1708 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006599674550704135
rmse: 0.08123838101971342
mae: 0.03737268995065382
r2: 0.7024235792476574
pearson: 0.8429729366488958

=== Experiment 1756 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007852973235569065
rmse: 0.08861700308388376
mae: 0.0386151715843379
r2: 0.6459128931660334
pearson: 0.8052719694703461

=== Experiment 1739 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006933966180434471
rmse: 0.08327044001585719
mae: 0.04215645454682035
r2: 0.6873505167961147
pearson: 0.8323382705543013

=== Experiment 1652 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006126251471790029
rmse: 0.0782703741641116
mae: 0.03168219053547841
r2: 0.7237700175064625
pearson: 0.8510157659539711

=== Experiment 1141 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.01141429763537969
rmse: 0.10683771635232424
mae: 0.04832338163749624
r2: 0.48533434343731063
pearson: 0.7434223901718348

=== Experiment 1736 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006852695574405962
rmse: 0.08278100974502522
mae: 0.035834079327762405
r2: 0.6910149726520111
pearson: 0.8320045288900224

=== Experiment 1367 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005648572224956517
rmse: 0.07515698387346659
mae: 0.029863451493827985
r2: 0.7453083645034709
pearson: 0.8649721333608649

=== Experiment 1374 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005931330914717842
rmse: 0.07701513432253326
mae: 0.03223865140859195
r2: 0.7325588996337495
pearson: 0.8589479679648734

=== Experiment 1345 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0064396666125625165
rmse: 0.08024753337369639
mae: 0.029410589422666358
r2: 0.7096382667535837
pearson: 0.8550686465696998

=== Experiment 1725 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007336228123709573
rmse: 0.0856517841244978
mae: 0.03418347697888681
r2: 0.6692127028228612
pearson: 0.8336606288934593

=== Experiment 1471 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006396042562690076
rmse: 0.07997526219206834
mae: 0.03611638224992975
r2: 0.7116052559619194
pearson: 0.8562931474735591

=== Experiment 1553 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007554219728744224
rmse: 0.08691501440340572
mae: 0.04646621707271396
r2: 0.6593835572972915
pearson: 0.8326198657090033

=== Experiment 1754 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007211052342387017
rmse: 0.08491791532054362
mae: 0.04553538000434893
r2: 0.6748568237086208
pearson: 0.8275085900816033

=== Experiment 1380 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009152333335024833
rmse: 0.09566782810864284
mae: 0.04058199714912777
r2: 0.5873253181736876
pearson: 0.7686823251968073

=== Experiment 1807 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007657764193205124
rmse: 0.08750865210483547
mae: 0.04602257786737709
r2: 0.6547147829681537
pearson: 0.8152216203687312

=== Experiment 1075 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005720594819994301
rmse: 0.07563461390127077
mae: 0.027207521131739073
r2: 0.7420608973927856
pearson: 0.8783610459394015

=== Experiment 1578 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006751657297938407
rmse: 0.08216846900081812
mae: 0.0355343065889429
r2: 0.6955707440675868
pearson: 0.8350327405678605

=== Experiment 1551 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006258271519056851
rmse: 0.07910923788696773
mae: 0.035997544213810496
r2: 0.7178172916816681
pearson: 0.85176647805495

=== Experiment 1322 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0054909815123186795
rmse: 0.0741011572940577
mae: 0.029523061493137732
r2: 0.7524140603753341
pearson: 0.8841386489857798

=== Experiment 1782 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00860734541929673
rmse: 0.09277578034862725
mae: 0.042618639746974406
r2: 0.6118985834263425
pearson: 0.7919296495813481

=== Experiment 1348 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005815857852460695
rmse: 0.07626177189431606
mae: 0.031381220193336445
r2: 0.737765529187343
pearson: 0.8589921877493111

=== Experiment 1612 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0056807830059905995
rmse: 0.07537096925203098
mae: 0.0321632483227118
r2: 0.7438559945636932
pearson: 0.8671710326535721

=== Experiment 1593 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007319997832489832
rmse: 0.08555698587777524
mae: 0.03732880514197553
r2: 0.6699445195104619
pearson: 0.8401896947173909

=== Experiment 1781 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006758317698817042
rmse: 0.08220898794424514
mae: 0.03546439599467586
r2: 0.6952704295234358
pearson: 0.8340322686663718

=== Experiment 1379 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0056225889315737075
rmse: 0.0749839244876774
mae: 0.03090650681215936
r2: 0.7464799397659798
pearson: 0.866982763606921

=== Experiment 1790 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00745782827938439
rmse: 0.08635871860666061
mae: 0.04120311268268355
r2: 0.6637298053238052
pearson: 0.8156196277862584

=== Experiment 1808 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007339828981902064
rmse: 0.08567280187960508
mae: 0.03734312814217339
r2: 0.6690503417118208
pearson: 0.8184487219503942

=== Experiment 1146 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.020667268919041734
rmse: 0.14376115232927753
mae: 0.06373190102742128
r2: 0.06812194080109513
pearson: 0.27870687083859735

=== Experiment 1751 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007513616257299641
rmse: 0.08668111822824877
mae: 0.034221108897893725
r2: 0.6612143499537734
pearson: 0.8165219530126582

=== Experiment 1368 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00663998558170211
rmse: 0.08148610667900455
mae: 0.03578188909213963
r2: 0.7006059726022004
pearson: 0.8394139732804758

=== Experiment 1814 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007010894555240577
rmse: 0.08373108476092124
mae: 0.0410866939581597
r2: 0.6838818502348744
pearson: 0.8294681533198787

=== Experiment 1587 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005859830841054029
rmse: 0.07654953194536221
mae: 0.02975451854831834
r2: 0.7357828064856619
pearson: 0.8642957253551128

=== Experiment 1803 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0064549062884836144
rmse: 0.08034243143248537
mae: 0.034438720606927375
r2: 0.7089511164737958
pearson: 0.8433725698553233

=== Experiment 1256 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0060127989367186555
rmse: 0.07754223969372213
mae: 0.03673562411783544
r2: 0.7288855423785515
pearson: 0.8567154986111862

=== Experiment 1116 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0067022939950110725
rmse: 0.08186753932427109
mae: 0.036465791123954115
r2: 0.6977965136701291
pearson: 0.8506058600758376

=== Experiment 1211 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0054730427485112886
rmse: 0.0739800158726077
mae: 0.029481159571234847
r2: 0.7532229113399557
pearson: 0.8690172820124632

=== Experiment 1771 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006910651393427239
rmse: 0.08313032775965243
mae: 0.032337512030940374
r2: 0.6884017702806483
pearson: 0.8324204663039048

=== Experiment 1109 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005881206657998621
rmse: 0.0766890256685963
mae: 0.028331116457171148
r2: 0.7348189803078466
pearson: 0.8577751649507785

=== Experiment 1434 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0075617535016378095
rmse: 0.08695834348489977
mae: 0.039684244453977655
r2: 0.6590438627933868
pearson: 0.8482008361982883

=== Experiment 1770 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008097100621198578
rmse: 0.0899838908983079
mae: 0.03904120963301166
r2: 0.6349052967966782
pearson: 0.8274338683723462

=== Experiment 1760 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00758513527263557
rmse: 0.08709268208429208
mae: 0.04093942404140088
r2: 0.6579895890302023
pearson: 0.81231903887004

=== Experiment 1559 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006359254806392121
rmse: 0.07974493592945023
mae: 0.03169934342315085
r2: 0.7132639997018644
pearson: 0.8493198555436329

=== Experiment 1335 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006850191974913612
rmse: 0.0827658865409754
mae: 0.03562614873763619
r2: 0.6911278588512025
pearson: 0.8318685063570042

=== Experiment 1358 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00618553929320429
rmse: 0.07864819955475326
mae: 0.029261335208132858
r2: 0.7210967557334593
pearson: 0.8507864325589761

=== Experiment 1835 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007437470210091554
rmse: 0.08624076883986805
mae: 0.04205780604290807
r2: 0.6646477417079476
pearson: 0.8178557812396151

=== Experiment 1239 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006298569731865861
rmse: 0.0793635289781513
mae: 0.036813897820171165
r2: 0.7160002630027096
pearson: 0.8507548962569408

=== Experiment 1555 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00702436962922913
rmse: 0.08381151251009093
mae: 0.035478573961196753
r2: 0.6832742650795642
pearson: 0.8424355174944791

=== Experiment 1484 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006977659631598851
rmse: 0.08353238672274875
mae: 0.04095335802614032
r2: 0.6853803983140713
pearson: 0.8338300170471309

=== Experiment 1665 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006824433538686889
rmse: 0.08261012975832255
mae: 0.03723709597671685
r2: 0.6922892954035109
pearson: 0.8358862876458456

=== Experiment 1788 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007177826018114574
rmse: 0.08472205154571373
mae: 0.038162959889178394
r2: 0.6763549840460414
pearson: 0.823736327653596

=== Experiment 1703 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007061837264841529
rmse: 0.08403473844096576
mae: 0.04277466383140741
r2: 0.6815848658805688
pearson: 0.8344776241050963

=== Experiment 1695 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00545401725561873
rmse: 0.07385131857738716
mae: 0.030398255920965473
r2: 0.7540807624407213
pearson: 0.868652604089945

=== Experiment 1523 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006603703204278297
rmse: 0.08126317249700689
mae: 0.0306446300117926
r2: 0.7022419290311444
pearson: 0.8505404828615881

=== Experiment 1592 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00691150481033517
rmse: 0.08313546060698268
mae: 0.036222117196650486
r2: 0.6883632900878871
pearson: 0.8317236652041039

=== Experiment 1761 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006652554884422155
rmse: 0.08156319564866346
mae: 0.03392069114216343
r2: 0.7000392282747268
pearson: 0.8368202567476174

=== Experiment 1262 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006173546466784983
rmse: 0.07857191907281497
mae: 0.03139549520970496
r2: 0.7216375069982592
pearson: 0.8654613328787303

=== Experiment 1791 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006994849322672096
rmse: 0.08363521580454071
mae: 0.046509528429299736
r2: 0.6846053227093405
pearson: 0.8378673470403745

=== Experiment 1325 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012524158129660625
rmse: 0.1119113851654988
mae: 0.04384920260060212
r2: 0.43529122223714356
pearson: 0.6628589508172404

=== Experiment 1176 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005908050645649152
rmse: 0.07686384485341045
mae: 0.033025573728732806
r2: 0.7336085967196269
pearson: 0.8605985849468754

=== Experiment 1646 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.012056045657129677
rmse: 0.10980002576106108
mae: 0.05116962083172288
r2: 0.4563982075915086
pearson: 0.6914702628914203

=== Experiment 1825 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006570406020696745
rmse: 0.08105804106130832
mae: 0.03423034031663533
r2: 0.7037432843836254
pearson: 0.8396907925752892

=== Experiment 1785 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006388556938030719
rmse: 0.07992844886541162
mae: 0.03454974795801837
r2: 0.7119427794831344
pearson: 0.8466562250766226

=== Experiment 1836 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00754340797520047
rmse: 0.08685279486119299
mae: 0.03706054599258151
r2: 0.6598710545059627
pearson: 0.8324509600803218

=== Experiment 1651 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008837821991082732
rmse: 0.09400969094238494
mae: 0.04442302122680955
r2: 0.6015064962450083
pearson: 0.7836585978222912

=== Experiment 1844 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007360844772356932
rmse: 0.085795365681119
mae: 0.036511110653978864
r2: 0.6681027489699676
pearson: 0.8184196484662809

=== Experiment 1234 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009663938060383685
rmse: 0.09830533078314566
mae: 0.04381458508839957
r2: 0.5642572862817166
pearson: 0.7648236516463899

=== Experiment 1767 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00803213922268822
rmse: 0.08962220273285086
mae: 0.03997843339606527
r2: 0.6378343776638125
pearson: 0.8039551065714685

=== Experiment 1351 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008146230201314166
rmse: 0.09025646902751162
mae: 0.04136621232294566
r2: 0.6326900656528482
pearson: 0.8098493076230101

=== Experiment 1186 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0057722836357127156
rmse: 0.07597554630085075
mae: 0.029397546080114335
r2: 0.7397302714420299
pearson: 0.870419087606963

=== Experiment 1717 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006427038629560891
rmse: 0.08016881332264368
mae: 0.037646744065056685
r2: 0.7102076569491267
pearson: 0.8459086213933523

=== Experiment 1706 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006504698355402017
rmse: 0.08065171018274825
mae: 0.035297351138648264
r2: 0.7067060140916097
pearson: 0.8420578465169444

=== Experiment 1485 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006031717996840404
rmse: 0.07766413584686567
mae: 0.03498672738938938
r2: 0.728032490284577
pearson: 0.8536694522164633

=== Experiment 1893 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007522877560723627
rmse: 0.08673452346513254
mae: 0.04355243557320635
r2: 0.6607967618585971
pearson: 0.8168918002801377

=== Experiment 1784 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006138371393028778
rmse: 0.07834775933636379
mae: 0.0364967786526414
r2: 0.7232235355921928
pearson: 0.8543492657451478

=== Experiment 1838 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007349184929819233
rmse: 0.08572738727979078
mae: 0.03845432140388742
r2: 0.6686284861381031
pearson: 0.8240612091554723

=== Experiment 1392 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010567428565176685
rmse: 0.10279799883838539
mae: 0.04506739098049674
r2: 0.5235192970771818
pearson: 0.7269188288217494

=== Experiment 1076 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005730436213546446
rmse: 0.07569964473857488
mae: 0.027542777921548887
r2: 0.7416171532890479
pearson: 0.8658302605570067

=== Experiment 1181 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006099024794212451
rmse: 0.07809625339420868
mae: 0.02841958109127766
r2: 0.7249976564150584
pearson: 0.8522642574367776

=== Experiment 1806 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007227683554164925
rmse: 0.08501578414720955
mae: 0.03560460552275208
r2: 0.6741069296895109
pearson: 0.8217801494023402

=== Experiment 1395 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0066673948378578535
rmse: 0.08165411709067616
mae: 0.043505363015424986
r2: 0.699370101305874
pearson: 0.8497692693600504

=== Experiment 1318 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005715090125803096
rmse: 0.0755982150966747
mae: 0.031764564540943
r2: 0.7423091016310658
pearson: 0.8656737280330365

=== Experiment 1777 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010756743209886227
rmse: 0.10371472031436149
mae: 0.04479746696725175
r2: 0.5149831830709757
pearson: 0.7714566373488628

=== Experiment 1713 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007622008282286285
rmse: 0.08730411377642112
mae: 0.03374979611416646
r2: 0.6563270012541041
pearson: 0.8125718484604698

=== Experiment 1663 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007084231716065118
rmse: 0.08416787817252563
mae: 0.031502145589659095
r2: 0.6805751099314488
pearson: 0.8490387282592612

=== Experiment 1832 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007352196754196113
rmse: 0.08574495177091251
mae: 0.037988894565349665
r2: 0.6684926843025549
pearson: 0.8211306107991894

=== Experiment 1424 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00633206807023542
rmse: 0.07957429277244894
mae: 0.033010417903764695
r2: 0.7144898376693725
pearson: 0.8488957399408953

=== Experiment 1661 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006634486114209201
rmse: 0.08145235487209194
mae: 0.034399561725404315
r2: 0.7008539411709548
pearson: 0.8375325938996379

=== Experiment 1285 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006231408235543453
rmse: 0.07893926928686035
mae: 0.02881043154513309
r2: 0.7190285453118519
pearson: 0.8510643020779411

=== Experiment 1847 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008649165717429066
rmse: 0.09300089094965201
mae: 0.05203285275636569
r2: 0.610012924590076
pearson: 0.8143677819090392

=== Experiment 1607 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006152800653554502
rmse: 0.07843978998922997
mae: 0.03499634760336823
r2: 0.7225729265858911
pearson: 0.8508914765149831

=== Experiment 1716 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006711235656002711
rmse: 0.08192213166173541
mae: 0.0316011567301595
r2: 0.6973933381115422
pearson: 0.836479114014649

=== Experiment 1575 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005977010145957494
rmse: 0.07731112562857621
mae: 0.033117826231012094
r2: 0.7304992432021209
pearson: 0.8551807491022086

=== Experiment 1752 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005740010805835381
rmse: 0.07576285901307699
mae: 0.03520571149566568
r2: 0.7411854391368402
pearson: 0.8664899685876234

=== Experiment 1563 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007132567326337357
rmse: 0.08445452815768588
mae: 0.033207055596632215
r2: 0.6783956785384024
pearson: 0.8431887607795799

=== Experiment 1891 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007251799083572345
rmse: 0.08515749575681723
mae: 0.03795643038456821
r2: 0.6730195710825866
pearson: 0.8297117712243243

=== Experiment 1909 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0072698833693736736
rmse: 0.085263611050516
mae: 0.037504363032518695
r2: 0.6722041585953075
pearson: 0.8217899849493245

=== Experiment 1704 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00694669594213377
rmse: 0.08334684122469052
mae: 0.03319264976116893
r2: 0.6867765374438899
pearson: 0.8496143804513945

=== Experiment 1698 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007183138500179489
rmse: 0.08475339816302052
mae: 0.035818906470157494
r2: 0.6761154465679365
pearson: 0.8288847149491796

=== Experiment 1697 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005955367754247665
rmse: 0.07717102924185776
mae: 0.03886158150323498
r2: 0.731475089118772
pearson: 0.8583141967662663

=== Experiment 1051 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005678254522432321
rmse: 0.07535419379458798
mae: 0.0319865595375854
r2: 0.7439700027744651
pearson: 0.8652091077513524

=== Experiment 1232 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005768519064461695
rmse: 0.07595076737243472
mae: 0.03007802949843247
r2: 0.7399000143028239
pearson: 0.8647197919916065

=== Experiment 1684 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006095806597611862
rmse: 0.0780756466358868
mae: 0.02883048856012221
r2: 0.7251427634833423
pearson: 0.8701864894004964

=== Experiment 1598 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006085768821327687
rmse: 0.0780113377742472
mae: 0.03866457588073474
r2: 0.7255953623980326
pearson: 0.8543100859873054

=== Experiment 1626 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005942125893688934
rmse: 0.07708518595481842
mae: 0.028284810896496076
r2: 0.7320721587831767
pearson: 0.8558878929706575

=== Experiment 1924 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008249289364018624
rmse: 0.09082559861635169
mae: 0.038948038730426106
r2: 0.6280431733663105
pearson: 0.8270516884213988

=== Experiment 1163 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006595599535946098
rmse: 0.08121329654647752
mae: 0.02825632697851999
r2: 0.7026073198695462
pearson: 0.8410745408552425

=== Experiment 1895 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007304317695703891
rmse: 0.08546530112100402
mae: 0.04845117407070015
r2: 0.6706515299767841
pearson: 0.8324773853634133

=== Experiment 1477 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007495753704261588
rmse: 0.08657802090751202
mae: 0.03615596409755076
r2: 0.6620197646083483
pearson: 0.8263711537812841

=== Experiment 1833 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006889360725300418
rmse: 0.08300217301553266
mae: 0.03354820957236702
r2: 0.6893617571357471
pearson: 0.8306698064802928

=== Experiment 1792 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007584873045772403
rmse: 0.08709117662411275
mae: 0.03966917295907865
r2: 0.6580014127240474
pearson: 0.8212979334455798

=== Experiment 1509 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007167609888445701
rmse: 0.08466173804290637
mae: 0.03299506422106502
r2: 0.6768156248363484
pearson: 0.8382945608838598

=== Experiment 1631 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006636321506415931
rmse: 0.08146362075439521
mae: 0.032029536506058554
r2: 0.7007711841441713
pearson: 0.8465401043188863

=== Experiment 1360 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005908417871573558
rmse: 0.07686623362422253
mae: 0.02911351121255493
r2: 0.7335920386643238
pearson: 0.8569753003359545

=== Experiment 1826 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006926718689694101
rmse: 0.08322691085036199
mae: 0.03682184701359577
r2: 0.6876773029637333
pearson: 0.8347645279350203

=== Experiment 1886 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006902142336973114
rmse: 0.0830791329815924
mae: 0.03838647645801858
r2: 0.6887854398910427
pearson: 0.8363653568820858

=== Experiment 1728 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0071109452409603684
rmse: 0.08432642077641128
mae: 0.03319794397244521
r2: 0.6793706088514435
pearson: 0.8594321716424691

=== Experiment 1419 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01017353167361119
rmse: 0.10086392652287135
mae: 0.04853169622545742
r2: 0.5412799345505896
pearson: 0.7477320225687106

=== Experiment 1387 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006595341337576232
rmse: 0.08121170689978281
mae: 0.03374964092628118
r2: 0.7026189619204015
pearson: 0.8490073392133538

=== Experiment 1873 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0063332174838050675
rmse: 0.07958151471167829
mae: 0.0356067016639153
r2: 0.7144380111174127
pearson: 0.8460799971467282

=== Experiment 1804 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006922245256746531
rmse: 0.08320003159101882
mae: 0.036815664916189955
r2: 0.6878790080864309
pearson: 0.8320241375247093

=== Experiment 1889 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006753553535009885
rmse: 0.08218000690563299
mae: 0.03516007729703476
r2: 0.6954852435726315
pearson: 0.834899203668708

=== Experiment 1921 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006908917104611714
rmse: 0.08311989596126618
mae: 0.03400292697992344
r2: 0.6884799686000214
pearson: 0.8393151039352801

=== Experiment 1423 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006369235421459336
rmse: 0.07980748975791267
mae: 0.03471488314516167
r2: 0.7128139781612883
pearson: 0.8443763387444969

=== Experiment 1820 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007116031943875609
rmse: 0.08435657617444896
mae: 0.04335329798490422
r2: 0.6791412516558244
pearson: 0.8295054738453522

=== Experiment 1660 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011363192495919699
rmse: 0.10659827623334112
mae: 0.04729499905246631
r2: 0.4876386516824699
pearson: 0.698918469426083

=== Experiment 1822 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00619889947353488
rmse: 0.07873309007993322
mae: 0.03229775654861431
r2: 0.7204943510826182
pearson: 0.8517667755600261

=== Experiment 1946 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006744342731425097
rmse: 0.08212394736874926
mae: 0.0374157679785194
r2: 0.695900554652285
pearson: 0.834568978333452

=== Experiment 1938 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007766627378774728
rmse: 0.08812847087505109
mae: 0.03547352366783526
r2: 0.6498061898451721
pearson: 0.8188158575266122

=== Experiment 1372 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006680434470083899
rmse: 0.0817339248420379
mae: 0.033805550857421825
r2: 0.6987821500279827
pearson: 0.8392158583845386

=== Experiment 1865 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006796166073573597
rmse: 0.08243886264119367
mae: 0.04143838769187093
r2: 0.6935638629640062
pearson: 0.8383126333954574

=== Experiment 1894 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00630512231578987
rmse: 0.07940480033215794
mae: 0.03823363185528627
r2: 0.7157048098775888
pearson: 0.8524520218485461

=== Experiment 1861 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007418487432345504
rmse: 0.08613064165757447
mae: 0.04082823067820245
r2: 0.6655036668015577
pearson: 0.8183513361637165

=== Experiment 1520 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006183602447353434
rmse: 0.07863588523920509
mae: 0.031552228493467206
r2: 0.7211840872604034
pearson: 0.8493075495268345

=== Experiment 1824 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008414609377987197
rmse: 0.09173117996617725
mae: 0.03960051051782877
r2: 0.6205889666993701
pearson: 0.8261563125021723

=== Experiment 1401 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066788484748749755
rmse: 0.08172422208179761
mae: 0.04197455665963376
r2: 0.6988536618539032
pearson: 0.839659545972179

=== Experiment 1088 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005255028623215673
rmse: 0.07249157622245272
mae: 0.025402414804424536
r2: 0.7630530722941803
pearson: 0.8741065362717787

=== Experiment 1780 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007209542775209858
rmse: 0.08490902646485743
mae: 0.03809381753427181
r2: 0.6749248894282274
pearson: 0.834153895127726

=== Experiment 1796 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007315637266483271
rmse: 0.08553149868021295
mae: 0.04213721673699929
r2: 0.6701411355124659
pearson: 0.8210300774445202

=== Experiment 1963 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007357644878919905
rmse: 0.08577671524906923
mae: 0.036452899778737335
r2: 0.6682470307566621
pearson: 0.8185625737299465

=== Experiment 1859 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00811767967879631
rmse: 0.09009816690031107
mae: 0.04113375221564416
r2: 0.6339773961471324
pearson: 0.8188328952382149

=== Experiment 1228 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006587273203169837
rmse: 0.08116201823001838
mae: 0.035494954319291304
r2: 0.7029827505497284
pearson: 0.8424275879707189

=== Experiment 1947 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006868866568575685
rmse: 0.0828786255229639
mae: 0.03548571573929283
r2: 0.6902858296422971
pearson: 0.8313055805627134

=== Experiment 1305 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005909475702811229
rmse: 0.07687311430410003
mae: 0.02953813921957821
r2: 0.7335443415194043
pearson: 0.8574012179555447

=== Experiment 1926 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007457028841204302
rmse: 0.0863540898927451
mae: 0.039402573353187455
r2: 0.6637658516394858
pearson: 0.8150158112634518

=== Experiment 1900 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00750960863890657
rmse: 0.08665799812427338
mae: 0.04533325061594547
r2: 0.6613950517032292
pearson: 0.8261196140581106

=== Experiment 1897 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007610076438239846
rmse: 0.0872357520643907
mae: 0.046294929497997725
r2: 0.6568650028505518
pearson: 0.8155472087391753

=== Experiment 1870 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00588568935174644
rmse: 0.07671824653722503
mae: 0.02906571835288125
r2: 0.7346168576197419
pearson: 0.8572074270161436

=== Experiment 1872 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006283407464172
rmse: 0.079267947268565
mae: 0.03152288334966405
r2: 0.7166839229796014
pearson: 0.846680028272731

=== Experiment 1495 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006351864489139002
rmse: 0.0796985852392563
mae: 0.030211550354744692
r2: 0.7135972258540803
pearson: 0.845430952361095

=== Experiment 1860 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009920797838422209
rmse: 0.09960320194864324
mae: 0.04140135790566793
r2: 0.5526755919425934
pearson: 0.7629760189273129

=== Experiment 1388 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007119505663977753
rmse: 0.08437716316621313
mae: 0.03752139317158058
r2: 0.6789846231453198
pearson: 0.8274281403682278

=== Experiment 1750 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006907765323938273
rmse: 0.08311296724301373
mae: 0.0340596487034374
r2: 0.6885319018836494
pearson: 0.8470735225107312

=== Experiment 1978 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006973596497462607
rmse: 0.0835080624698155
mae: 0.039137525790729835
r2: 0.6855636032439525
pearson: 0.8290731740938572

=== Experiment 1667 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007207486265622763
rmse: 0.08489691552478666
mae: 0.041904490577879955
r2: 0.6750176165403704
pearson: 0.8307607913567033

=== Experiment 1890 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006135144250501773
rmse: 0.07832716163950902
mae: 0.03438606618169125
r2: 0.7233690460283613
pearson: 0.8507413577782905

=== Experiment 1542 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005550923972829556
rmse: 0.07450452317027172
mae: 0.031578695269061666
r2: 0.7497112812135933
pearson: 0.8659118376078508

=== Experiment 1866 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006413182199973288
rmse: 0.08008234636905495
mae: 0.030626101200388927
r2: 0.7108324372605507
pearson: 0.8437465685232015

=== Experiment 1597 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006372194426638135
rmse: 0.07982602599802983
mae: 0.030346250786060194
r2: 0.7126805579201341
pearson: 0.8557346006903729

=== Experiment 1964 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007040026444962922
rmse: 0.08390486544273176
mae: 0.03490359745886406
r2: 0.6825683061492192
pearson: 0.826469880851687

=== Experiment 1451 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008258879879603429
rmse: 0.09087837960485116
mae: 0.03819340227295134
r2: 0.6276107412397004
pearson: 0.8177124586416612

=== Experiment 1980 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00717403898771081
rmse: 0.0846996988643455
mae: 0.040190563686239994
r2: 0.6765257395801476
pearson: 0.827137976810556

=== Experiment 1448 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006052896835148204
rmse: 0.07780036526359117
mae: 0.030614043389333195
r2: 0.7270775457867977
pearson: 0.8549991574740463

=== Experiment 1560 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00638845545192535
rmse: 0.07992781400692346
mae: 0.03417722195736659
r2: 0.711947355446958
pearson: 0.8612841076769222

=== Experiment 1941 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007369552218332137
rmse: 0.08584609611585221
mae: 0.041827823325087145
r2: 0.6677101340633855
pearson: 0.8191292476568717

=== Experiment 1568 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00813448660891409
rmse: 0.09019138877361901
mae: 0.046342154015029065
r2: 0.6332195790654171
pearson: 0.8349692856336989

=== Experiment 1594 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007954562493093092
rmse: 0.08918835402166077
mae: 0.03836679095932712
r2: 0.6413322782571267
pearson: 0.8104203633102318

=== Experiment 1943 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0069300297312184875
rmse: 0.08324680012600176
mae: 0.038854604414753996
r2: 0.6875280095586995
pearson: 0.8297511171318593

=== Experiment 1691 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005775970021994214
rmse: 0.07599980277602182
mae: 0.032409678441625914
r2: 0.7395640539070998
pearson: 0.8602740366569165

=== Experiment 1280 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006372310995119633
rmse: 0.07982675613551908
mae: 0.02967523163809651
r2: 0.7126753018986091
pearson: 0.8444839866170748

=== Experiment 1962 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066542920825815345
rmse: 0.08157384435333138
mae: 0.03712242593344695
r2: 0.6999608987743197
pearson: 0.8381904209271229

=== Experiment 1203 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006760796674668274
rmse: 0.08222406384184787
mae: 0.03986949918745346
r2: 0.6951586535933844
pearson: 0.8368698058759352

=== Experiment 1479 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005489196845289631
rmse: 0.07408911421585246
mae: 0.027227252806314984
r2: 0.7524945302261818
pearson: 0.8781273531802833

=== Experiment 1851 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0075287356918036395
rmse: 0.08676828736239779
mae: 0.04034996338784542
r2: 0.6605326213065601
pearson: 0.8218024827550414

=== Experiment 1794 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006633399409329344
rmse: 0.08144568379803403
mae: 0.04262381575542096
r2: 0.7009029402156923
pearson: 0.8443485325412734

=== Experiment 1898 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007984357993390059
rmse: 0.08935523484043931
mae: 0.03657336338712154
r2: 0.6399888122627391
pearson: 0.8347944451620948

=== Experiment 1977 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006277037809463309
rmse: 0.07922775908394297
mae: 0.0385380777464774
r2: 0.7169711279069169
pearson: 0.8492957757243021

=== Experiment 1854 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006739935116993186
rmse: 0.08209710784792108
mae: 0.04053321083213856
r2: 0.6960992920470803
pearson: 0.8371533210735741

=== Experiment 1757 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006492330119888637
rmse: 0.08057499686558255
mae: 0.034175200131311846
r2: 0.7072636923872311
pearson: 0.8411347278836058

=== Experiment 1902 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067383819231368975
rmse: 0.08208764781096421
mae: 0.03797587876708233
r2: 0.6961693248744475
pearson: 0.8358433910427798

=== Experiment 1776 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007104668216413822
rmse: 0.08428919394806088
mae: 0.04064439592003532
r2: 0.6796536371255175
pearson: 0.8257903483550013

=== Experiment 1876 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006485255428495023
rmse: 0.08053108361679373
mae: 0.02964734969612244
r2: 0.7075826871083595
pearson: 0.8433217655060169

=== Experiment 1819 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006065775037342162
rmse: 0.07788308569479102
mae: 0.030475407094749205
r2: 0.7264968733180024
pearson: 0.8588660685729196

=== Experiment 1939 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007200878396251564
rmse: 0.08485798958407843
mae: 0.04159883970083518
r2: 0.675315562461972
pearson: 0.825520896490687

=== Experiment 1929 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006739476510379141
rmse: 0.08209431472628992
mae: 0.03452892668102691
r2: 0.6961199704174594
pearson: 0.8367681355478667

=== Experiment 1190 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006925072323740556
rmse: 0.08321701943557314
mae: 0.03674787734106115
r2: 0.6877515368799576
pearson: 0.8303887044083316

=== Experiment 1960 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006922400250117774
rmse: 0.08320096303599961
mae: 0.037687726836923306
r2: 0.6878720195035392
pearson: 0.8309326841543568

=== Experiment 1877 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006813399978906174
rmse: 0.08254332183106138
mae: 0.03943272761891104
r2: 0.6927867937577823
pearson: 0.8414100811287627

=== Experiment 1602 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007443754657676105
rmse: 0.08627719662620074
mae: 0.03386962723478341
r2: 0.6643643787323572
pearson: 0.8184179412193169

=== Experiment 1933 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007199377610256899
rmse: 0.08484914619639315
mae: 0.034602259627493574
r2: 0.6753832322419251
pearson: 0.8263586261832255

=== Experiment 1932 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007602274936270534
rmse: 0.08719102554890919
mae: 0.04458775184890706
r2: 0.6572167691406445
pearson: 0.8290078661931102

=== Experiment 1081 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006676958297094984
rmse: 0.0817126568965603
mae: 0.031513109834172034
r2: 0.6989388891380721
pearson: 0.8437587011149515

=== Experiment 1722 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0060884311696303
rmse: 0.07802839976335732
mae: 0.03452010287925282
r2: 0.7254753182848568
pearson: 0.8562030298381235

=== Experiment 1918 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006396803115496789
rmse: 0.07998001697609715
mae: 0.03932672609250705
r2: 0.7115709629706096
pearson: 0.846624365501248

=== Experiment 1848 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0074811598334275885
rmse: 0.08649369822956808
mae: 0.04629837695431082
r2: 0.6626777958209995
pearson: 0.8193664402531

=== Experiment 1350 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006286996782592011
rmse: 0.0792905844510684
mae: 0.03200218043021327
r2: 0.716522082191823
pearson: 0.8478413527740248

=== Experiment 1937 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006528818617415978
rmse: 0.08080110529823202
mae: 0.03594706115718457
r2: 0.7056184420935393
pearson: 0.840616741385032

=== Experiment 1730 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00898223516319706
rmse: 0.09477465464562274
mae: 0.04699318221156206
r2: 0.5949949698754711
pearson: 0.7865124814879644

=== Experiment 1094 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005939211829255663
rmse: 0.07706628205159285
mae: 0.03378245018705762
r2: 0.732203552665895
pearson: 0.8574200543346747

=== Experiment 1538 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008692477658757402
rmse: 0.09323345782902939
mae: 0.04337349101772455
r2: 0.6080600082186247
pearson: 0.7962089493881123

=== Experiment 1677 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006671645599544298
rmse: 0.08168014201471677
mae: 0.04057377506903857
r2: 0.6991784363323356
pearson: 0.8396982416500761

=== Experiment 1532 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005551696982099931
rmse: 0.07450971065639653
mae: 0.028703397234263947
r2: 0.7496764265658193
pearson: 0.8663277026637852

=== Experiment 1546 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00870990684661324
rmse: 0.09332688169339656
mae: 0.03567220583180639
r2: 0.6072741338093772
pearson: 0.784303073724684

=== Experiment 1420 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006799848115899353
rmse: 0.08246119157457861
mae: 0.04618506449659083
r2: 0.6933978412961286
pearson: 0.8448065269355916

=== Experiment 1816 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006758886587798447
rmse: 0.08221244788837301
mae: 0.03274885211394194
r2: 0.6952447785696514
pearson: 0.8348682824351502

=== Experiment 1357 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009299446833217297
rmse: 0.0964336395311164
mae: 0.04337421556412901
r2: 0.5806920352897916
pearson: 0.7753973875852238

=== Experiment 1522 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006731664680374483
rmse: 0.08204672254498947
mae: 0.0314256810493101
r2: 0.6964722023941201
pearson: 0.8434167430796135

=== Experiment 1910 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006036909828703538
rmse: 0.0776975535567468
mae: 0.03026063794029632
r2: 0.7277983928709681
pearson: 0.8567822922164241

=== Experiment 1343 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007543784190590199
rmse: 0.08685496065620087
mae: 0.03759135956164446
r2: 0.6598540911196249
pearson: 0.8227924020109282

=== Experiment 1952 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007169990101588784
rmse: 0.08467579407120304
mae: 0.031393552086741
r2: 0.6767083020733388
pearson: 0.8270952455931171

=== Experiment 1733 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005834185510878618
rmse: 0.0763818401904446
mae: 0.02968803968628698
r2: 0.7369391431358983
pearson: 0.8594147651229647

=== Experiment 1759 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.015511228445996632
rmse: 0.12454408234033695
mae: 0.06018654093780534
r2: 0.3006055363837402
pearson: 0.5700349298544429

=== Experiment 1762 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006605320083077322
rmse: 0.0812731202986407
mae: 0.0362070162734555
r2: 0.7021690246777393
pearson: 0.8442732395495782

=== Experiment 1447 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00634262168115438
rmse: 0.07964057810660581
mae: 0.036664217735791226
r2: 0.7140139799980361
pearson: 0.8456630328028313

=== Experiment 1974 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007606600604499724
rmse: 0.08721582771779285
mae: 0.053101713794349484
r2: 0.6570217266640626
pearson: 0.8289283019378045

=== Experiment 1981 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006318803864592793
rmse: 0.07949090428843285
mae: 0.03273256130341869
r2: 0.7150879148637751
pearson: 0.8480636682220478

=== Experiment 1068 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0066330747377249715
rmse: 0.08144369059494401
mae: 0.034742307115649386
r2: 0.7009175795154343
pearson: 0.8386362252343319

=== Experiment 1685 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007300424008364088
rmse: 0.08544251873841319
mae: 0.030610757420058206
r2: 0.6708270946251929
pearson: 0.8500426281723611

=== Experiment 1925 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0073214764447352
rmse: 0.08556562653738474
mae: 0.048839808065296614
r2: 0.6698778495351052
pearson: 0.830114709064662

=== Experiment 1078 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072386730388293435
rmse: 0.08508039162362467
mae: 0.03507626100533044
r2: 0.6736114186628344
pearson: 0.8262165116856982

=== Experiment 1852 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006260595254289048
rmse: 0.07912392340050542
mae: 0.04075224144534768
r2: 0.7177125154828023
pearson: 0.8519232872215682

=== Experiment 1498 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007298485629997756
rmse: 0.08543117481340026
mae: 0.038272838430456146
r2: 0.6709144952525854
pearson: 0.820414303466099

=== Experiment 1513 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006362085768428417
rmse: 0.07976268405983099
mae: 0.035292762707626685
r2: 0.7131363528696495
pearson: 0.8540406063928389

=== Experiment 1623 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006566784849488871
rmse: 0.08103570107976404
mae: 0.03691797096547234
r2: 0.7039065613995892
pearson: 0.8509702027503019

=== Experiment 1591 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006152614597186373
rmse: 0.07843860399819959
mae: 0.03241275463698765
r2: 0.7225813157856402
pearson: 0.8575165139388347

=== Experiment 1243 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005682018652417329
rmse: 0.07537916590422933
mae: 0.029546487200700005
r2: 0.7438002798101615
pearson: 0.8626132451916472

=== Experiment 1084 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005745620897846165
rmse: 0.07579987399624201
mae: 0.03188673672427218
r2: 0.7409324825572641
pearson: 0.8610202135311926

=== Experiment 1543 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00547000321118558
rmse: 0.07395947005749554
mae: 0.029212049454193055
r2: 0.7533599627401691
pearson: 0.8761939816146529

=== Experiment 1204 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005577029760589543
rmse: 0.07467951366063884
mae: 0.02847878514776917
r2: 0.7485341827335312
pearson: 0.8731174571764245

=== Experiment 1968 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008220707045285132
rmse: 0.09066811482150235
mae: 0.04156671450289249
r2: 0.6293319375379631
pearson: 0.8013864499218004

=== Experiment 1694 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00809993490916308
rmse: 0.08999963838351285
mae: 0.05218994246988602
r2: 0.6347774999998244
pearson: 0.8124617482210764

=== Experiment 1746 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.01074098252735338
rmse: 0.10363871152881717
mae: 0.04303252720369892
r2: 0.5156938253095751
pearson: 0.7514660157177663

=== Experiment 1429 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060202067118882326
rmse: 0.07758999105482764
mae: 0.02981324939971139
r2: 0.7285515290565665
pearson: 0.8575696944764349

=== Experiment 1828 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006525983278649114
rmse: 0.08078355821978328
mae: 0.03380550411741938
r2: 0.7057462862706091
pearson: 0.8478634348423423

=== Experiment 1533 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005405906791073961
rmse: 0.07352487192150668
mae: 0.026970804576561428
r2: 0.7562500421120102
pearson: 0.8713341986066693

=== Experiment 1930 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006905957082783407
rmse: 0.08310208831782392
mae: 0.03557014250361196
r2: 0.6886134346814546
pearson: 0.8303138046115792

=== Experiment 1389 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00631808650590111
rmse: 0.07948639195422767
mae: 0.034652970810235446
r2: 0.7151202602514494
pearson: 0.8529099421243297

=== Experiment 1686 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005900841212054179
rmse: 0.07681693310757844
mae: 0.0349691465970954
r2: 0.7339336669073098
pearson: 0.8571991430227276

=== Experiment 1385 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007406568823462074
rmse: 0.08606142471201644
mae: 0.03989752302984254
r2: 0.6660410716303304
pearson: 0.8173849401667185

=== Experiment 1454 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0056464709440938315
rmse: 0.07514300329434426
mae: 0.02909719229334054
r2: 0.7454031103327251
pearson: 0.8645105782128396

=== Experiment 1381 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005869860181362235
rmse: 0.07661501276748725
mae: 0.03214154448285639
r2: 0.7353305879454151
pearson: 0.8589117092377575

=== Experiment 1140 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005809409705778039
rmse: 0.07621948376745961
mae: 0.0357240783866372
r2: 0.7380562732832174
pearson: 0.8610523139388063

=== Experiment 1871 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007552591355109393
rmse: 0.08690564627864747
mae: 0.04809709416405459
r2: 0.6594569799477841
pearson: 0.8392171981806996

=== Experiment 1957 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007078323839581629
rmse: 0.08413277506169417
mae: 0.038518775139243545
r2: 0.6808414934818341
pearson: 0.8267423192448474

=== Experiment 1940 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00793941691247096
rmse: 0.08910340572879893
mae: 0.0395510163095214
r2: 0.642015185821299
pearson: 0.8081571263840447

=== Experiment 1103 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006277139153437497
rmse: 0.07922839865501194
mae: 0.03655924813585138
r2: 0.7169665583517257
pearson: 0.8483679190833956

=== Experiment 1473 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0069397467861351935
rmse: 0.08330514261517828
mae: 0.03681786397003714
r2: 0.6870898718293083
pearson: 0.8311950671635615

=== Experiment 1415 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007180316945107804
rmse: 0.084736750852908
mae: 0.03433223995068662
r2: 0.6762426692442576
pearson: 0.8573746836406766

=== Experiment 1882 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006382441025630391
rmse: 0.07989018103390673
mae: 0.03798408491776738
r2: 0.7122185432814464
pearson: 0.8480488108771255

=== Experiment 1583 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005818371996123693
rmse: 0.07627825375638651
mae: 0.03252180881914918
r2: 0.7376521675561376
pearson: 0.8611323158078878

=== Experiment 1535 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008305365972597486
rmse: 0.09113378063373365
mae: 0.03771672842753434
r2: 0.6255147037666926
pearson: 0.8060683341112966

=== Experiment 1972 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007297275521020493
rmse: 0.0854240921580118
mae: 0.04342119744691484
r2: 0.6709690585337604
pearson: 0.8359954172068971

=== Experiment 1129 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006193627892832922
rmse: 0.07869960541726319
mae: 0.03070836056419963
r2: 0.7207320443362693
pearson: 0.8495180498532662

=== Experiment 1699 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006738958164717559
rmse: 0.08209115765243878
mae: 0.03439515616472096
r2: 0.6961433423951988
pearson: 0.8391341136408507

=== Experiment 1875 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007013263973467525
rmse: 0.08374523254172457
mae: 0.04736317756541961
r2: 0.683775014209882
pearson: 0.8364071100998516

=== Experiment 1216 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006197255320390176
rmse: 0.07872264807785734
mae: 0.031685058407757136
r2: 0.7205684852242658
pearson: 0.8488690413421109

=== Experiment 1987 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006536414374846636
rmse: 0.0808480944416542
mae: 0.03835327588564856
r2: 0.7052759527341395
pearson: 0.8410437639440865

=== Experiment 1856 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006243862017698469
rmse: 0.07901811195984418
mae: 0.032849078742947714
r2: 0.7184670097558109
pearson: 0.8508971703828939

=== Experiment 1599 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0064925920033625005
rmse: 0.08057662194062556
mae: 0.03618985616373164
r2: 0.7072518841766584
pearson: 0.8666558835036511

=== Experiment 1689 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005787556337226699
rmse: 0.07607599054384175
mae: 0.034305928547247774
r2: 0.7390416320527944
pearson: 0.8608919277695578

=== Experiment 1696 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005953208691628147
rmse: 0.07715703915799353
mae: 0.0318303074969301
r2: 0.7315724403020099
pearson: 0.8571114245855174

=== Experiment 1903 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.012442821639655863
rmse: 0.11154739638223683
mae: 0.04442477183823508
r2: 0.4389586487725311
pearson: 0.7267549910025302

=== Experiment 1206 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005772239268347202
rmse: 0.07597525431577838
mae: 0.030008885642234518
r2: 0.7397322719470132
pearson: 0.8679215907456641

=== Experiment 1518 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0068697901627906
rmse: 0.08288419730437521
mae: 0.031675296410975634
r2: 0.6902441851856513
pearson: 0.8669198472093

=== Experiment 1229 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005867774621728294
rmse: 0.07660140091230899
mae: 0.030339066566265827
r2: 0.7354246249113845
pearson: 0.8598415804302766

=== Experiment 1880 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006418826192636598
rmse: 0.08011757730134254
mae: 0.03660275403734766
r2: 0.7105779521154733
pearson: 0.8439491051010803

=== Experiment 1571 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006134974477082724
rmse: 0.07832607788650421
mae: 0.0292584234573653
r2: 0.7233767010371033
pearson: 0.8531009761164854

=== Experiment 1292 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007454180129237297
rmse: 0.08633759395093946
mae: 0.04287131101802187
r2: 0.6638942988082639
pearson: 0.8164999483053731

=== Experiment 1288 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006598875536224075
rmse: 0.08123346315542675
mae: 0.035115575654855485
r2: 0.7024596064588902
pearson: 0.8476688735501274

=== Experiment 1502 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006407999130533995
rmse: 0.08004997895398847
mae: 0.03565233625348727
r2: 0.711066139580325
pearson: 0.8437153496816615

=== Experiment 1384 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0063506980942702905
rmse: 0.07969126736519058
mae: 0.03168596439623099
r2: 0.7136498180853408
pearson: 0.8468626879804609

=== Experiment 1934 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007761392222890182
rmse: 0.08809876402589416
mae: 0.04745635004133221
r2: 0.6500422407198383
pearson: 0.8307632300874386

=== Experiment 1778 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007399235042384188
rmse: 0.08601880632968693
mae: 0.03569443904093197
r2: 0.6663717485912884
pearson: 0.8168578309852723

=== Experiment 1688 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006179186130289086
rmse: 0.07860779942403352
mae: 0.029896943151626342
r2: 0.721383217053065
pearson: 0.8617890100902241

=== Experiment 1793 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006661946284018275
rmse: 0.08162074665192837
mae: 0.03438933974168992
r2: 0.6996157742004085
pearson: 0.8460474063508682

=== Experiment 1664 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006035291527943277
rmse: 0.07768713875502996
mae: 0.02949876431936211
r2: 0.7278713613399799
pearson: 0.8532823751139792

=== Experiment 1519 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006172448032873393
rmse: 0.07856492877151607
mae: 0.030485578335734405
r2: 0.7216870348998752
pearson: 0.850550865139716

=== Experiment 1183 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006624741173818751
rmse: 0.08139251300837658
mae: 0.03367870354283108
r2: 0.701293336244084
pearson: 0.8452395340697995

=== Experiment 1613 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006304768434376819
rmse: 0.07940257196323566
mae: 0.03659162981733261
r2: 0.7157207662347482
pearson: 0.8479985776923463

=== Experiment 1896 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00706057673786047
rmse: 0.08402723807111875
mae: 0.03786219133035592
r2: 0.6816417024873443
pearson: 0.8338879600255759

=== Experiment 1605 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00709360349356569
rmse: 0.08422353289648736
mae: 0.03872556448197098
r2: 0.6801525406087828
pearson: 0.8262021082725824

=== Experiment 1841 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008824527669864187
rmse: 0.09393895714699087
mae: 0.039624587177986816
r2: 0.6021059313374745
pearson: 0.7786323349136471

=== Experiment 1850 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0064559598098064275
rmse: 0.08034898760909454
mae: 0.030566633999520317
r2: 0.7089036136610403
pearson: 0.8454053567819383

=== Experiment 1992 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006840226506231355
rmse: 0.08270566187529942
mae: 0.042623781726394736
r2: 0.6915771974479474
pearson: 0.8395621671871095

=== Experiment 1984 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006369472864731279
rmse: 0.07980897734422662
mae: 0.03134391026689374
r2: 0.7128032719486632
pearson: 0.854814495537822

=== Experiment 1996 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006339228199349825
rmse: 0.07961927027642131
mae: 0.03175306844384214
r2: 0.7141669905990178
pearson: 0.8598435935933919

=== Experiment 1954 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007330451649657573
rmse: 0.08561805679678541
mae: 0.04742035258095519
r2: 0.6694731614954452
pearson: 0.8283257436498226

=== Experiment 1935 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007183277757901487
rmse: 0.0847542197055786
mae: 0.03745155526898287
r2: 0.6761091674985436
pearson: 0.8294946472224992

=== Experiment 1537 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005446402120800319
rmse: 0.0737997433654096
mae: 0.029618198524903456
r2: 0.7544241255180064
pearson: 0.8703699644480914

=== Experiment 1786 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006012102972155266
rmse: 0.07753775191579432
mae: 0.03264018452359236
r2: 0.7289169231143973
pearson: 0.8540078284541409

=== Experiment 1985 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0068961970110514225
rmse: 0.0830433441706885
mae: 0.04106640826866809
r2: 0.6890535120201137
pearson: 0.8331302252976618

=== Experiment 1714 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005702133740537995
rmse: 0.07551247407242061
mae: 0.03182982816157243
r2: 0.7428932993401279
pearson: 0.865725097368554

=== Experiment 1437 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006486379894376934
rmse: 0.08053806487852147
mae: 0.028406917089914926
r2: 0.7075319854366593
pearson: 0.8536113970594912

=== Experiment 1577 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00805262700949316
rmse: 0.08973643078200269
mae: 0.03716414237951978
r2: 0.6369105923741423
pearson: 0.8357766707405048

=== Experiment 1702 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.011366452665055763
rmse: 0.10661356698401833
mae: 0.04511697263436336
r2: 0.4874916520910345
pearson: 0.7316122416270079

=== Experiment 1377 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006724183285037235
rmse: 0.08200111758407463
mae: 0.03400060298958155
r2: 0.6968095352170629
pearson: 0.8362145055459091

=== Experiment 1683 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005906598118000242
rmse: 0.07685439556720385
mae: 0.03477890348033208
r2: 0.7336740905520098
pearson: 0.8571947769321189

=== Experiment 1857 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00525196901978562
rmse: 0.072470469984578
mae: 0.032221253037598034
r2: 0.763191028466207
pearson: 0.8739482449136744

=== Experiment 1709 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006314391390702813
rmse: 0.07946314485786989
mae: 0.0323894577644661
r2: 0.7152868713693961
pearson: 0.8465737729303268

=== Experiment 1768 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008313186785004227
rmse: 0.09117667895358016
mae: 0.039327591178779424
r2: 0.6251620667774753
pearson: 0.7928237091113259

=== Experiment 1821 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006436756779998303
rmse: 0.08022940096995804
mae: 0.03211142355022336
r2: 0.7097694698231889
pearson: 0.8435605284625906

=== Experiment 1496 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006476152205318675
rmse: 0.08047454383417575
mae: 0.031737496874304355
r2: 0.7079931474347461
pearson: 0.8438063569138259

=== Experiment 1914 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006894040045820682
rmse: 0.08303035617062401
mae: 0.036803917015335634
r2: 0.6891507686330147
pearson: 0.8319799286959249

=== Experiment 1383 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059509541773392184
rmse: 0.07714242786780319
mae: 0.032968648019024265
r2: 0.7316740953590102
pearson: 0.8570441787712333

=== Experiment 1566 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006285798783318635
rmse: 0.07928302960481919
mae: 0.03051800808659371
r2: 0.7165760994517798
pearson: 0.847559121281473

=== Experiment 1341 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007197586368297163
rmse: 0.08483859008904594
mae: 0.047980306480662
r2: 0.6754639985535038
pearson: 0.8350072582288811

=== Experiment 1764 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006702715022418257
rmse: 0.08187011067794069
mae: 0.03362141639731566
r2: 0.6977775297296476
pearson: 0.8354973017516264

=== Experiment 1961 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006646363198762689
rmse: 0.08152523044286775
mae: 0.044074038068213826
r2: 0.7003184086559426
pearson: 0.8431282067246079

=== Experiment 1879 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060229906362539954
rmse: 0.07760792895222753
mae: 0.03834106087985783
r2: 0.728426003132213
pearson: 0.8641783608245165

=== Experiment 1666 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008849052637841822
rmse: 0.09406940330331548
mae: 0.0428690023163394
r2: 0.6010001113256266
pearson: 0.7811552463026062

=== Experiment 1494 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006415506795389186
rmse: 0.08009685883596925
mae: 0.035114312917734515
r2: 0.7107276222763805
pearson: 0.8493704072981959

=== Experiment 1628 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005715998881520947
rmse: 0.07560422528880874
mae: 0.030686911968035097
r2: 0.7422681262357218
pearson: 0.8620725752667877

=== Experiment 1452 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006995537100787308
rmse: 0.08363932747689515
mae: 0.03133152485213788
r2: 0.6845743110968397
pearson: 0.8413219036214196

=== Experiment 1863 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005868739875544985
rmse: 0.07660770115037381
mae: 0.03185530826736752
r2: 0.7353811020416305
pearson: 0.8598162224297872

=== Experiment 1569 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007247883241845993
rmse: 0.08513450089033231
mae: 0.029952568831790885
r2: 0.6731961346625339
pearson: 0.8249527927893874

=== Experiment 1640 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006212108122925242
rmse: 0.0788169278957588
mae: 0.030461233749977848
r2: 0.719898778895178
pearson: 0.8618335190166287

=== Experiment 1391 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006820950332676175
rmse: 0.08258904487083123
mae: 0.03421274113662137
r2: 0.6924463516294577
pearson: 0.8345948719159691

=== Experiment 1729 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00652032659934759
rmse: 0.08074853930163436
mae: 0.030353591473365332
r2: 0.7060013434506198
pearson: 0.8402899758900798

=== Experiment 1647 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005784828507819287
rmse: 0.07605806011080803
mae: 0.03555964616659007
r2: 0.739164628680165
pearson: 0.8610847292591488

=== Experiment 1811 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006516820942593827
rmse: 0.080726829137492
mae: 0.03248494059611436
r2: 0.7061594119706894
pearson: 0.8417767238533098

=== Experiment 1408 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006563924453171343
rmse: 0.08101805016890584
mae: 0.03360298129520953
r2: 0.7040355354105898
pearson: 0.8391097938713676

=== Experiment 1443 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012642769686793946
rmse: 0.11244007153499123
mae: 0.05334493383420177
r2: 0.42994307933094056
pearson: 0.657676568621214

=== Experiment 1818 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006158278887592102
rmse: 0.07847470221410274
mae: 0.02892121304124042
r2: 0.7223259154242908
pearson: 0.8511199782653377

=== Experiment 1319 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00561894419977652
rmse: 0.07495961712666707
mae: 0.028838485609251285
r2: 0.7466442791185459
pearson: 0.8641219004683973

=== Experiment 1266 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005631772242637373
rmse: 0.0750451347033062
mae: 0.02968010078329289
r2: 0.7460658683120035
pearson: 0.8651772071324141

=== Experiment 1481 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006936565690364729
rmse: 0.08328604739309417
mae: 0.03292244589338392
r2: 0.6872333060374911
pearson: 0.835505839466729

=== Experiment 1080 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006222522711082994
rmse: 0.07888296844746015
mae: 0.033575523643593584
r2: 0.7194291896989555
pearson: 0.8508327945742489

=== Experiment 1441 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007562717752241126
rmse: 0.08696388763297744
mae: 0.04051920732597444
r2: 0.6590003851580823
pearson: 0.8267373486343436

=== Experiment 1596 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00595735918652747
rmse: 0.07718393088284291
mae: 0.031586895581275136
r2: 0.7313852963137719
pearson: 0.8559897146463016

=== Experiment 1622 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0077296302946639655
rmse: 0.0879183160363298
mae: 0.03931201541702152
r2: 0.6514743720840646
pearson: 0.8180121413847357

=== Experiment 1445 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010753333285395048
rmse: 0.10369828005032218
mae: 0.044068710384153775
r2: 0.5151369350654604
pearson: 0.7678326538973116

=== Experiment 1284 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006294842962005277
rmse: 0.0793400463952806
mae: 0.03529753629827878
r2: 0.7161683014154465
pearson: 0.8497005996938088

=== Experiment 1834 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0070542025786380705
rmse: 0.08398930038188239
mae: 0.03952981241874352
r2: 0.6819291105212002
pearson: 0.8279674318507741

=== Experiment 1908 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005954831610278732
rmse: 0.07716755542505369
mae: 0.03447073041210247
r2: 0.7314992636143561
pearson: 0.856131394369471

=== Experiment 1639 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006324458829033247
rmse: 0.07952646621743763
mae: 0.033900896084158666
r2: 0.7148329350060951
pearson: 0.8523041406810719

=== Experiment 1579 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0072575406007658345
rmse: 0.08519120025428585
mae: 0.03135954591170124
r2: 0.6727606885993671
pearson: 0.8357967984283109

=== Experiment 1969 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01265432091949857
rmse: 0.11249142598215461
mae: 0.0499655261233398
r2: 0.42942223933237533
pearson: 0.6560966176570442

=== Experiment 1476 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006075137391998806
rmse: 0.07794316770569956
mae: 0.028166117871043773
r2: 0.726074728867222
pearson: 0.8691672759969643

=== Experiment 1721 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007435807056642829
rmse: 0.0862311257994631
mae: 0.03444954160931278
r2: 0.6647227325649407
pearson: 0.8215920340129742

=== Experiment 1787 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010249184166822698
rmse: 0.10123825446353121
mae: 0.04427316545462075
r2: 0.5378687969288916
pearson: 0.7418804409001686

=== Experiment 1427 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00630872526110884
rmse: 0.07942748429296272
mae: 0.0312615531253919
r2: 0.7155423546589339
pearson: 0.8489260965392865

=== Experiment 1486 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005811903386037782
rmse: 0.07623584056097094
mae: 0.030457982557048024
r2: 0.7379438343378584
pearson: 0.8593163078158222

=== Experiment 1687 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007292870831890309
rmse: 0.08539830696149842
mae: 0.044370059145032725
r2: 0.6711676640279883
pearson: 0.8282334346121755

=== Experiment 1680 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007447991483585404
rmse: 0.08630174670066304
mae: 0.048289931919690686
r2: 0.6641733421168765
pearson: 0.8273961086431836

=== Experiment 1643 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005690987631045955
rmse: 0.07543863486997862
mae: 0.032152586382314094
r2: 0.7433958725113462
pearson: 0.8665849927364194

=== Experiment 1907 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008194014205384587
rmse: 0.09052079432585966
mae: 0.04357749759181683
r2: 0.6305355059406614
pearson: 0.79507338166913

=== Experiment 1130 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007289592186453333
rmse: 0.08537910860657502
mae: 0.03901084017082364
r2: 0.6713154967076993
pearson: 0.8222761478100828

=== Experiment 1428 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006463984488644206
rmse: 0.08039890850405997
mae: 0.03294992212656699
r2: 0.7085417844241763
pearson: 0.8430665046635998

=== Experiment 1999 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005573872493596799
rmse: 0.07465837189221848
mae: 0.030197805022014787
r2: 0.7486765425126145
pearson: 0.865278735020178

=== Experiment 1809 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00645173499669185
rmse: 0.08032269291235106
mae: 0.038273928696859624
r2: 0.7090941086248941
pearson: 0.8537506462479675

=== Experiment 1681 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006377152097798962
rmse: 0.07985707293533217
mae: 0.035561366920402133
r2: 0.7124570187095303
pearson: 0.8464374532676259

=== Experiment 1670 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006243931829157214
rmse: 0.07901855370200858
mae: 0.037665321425879965
r2: 0.7184638619878625
pearson: 0.8505866206830821

=== Experiment 1769 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009574923859629996
rmse: 0.09785153989401493
mae: 0.036631464842878915
r2: 0.5682708974155544
pearson: 0.7552301924457574

=== Experiment 1294 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006086131854079607
rmse: 0.07801366453435966
mae: 0.028916798486450745
r2: 0.7255789934110314
pearson: 0.8549543399852612

=== Experiment 1951 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006523732727423827
rmse: 0.0807696275057885
mae: 0.04157711963271347
r2: 0.705847762634814
pearson: 0.843758030625409

=== Experiment 1737 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0052351250078172375
rmse: 0.0723541637213591
mae: 0.027606643325385507
r2: 0.7639505175522447
pearson: 0.8741306238935395

=== Experiment 1344 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006256682763225508
rmse: 0.07909919571794335
mae: 0.030075762026121527
r2: 0.7178889279828384
pearson: 0.8600142791063452

=== Experiment 2000 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005882211448703433
rmse: 0.07669557646111953
mae: 0.03035196735060722
r2: 0.734773674737209
pearson: 0.8589891005419427

=== Experiment 1246 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006944185592333412
rmse: 0.08333178020619392
mae: 0.03564912299350475
r2: 0.6868897280114985
pearson: 0.8293493523307512

=== Experiment 1658 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005181906553909728
rmse: 0.07198546071193633
mae: 0.028381245546394714
r2: 0.7663501142157015
pearson: 0.8770273311648735

=== Experiment 1920 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006261428129373493
rmse: 0.07912918633079385
mae: 0.029944006227904628
r2: 0.7176749615118211
pearson: 0.8481599621981456

=== Experiment 1212 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005603883945781435
rmse: 0.07485909394176124
mae: 0.03295182258364151
r2: 0.7473233393426596
pearson: 0.8667629356979722

=== Experiment 1299 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0063656665846819165
rmse: 0.07978512759081054
mae: 0.030435830478373796
r2: 0.7129748954408169
pearson: 0.8504851691941131

=== Experiment 1603 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008266013344531527
rmse: 0.09091761844951465
mae: 0.04009050375827225
r2: 0.6272890964457706
pearson: 0.7930363557383455

=== Experiment 1313 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005822207294613503
rmse: 0.07630338979765908
mae: 0.03807684717065536
r2: 0.737479235635276
pearson: 0.8621780393638575

=== Experiment 1922 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005722216356725019
rmse: 0.07564533268302162
mae: 0.028676392621012268
r2: 0.7419877830152904
pearson: 0.8628711861558048

=== Experiment 1675 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007351374841714406
rmse: 0.08574015886219483
mae: 0.04137372231630488
r2: 0.6685297439746039
pearson: 0.8206173204906133

=== Experiment 1998 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00735002184120948
rmse: 0.08573226837783705
mae: 0.035731529167438394
r2: 0.6685907501718698
pearson: 0.8324903466609215

=== Experiment 1855 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006189694565730855
rmse: 0.07867461195157467
mae: 0.03196413880552007
r2: 0.7209093963241118
pearson: 0.8581989903025095

=== Experiment 1858 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072422568942140265
rmse: 0.08510145059994
mae: 0.03602599362194457
r2: 0.6734498242009144
pearson: 0.8424295779538807

=== Experiment 1638 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008067727472042676
rmse: 0.08982052923492867
mae: 0.05308862040482766
r2: 0.6362297191640061
pearson: 0.8231258371281464

=== Experiment 1772 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006285465149667816
rmse: 0.07928092550965721
mae: 0.028634854675892427
r2: 0.7165911428462517
pearson: 0.8466956145872814

=== Experiment 1988 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006384949626056023
rmse: 0.07990587979652075
mae: 0.03438204374618083
r2: 0.7121054315923743
pearson: 0.844623060372134

=== Experiment 1610 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006192890902270518
rmse: 0.07869492297645711
mae: 0.039440967393852146
r2: 0.7207652749163538
pearson: 0.8587414756507448

=== Experiment 1614 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010537238194752514
rmse: 0.1026510506266376
mae: 0.04659826573836813
r2: 0.5248805675918093
pearson: 0.7423336073556703

=== Experiment 1734 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008802284490857374
rmse: 0.09382049078350302
mae: 0.04120371105069983
r2: 0.6031088664889197
pearson: 0.8231229194862252

=== Experiment 1425 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006813311266604654
rmse: 0.082542784461179
mae: 0.030323909028187122
r2: 0.6927907937564124
pearson: 0.842858138912217

=== Experiment 1727 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007273924735382991
rmse: 0.08528730700041473
mae: 0.03270890096877453
r2: 0.6720219351806886
pearson: 0.8340724478429353

=== Experiment 1574 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007092124479130101
rmse: 0.08421475214669993
mae: 0.035509925174301585
r2: 0.6802192287187194
pearson: 0.8322321140431268

=== Experiment 1842 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006882150672120834
rmse: 0.08295872872772843
mae: 0.03307941464268024
r2: 0.6896868552602274
pearson: 0.8475885116238088

=== Experiment 1949 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006357953755321172
rmse: 0.07973677793415766
mae: 0.035407809197241985
r2: 0.7133226635220171
pearson: 0.8447460577128839

=== Experiment 1565 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006782858732284119
rmse: 0.08235811272900878
mae: 0.03421430510752232
r2: 0.6941638851257401
pearson: 0.8417188744784035

=== Experiment 1774 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009226701250201233
rmse: 0.09605571950800865
mae: 0.04311611542140819
r2: 0.5839721016102064
pearson: 0.7704349745786007

=== Experiment 1911 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006419427814404184
rmse: 0.08012133183119327
mae: 0.03834597466581902
r2: 0.7105508252547665
pearson: 0.850679924914165

=== Experiment 1161 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006982674003193987
rmse: 0.08356239586796196
mae: 0.03760445134503636
r2: 0.6851543025058404
pearson: 0.8290557453213532

=== Experiment 1982 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008863567673152493
rmse: 0.09414652236356101
mae: 0.04080755495681247
r2: 0.6003456347720242
pearson: 0.7798838251074065

=== Experiment 1853 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006272389486124996
rmse: 0.07919841845722045
mae: 0.03479096438260879
r2: 0.7171807187603578
pearson: 0.8477458260382055

=== Experiment 1557 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008056643611062016
rmse: 0.0897588079859688
mae: 0.03967162673985048
r2: 0.636729485577244
pearson: 0.8242439488929767

=== Experiment 1942 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.010528816618935975
rmse: 0.10261002201995659
mae: 0.044376737205018287
r2: 0.5252602927387583
pearson: 0.7333906506893048

=== Experiment 1869 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007166236802551619
rmse: 0.08465362840747949
mae: 0.03267417123366071
r2: 0.6768775366749713
pearson: 0.8256519805343044

=== Experiment 1433 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005299296873933828
rmse: 0.0727962696429826
mae: 0.02726840718065004
r2: 0.7610570363532458
pearson: 0.8769125284671578

=== Experiment 1738 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006781357632787091
rmse: 0.08234899897865845
mae: 0.03655818099540091
r2: 0.6942315690413765
pearson: 0.8361149859442731

=== Experiment 1976 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006129976860190899
rmse: 0.07829416874960037
mae: 0.03259570931588566
r2: 0.7236020413831361
pearson: 0.8526404979009139

=== Experiment 1668 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007031830527114982
rmse: 0.08385601067970609
mae: 0.0362239389285298
r2: 0.6829378564776833
pearson: 0.8385771109793453

=== Experiment 1813 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006345752719891477
rmse: 0.0796602329891865
mae: 0.03314440530694047
r2: 0.7138728028394556
pearson: 0.8462492692473906

=== Experiment 1567 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062420529979520765
rmse: 0.07900666426290935
mae: 0.03151778723190225
r2: 0.7185485776599653
pearson: 0.8478079612182419

=== Experiment 1548 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006160898198746271
rmse: 0.07849138932867905
mae: 0.039363681109772015
r2: 0.7222078118371964
pearson: 0.8528923334681001

=== Experiment 1291 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007430901209394699
rmse: 0.08620267518699579
mae: 0.038573168406867496
r2: 0.6649439350581305
pearson: 0.818145314782354

=== Experiment 1979 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007234815836848447
rmse: 0.08505772061869779
mae: 0.03362474318912972
r2: 0.6737853382024683
pearson: 0.8352787182662444

=== Experiment 1953 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006505812784648441
rmse: 0.08065861878713546
mae: 0.03153252183112995
r2: 0.7066557649673861
pearson: 0.8422060358839555

=== Experiment 1530 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006255343678521011
rmse: 0.0790907306738344
mae: 0.030320296930407612
r2: 0.7179493067227878
pearson: 0.8562150543321886

=== Experiment 1528 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006661008618935239
rmse: 0.0816150024133752
mae: 0.03514258610093668
r2: 0.6996580531063049
pearson: 0.842557711734499

=== Experiment 1672 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00950971447095781
rmse: 0.09751776489931366
mae: 0.0414874673790084
r2: 0.5712111600499366
pearson: 0.7593737763592106

=== Experiment 1810 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007979082524419335
rmse: 0.08932571032138135
mae: 0.041165517219536055
r2: 0.6402266808367175
pearson: 0.8051790185869969

=== Experiment 1862 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007186342333663321
rmse: 0.08477229697055118
mae: 0.02845024576197232
r2: 0.6759709871262656
pearson: 0.8329167576634783

=== Experiment 1740 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009873386478663971
rmse: 0.0993649157331901
mae: 0.04844658303937489
r2: 0.5548133492867571
pearson: 0.7455767544541843

=== Experiment 1270 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005838847401809379
rmse: 0.07641235110771935
mae: 0.029890729881916128
r2: 0.7367289405256854
pearson: 0.8607354171121879

=== Experiment 1363 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006847033346338484
rmse: 0.08274680263513802
mae: 0.03153930886736347
r2: 0.6912702800234364
pearson: 0.8337225422465464

=== Experiment 1453 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061687362736330695
rmse: 0.07854130297896178
mae: 0.030322526542417037
r2: 0.7218543964903512
pearson: 0.8605405722054122

=== Experiment 1251 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007221736846019125
rmse: 0.0849808028087469
mae: 0.03671415977509312
r2: 0.6743750641424586
pearson: 0.8289327336258103

=== Experiment 1521 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007447146409617024
rmse: 0.08629685051968597
mae: 0.042621992532924556
r2: 0.6642114461301654
pearson: 0.8226970560917718

=== Experiment 1627 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009030052096093916
rmse: 0.09502658625928807
mae: 0.043793939633380435
r2: 0.5928389254169935
pearson: 0.7816361344313975

=== Experiment 1110 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005763933362083061
rmse: 0.07592057271967237
mae: 0.028983615608183563
r2: 0.7401067816047545
pearson: 0.8611881872714329

=== Experiment 1995 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006004405437570502
rmse: 0.07748809868341397
mae: 0.03035236033714937
r2: 0.7292640015608802
pearson: 0.8557226869655273

=== Experiment 1912 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005765113881781726
rmse: 0.07592834702389963
mae: 0.030971458214767045
r2: 0.7400535524911281
pearson: 0.8612669939739441

=== Experiment 1993 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006086542962868973
rmse: 0.07801629934102856
mae: 0.03010942615016034
r2: 0.7255604566966616
pearson: 0.8544534811050672

=== Experiment 1461 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00632684917768116
rmse: 0.07954149343381202
mae: 0.031349803087136446
r2: 0.7147251552376337
pearson: 0.8567242743998112

=== Experiment 1827 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006914692915969778
rmse: 0.08315463255868417
mae: 0.03889271100362694
r2: 0.6882195398080153
pearson: 0.8299192516783057

=== Experiment 1616 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005822837936176331
rmse: 0.07630752214674731
mae: 0.027703627814856146
r2: 0.7374508002847746
pearson: 0.8593427706171142

=== Experiment 1906 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007274351361545398
rmse: 0.08528980807544004
mae: 0.035170208180336546
r2: 0.6720026987947982
pearson: 0.8568783535182541

=== Experiment 1905 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006013721813130423
rmse: 0.07754819026341249
mae: 0.02968640562896752
r2: 0.7288439302873339
pearson: 0.8542314481495851

=== Experiment 1945 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00686235417827371
rmse: 0.08283932748564361
mae: 0.03558426678680252
r2: 0.6905794704546908
pearson: 0.8322847352561717

=== Experiment 1789 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005358929923746425
rmse: 0.07320471244220843
mae: 0.033567336407429677
r2: 0.7583682083837082
pearson: 0.8730411076412151

=== Experiment 1931 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0109033493361592
rmse: 0.10441910426813285
mae: 0.047946077681691915
r2: 0.5083727773636182
pearson: 0.7147270484940256

=== Experiment 1867 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006311305043067203
rmse: 0.07944372248999415
mae: 0.03537015349456246
r2: 0.7154260334258802
pearson: 0.8466974361790292

=== Experiment 1115 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009532694799561625
rmse: 0.09763552017355992
mae: 0.04687621421001787
r2: 0.5701749871476067
pearson: 0.7660859197416663

=== Experiment 1200 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072049055681821204
rmse: 0.08488171515810763
mae: 0.033321452651077985
r2: 0.6751339790521171
pearson: 0.8512354949928845

=== Experiment 1742 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006208554051019413
rmse: 0.07879437829578588
mae: 0.030609587586253463
r2: 0.7200590304331449
pearson: 0.8552131648971781

=== Experiment 1337 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007610429054270995
rmse: 0.0872377730932593
mae: 0.03883033597720054
r2: 0.6568491035489055
pearson: 0.818671390340818

=== Experiment 1868 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007693162533639024
rmse: 0.08771067514070921
mae: 0.03919648190536944
r2: 0.65311868737799
pearson: 0.81091706884572

=== Experiment 1615 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008454391508282255
rmse: 0.0919477651075993
mae: 0.04024532659949345
r2: 0.6187952079538207
pearson: 0.8060123279377961

=== Experiment 1812 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006102698380572752
rmse: 0.07811976946057095
mae: 0.027071883780355167
r2: 0.7248320160229412
pearson: 0.8526930475125658

=== Experiment 1775 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.02009107514092698
rmse: 0.14174298974173988
mae: 0.054160311620639126
r2: 0.0941022646540135
pearson: 0.3926164688624552

=== Experiment 1547 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006148716461036417
rmse: 0.07841375173422335
mae: 0.037485674386151455
r2: 0.722757081028941
pearson: 0.8528402604817087

=== Experiment 1744 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005735347880348935
rmse: 0.07573207959873368
mae: 0.02886709917374454
r2: 0.7413956883947135
pearson: 0.8610777778057326

=== Experiment 1673 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0058479863281784865
rmse: 0.07647212778639344
mae: 0.02958636294154214
r2: 0.7363168703580512
pearson: 0.8605523108695922

=== Experiment 1845 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005561940599983004
rmse: 0.07457841913035569
mae: 0.02837217810775909
r2: 0.7492145463440327
pearson: 0.8665301123108525

=== Experiment 1990 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005775643131235301
rmse: 0.07599765214291361
mae: 0.034962256732547735
r2: 0.7395787932675436
pearson: 0.8617031521881395

=== Experiment 1153 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0053822093955529235
rmse: 0.07336354268676591
mae: 0.030215682318890904
r2: 0.7573185472460324
pearson: 0.8710953443509295

=== Experiment 1720 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005962229930857588
rmse: 0.07721547727533379
mae: 0.03674711017448297
r2: 0.7311656765957055
pearson: 0.8566333411918313

=== Experiment 1800 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00850712823200809
rmse: 0.09223409473729381
mae: 0.04231760911016553
r2: 0.61641733229223
pearson: 0.7895417163784636

=== Experiment 1966 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007637642190364572
rmse: 0.08739360497407446
mae: 0.03828625025633117
r2: 0.6556220752198632
pearson: 0.8138472502351225

=== Experiment 1036 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006721230475837528
rmse: 0.08198311091827101
mae: 0.03992984944662565
r2: 0.6969426760842468
pearson: 0.8393736265125246

=== Experiment 1755 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007019168333557656
rmse: 0.08378047704302988
mae: 0.037002053456642814
r2: 0.6835087892121257
pearson: 0.8277588646629876

=== Experiment 1765 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0072819050257808575
rmse: 0.08533407892384412
mae: 0.03669845963871071
r2: 0.6716621073990431
pearson: 0.8262913788766588

=== Experiment 1166 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005994674399093997
rmse: 0.07742528268656174
mae: 0.031327370583450094
r2: 0.7297027698028282
pearson: 0.8544248083582359

=== Experiment 1511 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008231199160351482
rmse: 0.09072595637606408
mae: 0.03268980951286114
r2: 0.6288588526875544
pearson: 0.8274743616303561

=== Experiment 1435 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00603337508948972
rmse: 0.07767480344030309
mae: 0.029784397073213377
r2: 0.7279577727063615
pearson: 0.8535416806951577

=== Experiment 1843 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0067276252362524986
rmse: 0.08202210212041934
mae: 0.029977160401733743
r2: 0.6966543391516851
pearson: 0.8523949596127062

=== Experiment 1959 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006765243498752389
rmse: 0.08225110028900762
mae: 0.03774728033864791
r2: 0.6949581482526286
pearson: 0.8390964608491155

=== Experiment 1464 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00545502540344463
rmse: 0.0738581437855341
mae: 0.027810323656152276
r2: 0.7540353054989712
pearson: 0.8710573630142137

=== Experiment 1928 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007033561747205883
rmse: 0.08386633262046149
mae: 0.03439393413142026
r2: 0.682859796525753
pearson: 0.8303722948690115

=== Experiment 1763 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00638245595851646
rmse: 0.0798902744926844
mae: 0.03467376482844031
r2: 0.7122178699641859
pearson: 0.8529125967622274

=== Experiment 1829 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059002520542251295
rmse: 0.07681309819441688
mae: 0.03318123118349016
r2: 0.7339602317745133
pearson: 0.8570296457581121

=== Experiment 1222 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006876714271135532
rmse: 0.08292595655845962
mae: 0.029797749425951973
r2: 0.6899319801879115
pearson: 0.837001688338485

=== Experiment 1874 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006034987944931626
rmse: 0.07768518484840997
mae: 0.030045478201534276
r2: 0.7278850497643581
pearson: 0.8580768298203079

=== Experiment 1915 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006056768263749483
rmse: 0.07782524181619664
mae: 0.03300743221224399
r2: 0.7269029847751123
pearson: 0.8531309920642155

=== Experiment 1554 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00616486718700738
rmse: 0.07851666821132555
mae: 0.03299813232836919
r2: 0.7220288519033883
pearson: 0.8544172263157223

=== Experiment 1412 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007450144915288875
rmse: 0.08631422197580695
mae: 0.034027137427243344
r2: 0.6640762448291648
pearson: 0.8173882504583656

=== Experiment 1700 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005675593674327368
rmse: 0.07533653611845562
mae: 0.028796122174695203
r2: 0.7440899792443888
pearson: 0.8658170991078642

=== Experiment 1991 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005809885308646144
rmse: 0.07622260365958476
mae: 0.03307275424592041
r2: 0.7380348285592234
pearson: 0.8603549034503181

=== Experiment 1601 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005963536024449107
rmse: 0.07722393427201897
mae: 0.03511211059462275
r2: 0.7311067854105195
pearson: 0.8563728247550593

=== Experiment 1446 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007889224777556892
rmse: 0.08882130812793117
mae: 0.03414826193068209
r2: 0.6442783270933261
pearson: 0.8525248746941881

=== Experiment 1758 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006527838146885329
rmse: 0.08079503788528927
mae: 0.03279506224582088
r2: 0.7056626510782282
pearson: 0.8420508972390824

=== Experiment 1783 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006655358564149618
rmse: 0.08158038100027247
mae: 0.03616797679319347
r2: 0.6999128115898092
pearson: 0.836957610555295

=== Experiment 1773 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006287605517770122
rmse: 0.07929442299285695
mae: 0.029255908047443036
r2: 0.716494634590567
pearson: 0.8576560263711165

=== Experiment 1399 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007332562584532683
rmse: 0.08563038353605969
mae: 0.037506971388172834
r2: 0.66937798037102
pearson: 0.829302366164428

=== Experiment 1948 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059087580557074535
rmse: 0.07686844642444293
mae: 0.029323266616638678
r2: 0.733576699911453
pearson: 0.8566359761792965

=== Experiment 1830 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008587058960298679
rmse: 0.09266638527696372
mae: 0.041681220861921
r2: 0.6128132909338111
pearson: 0.7932457297474789

=== Experiment 1837 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006488916042958137
rmse: 0.08055380837029455
mae: 0.0332131103446202
r2: 0.7074176316133773
pearson: 0.8458617809742669

=== Experiment 1779 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008291391895192658
rmse: 0.09105708042317553
mae: 0.036747015818645846
r2: 0.6261447887664144
pearson: 0.8298527138531511

=== Experiment 1552 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008342951812427685
rmse: 0.09133976030419438
mae: 0.04185914206093671
r2: 0.6238199747915419
pearson: 0.798983541916822

=== Experiment 1989 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005707876645438734
rmse: 0.0755504907028322
mae: 0.028595689951829675
r2: 0.7426343542857994
pearson: 0.8639695019876131

=== Experiment 1815 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007836708153562863
rmse: 0.08852518372510086
mae: 0.039654075360084376
r2: 0.646646278554883
pearson: 0.806754871157875

=== Experiment 1117 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005766956492000745
rmse: 0.07594047993001325
mae: 0.029342805331531944
r2: 0.7399704700073473
pearson: 0.8608725709125198

=== Experiment 1207 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006145829075381793
rmse: 0.07839533835236502
mae: 0.031995790774619406
r2: 0.7228872719772723
pearson: 0.8518036156860063

=== Experiment 1881 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006258128769671116
rmse: 0.07910833565226306
mae: 0.03230652819264467
r2: 0.7178237281886419
pearson: 0.8486021771475477

=== Experiment 1956 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00684793361017745
rmse: 0.08275224232718682
mae: 0.038055182589596614
r2: 0.6912296875231738
pearson: 0.8385008381423773

=== Experiment 1108 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005711394693772281
rmse: 0.07557376987931912
mae: 0.02857214411174157
r2: 0.7424757270348513
pearson: 0.8619551650228038

=== Experiment 1885 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006265951067964256
rmse: 0.07915776063004977
mae: 0.034994432770650674
r2: 0.7174710242014611
pearson: 0.8473066464382281

=== Experiment 1656 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0070763928821858165
rmse: 0.08412129862398593
mae: 0.03468511720104346
r2: 0.6809285595009318
pearson: 0.8407481518706121

=== Experiment 1745 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006025534511289152
rmse: 0.07762431649482752
mae: 0.02985666170822638
r2: 0.7283113009265227
pearson: 0.8561163184402728

=== Experiment 1573 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0056692585660023
rmse: 0.07529447898752138
mae: 0.0337750132173307
r2: 0.7443756264904717
pearson: 0.8632471973988578

=== Experiment 1634 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006800267327416084
rmse: 0.0824637334069716
mae: 0.03113202852399638
r2: 0.6933789392333485
pearson: 0.8338815815319942

=== Experiment 1797 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007451585747173557
rmse: 0.08632256800613358
mae: 0.036852943541253784
r2: 0.6640112783536432
pearson: 0.8153787146835295

=== Experiment 1802 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005641829588505779
rmse: 0.07511211346051833
mae: 0.03142789825912443
r2: 0.7456123870133735
pearson: 0.8642000298327936

=== Experiment 1692 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009025172511563356
rmse: 0.09500090795125779
mae: 0.038595679988683324
r2: 0.593058943735807
pearson: 0.7746513571233533

=== Experiment 1536 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007412735981635203
rmse: 0.08609724723610623
mae: 0.03427873026403221
r2: 0.665762997182678
pearson: 0.8399769050767778

=== Experiment 1462 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007334193491180549
rmse: 0.08563990595032522
mae: 0.03776541644262017
r2: 0.6693044435080846
pearson: 0.820897672570586

=== Experiment 1329 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006751241661701978
rmse: 0.08216593979077935
mae: 0.03938435404262386
r2: 0.6955894849225519
pearson: 0.8393537918438916

=== Experiment 1975 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005818245486810698
rmse: 0.07627742448988888
mae: 0.029449853828569395
r2: 0.7376578718053796
pearson: 0.8644662690312855

=== Experiment 1440 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006573262412087178
rmse: 0.08107565856708891
mae: 0.034147686823084535
r2: 0.7036144909530249
pearson: 0.8422107378436146

=== Experiment 1944 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006860130762107633
rmse: 0.08282590634643024
mae: 0.030362086200118973
r2: 0.6906797233110191
pearson: 0.843836184215405

=== Experiment 1347 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0057694189478568
rmse: 0.07595669126454101
mae: 0.02824357629054428
r2: 0.7398594389566076
pearson: 0.8604152116435808

=== Experiment 1079 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006450892153967962
rmse: 0.08031744613698796
mae: 0.034319548049389
r2: 0.7091321120323508
pearson: 0.846521651945155

=== Experiment 1986 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005939943493313192
rmse: 0.07707102888448546
mae: 0.02878990284957473
r2: 0.732170562255571
pearson: 0.8599341591043682

=== Experiment 1766 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00781843518040315
rmse: 0.08842191572457107
mae: 0.03364465128587379
r2: 0.647470198872104
pearson: 0.8062877937820621

=== Experiment 1630 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010008553974825363
rmse: 0.10004276073172592
mae: 0.04642815005317736
r2: 0.5487187063766048
pearson: 0.7464064504882149

=== Experiment 1965 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005617233780242147
rmse: 0.07494820731840186
mae: 0.03209313717158814
r2: 0.7467214011825376
pearson: 0.8673119448175246

=== Experiment 1899 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006165905018227069
rmse: 0.07852327691982212
mae: 0.030424310249467033
r2: 0.7219820565504769
pearson: 0.8538186019235369

=== Experiment 1636 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005405936000863032
rmse: 0.07352507056006836
mae: 0.028094775271938485
r2: 0.7562487250554772
pearson: 0.8712894410932592

=== Experiment 1983 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007055653980863992
rmse: 0.08399794033703441
mae: 0.03755769132064781
r2: 0.6818636674336443
pearson: 0.8265096033008551

=== Experiment 1707 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00695577853414257
rmse: 0.08340131014643937
mae: 0.03905399612792051
r2: 0.6863670073677675
pearson: 0.8285486871484078

=== Experiment 1927 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006547973531247018
rmse: 0.08091954974693703
mae: 0.0302790986577795
r2: 0.70475475545962
pearson: 0.8580501673640584

=== Experiment 1669 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008249822044144097
rmse: 0.09082853100289631
mae: 0.03994346637530362
r2: 0.6280191550539033
pearson: 0.7959906595775582

=== Experiment 1489 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005774257871010184
rmse: 0.07598853776070562
mae: 0.0312912131413039
r2: 0.7396412540413946
pearson: 0.8606022811397775

=== Experiment 1743 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.004805015643914688
rmse: 0.06931822014387479
mae: 0.029744194866480854
r2: 0.7833439594650027
pearson: 0.8857186551239976

=== Experiment 1839 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005756644213581978
rmse: 0.07587255243882321
mae: 0.03203431443427517
r2: 0.7404354461024689
pearson: 0.8660219198135869

=== Experiment 1642 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009370832972612288
rmse: 0.09680306282660837
mae: 0.04490018530281456
r2: 0.5774732656839141
pearson: 0.7697042199384102

=== Experiment 1671 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005852448628982093
rmse: 0.07650129821762565
mae: 0.028734022276930594
r2: 0.7361156671788256
pearson: 0.8596512987607852

=== Experiment 1904 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005789714844730969
rmse: 0.0760901757438565
mae: 0.03320222374125948
r2: 0.7389443058994586
pearson: 0.8615920423290634

=== Experiment 1290 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00562827921572273
rmse: 0.07502185825293006
mae: 0.028906248686367337
r2: 0.7462233673581861
pearson: 0.8647198565713482

=== Experiment 1465 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005295814105196739
rmse: 0.07277234437062433
mae: 0.029654427471046976
r2: 0.7612140728627176
pearson: 0.8739294088702638

=== Experiment 1458 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005916083323113196
rmse: 0.07691607974353085
mae: 0.02829015295597587
r2: 0.7332464068282247
pearson: 0.8573234186223703

=== Experiment 1690 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006132717193323512
rmse: 0.07831166703195323
mae: 0.03029218729139261
r2: 0.7234784809682996
pearson: 0.8508969990926186

=== Experiment 1474 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005716770605169042
rmse: 0.07560932882369108
mae: 0.027957689271870603
r2: 0.7422333295560909
pearson: 0.8631102324753933

=== Experiment 1888 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006168767332861666
rmse: 0.07854150070416063
mae: 0.032828710709615176
r2: 0.7218529960434039
pearson: 0.8502679478810417

=== Experiment 1735 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006056944698848386
rmse: 0.07782637534183631
mae: 0.033010795530917895
r2: 0.7268950293941576
pearson: 0.8527672889960812

=== Experiment 1817 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00560174076835004
rmse: 0.07484477782951887
mae: 0.028233421695442452
r2: 0.7474199742697567
pearson: 0.865347707053054

=== Experiment 1659 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007369462229413081
rmse: 0.08584557198489087
mae: 0.03807882864958478
r2: 0.6677141916241386
pearson: 0.832610404910622

=== Experiment 1463 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006074151953734728
rmse: 0.07793684593139966
mae: 0.0304549055897214
r2: 0.7261191618448246
pearson: 0.8530079468972682

=== Experiment 1710 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005749367466609564
rmse: 0.07582458352414186
mae: 0.03497404337969085
r2: 0.7407635514207138
pearson: 0.8616578755014435

=== Experiment 1359 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006912363753348255
rmse: 0.08314062637091602
mae: 0.030097327328777693
r2: 0.6883245607254769
pearson: 0.834336291635032

=== Experiment 1831 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006695619215717119
rmse: 0.08182676344398035
mae: 0.038667162595521186
r2: 0.6980974765306984
pearson: 0.8366813249744474

=== Experiment 1611 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006204481983243326
rmse: 0.07876853422048252
mae: 0.03129763228966753
r2: 0.7202426381769147
pearson: 0.8497050118658556

=== Experiment 1971 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006015106732619768
rmse: 0.07755711916142688
mae: 0.028711444534892668
r2: 0.7287814848771088
pearson: 0.8545275839643915

=== Experiment 1411 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00597670692653113
rmse: 0.07730916457012797
mae: 0.029030969287271204
r2: 0.7305129152325984
pearson: 0.8547511345078617

=== Experiment 1958 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005831304783487196
rmse: 0.0763629804518341
mae: 0.030312757040041532
r2: 0.7370690338660723
pearson: 0.8589186822109478

=== Experiment 1726 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007529245424641185
rmse: 0.08677122463490523
mae: 0.03753484504039323
r2: 0.6605096376772658
pearson: 0.8138929558010549

=== Experiment 1864 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00600334677642579
rmse: 0.07748126726135672
mae: 0.02865157824908107
r2: 0.7293117361259429
pearson: 0.85555070235693

=== Experiment 1805 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006412224518939487
rmse: 0.08007636679407656
mae: 0.03566587072573118
r2: 0.7108756186768572
pearson: 0.8591072301123953

=== Experiment 1747 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005919688114205651
rmse: 0.07693950944869386
mae: 0.030193670410912952
r2: 0.7330838683844558
pearson: 0.857783210591683

=== Experiment 1231 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005941691043248494
rmse: 0.07708236531949765
mae: 0.02796526580887599
r2: 0.7320917659981437
pearson: 0.8569040913179515

=== Experiment 1994 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066591095204731904
rmse: 0.08160336709029346
mae: 0.031938149779045036
r2: 0.6997436826201014
pearson: 0.8460306409483376

=== Experiment 1795 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00702956235288708
rmse: 0.08384248536921529
mae: 0.037154502648888
r2: 0.6830401274553197
pearson: 0.8345213503921498

=== Experiment 1887 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007383576742220555
rmse: 0.0859277414006708
mae: 0.03773319811019752
r2: 0.6670777744539218
pearson: 0.820422383858186

=== Experiment 1799 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006424969779381141
rmse: 0.08015590919814422
mae: 0.034099977792783065
r2: 0.7103009404931611
pearson: 0.8491109159532857

=== Experiment 1997 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006088742528875719
rmse: 0.0780303949040098
mae: 0.03615398812611814
r2: 0.7254612792335206
pearson: 0.8558738294728722

=== Experiment 1801 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007715773796609822
rmse: 0.08783947743816457
mae: 0.03846273984029428
r2: 0.6520991554826147
pearson: 0.8116202503089524

=== Experiment 1917 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00616320744722923
rmse: 0.07850609815313221
mae: 0.03836479599717142
r2: 0.7221036888394777
pearson: 0.8585290838340113

=== Experiment 1936 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006143706125974138
rmse: 0.07838179715963482
mae: 0.02755187012338004
r2: 0.7229829948316819
pearson: 0.8682385887690877

=== Experiment 1649 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005892554173041556
rmse: 0.07676297397210166
mae: 0.029284539340141517
r2: 0.7343073258489841
pearson: 0.8638718596129037

=== Experiment 1916 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009483677030725975
rmse: 0.09738417238302113
mae: 0.04690124725874987
r2: 0.5723851767700363
pearson: 0.7566712520327975

=== Experiment 1732 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062604847275368955
rmse: 0.07912322495662633
mae: 0.038126846499179204
r2: 0.7177174990854104
pearson: 0.849648636743187

=== Experiment 1967 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006400774355678885
rmse: 0.08000483957660864
mae: 0.029823537514638582
r2: 0.7113919014986754
pearson: 0.8542626171232016

=== Experiment 1641 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006006382642642776
rmse: 0.07750085575426104
mae: 0.029900847721231923
r2: 0.729174850254406
pearson: 0.8548189999653693

=== Experiment 1582 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0076607109134067975
rmse: 0.08752548722176186
mae: 0.04106014804059743
r2: 0.6545819166512108
pearson: 0.8105190912408379

=== Experiment 1679 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007112299285733186
rmse: 0.08433444898576847
mae: 0.04118859110771222
r2: 0.6793095555685977
pearson: 0.833752288148856

=== Experiment 1883 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0074219745754447826
rmse: 0.08615088261558777
mae: 0.038162894903642566
r2: 0.6653464330540202
pearson: 0.8162069161889055

=== Experiment 1798 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005779889444618304
rmse: 0.07602558414519617
mae: 0.028450083888720834
r2: 0.7393873288660493
pearson: 0.8631330662628913

=== Experiment 1558 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.004771223123812116
rmse: 0.06907404088231783
mae: 0.030463043097138015
r2: 0.7848676493232022
pearson: 0.8861932679959597

=== Experiment 1923 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057081586764222165
rmse: 0.07555235718640561
mae: 0.02743364352941248
r2: 0.7426216376328856
pearson: 0.8637662204434587

=== Experiment 1884 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007054589842374674
rmse: 0.08399160578518948
mae: 0.03513122938901972
r2: 0.6819116489697645
pearson: 0.8263451366350909

=== Experiment 1529 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00668796437845842
rmse: 0.08177997541243467
mae: 0.036493963727771014
r2: 0.6984426297735418
pearson: 0.8368329083911452

=== Experiment 1741 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006008276909384536
rmse: 0.07751307573167598
mae: 0.0293768628976895
r2: 0.7290894386007505
pearson: 0.8585065405663984

=== Experiment 1823 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005548734122648819
rmse: 0.07448982563175201
mae: 0.028572671734272033
r2: 0.7498100205944158
pearson: 0.8782245132555443

=== Experiment 1503 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0057652953219030165
rmse: 0.07592954182597848
mae: 0.02611611058669381
r2: 0.740045371435917
pearson: 0.8701980003986443

=== Experiment 1955 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005648124267558646
rmse: 0.0751540036695228
mae: 0.030183953280320103
r2: 0.7453285627053798
pearson: 0.8649219674469713

=== Experiment 1840 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005902580071601271
rmse: 0.076828250478592
mae: 0.028547872919820142
r2: 0.7338552624956618
pearson: 0.8592337333855017

=== Experiment 1492 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007779214686384978
rmse: 0.08819985649866431
mae: 0.03921369258722871
r2: 0.6492386336851238
pearson: 0.813522071777556

=== Experiment 1919 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006579136818050155
rmse: 0.08111187840292046
mae: 0.03980679396479736
r2: 0.7033496165736779
pearson: 0.8474848891703652

=== Experiment 1491 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006900855944379837
rmse: 0.08307139064912683
mae: 0.03344839072349754
r2: 0.6888434427668888
pearson: 0.8372403654448447

=== Experiment 1846 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006990376265962341
rmse: 0.08360847006112683
mae: 0.04005650448946759
r2: 0.6848070108676403
pearson: 0.8353547310246303

=== Experiment 1580 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007860779213890449
rmse: 0.08866103548848529
mae: 0.03371486294029716
r2: 0.6455609250391962
pearson: 0.8253625926010734

=== Experiment 1701 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057927944246951345
rmse: 0.07611040943717971
mae: 0.03243207998699634
r2: 0.7388054489943012
pearson: 0.8618275642801142

=== Experiment 1950 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005756933778630591
rmse: 0.0758744606480375
mae: 0.029211432675155885
r2: 0.740422389741874
pearson: 0.8622322651454383

=== Experiment 1220 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064871705170826575
rmse: 0.08054297310804126
mae: 0.03055420898612057
r2: 0.7074963366068379
pearson: 0.8434877221783672

=== Experiment 1514 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005651309134183187
rmse: 0.0751751896185383
mae: 0.031468573551771545
r2: 0.7451849584710463
pearson: 0.8713752344853484

=== Experiment 1849 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005608010414456071
rmse: 0.07488665044222549
mae: 0.02732173730634588
r2: 0.7471372786863182
pearson: 0.8656747007957507

=== Experiment 1913 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006093360116006769
rmse: 0.07805997768387311
mae: 0.028839293754277344
r2: 0.725253074262137
pearson: 0.8526056494405851

=== Experiment 1421 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007352863100296361
rmse: 0.08574883731162984
mae: 0.03821015895967842
r2: 0.668462639050176
pearson: 0.8281393377843459

=== Experiment 1749 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006950077228786683
rmse: 0.08336712318886075
mae: 0.038750646673028445
r2: 0.6866240767169282
pearson: 0.8289794124257365

=== Experiment 1653 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.004503179884531208
rmse: 0.06710573659927449
mae: 0.02433501860124148
r2: 0.7969536010075267
pearson: 0.8959013369481559

=== Experiment 1970 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006072016025642612
rmse: 0.0779231417849833
mae: 0.03210674871366525
r2: 0.7262154699024044
pearson: 0.8524821322846943

=== Experiment 1973 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006867138106298477
rmse: 0.08286819719469271
mae: 0.035840135669729385
r2: 0.6903637652456219
pearson: 0.8384312932199258

=== Experiment 1135 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005349920314584387
rmse: 0.07314314947132361
mae: 0.031026433489508665
r2: 0.7587744476953924
pearson: 0.8726300633700976

=== Experiment 1892 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006006790843762186
rmse: 0.07750348923604786
mae: 0.027231040104397718
r2: 0.7291564446455947
pearson: 0.8547882201012066

=== Experiment 1196 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00596920353515309
rmse: 0.07726062085663751
mae: 0.033238406549692255
r2: 0.7308512398473392
pearson: 0.8560403352348097

=== Experiment 1748 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006007735610777598
rmse: 0.07750958399306242
mae: 0.028291445207385493
r2: 0.7291138455166934
pearson: 0.8549210381963387

=== Experiment 1693 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005524475007018389
rmse: 0.07432681216773923
mae: 0.030836193409744833
r2: 0.7509038534409387
pearson: 0.8718273679882469

=== Experiment 1468 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.004955826160842645
rmse: 0.07039762894332909
mae: 0.02554732023537728
r2: 0.7765439796335056
pearson: 0.8831737546249083

=== Experiment 1719 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006474707672040423
rmse: 0.0804655682390948
mae: 0.03176781065654019
r2: 0.7080582808044746
pearson: 0.8442394778353896

=== Experiment 1505 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0051965716607308395
rmse: 0.07208725033409749
mae: 0.030052436227766374
r2: 0.7656888710037447
pearson: 0.878668385108035

=== Experiment 1366 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.004634156792351535
rmse: 0.06807464133105319
mae: 0.024262212340379282
r2: 0.7910479098812538
pearson: 0.8896232168930005

=== Experiment 1901 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00743415311146077
rmse: 0.08622153507947286
mae: 0.03834815482344633
r2: 0.664797308225243
pearson: 0.8272045277328696

=== Experiment 1586 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006420034527106788
rmse: 0.08012511795377769
mae: 0.028778816809641276
r2: 0.7105234688460409
pearson: 0.8504113390459954

=== Experiment 1878 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0066335118250261515
rmse: 0.08144637392190122
mae: 0.02834852804661074
r2: 0.7008978714414011
pearson: 0.8563905357776831

=== Experiment 1413 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00613057522296221
rmse: 0.07829798990371471
mae: 0.033328596282897166
r2: 0.7235750614691374
pearson: 0.8539654846747409

=== Experiment 1442 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005771313532125383
rmse: 0.07596916171793251
mae: 0.028092009920027753
r2: 0.7397740129858075
pearson: 0.8619930135190893

=== Experiment 2014 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008984974665636778
rmse: 0.09478910626035451
mae: 0.04170489861007766
r2: 0.5948714469162115
pearson: 0.8097316326787695

=== Experiment 2035 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008421879269769998
rmse: 0.09177079747811935
mae: 0.04401677019083671
r2: 0.6202611704786077
pearson: 0.8154772375284994

=== Experiment 2039 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007575020804762088
rmse: 0.0870345954477993
mae: 0.037359344986157596
r2: 0.6584456459348997
pearson: 0.8141676256324858

=== Experiment 2010 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007915814480818956
rmse: 0.08897086310033726
mae: 0.04015314012374746
r2: 0.6430794090762632
pearson: 0.8037966078589984

=== Experiment 2042 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006827107241338127
rmse: 0.08262631082977218
mae: 0.03741912304990416
r2: 0.6921687393277529
pearson: 0.8348013539022155

=== Experiment 2007 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007235140844804335
rmse: 0.0850596311113817
mae: 0.035596286816687556
r2: 0.6737706837367845
pearson: 0.8225442375061651

=== Experiment 2020 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0063896894778346655
rmse: 0.0799355332617145
mae: 0.03938756493379928
r2: 0.7118917137618435
pearson: 0.8471956353601519

=== Experiment 2015 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00792954830718996
rmse: 0.08904801124780924
mae: 0.038826100673845836
r2: 0.6424601568899138
pearson: 0.8140004670710146

=== Experiment 2025 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0065359603488389624
rmse: 0.08084528649735223
mae: 0.041578498720406205
r2: 0.7052964245669933
pearson: 0.8439281406407768

=== Experiment 2064 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007394523220545256
rmse: 0.08599141364430088
mae: 0.037309098678303676
r2: 0.6665842025641744
pearson: 0.8177908105402677

=== Experiment 2051 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006928950919822786
rmse: 0.08324032027703153
mae: 0.038257137888368764
r2: 0.6875766526897105
pearson: 0.8307406458744592

=== Experiment 2023 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007423840336216041
rmse: 0.08616171038353429
mae: 0.04097721749575956
r2: 0.6652623067220282
pearson: 0.8266502157102991

=== Experiment 2005 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0071243222222868805
rmse: 0.08440570017650988
mae: 0.0433285048371059
r2: 0.6787674466510907
pearson: 0.8270549289278721

=== Experiment 2073 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007314964371007957
rmse: 0.0855275649776606
mae: 0.03611777632017339
r2: 0.6701714760732838
pearson: 0.8253340776524554

=== Experiment 2063 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007419421688679826
rmse: 0.08613606497095061
mae: 0.037558935185266845
r2: 0.6654615415946408
pearson: 0.8259905172536082

=== Experiment 2036 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007166583581759057
rmse: 0.08465567660682334
mae: 0.045855645020971995
r2: 0.6768619005531371
pearson: 0.8296496605864287

=== Experiment 2013 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007561979651302501
rmse: 0.08695964380850753
mae: 0.037352029591967185
r2: 0.6590336658045419
pearson: 0.8314373130254168

=== Experiment 2033 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007030966616356551
rmse: 0.08385085936564128
mae: 0.03692894125871828
r2: 0.6829768098335449
pearson: 0.8329516386059109

=== Experiment 2043 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007257771919271429
rmse: 0.08519255788665714
mae: 0.033349664617141464
r2: 0.6727502585497609
pearson: 0.8344057561297382

=== Experiment 2069 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006761414406228315
rmse: 0.08222782014761376
mae: 0.03501005057308657
r2: 0.6951308003492265
pearson: 0.8412033517846099

=== Experiment 2147 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006878891805915861
rmse: 0.08293908491125195
mae: 0.03847256542539701
r2: 0.6898337961030728
pearson: 0.8313131577376547

=== Experiment 2002 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006660634219344602
rmse: 0.08161270868770747
mae: 0.04172296474358927
r2: 0.6996749346190607
pearson: 0.8407494180693496

=== Experiment 2074 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008297456426550037
rmse: 0.09109037504890424
mae: 0.05423848092535819
r2: 0.6258713417167145
pearson: 0.8162278474522007

=== Experiment 2055 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007563636206029368
rmse: 0.086969168134629
mae: 0.035323995767988246
r2: 0.6589589724810131
pearson: 0.8118482693758127

=== Experiment 2024 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006037997415947495
rmse: 0.07770455209283106
mae: 0.03690934127783089
r2: 0.7277493540408219
pearson: 0.8552182797312652

=== Experiment 2008 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072529297398853075
rmse: 0.08516413411692335
mae: 0.04015595389055935
r2: 0.672968590287075
pearson: 0.8300793296235986

=== Experiment 2083 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007372842826147611
rmse: 0.08586525971629977
mae: 0.04021635100091081
r2: 0.6675617620052929
pearson: 0.8182345559757379

=== Experiment 2131 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00635956814907127
rmse: 0.07974690056090751
mae: 0.03748681043438008
r2: 0.7132498712183742
pearson: 0.8454412887209168

=== Experiment 2088 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0076360150364933104
rmse: 0.08738429513644491
mae: 0.04161923599432611
r2: 0.6556954428717532
pearson: 0.8117777563601681

=== Experiment 2135 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006866649313367006
rmse: 0.08286524792316141
mae: 0.032731553416903376
r2: 0.6903858047037679
pearson: 0.8419961444865788

=== Experiment 2118 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007192167479583039
rmse: 0.08480664761434117
mae: 0.03810939363535275
r2: 0.6757083338605883
pearson: 0.8237578108207793

=== Experiment 2089 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007908616313089901
rmse: 0.08893040151202457
mae: 0.049768648970992355
r2: 0.6434039712910102
pearson: 0.8212260699610895

=== Experiment 2104 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0068419529644869964
rmse: 0.08271609858115285
mae: 0.04154688575077986
r2: 0.691499352205073
pearson: 0.835492883980892

=== Experiment 2201 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006899813460101755
rmse: 0.08306511578335249
mae: 0.035729620716734516
r2: 0.6888904479241551
pearson: 0.8329207107518609

=== Experiment 2037 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005867768041319583
rmse: 0.07660135796002303
mae: 0.030322705604887635
r2: 0.7354249216191171
pearson: 0.8586617580287247

=== Experiment 2200 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006302739620974992
rmse: 0.07938979544610876
mae: 0.033493231325773705
r2: 0.7158122445380874
pearson: 0.8461791770771304

=== Experiment 2027 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006451986474289763
rmse: 0.08032425831770726
mae: 0.04149227480097372
r2: 0.7090827696106881
pearson: 0.8470668791252568

=== Experiment 2101 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00781014818215176
rmse: 0.08837504275615238
mae: 0.053519378980119124
r2: 0.6478438559758724
pearson: 0.8238864980018081

=== Experiment 2159 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00682838324410564
rmse: 0.08263403199714776
mae: 0.03224802322744914
r2: 0.6921112049245775
pearson: 0.835845451952772

=== Experiment 2011 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006723959560360771
rmse: 0.08199975341646322
mae: 0.040314918964133625
r2: 0.6968196228642558
pearson: 0.8375987702245126

=== Experiment 2122 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007810364557766638
rmse: 0.08837626693726454
mae: 0.04857946586905559
r2: 0.6478340996946317
pearson: 0.8123802433909383

=== Experiment 2126 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008158391404687161
rmse: 0.09032381416153307
mae: 0.03888653671734185
r2: 0.6321417223453147
pearson: 0.7951444370091902

=== Experiment 2093 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007273934404580814
rmse: 0.0852873636864267
mae: 0.04725399291403112
r2: 0.6720214992008144
pearson: 0.8296310632997851

=== Experiment 2140 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007627206800570499
rmse: 0.08733388117203139
mae: 0.037703601382077366
r2: 0.6560926023527116
pearson: 0.8370979712102763

=== Experiment 2125 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006732397825073105
rmse: 0.08205119027212893
mae: 0.037447331142748945
r2: 0.696439145222341
pearson: 0.8376194468778554

=== Experiment 2100 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007436015748102898
rmse: 0.08623233586133973
mae: 0.03573910514383369
r2: 0.6647133227588585
pearson: 0.8224392213716323

=== Experiment 2095 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007419936335015282
rmse: 0.08613905232248195
mae: 0.03318891907114888
r2: 0.6654383364178882
pearson: 0.8337278343051877

=== Experiment 2017 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006025973282844978
rmse: 0.0776271426940666
mae: 0.03740299932593311
r2: 0.7282915169101887
pearson: 0.8579660096911653

=== Experiment 2150 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006491089840739846
rmse: 0.08056730007105765
mae: 0.03995704416095024
r2: 0.7073196160281616
pearson: 0.8441406551582692

=== Experiment 2212 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007232431364878591
rmse: 0.08504370267620402
mae: 0.03770477141420817
r2: 0.673892852994103
pearson: 0.8254923722583678

=== Experiment 2132 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007767293331621234
rmse: 0.0881322491011164
mae: 0.04604352248097545
r2: 0.6497761623244316
pearson: 0.8149192476963407

=== Experiment 2205 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007336374652077251
rmse: 0.08565263949276317
mae: 0.033073179169453806
r2: 0.6692060959232548
pearson: 0.8304901872398085

=== Experiment 2231 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007896169306624035
rmse: 0.0888603922263684
mae: 0.03907320302917793
r2: 0.6439652013342099
pearson: 0.8065136915083985

=== Experiment 2204 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006580765062975242
rmse: 0.08112191481329346
mae: 0.04041212382136447
r2: 0.7032761997266512
pearson: 0.8407334623983538

=== Experiment 2142 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0064225908202364375
rmse: 0.08014106824990816
mae: 0.033175393500878184
r2: 0.7104082067139357
pearson: 0.846581874738405

=== Experiment 2127 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007691632312106186
rmse: 0.08770195158664479
mae: 0.047765797566254604
r2: 0.6531876843934038
pearson: 0.8158886008619349

=== Experiment 2223 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007401762428848147
rmse: 0.08603349597016355
mae: 0.03932439633612877
r2: 0.6662577898480164
pearson: 0.8182144677619185

=== Experiment 2119 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007119874024449767
rmse: 0.08437934595888834
mae: 0.04103895665635897
r2: 0.6789680139337646
pearson: 0.8276800859415483

=== Experiment 2237 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007413859840719962
rmse: 0.08610377367293469
mae: 0.0424090879726535
r2: 0.6657123228711944
pearson: 0.8216051657480152

=== Experiment 2176 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00790925860851489
rmse: 0.08893401266396839
mae: 0.05141871739924762
r2: 0.6433750104729934
pearson: 0.8219932212020727

=== Experiment 2207 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00695245317926979
rmse: 0.08338137189606436
mae: 0.031433188786603616
r2: 0.6865169461553805
pearson: 0.8332811414260471

=== Experiment 2096 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007303655978922429
rmse: 0.0854614297734506
mae: 0.04202587410460501
r2: 0.6706813664952163
pearson: 0.8331717232836195

=== Experiment 2097 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006717149642479517
rmse: 0.08195821888303526
mae: 0.032213498899094056
r2: 0.6971266790642474
pearson: 0.8433942312354881

=== Experiment 2151 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006316495934228772
rmse: 0.07947638601640598
mae: 0.031393975853573626
r2: 0.715191978428091
pearson: 0.8479483519579296

=== Experiment 2240 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007328302391002461
rmse: 0.08560550444336194
mae: 0.03991711337577232
r2: 0.6695700706222432
pearson: 0.8225034613795196

=== Experiment 2225 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008054429260104173
rmse: 0.08974647213180122
mae: 0.036393813450068806
r2: 0.6368293296873226
pearson: 0.8276095043075026

=== Experiment 2216 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006989759309218898
rmse: 0.08360478042085212
mae: 0.04608802951133755
r2: 0.6848348291756587
pearson: 0.8358081356158598

=== Experiment 2247 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00626058319920167
rmse: 0.07912384722194485
mae: 0.03390611192385774
r2: 0.7177130590413864
pearson: 0.847231558577455

=== Experiment 2077 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007224450798035548
rmse: 0.08499676933881398
mae: 0.04313312161124379
r2: 0.6742526932405397
pearson: 0.8306991597740226

=== Experiment 2198 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006340195603855678
rmse: 0.07962534523539397
mae: 0.031943413779485536
r2: 0.7141233707556363
pearson: 0.8451090197485956

=== Experiment 2185 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006444042099961672
rmse: 0.08027479118603593
mae: 0.03693765798247791
r2: 0.7094409779525551
pearson: 0.8434584639179834

=== Experiment 2038 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012139234749544937
rmse: 0.11017819543605231
mae: 0.04474779188694755
r2: 0.45264724802882417
pearson: 0.6823458276802216

=== Experiment 2120 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006815065433126243
rmse: 0.08255340957904914
mae: 0.033659417365437545
r2: 0.6927116991600217
pearson: 0.8329785096136992

=== Experiment 2260 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00746154073026709
rmse: 0.08638021029302423
mae: 0.03709078994019992
r2: 0.6635624125474788
pearson: 0.8243775367526976

=== Experiment 2286 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0075247297731719
rmse: 0.08674520028896066
mae: 0.03903141326433009
r2: 0.6607132464145257
pearson: 0.8238562644918642

=== Experiment 2021 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007393018225076544
rmse: 0.0859826623516424
mae: 0.033146285347853736
r2: 0.6666520621474596
pearson: 0.8187207740807881

=== Experiment 2155 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0065042832587697205
rmse: 0.08064913675154695
mae: 0.032842019115347586
r2: 0.7067247306160619
pearson: 0.843866088629362

=== Experiment 2091 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007708746678729211
rmse: 0.08779946855607505
mae: 0.04042178352404205
r2: 0.6524160051349802
pearson: 0.8261304156807772

=== Experiment 2144 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008330804631653624
rmse: 0.09127324159716046
mae: 0.04962593112055065
r2: 0.6243676858262612
pearson: 0.8012249326084596

=== Experiment 2203 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0065473651567253
rmse: 0.08091579052771652
mae: 0.03683633242488503
r2: 0.704782186799038
pearson: 0.8401180383037654

=== Experiment 2019 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007071288363645809
rmse: 0.08409095292387767
mae: 0.04727160261793178
r2: 0.6811587199952305
pearson: 0.8396463584486059

=== Experiment 2309 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007765712017855322
rmse: 0.0881232773894351
mae: 0.04788992357452619
r2: 0.649847463066147
pearson: 0.8135201318295414

=== Experiment 2255 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0068865154405323235
rmse: 0.08298503142454261
mae: 0.036655058365639336
r2: 0.6894900497736347
pearson: 0.8403741103072079

=== Experiment 2259 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008245329883127223
rmse: 0.09080379883643208
mae: 0.05423438491013018
r2: 0.6282217046170029
pearson: 0.8187717229581942

=== Experiment 2219 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0061280283641197315
rmse: 0.07828172433026583
mae: 0.03877575972681526
r2: 0.7236898982133864
pearson: 0.8543434703090221

=== Experiment 2171 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00705755793822132
rmse: 0.08400927292996482
mae: 0.035476012224534816
r2: 0.6817778188343984
pearson: 0.8331435061759358

=== Experiment 2271 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008797360468214197
rmse: 0.09379424538965168
mae: 0.04435698826520675
r2: 0.6033308885026726
pearson: 0.7833141587379483

=== Experiment 2269 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006782405991071951
rmse: 0.08235536406981607
mae: 0.035977617337580224
r2: 0.69418429902773
pearson: 0.8382638557765725

=== Experiment 2054 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006843742627577506
rmse: 0.08272691598009385
mae: 0.04489705851989129
r2: 0.6914186570840128
pearson: 0.8375401833631579

=== Experiment 2121 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005310279977830702
rmse: 0.07287166786777081
mae: 0.027850582398653262
r2: 0.7605618130325695
pearson: 0.8725378000246835

=== Experiment 2295 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00700119783755386
rmse: 0.08367316079576449
mae: 0.041380711830599694
r2: 0.6843190709675167
pearson: 0.8295740411862512

=== Experiment 2214 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006534200192715686
rmse: 0.08083439981045004
mae: 0.0332982764111797
r2: 0.705375789231888
pearson: 0.8453166127009789

=== Experiment 2264 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007685563333126774
rmse: 0.08766734473637704
mae: 0.037621310947245044
r2: 0.6534613319844236
pearson: 0.8157246253070284

=== Experiment 2004 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0078069397680247815
rmse: 0.08835688862802256
mae: 0.03300040549680608
r2: 0.6479885219567267
pearson: 0.8296497687381469

=== Experiment 2195 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00575191979159253
rmse: 0.07584141211496875
mae: 0.030207361276587907
r2: 0.7406484682105958
pearson: 0.870408442631339

=== Experiment 2184 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006813770998571809
rmse: 0.0825455692243491
mae: 0.03417821506186575
r2: 0.6927700646443571
pearson: 0.8323708816501472

=== Experiment 2324 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007461962606069412
rmse: 0.08638265222872826
mae: 0.03477703982731854
r2: 0.663543390353234
pearson: 0.830392379338977

=== Experiment 2178 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007430816659743662
rmse: 0.08620218477361036
mae: 0.044104692936522916
r2: 0.6649477473646825
pearson: 0.8280122658034239

=== Experiment 2221 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006907074251604848
rmse: 0.08310880971115425
mae: 0.04100165748113158
r2: 0.6885630620309995
pearson: 0.8363893829346681

=== Experiment 2046 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005863651842967706
rmse: 0.07657448558735283
mae: 0.03411680537041964
r2: 0.7356105191911257
pearson: 0.8596518600942016

=== Experiment 2280 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007371687009686149
rmse: 0.08585852904450524
mae: 0.038036479032131046
r2: 0.6676138772608267
pearson: 0.8172753011587384

=== Experiment 2209 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00686141368623969
rmse: 0.08283365068762628
mae: 0.04387972695297524
r2: 0.6906218768265626
pearson: 0.8379656188230465

=== Experiment 2303 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008521583071412542
rmse: 0.09231242100287773
mae: 0.03749903850549389
r2: 0.6157655699455445
pearson: 0.8028184339979083

=== Experiment 2087 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007024309765992115
rmse: 0.0838111553791744
mae: 0.04187694010490158
r2: 0.6832769642865734
pearson: 0.830700974931659

=== Experiment 2075 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006947412994639857
rmse: 0.08335114273145784
mae: 0.03180023495695181
r2: 0.6867442058619895
pearson: 0.8454030023996116

=== Experiment 2248 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006367572807884805
rmse: 0.07979707267741597
mae: 0.03429488241098371
r2: 0.7128889446755917
pearson: 0.8459986114427762

=== Experiment 2192 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008094717784884827
rmse: 0.08997064957465199
mae: 0.04834146261803226
r2: 0.635012737837299
pearson: 0.8194604438327432

=== Experiment 2288 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007711108209060818
rmse: 0.08781291595807998
mae: 0.039479208001719265
r2: 0.6523095247717172
pearson: 0.8083397082597373

=== Experiment 2245 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005689204763836405
rmse: 0.07542681727234953
mae: 0.03497456911087622
r2: 0.7434762612091248
pearson: 0.8676034840530148

=== Experiment 2072 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.012399434848127368
rmse: 0.11135274962086643
mae: 0.043581994375008745
r2: 0.44091494010656374
pearson: 0.6664017169185971

=== Experiment 2321 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008244548198066918
rmse: 0.09079949448134014
mae: 0.037331089009167956
r2: 0.6282569504523263
pearson: 0.815033076493241

=== Experiment 2191 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005403369465154695
rmse: 0.07350761501473636
mae: 0.028546864816519458
r2: 0.7563644490209473
pearson: 0.8722756275504274

=== Experiment 2262 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00752166116730542
rmse: 0.08672751101758554
mae: 0.03851460121931472
r2: 0.6608516085024485
pearson: 0.8136984112271551

=== Experiment 2229 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005779507505090081
rmse: 0.07602307218923793
mae: 0.031836453065180084
r2: 0.7394045503512725
pearson: 0.8599728914387651

=== Experiment 2336 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008356955557993933
rmse: 0.09141638560998752
mae: 0.037349393502093174
r2: 0.6231885520674798
pearson: 0.8057714562303344

=== Experiment 2228 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006427199395232813
rmse: 0.08016981598602316
mae: 0.03725439194086431
r2: 0.7102004080957381
pearson: 0.84483286573854

=== Experiment 2006 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007220409990712596
rmse: 0.08497299565575286
mae: 0.042180340316732644
r2: 0.6744348914642364
pearson: 0.8242303367463433

=== Experiment 2347 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007172911840705399
rmse: 0.08469304481895429
mae: 0.03617749265190111
r2: 0.6765765621425262
pearson: 0.8231329030537824

=== Experiment 2362 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007449153133765448
rmse: 0.08630847660436053
mae: 0.038129922204030546
r2: 0.6641209638215395
pearson: 0.8249020134691939

=== Experiment 2281 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006891724749333871
rmse: 0.08301641252989599
mae: 0.03597840932302614
r2: 0.6892551643325651
pearson: 0.8366752561113258

=== Experiment 2232 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007236166044582423
rmse: 0.08506565725710008
mae: 0.04189782864580204
r2: 0.6737244579300155
pearson: 0.8237472709784708

=== Experiment 2421 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006726097435428658
rmse: 0.08201278824322862
mae: 0.03455488780313555
r2: 0.6967232270183955
pearson: 0.8357683486110232

=== Experiment 2252 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007548615552545724
rmse: 0.08688276901978738
mae: 0.04341335705040404
r2: 0.6596362471354954
pearson: 0.8287469668805946

=== Experiment 2283 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006553727292394849
rmse: 0.0809550942955096
mae: 0.033835926649881956
r2: 0.7044953209018883
pearson: 0.8461520323632444

=== Experiment 2431 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007747616705733049
rmse: 0.08802054706563149
mae: 0.042891094622293326
r2: 0.6506633727253845
pearson: 0.8165513509536155

=== Experiment 2244 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007282655098583504
rmse: 0.08533847373010314
mae: 0.04077253027032878
r2: 0.671628286946504
pearson: 0.8218212034428773

=== Experiment 2085 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005840141215960122
rmse: 0.07642081664023306
mae: 0.03309087576791844
r2: 0.7366706030150795
pearson: 0.8597975031735122

=== Experiment 2456 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007261460825447816
rmse: 0.08521420553785511
mae: 0.03924689318601056
r2: 0.6725839273938792
pearson: 0.8207383871401464

=== Experiment 2414 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0075832045362837705
rmse: 0.08708159700122507
mae: 0.03210938887456258
r2: 0.6580766450825227
pearson: 0.8371809087984915

=== Experiment 2345 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006960413678953429
rmse: 0.08342909372007723
mae: 0.03374659105073933
r2: 0.6861580107283292
pearson: 0.836403003883467

=== Experiment 2467 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008487512350982804
rmse: 0.09212769589533218
mae: 0.03546644605178581
r2: 0.6173018037340505
pearson: 0.811003089223149

=== Experiment 2177 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006370382587443132
rmse: 0.07981467651656012
mae: 0.03711497601086288
r2: 0.7127622529519854
pearson: 0.8447600350709645

=== Experiment 2484 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006934477818136226
rmse: 0.08327351210400716
mae: 0.03607881223526135
r2: 0.6873274472773316
pearson: 0.8297320208494356

=== Experiment 2305 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006090240725061279
rmse: 0.07803999439429297
mae: 0.03499124509486605
r2: 0.7253937262269202
pearson: 0.8538541294883756

=== Experiment 2372 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006975945375127041
rmse: 0.08352212506352458
mae: 0.036605226708793896
r2: 0.6854576933838832
pearson: 0.8291118459827598

=== Experiment 2279 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007261863351941112
rmse: 0.08521656735600837
mae: 0.03909453333844096
r2: 0.6725657776514621
pearson: 0.8223412355669205

=== Experiment 2158 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006003888826328653
rmse: 0.0774847651240465
mae: 0.03036070101113684
r2: 0.7292872953343973
pearson: 0.8574308067853531

=== Experiment 2441 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006336566827416556
rmse: 0.0796025554075782
mae: 0.03189691239251657
r2: 0.7142869906881295
pearson: 0.8480146678495103

=== Experiment 2285 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007243933191068043
rmse: 0.08511129884491273
mae: 0.03403704933068404
r2: 0.6733742407135613
pearson: 0.8286406884931766

=== Experiment 2045 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006362866977453624
rmse: 0.07976758099286717
mae: 0.029838718146499795
r2: 0.7131011284985393
pearson: 0.8454946958315016

=== Experiment 2472 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007284194572034299
rmse: 0.08534749306238759
mae: 0.036921096636437584
r2: 0.6715588727661819
pearson: 0.8207593021622955

=== Experiment 2301 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0074654531358193015
rmse: 0.08640285374812166
mae: 0.03727669304743647
r2: 0.6633860039031109
pearson: 0.8302896010130061

=== Experiment 2319 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0065639935906614665
rmse: 0.08101847684733074
mae: 0.036338107672895616
r2: 0.7040324180315898
pearson: 0.8396139644902897

=== Experiment 2332 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006273674631037593
rmse: 0.07920653149228031
mae: 0.03425551123401352
r2: 0.7171227721418844
pearson: 0.8503362998013502

=== Experiment 2263 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007931762737911822
rmse: 0.08906044429437697
mae: 0.03651671178697376
r2: 0.6423603091833108
pearson: 0.8373919380778894

=== Experiment 2396 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007788728168662275
rmse: 0.08825377141325053
mae: 0.041044817028078766
r2: 0.6488096749564443
pearson: 0.8131588042193995

=== Experiment 2399 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007325716450463623
rmse: 0.08559039928907694
mae: 0.04402821291078806
r2: 0.6696866695429798
pearson: 0.8340567784237433

=== Experiment 2123 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007053643433990177
rmse: 0.08398597164997365
mae: 0.03316523016557457
r2: 0.6819543221072754
pearson: 0.8304857667255507

=== Experiment 2402 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00722073610479316
rmse: 0.08497491456184678
mae: 0.04146574196236994
r2: 0.6744201871238773
pearson: 0.8268459725558145

=== Experiment 2365 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0064628987052049456
rmse: 0.08039215574423257
mae: 0.03778179195753794
r2: 0.7085907419215627
pearson: 0.8539037142990538

=== Experiment 2261 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0074563187577047765
rmse: 0.0863499783306561
mae: 0.038672147719104995
r2: 0.6637978689919478
pearson: 0.8171335449284737

=== Experiment 2457 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006575133835913956
rmse: 0.0810871989645342
mae: 0.037427647442154856
r2: 0.7035301092763522
pearson: 0.8397364668291134

=== Experiment 2029 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006031248424302914
rmse: 0.0776611126903479
mae: 0.035431607582780224
r2: 0.7280536631036177
pearson: 0.8551093547016954

=== Experiment 2166 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006186494826402419
rmse: 0.07865427405044445
mae: 0.03397973846715326
r2: 0.7210536711621179
pearson: 0.8547651171390329

=== Experiment 2481 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007400784943165299
rmse: 0.08602781493892135
mae: 0.03514703750288831
r2: 0.6663018642472318
pearson: 0.8239341323209985

=== Experiment 2049 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005913394329460594
rmse: 0.07689859770802453
mae: 0.03024931979940622
r2: 0.733367652368504
pearson: 0.8576407890809117

=== Experiment 2451 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006750536358924576
rmse: 0.08216164773739981
mae: 0.03622276913651856
r2: 0.6956212867143574
pearson: 0.8347653404497667

=== Experiment 2133 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005442435078802408
rmse: 0.07377286139768749
mae: 0.029877770304978485
r2: 0.7546029976956634
pearson: 0.8699695406768342

=== Experiment 2440 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007927625975277713
rmse: 0.08903721679880675
mae: 0.04183074282709779
r2: 0.642546833989754
pearson: 0.8033021497252472

=== Experiment 2078 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006644298386739819
rmse: 0.08151256582110404
mae: 0.040159487627926045
r2: 0.7004115101212607
pearson: 0.8416787477406836

=== Experiment 2092 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006210575621157077
rmse: 0.0788072053885752
mae: 0.03155021487243809
r2: 0.7199678787253988
pearson: 0.8493778034647165

=== Experiment 2465 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006542028380945709
rmse: 0.0808828064606175
mae: 0.03912170688662117
r2: 0.7050228196700452
pearson: 0.8430168032326941

=== Experiment 2383 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007059663447881265
rmse: 0.08402180340769451
mae: 0.03565476743391201
r2: 0.681682882330537
pearson: 0.8258965768495676

=== Experiment 2353 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0061194895340690985
rmse: 0.07822716621525477
mae: 0.03232830023325246
r2: 0.7240749103021429
pearson: 0.8550637324850913

=== Experiment 2410 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007117968555863986
rmse: 0.08436805411922207
mae: 0.04395689905552796
r2: 0.6790539306736367
pearson: 0.8332577593403789

=== Experiment 2163 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007011471723184898
rmse: 0.08373453124717961
mae: 0.03279801948929132
r2: 0.683855825986296
pearson: 0.8330182176975423

=== Experiment 2391 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007184833751716276
rmse: 0.08476339865600173
mae: 0.036280089841989875
r2: 0.67603900842228
pearson: 0.8234265995573431

=== Experiment 2447 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006472852896036805
rmse: 0.08045404213609658
mae: 0.03543459763407936
r2: 0.7081419118381285
pearson: 0.8417202239679896

=== Experiment 2148 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006405685684519742
rmse: 0.08003552763941613
mae: 0.03541447639681186
r2: 0.7111704518428823
pearson: 0.8434178494396823

=== Experiment 2335 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006983748885842499
rmse: 0.0835688272374484
mae: 0.04055498367366318
r2: 0.6851058365203109
pearson: 0.8297325917216812

=== Experiment 2307 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005853112338950249
rmse: 0.07650563599467851
mae: 0.030026505598235137
r2: 0.7360857407884824
pearson: 0.8581584097618459

=== Experiment 2153 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006272519144698386
rmse: 0.07919923702093591
mae: 0.03543431074938153
r2: 0.7171748725123508
pearson: 0.851302781364399

=== Experiment 2111 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006882029980935318
rmse: 0.08295800130750088
mae: 0.030822752484532843
r2: 0.689692297172662
pearson: 0.8452045248724727

=== Experiment 2464 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007110022937529028
rmse: 0.08432095194866474
mae: 0.03606587637991483
r2: 0.6794121951072258
pearson: 0.8336168607099397

=== Experiment 2378 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007213016882023536
rmse: 0.0849294818188804
mae: 0.0458812495981822
r2: 0.6747682434810701
pearson: 0.829904030086294

=== Experiment 2018 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010018482341661873
rmse: 0.10009236904810413
mae: 0.043958726305466475
r2: 0.5482710406857547
pearson: 0.7780428091895588

=== Experiment 2208 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005938393836848929
rmse: 0.07706097479820073
mae: 0.034255167651630704
r2: 0.7322404355834886
pearson: 0.8562363675698129

=== Experiment 2359 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006617657124947578
rmse: 0.08134898355202466
mae: 0.04214816393897125
r2: 0.7016127528897567
pearson: 0.8424026759640261

=== Experiment 2377 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0070841660831198324
rmse: 0.08416748827854989
mae: 0.04147957700402547
r2: 0.6805780692920644
pearson: 0.8289842598094994

=== Experiment 2448 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007831017954656707
rmse: 0.08849303901808722
mae: 0.040517990915067026
r2: 0.6469028471191136
pearson: 0.8247747088553138

=== Experiment 2450 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007768914728017322
rmse: 0.08814144727662078
mae: 0.048239225313804604
r2: 0.6497030542745632
pearson: 0.8264152155896901

=== Experiment 2458 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006368117969567336
rmse: 0.07980048852962829
mae: 0.0313227271353509
r2: 0.7128643635752705
pearson: 0.8589135261191143

=== Experiment 2136 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011833454729569967
rmse: 0.1087816837963541
mae: 0.05295943841743846
r2: 0.4664347345454165
pearson: 0.6859651999216527

=== Experiment 2387 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006989625588047758
rmse: 0.08360398069498699
mae: 0.043574613724013445
r2: 0.6848408586044104
pearson: 0.835782249715166

=== Experiment 2022 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009788300453030956
rmse: 0.09893584008351552
mae: 0.04207251728310214
r2: 0.558649840733326
pearson: 0.7912967846928556

=== Experiment 2167 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005915364903366855
rmse: 0.07691140944857827
mae: 0.027583793505966136
r2: 0.7332788000583865
pearson: 0.8676461072848562

=== Experiment 2258 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00580572911970791
rmse: 0.07619533528837517
mae: 0.03134715339478859
r2: 0.7382222292891698
pearson: 0.863692976703889

=== Experiment 2439 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006880880262114583
rmse: 0.0829510714946744
mae: 0.03666738843015987
r2: 0.6897441374882589
pearson: 0.8309132691194738

=== Experiment 2058 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006655546928975026
rmse: 0.08158153546590691
mae: 0.03010116777774924
r2: 0.6999043183027372
pearson: 0.8591913893885301

=== Experiment 2296 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005876338621530858
rmse: 0.07665728029046463
mae: 0.028059417852971245
r2: 0.7350384779295914
pearson: 0.8598332645063674

=== Experiment 2445 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008056660497244426
rmse: 0.08975890205012774
mae: 0.04087734631179261
r2: 0.6367287241867112
pearson: 0.8207488095693237

=== Experiment 2400 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0066141743766582085
rmse: 0.08132757451601645
mae: 0.03382572715601175
r2: 0.7017697884772498
pearson: 0.8381373565245571

=== Experiment 2275 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007736426018519327
rmse: 0.08795695548687055
mae: 0.05097745596055153
r2: 0.6511679558864532
pearson: 0.8207430349736581

=== Experiment 2012 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006063777311592905
rmse: 0.07787025948070872
mae: 0.028997655828137254
r2: 0.7265869498927702
pearson: 0.8707537749485957

=== Experiment 2471 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007128367579729597
rmse: 0.08442966054491512
mae: 0.04162395831568797
r2: 0.6785850432644966
pearson: 0.8285094228890617

=== Experiment 2322 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0069065516378752296
rmse: 0.08310566549805873
mae: 0.038129524809717735
r2: 0.6885866264540479
pearson: 0.8384290179287978

=== Experiment 2436 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076361517521889
rmse: 0.08738507739991365
mae: 0.03210272269259283
r2: 0.6556892784212125
pearson: 0.8104149876332722

=== Experiment 2294 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009216451933005866
rmse: 0.09600235378888304
mae: 0.04368285207692707
r2: 0.5844342388114758
pearson: 0.7682966113235974

=== Experiment 2338 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0066085847656585225
rmse: 0.08129320245665392
mae: 0.03563175009579674
r2: 0.7020218215770496
pearson: 0.8407908284794103

=== Experiment 2137 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00612190215745484
rmse: 0.0782425853193441
mae: 0.03181613640219229
r2: 0.7239661261755567
pearson: 0.8528589584020618

=== Experiment 2357 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006595877250551149
rmse: 0.08121500631380354
mae: 0.0397490431197757
r2: 0.702594797840236
pearson: 0.839972551347029

=== Experiment 2434 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007249181110890465
rmse: 0.0851421230114123
mae: 0.04816630950102866
r2: 0.6731376143185546
pearson: 0.836304270349858

=== Experiment 2149 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00607113649679576
rmse: 0.0779174980142186
mae: 0.031199817208276984
r2: 0.7262551274709985
pearson: 0.8530592143115412

=== Experiment 2040 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008585052329690706
rmse: 0.09265555746791827
mae: 0.03906952749113149
r2: 0.6129037690247389
pearson: 0.8120369031157284

=== Experiment 2361 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006069146971183354
rmse: 0.07790473009505491
mae: 0.03215975690024421
r2: 0.7263448343052047
pearson: 0.8626124768899023

=== Experiment 2071 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006901960842720259
rmse: 0.0830780406769458
mae: 0.03500138865039339
r2: 0.6887936233870221
pearson: 0.8488712669850281

=== Experiment 2181 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006394903739616996
rmse: 0.07996814202929187
mae: 0.037545740567621076
r2: 0.7116566049930503
pearson: 0.8436145741359199

=== Experiment 2170 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.012474330140749779
rmse: 0.11168854077634724
mae: 0.048964306539723564
r2: 0.4375379443261582
pearson: 0.6616711924644634

=== Experiment 2235 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006647826862847481
rmse: 0.08153420670398089
mae: 0.035161463372896364
r2: 0.7002524126865675
pearson: 0.8374317759166812

=== Experiment 2154 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006923085657481755
rmse: 0.08320508192100862
mae: 0.038336462189512095
r2: 0.6878411147871684
pearson: 0.8352875806708037

=== Experiment 2363 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007536820381303918
rmse: 0.08681486267514289
mae: 0.03818022539548701
r2: 0.6601680862153365
pearson: 0.8135882801553235

=== Experiment 2476 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0074060178890663604
rmse: 0.08605822383169641
mae: 0.040515453105741034
r2: 0.6660659130197484
pearson: 0.8231988466769802

=== Experiment 2384 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006738416126212955
rmse: 0.08208785614335994
mae: 0.03256519899770719
r2: 0.6961677826728019
pearson: 0.8359549285469297

=== Experiment 2234 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006566812366055136
rmse: 0.08103587086010204
mae: 0.03128498015885015
r2: 0.7039053206897273
pearson: 0.8459008637218781

=== Experiment 2349 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0067008134570617816
rmse: 0.08185849654777311
mae: 0.030502079239519538
r2: 0.6978632704746279
pearson: 0.8393360841182557

=== Experiment 2409 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006457845025803603
rmse: 0.08036071817625576
mae: 0.03677027238709715
r2: 0.7088186101014748
pearson: 0.8506993050415668

=== Experiment 2382 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0064209661584815995
rmse: 0.0801309313466504
mae: 0.03327726940739507
r2: 0.7104814619973931
pearson: 0.8441308283762109

=== Experiment 2105 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007425494126142817
rmse: 0.08617130686105913
mae: 0.04214752521466854
r2: 0.6651877380621185
pearson: 0.8185283897048794

=== Experiment 2186 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006247112204911228
rmse: 0.07903867537421935
mae: 0.03315217332188581
r2: 0.7183204602449045
pearson: 0.8483343648695613

=== Experiment 2433 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006537795136930018
rmse: 0.08085663322776937
mae: 0.03586544504198683
r2: 0.7052136947795196
pearson: 0.8446833058967883

=== Experiment 2333 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007248750195032577
rmse: 0.08513959240584006
mae: 0.0413330299669204
r2: 0.6731570441249262
pearson: 0.8284473755295737

=== Experiment 2099 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006679876307078805
rmse: 0.08173051025827996
mae: 0.03941203780813458
r2: 0.6988073173522162
pearson: 0.8487506772324797

=== Experiment 2473 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007567883929717607
rmse: 0.08699358556650948
mae: 0.04348826203804843
r2: 0.6587674444894764
pearson: 0.8231679922586761

=== Experiment 2403 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006334839032275701
rmse: 0.07959170203152903
mae: 0.039721354067755295
r2: 0.7143648962105698
pearson: 0.8483062312051507

=== Experiment 2059 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006691143412868347
rmse: 0.08179940961197915
mae: 0.03362286830576859
r2: 0.698299288511198
pearson: 0.8443882016300482

=== Experiment 2328 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00705644864421545
rmse: 0.08400267045883393
mae: 0.039437562192356335
r2: 0.6818278364128852
pearson: 0.826880680261676

=== Experiment 2419 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0065174548276262815
rmse: 0.08073075515332606
mae: 0.03445212131717964
r2: 0.7061308303735703
pearson: 0.8492359566116233

=== Experiment 2371 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006248218012237192
rmse: 0.07904567042056884
mae: 0.03722624623128565
r2: 0.7182705998792797
pearson: 0.8532178063535906

=== Experiment 2106 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006232717474436148
rmse: 0.0789475615483857
mae: 0.03475611376227283
r2: 0.7189695123064204
pearson: 0.8480552836578241

=== Experiment 2394 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006731753586909537
rmse: 0.08204726434750605
mae: 0.03549894404293847
r2: 0.6964681936375854
pearson: 0.8370167559894119

=== Experiment 2061 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006221779421272158
rmse: 0.07887825696142226
mae: 0.030966827160107024
r2: 0.7194627043093793
pearson: 0.8486587142490175

=== Experiment 2366 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006259335937565677
rmse: 0.07911596512440253
mae: 0.03416277894529097
r2: 0.7177692975195915
pearson: 0.8472501257521222

=== Experiment 2341 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006963730619736141
rmse: 0.08344897015383798
mae: 0.036797091176300374
r2: 0.686008451328334
pearson: 0.8298307757619262

=== Experiment 2352 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006194876488435434
rmse: 0.07870753768499834
mae: 0.030823632676254167
r2: 0.720675745710101
pearson: 0.8534091095069938

=== Experiment 2317 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005927481847074539
rmse: 0.07699014123298216
mae: 0.033768230742265294
r2: 0.7327324523996819
pearson: 0.8561196613512897

=== Experiment 2302 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007437995011104049
rmse: 0.08624381143655496
mae: 0.040319389436785154
r2: 0.6646240786612767
pearson: 0.8321635877808181

=== Experiment 2438 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.011635713260383408
rmse: 0.10786896337864478
mae: 0.05042778421774869
r2: 0.47535081035836224
pearson: 0.693951031299111

=== Experiment 2351 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007762046073376396
rmse: 0.0881024748425173
mae: 0.05274213937831931
r2: 0.6500127588891964
pearson: 0.8266712709024142

=== Experiment 2070 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006214172197089922
rmse: 0.07883002091265688
mae: 0.03454960141203684
r2: 0.719805710699561
pearson: 0.8501021146546954

=== Experiment 2108 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005891813529126521
rmse: 0.0767581495942061
mae: 0.03343530528228932
r2: 0.7343407211571307
pearson: 0.8576678792406061

=== Experiment 2412 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006957996808940777
rmse: 0.0834146078869929
mae: 0.03882600558429404
r2: 0.6862669863334538
pearson: 0.8304537050378379

=== Experiment 2315 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006428036988119668
rmse: 0.08017503968268222
mae: 0.03236111861109459
r2: 0.7101626414011226
pearson: 0.8578341717178063

=== Experiment 2098 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006601367854379234
rmse: 0.0812488021719658
mae: 0.03380855389497028
r2: 0.702347228930227
pearson: 0.8403042567901801

=== Experiment 2369 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005636520439390573
rmse: 0.07507676364488931
mae: 0.02926874516747583
r2: 0.745851774210243
pearson: 0.8663061948727859

=== Experiment 2067 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005929715602851796
rmse: 0.07700464663156241
mae: 0.030253866109503348
r2: 0.7326317333348364
pearson: 0.8564046814775504

=== Experiment 2339 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007564206900532432
rmse: 0.08697244908896398
mae: 0.04075177234281136
r2: 0.6589332401170518
pearson: 0.8118712035552469

=== Experiment 2053 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006004922301770733
rmse: 0.07749143373154695
mae: 0.03107291684525964
r2: 0.7292406963815812
pearson: 0.8554600182278032

=== Experiment 2430 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006664526714843328
rmse: 0.08163655256589004
mae: 0.03649288388327526
r2: 0.6994994237102414
pearson: 0.8377036657617858

=== Experiment 2230 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006696972827066161
rmse: 0.08183503422780589
mae: 0.03145160163552392
r2: 0.6980364427907402
pearson: 0.8363322480121036

=== Experiment 2289 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006628929132908225
rmse: 0.08141823587445397
mae: 0.04161955242382667
r2: 0.7011045030120013
pearson: 0.8416288252903816

=== Experiment 2066 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005944900334177368
rmse: 0.07710317979290716
mae: 0.029135899620929932
r2: 0.7319470604826778
pearson: 0.8562555594281712

=== Experiment 2470 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006477949781207776
rmse: 0.08048571165870236
mae: 0.04134813813261699
r2: 0.7079120955290802
pearson: 0.8481346252651755

=== Experiment 2483 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012066081663339726
rmse: 0.10984571754665598
mae: 0.04645205937931849
r2: 0.45594568848868167
pearson: 0.7383950555419387

=== Experiment 2446 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0073418912797963394
rmse: 0.08568483693044143
mae: 0.034453398355579624
r2: 0.6689573536074546
pearson: 0.8215302571585917

=== Experiment 2139 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0060963774590621495
rmse: 0.07807930237304986
mae: 0.03684262462516372
r2: 0.7251170235918041
pearson: 0.8547298436636123

=== Experiment 2115 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011859130543721513
rmse: 0.1088996351863564
mae: 0.05254069454996265
r2: 0.4652770233861133
pearson: 0.6924496922007723

=== Experiment 2182 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005789853403946399
rmse: 0.07609108623187344
mae: 0.040115220975107176
r2: 0.7389380583254195
pearson: 0.8648731386043707

=== Experiment 2278 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0063663699446427505
rmse: 0.07978953530785068
mae: 0.037875904094971165
r2: 0.7129431812497553
pearson: 0.8482121762291408

=== Experiment 2107 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00620775295107534
rmse: 0.07878929464765717
mae: 0.027275595795005634
r2: 0.7200951516770322
pearson: 0.8491561197202061

=== Experiment 2418 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006934294739442339
rmse: 0.08327241283547834
mae: 0.03318031785566552
r2: 0.6873357022150599
pearson: 0.832520810275381

=== Experiment 2424 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010976947401606908
rmse: 0.10477092822728501
mae: 0.05047873184619991
r2: 0.5050542729855669
pearson: 0.7115953055841652

=== Experiment 2143 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011039984138844378
rmse: 0.1050713288144981
mae: 0.04694980584790811
r2: 0.5022119742481191
pearson: 0.7118320372981586

=== Experiment 2266 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006384023923274761
rmse: 0.07990008712933147
mae: 0.03237086139647172
r2: 0.7121471711233482
pearson: 0.8467877911413757

=== Experiment 2016 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0061570228585183994
rmse: 0.07846669904181264
mae: 0.03254424916103942
r2: 0.7223825492224034
pearson: 0.8526143705563147

=== Experiment 2292 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006121554815117052
rmse: 0.07824036563767485
mae: 0.03367898343087971
r2: 0.7239817876886905
pearson: 0.8522201905983807

=== Experiment 2346 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005590111467101982
rmse: 0.07476704800312757
mae: 0.028751586097316883
r2: 0.7479443343445847
pearson: 0.8671272969428013

=== Experiment 2103 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009669385760426088
rmse: 0.09833303493956691
mae: 0.04559386659452403
r2: 0.5640116518845208
pearson: 0.7742374111894464

=== Experiment 2056 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006324756158634671
rmse: 0.07952833557062962
mae: 0.03684162719669046
r2: 0.7148195285452317
pearson: 0.8463278174501465

=== Experiment 2486 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0066957269111994795
rmse: 0.08182742151137037
mae: 0.03922945490548708
r2: 0.6980926205887992
pearson: 0.8362205929623592

=== Experiment 2243 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005951769876840771
rmse: 0.07714771465727789
mae: 0.02939927313071038
r2: 0.7316373158275018
pearson: 0.8557661263743773

=== Experiment 2080 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0060542216175316806
rmse: 0.07780887878341186
mae: 0.032559840494005995
r2: 0.7270178119322082
pearson: 0.8531445540474951

=== Experiment 2081 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0076436274593794625
rmse: 0.08742784144298349
mae: 0.036277583880153216
r2: 0.6553522020743003
pearson: 0.8135608287470616

=== Experiment 2306 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005962837580589233
rmse: 0.07721941194148808
mae: 0.033125103261346885
r2: 0.7311382779367526
pearson: 0.8597444954137708

=== Experiment 2360 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.016092182619390355
rmse: 0.12685496686921785
mae: 0.052939234127620784
r2: 0.27441056840290334
pearson: 0.5277919182180097

=== Experiment 2130 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006790886733273038
rmse: 0.08240683669012565
mae: 0.03958097489106851
r2: 0.6938019060945432
pearson: 0.8382507319907744

=== Experiment 2084 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005311366192457631
rmse: 0.07287912041495583
mae: 0.02768880059975634
r2: 0.7605128360931209
pearson: 0.872278477894945

=== Experiment 2217 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.010657156014688729
rmse: 0.1032335023850723
mae: 0.04378052561696958
r2: 0.5194735258707546
pearson: 0.7215168466031203

=== Experiment 2257 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006364172591920998
rmse: 0.07977576443959028
mae: 0.028838431180018713
r2: 0.7130422589168504
pearson: 0.8620783669793713

=== Experiment 2173 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005866513900944346
rmse: 0.07659317137280808
mae: 0.03057334363979955
r2: 0.7354814702566471
pearson: 0.8584222887174975

=== Experiment 2368 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007065286180914104
rmse: 0.08405525671196361
mae: 0.042829590506417235
r2: 0.6814293557728404
pearson: 0.8353082067146135

=== Experiment 2411 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006498990525580703
rmse: 0.08061631674531343
mae: 0.03516725943282804
r2: 0.7069633776260447
pearson: 0.8427904519363124

=== Experiment 2165 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006237935093855188
rmse: 0.07898059947768939
mae: 0.03028646398346299
r2: 0.7187342521432658
pearson: 0.8506542854282756

=== Experiment 2238 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007993922986872112
rmse: 0.08940874110998383
mae: 0.03882435813459971
r2: 0.6395575309165089
pearson: 0.8094186784566436

=== Experiment 2350 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00739019849403707
rmse: 0.08596626369708683
mae: 0.03449194677013501
r2: 0.6667792025789709
pearson: 0.8330437039534083

=== Experiment 2358 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006131441871572025
rmse: 0.07830352400481108
mae: 0.036420843237465084
r2: 0.723535984664762
pearson: 0.8509766245310495

=== Experiment 2112 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007260710872674128
rmse: 0.08520980502661726
mae: 0.035744389928196084
r2: 0.6726177424343655
pearson: 0.844306493502238

=== Experiment 2086 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006025387744214903
rmse: 0.0776233711211701
mae: 0.0334074181410402
r2: 0.7283179185893036
pearson: 0.8590223835248344

=== Experiment 2348 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0061721636903644085
rmse: 0.07856311914864639
mae: 0.038556155547528254
r2: 0.7216998557784565
pearson: 0.8535897350342074

=== Experiment 2344 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007065303042177685
rmse: 0.08405535701058967
mae: 0.035710264764364966
r2: 0.6814285955058867
pearson: 0.8404124687533112

=== Experiment 2304 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006975899615995096
rmse: 0.08352185112888182
mae: 0.04083897243510211
r2: 0.6854597566430025
pearson: 0.8430664404738771

=== Experiment 2274 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006233832606140907
rmse: 0.07895462371603647
mae: 0.03777538912800901
r2: 0.7189192315086597
pearson: 0.8502619949941698

=== Experiment 2415 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006408568308484266
rmse: 0.08005353401620859
mae: 0.030707799829566715
r2: 0.7110404755970617
pearson: 0.844433137180279

=== Experiment 2009 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006655823361462361
rmse: 0.0815832296581985
mae: 0.031654617196584925
r2: 0.6998918540835505
pearson: 0.8466277463700643

=== Experiment 2437 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006235015045406654
rmse: 0.0789621114548405
mae: 0.03642129630131552
r2: 0.7188659158425986
pearson: 0.8488417472306267

=== Experiment 2452 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005914902202621
rmse: 0.07690840137865954
mae: 0.03267290519915345
r2: 0.7332996630313671
pearson: 0.8569565663561337

=== Experiment 2110 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0069119020397227756
rmse: 0.08313784962171426
mae: 0.03638488173000161
r2: 0.6883453791896341
pearson: 0.8312818226446652

=== Experiment 2050 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007057802057004291
rmse: 0.08401072584500321
mae: 0.03497487458760266
r2: 0.681766811625919
pearson: 0.8453175199526437

=== Experiment 2044 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008814603642151487
rmse: 0.09388612060444018
mae: 0.042303674472980446
r2: 0.6025534013791413
pearson: 0.7837665140316452

=== Experiment 2197 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006384425704054771
rmse: 0.0799026013597478
mae: 0.0349930909324057
r2: 0.7121290550048149
pearson: 0.8493257411262706

=== Experiment 2468 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006940508044630284
rmse: 0.08330971158652684
mae: 0.0395865137538532
r2: 0.6870555470188202
pearson: 0.8302523760522053

=== Experiment 2128 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006280564340110378
rmse: 0.07925001160952835
mae: 0.029003951175652423
r2: 0.7168121181921854
pearson: 0.8563134365685131

=== Experiment 2463 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006497966150417519
rmse: 0.08060996309649024
mae: 0.03282455938055872
r2: 0.7070095662512907
pearson: 0.8480902581938733

=== Experiment 2320 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006863883357913563
rmse: 0.08284855676421625
mae: 0.03425512604716114
r2: 0.6905105204177733
pearson: 0.8313280280677029

=== Experiment 2065 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00671376631844433
rmse: 0.08193757574180682
mae: 0.03089461092786799
r2: 0.6972792316558791
pearson: 0.8479840957983551

=== Experiment 2052 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062461651062961756
rmse: 0.07903268378523012
mae: 0.036083099025960866
r2: 0.7183631645046071
pearson: 0.8491868668468866

=== Experiment 2375 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007001013379684036
rmse: 0.08367205853619257
mae: 0.034448689433552865
r2: 0.6843273880916809
pearson: 0.8300992750656591

=== Experiment 2395 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006003702099228015
rmse: 0.07748356018684231
mae: 0.034280228015043164
r2: 0.7292957147771801
pearson: 0.8546260247633052

=== Experiment 2290 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005851822587542768
rmse: 0.07649720640352017
mae: 0.028066308190827928
r2: 0.7361438951117826
pearson: 0.8664502731116798

=== Experiment 2109 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008797817611330867
rmse: 0.09379668230449767
mae: 0.04487205176732094
r2: 0.6033102761207458
pearson: 0.7862676064857131

=== Experiment 2161 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005871937328349858
rmse: 0.07662856731239244
mae: 0.02984187379147644
r2: 0.7352369303019655
pearson: 0.8600861773324677

=== Experiment 2461 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006818993439236423
rmse: 0.08257719684777647
mae: 0.03588615215125658
r2: 0.6925345870933615
pearson: 0.8361326128499181

=== Experiment 2047 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005618369548384582
rmse: 0.07495578395550662
mae: 0.031165548773270536
r2: 0.746670189896885
pearson: 0.8642257251201059

=== Experiment 2406 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005764926731705961
rmse: 0.07592711460147791
mae: 0.033722138413109747
r2: 0.7400619910056734
pearson: 0.8612577472993631

=== Experiment 2380 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0063913359593446165
rmse: 0.07994583140692588
mae: 0.039110889383215504
r2: 0.7118174746352317
pearson: 0.8489502127924184

=== Experiment 2364 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006691492098341886
rmse: 0.08180154092889623
mae: 0.0327209249775248
r2: 0.6982835664366644
pearson: 0.8390834048017924

=== Experiment 2172 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005469048733152235
rmse: 0.07395301706591986
mae: 0.03257457181658325
r2: 0.7534029997345948
pearson: 0.8690452373739016

=== Experiment 2224 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0060126424300001495
rmse: 0.0775412305164172
mae: 0.03536172094481464
r2: 0.7288925991976051
pearson: 0.8558550535157278

=== Experiment 2422 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006763029833996152
rmse: 0.08223764243943373
mae: 0.03475518239982426
r2: 0.695057961422179
pearson: 0.8337789654542622

=== Experiment 2031 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006248415699927484
rmse: 0.07904692087568929
mae: 0.03303457481453606
r2: 0.7182616862283335
pearson: 0.8483160631129575

=== Experiment 2169 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005343919045412918
rmse: 0.07310211382315096
mae: 0.03175696821985034
r2: 0.7590450422809731
pearson: 0.8736304770187642

=== Experiment 2388 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006956306967426481
rmse: 0.0834044781017571
mae: 0.03460304593066532
r2: 0.6863431805435773
pearson: 0.8284678063187783

=== Experiment 2026 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006147556499430813
rmse: 0.07840635496840044
mae: 0.03385235019321987
r2: 0.7228093831871995
pearson: 0.8541096322065304

=== Experiment 2293 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005873454675090901
rmse: 0.07663846733260589
mae: 0.03625318276415754
r2: 0.7351685138052472
pearson: 0.8604299505058887

=== Experiment 2427 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0054899867434238135
rmse: 0.07409444475413669
mae: 0.02975837641672682
r2: 0.7524589140669723
pearson: 0.867777512033057

=== Experiment 2193 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006616620396703516
rmse: 0.08134261119919568
mae: 0.03785907807156347
r2: 0.7016594985099824
pearson: 0.8429003715147193

=== Experiment 2449 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005972047103377187
rmse: 0.0772790211077831
mae: 0.03532485880779494
r2: 0.7307230246076647
pearson: 0.8621639204848379

=== Experiment 2273 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006340153736277888
rmse: 0.07962508233137275
mae: 0.02960314180496542
r2: 0.7141252585462914
pearson: 0.8494028893279257

=== Experiment 2325 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010268281788163414
rmse: 0.10133253074982097
mae: 0.042754973634142865
r2: 0.5370076935881448
pearson: 0.7582553330342419

=== Experiment 2222 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0059747066050640195
rmse: 0.07729622633132888
mae: 0.027707881554700137
r2: 0.7306031088471416
pearson: 0.8599958692313476

=== Experiment 2495 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007930804904706832
rmse: 0.08905506669868274
mae: 0.039900351247329414
r2: 0.6424034974609489
pearson: 0.814467975104997

=== Experiment 2298 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005516211530046236
rmse: 0.07427120256227333
mae: 0.029754422432983582
r2: 0.7512764499805786
pearson: 0.8668897982973148

=== Experiment 2094 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0052891490836061
rmse: 0.07272653630969991
mae: 0.02646156535760192
r2: 0.761514595752739
pearson: 0.8732477322475549

=== Experiment 2188 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007333736871552606
rmse: 0.08563723998093707
mae: 0.037751671026933144
r2: 0.6693250322861348
pearson: 0.8340654873531674

=== Experiment 2490 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008161387394413211
rmse: 0.09034039735585189
mae: 0.04124105657661544
r2: 0.6320066344872031
pearson: 0.8181231966401761

=== Experiment 2060 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005240196701819121
rmse: 0.07238920293675792
mae: 0.029724039843828078
r2: 0.763721837101923
pearson: 0.8749597762652752

=== Experiment 2482 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066313693778698455
rmse: 0.08143322035797089
mae: 0.03525845088113129
r2: 0.7009944734407157
pearson: 0.8389665475050273

=== Experiment 2141 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059970810561594005
rmse: 0.07744082293054097
mae: 0.035114379578117846
r2: 0.7295942546949999
pearson: 0.8577359029310031

=== Experiment 2162 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008424200768753337
rmse: 0.09178344496015246
mae: 0.03716697395776729
r2: 0.6201564951112188
pearson: 0.832454649072528

=== Experiment 2511 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008193114627732015
rmse: 0.09051582528890743
mae: 0.035513881689370495
r2: 0.6305760675010814
pearson: 0.7982831879532709

=== Experiment 2573 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008334722038163262
rmse: 0.0912946988502797
mae: 0.035133209822670555
r2: 0.6241910516909244
pearson: 0.8314460763154092

=== Experiment 2373 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007478781631041884
rmse: 0.08647994930064358
mae: 0.031126858462493696
r2: 0.662785027919844
pearson: 0.8378560154476788

=== Experiment 2299 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006253818845526568
rmse: 0.07908109031574216
mae: 0.02938076941605377
r2: 0.7180180607713769
pearson: 0.850287057516659

=== Experiment 2597 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0069492651715355675
rmse: 0.08336225267790913
mae: 0.0358521092495396
r2: 0.6866606920209672
pearson: 0.8296855572435397

=== Experiment 2549 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007699575978281138
rmse: 0.08774722775268252
mae: 0.04845565656111224
r2: 0.65282950798185
pearson: 0.8173908670674126

=== Experiment 2355 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006889170507133513
rmse: 0.08300102714505112
mae: 0.034629389797938066
r2: 0.6893703339891712
pearson: 0.831063434751726

=== Experiment 2657 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008118016927441449
rmse: 0.09010003844306311
mae: 0.03860569058374477
r2: 0.6339621897541574
pearson: 0.8140590949863553

=== Experiment 2690 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008722274475933607
rmse: 0.09339311792596715
mae: 0.038787015837811016
r2: 0.6067164828467317
pearson: 0.8048791938779424

=== Experiment 2220 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006947345196248411
rmse: 0.08335073602703465
mae: 0.028934260872649522
r2: 0.6867472628616192
pearson: 0.8369833093460205

=== Experiment 2227 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005687584504154184
rmse: 0.0754160758999975
mae: 0.031043463977261484
r2: 0.7435493180050654
pearson: 0.8625469060974158

=== Experiment 2633 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007523057402480363
rmse: 0.08673556019580644
mae: 0.03553500891089698
r2: 0.6607886528729403
pearson: 0.8386993473234188

=== Experiment 2277 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005597609403431528
rmse: 0.0748171731852489
mae: 0.028939259699204713
r2: 0.7476062556955797
pearson: 0.8664793792626299

=== Experiment 2267 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005898880391277451
rmse: 0.07680416910088574
mae: 0.03076254290713739
r2: 0.7340220794531092
pearson: 0.856757379496722

=== Experiment 2313 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008310185895390022
rmse: 0.09116022101437678
mae: 0.04067990758479045
r2: 0.6252973755693874
pearson: 0.7933497503184929

=== Experiment 2525 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007740887305859915
rmse: 0.08798231246028894
mae: 0.03779147615242259
r2: 0.6509667984038288
pearson: 0.8184724590348398

=== Experiment 2606 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007316881650063544
rmse: 0.08553877278792082
mae: 0.04072322433681815
r2: 0.6700850268045264
pearson: 0.8204666162355436

=== Experiment 2116 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006273404578273605
rmse: 0.07920482673596102
mae: 0.03283706529768988
r2: 0.7171349487021533
pearson: 0.8502509301155039

=== Experiment 2531 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006533753650128041
rmse: 0.08083163768060153
mae: 0.03328213764597724
r2: 0.7053959236406264
pearson: 0.8399945421847012

=== Experiment 2323 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.015985779975988032
rmse: 0.1264348843317699
mae: 0.052320575176287104
r2: 0.27920821676252927
pearson: 0.5356372050862445

=== Experiment 2551 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007454912277190529
rmse: 0.08634183387669346
mae: 0.03489330364158454
r2: 0.6638612865792957
pearson: 0.8247466093233278

=== Experiment 2658 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007904021884724863
rmse: 0.08890456616352652
mae: 0.04010773295281527
r2: 0.6436111320438274
pearson: 0.805927709231292

=== Experiment 2581 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006857642917267982
rmse: 0.0828108864658988
mae: 0.03627422694691362
r2: 0.6907918991398547
pearson: 0.8336216550430308

=== Experiment 2501 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006052736934427165
rmse: 0.07779933762203355
mae: 0.02917026190265235
r2: 0.7270847556399318
pearson: 0.855332710044136

=== Experiment 2432 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00910116644922891
rmse: 0.09540003380098411
mae: 0.04397242407825792
r2: 0.5896324105339572
pearson: 0.7973322922094823

=== Experiment 2607 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007159565872607748
rmse: 0.08461421791051282
mae: 0.041604521236609904
r2: 0.6771783259700397
pearson: 0.8328808880159887

=== Experiment 2334 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006074242395096397
rmse: 0.07793742615134527
mae: 0.029493564953533648
r2: 0.7261150838836332
pearson: 0.8612369488682691

=== Experiment 2753 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0069033126083799445
rmse: 0.08308617579585634
mae: 0.03783720916385644
r2: 0.6887326728683846
pearson: 0.8322923587456059

=== Experiment 2429 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00825952511648108
rmse: 0.09088192953762085
mae: 0.03708855976565352
r2: 0.6275816477928777
pearson: 0.7970901323234616

=== Experiment 2527 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006712752131638337
rmse: 0.08193138673083922
mae: 0.03162332485137179
r2: 0.6973249608925836
pearson: 0.8364772701203183

=== Experiment 2660 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00738977581099884
rmse: 0.08596380523801188
mae: 0.043413606381078565
r2: 0.6667982611711276
pearson: 0.8201362596970609

=== Experiment 2684 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006869017128842149
rmse: 0.08287953383581588
mae: 0.038612203509151274
r2: 0.6902790409461488
pearson: 0.8331966055244537

=== Experiment 2164 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006373688822659845
rmse: 0.07983538578011536
mae: 0.0355783631235169
r2: 0.7126131762612491
pearson: 0.8462981824061763

=== Experiment 2577 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007462741555348953
rmse: 0.086387160824679
mae: 0.03772739755931281
r2: 0.6635082678730613
pearson: 0.8147886164986036

=== Experiment 2545 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007348981357051041
rmse: 0.08572619994523868
mae: 0.037929305866922135
r2: 0.6686376651446218
pearson: 0.8180281042554686

=== Experiment 2747 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067958196997651606
rmse: 0.08243676182241245
mae: 0.031232409744194924
r2: 0.6935794808065778
pearson: 0.8349385000256941

=== Experiment 2608 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007069107684850582
rmse: 0.0840779857325958
mae: 0.03530121876032977
r2: 0.681257045842317
pearson: 0.8372910129009175

=== Experiment 2523 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006948712622101582
rmse: 0.08335893846553939
mae: 0.03303416596837758
r2: 0.6866856062317499
pearson: 0.8357183871804535

=== Experiment 2584 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006934691752956323
rmse: 0.08327479662512736
mae: 0.031436775077208726
r2: 0.6873178010504533
pearson: 0.8368636858329648

=== Experiment 2117 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006587689279876883
rmse: 0.08116458143720623
mae: 0.04084758203356738
r2: 0.7029639898341372
pearson: 0.8453145215393366

=== Experiment 2776 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007615817966239246
rmse: 0.08726865397288562
mae: 0.039875137281813
r2: 0.6566061198800985
pearson: 0.8108739805201793

=== Experiment 2586 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007197900572442236
rmse: 0.0848404418449258
mae: 0.03872717985062852
r2: 0.6754498312268943
pearson: 0.826298332390535

=== Experiment 2691 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006532032662232867
rmse: 0.0808209914702416
mae: 0.036376911257879355
r2: 0.7054735222273552
pearson: 0.8403055723355554

=== Experiment 2561 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00840143846155032
rmse: 0.09165936101430296
mae: 0.05348085449240291
r2: 0.6211828375245454
pearson: 0.8070708433138799

=== Experiment 2582 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007139901667653752
rmse: 0.08449793883671809
mae: 0.03527224303137131
r2: 0.6780649763165367
pearson: 0.8269218129990956

=== Experiment 2798 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007040879862773611
rmse: 0.0839099509162865
mae: 0.037763031108553426
r2: 0.6825298259157531
pearson: 0.8275721808127504

=== Experiment 2802 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007030977018148267
rmse: 0.08385092139117058
mae: 0.036214939282383826
r2: 0.6829763408213346
pearson: 0.8319357759567203

=== Experiment 2310 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005472032671795881
rmse: 0.07397318887134637
mae: 0.030459026979272556
r2: 0.7532684552544846
pearson: 0.8723243526535222

=== Experiment 2777 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007102540492951224
rmse: 0.08427657143566784
mae: 0.03766146764742767
r2: 0.6797495752399633
pearson: 0.8252833001709997

=== Experiment 2312 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008799497319414437
rmse: 0.09380563586168177
mae: 0.038564585036554876
r2: 0.6032345388225538
pearson: 0.8205222842548693

=== Experiment 2571 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007018408140416494
rmse: 0.08377594010464158
mae: 0.03779585642826268
r2: 0.6835430659862741
pearson: 0.8334155948979863

=== Experiment 2699 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008270907076942877
rmse: 0.09094452747110668
mae: 0.053649127907780716
r2: 0.627068440205239
pearson: 0.8087999362536885

=== Experiment 2614 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006921550787234854
rmse: 0.08319585799326103
mae: 0.03810417751564108
r2: 0.687910321411053
pearson: 0.8298948934910408

=== Experiment 2708 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007006439318946584
rmse: 0.08370447609863277
mae: 0.03843732576403935
r2: 0.684082734878471
pearson: 0.8293654201133979

=== Experiment 2834 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007300845297692271
rmse: 0.0854449840405642
mae: 0.04551945091667923
r2: 0.6708080988748087
pearson: 0.8263372771396832

=== Experiment 2194 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006783438191008905
rmse: 0.08236163057521934
mae: 0.03193766800383981
r2: 0.6941377575868788
pearson: 0.8487472367573557

=== Experiment 2760 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006360957628259782
rmse: 0.07975561189195267
mae: 0.037485634741145406
r2: 0.7131872202133207
pearson: 0.8455813701826246

=== Experiment 2520 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00773191545908293
rmse: 0.0879313110278866
mae: 0.038306574816488394
r2: 0.6513713350261912
pearson: 0.8083044385169942

=== Experiment 2635 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00652744959189727
rmse: 0.08079263327740513
mae: 0.0326700406657898
r2: 0.7056801708516236
pearson: 0.8417826182473034

=== Experiment 2847 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006859020172609653
rmse: 0.08281920171439502
mae: 0.03593201954931887
r2: 0.6907297993026752
pearson: 0.8321786907348521

=== Experiment 2398 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006472761905595411
rmse: 0.08045347665325228
mae: 0.04111806145631507
r2: 0.708146014557083
pearson: 0.8475618088800982

=== Experiment 2854 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006668618212723998
rmse: 0.08166160794843559
mae: 0.0358597115365145
r2: 0.699314939871608
pearson: 0.8378499561128107

=== Experiment 2558 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007869634482212038
rmse: 0.0887109603274141
mae: 0.03750205792119141
r2: 0.6451616448880774
pearson: 0.8069817780881879

=== Experiment 2622 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006167034188430304
rmse: 0.07853046662557345
mae: 0.034011137317932975
r2: 0.7219311427629989
pearson: 0.850923807205115

=== Experiment 2552 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006810607993907854
rmse: 0.08252640785777517
mae: 0.03710785797145512
r2: 0.6929126831325099
pearson: 0.8477454564183967

=== Experiment 2842 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007509871435958157
rmse: 0.08665951439950582
mae: 0.04422502513074039
r2: 0.6613832022998389
pearson: 0.8175459593004237

=== Experiment 2190 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005679281167776728
rmse: 0.07536100561813601
mae: 0.036539350517618086
r2: 0.7439237117877471
pearson: 0.8665333161639537

=== Experiment 2477 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006708515057958306
rmse: 0.08190552519798835
mae: 0.037015752790990446
r2: 0.6975160086799376
pearson: 0.8373354237045189

=== Experiment 2718 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00721249179301843
rmse: 0.0849263904391234
mae: 0.04923355261560935
r2: 0.6747919195132008
pearson: 0.8365708834417844

=== Experiment 2236 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005985719562134276
rmse: 0.07736743218004767
mae: 0.030486101692808874
r2: 0.7301065394600168
pearson: 0.8644879616569529

=== Experiment 2474 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.011131033135745828
rmse: 0.10550371147853438
mae: 0.04535096289000336
r2: 0.49810661505156084
pearson: 0.7260483989070726

=== Experiment 2124 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0066089929601761644
rmse: 0.08129571304918953
mae: 0.03448817752680816
r2: 0.7020034162659093
pearson: 0.8409813454498383

=== Experiment 2160 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005665542961103787
rmse: 0.07526980112305191
mae: 0.03143602692466122
r2: 0.7445431614799828
pearson: 0.8725501528114432

=== Experiment 2610 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006495159276712202
rmse: 0.08059255100015263
mae: 0.03631562396875304
r2: 0.7071361269512637
pearson: 0.8426128524535678

=== Experiment 2602 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005795412650229236
rmse: 0.07612760767441228
mae: 0.02954775977112308
r2: 0.7386873943573378
pearson: 0.8595006783458228

=== Experiment 2698 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007297971536180014
rmse: 0.08542816594180173
mae: 0.037491015313639846
r2: 0.6709376755165573
pearson: 0.8264089759244907

=== Experiment 2502 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006534464480304635
rmse: 0.08083603454094365
mae: 0.0404562399146937
r2: 0.705363872620824
pearson: 0.842841041853812

=== Experiment 2254 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00822486199459685
rmse: 0.09069102488447713
mae: 0.04049312723448631
r2: 0.629144592702231
pearson: 0.8061479792029662

=== Experiment 2206 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007536652589088688
rmse: 0.08681389629021778
mae: 0.036339973089469714
r2: 0.6601756518924702
pearson: 0.8226232219512282

=== Experiment 2741 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00657773901483168
rmse: 0.08110326143153357
mae: 0.03798582544521165
r2: 0.7034126429055788
pearson: 0.8392194025059393

=== Experiment 2331 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005836033385138973
rmse: 0.0763939355259236
mae: 0.031238670213751752
r2: 0.7368558232988104
pearson: 0.8649857916535307

=== Experiment 2711 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007146718437236154
rmse: 0.08453826611207586
mae: 0.04041878846117896
r2: 0.6777576111763561
pearson: 0.8281173571586478

=== Experiment 2625 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006249184593061021
rmse: 0.07905178424969939
mae: 0.03619522026709792
r2: 0.718227017175358
pearson: 0.8483285817987718

=== Experiment 2537 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006110874850368567
rmse: 0.07817208485366478
mae: 0.0316711297177688
r2: 0.7244633425985877
pearson: 0.8572813675393208

=== Experiment 2337 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011854784752704016
rmse: 0.10887968016440908
mae: 0.04853303650337876
r2: 0.4654729731903634
pearson: 0.7560726060163581

=== Experiment 2744 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0071788053728006115
rmse: 0.08472783115836621
mae: 0.032119629484896585
r2: 0.6763108253742951
pearson: 0.8280848930703886

=== Experiment 2935 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006751914239734589
rmse: 0.08217003249198937
mae: 0.037253374331913794
r2: 0.6955591586750857
pearson: 0.8341654686732524

=== Experiment 2284 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00570554947387843
rmse: 0.0755350877002101
mae: 0.028977807409094468
r2: 0.742739285427187
pearson: 0.8619756053537597

=== Experiment 2781 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00722177321914952
rmse: 0.08498101681640154
mae: 0.0406071051066507
r2: 0.674373424094018
pearson: 0.8275058098248151

=== Experiment 2499 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006312612403297082
rmse: 0.07945195028000937
mae: 0.031775463956124704
r2: 0.7153670851285283
pearson: 0.8490441440984675

=== Experiment 2630 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006823859167479779
rmse: 0.08260665328821752
mae: 0.03792445072075227
r2: 0.6923151935484392
pearson: 0.8397674983363446

=== Experiment 2668 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007539949956763534
rmse: 0.08683288522652886
mae: 0.04351382255863041
r2: 0.6600269750352938
pearson: 0.8233110719177411

=== Experiment 2647 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006649976504254721
rmse: 0.08154738808971579
mae: 0.03080716201806679
r2: 0.7001554863016444
pearson: 0.8394341306955998

=== Experiment 2613 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006852248090870543
rmse: 0.08277830688574479
mae: 0.035237089571151445
r2: 0.6910351494876704
pearson: 0.8315466494574955

=== Experiment 2624 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006460991609050872
rmse: 0.08038029366113857
mae: 0.042218216615454024
r2: 0.7086767320477726
pearson: 0.8520342313885375

=== Experiment 2682 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007479791854016741
rmse: 0.08648578989647225
mae: 0.03866625025473395
r2: 0.6627394774105408
pearson: 0.8150915926284378

=== Experiment 2799 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006691665956914543
rmse: 0.08180260360718687
mae: 0.04203761998932909
r2: 0.6982757272301443
pearson: 0.839830093547208

=== Experiment 2956 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006926702949559429
rmse: 0.0832268162887385
mae: 0.03786856512308989
r2: 0.6876780126794779
pearson: 0.8362022396952711

=== Experiment 2342 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006410259405394498
rmse: 0.08006409560717274
mae: 0.03253424140644228
r2: 0.7109642247816881
pearson: 0.847072119198337

=== Experiment 2945 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007017292403139909
rmse: 0.08376928078442544
mae: 0.03787146177415361
r2: 0.6835933740890017
pearson: 0.8281500901203499

=== Experiment 2386 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0065132174127093935
rmse: 0.08070450676826786
mae: 0.03420539604473928
r2: 0.7063218935471438
pearson: 0.8464200220989505

=== Experiment 2326 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006552369761068695
rmse: 0.08094670938999741
mae: 0.031120031726374616
r2: 0.7045565313918892
pearson: 0.8395032547290399

=== Experiment 2679 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007195270845575431
rmse: 0.08482494235527326
mae: 0.04079650156596019
r2: 0.6755684044538871
pearson: 0.8300400130184605

=== Experiment 2917 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007539369262835703
rmse: 0.08682954141785906
mae: 0.04073279214457755
r2: 0.660053158268917
pearson: 0.8151262440783573

=== Experiment 2825 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006644762945909818
rmse: 0.08151541538819401
mae: 0.034705878483480966
r2: 0.7003905633527535
pearson: 0.8368967501513339

=== Experiment 2413 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005802070327206163
rmse: 0.07617132220991153
mae: 0.03297899482892324
r2: 0.7383872026327158
pearson: 0.8599806329558866

=== Experiment 2920 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007825157999826577
rmse: 0.08845992312808426
mae: 0.0371521542836185
r2: 0.6471670699032409
pearson: 0.8053654117915587

=== Experiment 2517 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006652945746865788
rmse: 0.0815655916846423
mae: 0.036610267371464694
r2: 0.7000216044591749
pearson: 0.8376730199772934

=== Experiment 2183 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0059029419941411965
rmse: 0.07683060584260153
mae: 0.03023094160505396
r2: 0.733838943567631
pearson: 0.860142556820118

=== Experiment 2855 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007811302178348445
rmse: 0.08838157148607648
mae: 0.03769356327008475
r2: 0.6477918227952764
pearson: 0.8123703465857885

=== Experiment 2565 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006071450018764434
rmse: 0.07791950987246028
mae: 0.032366164028873254
r2: 0.7262409909034225
pearson: 0.854991818218082

=== Experiment 2983 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006882145708774421
rmse: 0.08295869881317101
mae: 0.04252455389783554
r2: 0.6896870790553324
pearson: 0.8341717223024345

=== Experiment 2563 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006865291488003934
rmse: 0.08285705454578948
mae: 0.040344334577241486
r2: 0.690447028451764
pearson: 0.8317218707517469

=== Experiment 2686 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007594881111165087
rmse: 0.08714861508460756
mae: 0.045565108365057186
r2: 0.6575501534603777
pearson: 0.8219025084509234

=== Experiment 2888 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007838332825629173
rmse: 0.08853435957654616
mae: 0.03528710755536724
r2: 0.6465730228064861
pearson: 0.8057806322444135

=== Experiment 2068 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007913620523890767
rmse: 0.08895853260868666
mae: 0.05248732035677387
r2: 0.6431783336285208
pearson: 0.8183489051536994

=== Experiment 2882 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006802612912593666
rmse: 0.08247795410043623
mae: 0.034902783885216795
r2: 0.6932731778300623
pearson: 0.8403609710538804

=== Experiment 2644 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011356826806524738
rmse: 0.10656841373748949
mae: 0.04441079499141939
r2: 0.4879256778155343
pearson: 0.7376808639932205

=== Experiment 2865 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007243141706394481
rmse: 0.08510664901401348
mae: 0.042566511378746896
r2: 0.6734099284091333
pearson: 0.8318958341160781

=== Experiment 2562 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006160787344613522
rmse: 0.0784906831707657
mae: 0.030862267198909686
r2: 0.7222128102012514
pearson: 0.855863214866838

=== Experiment 2560 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0063310742621768695
rmse: 0.07956804799777904
mae: 0.0325550013973914
r2: 0.714534648037324
pearson: 0.8484163942408933

=== Experiment 2343 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005784138223749686
rmse: 0.07605352209956937
mae: 0.03199338196690577
r2: 0.7391957532850462
pearson: 0.8600178037748606

=== Experiment 2759 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006831655104270518
rmse: 0.08265382691848285
mae: 0.037409115836381006
r2: 0.6919636781897991
pearson: 0.8337762658181503

=== Experiment 2656 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007445488263859757
rmse: 0.08628724276426822
mae: 0.0500854014534769
r2: 0.6642862111925547
pearson: 0.831473723998082

=== Experiment 2840 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006635709226399959
rmse: 0.08145986267113368
mae: 0.03289465120274292
r2: 0.7007987915806064
pearson: 0.8375123349124942

=== Experiment 2851 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0065660675556506376
rmse: 0.0810312751698419
mae: 0.03969932130717085
r2: 0.7039389038630499
pearson: 0.8417515381160844

=== Experiment 2210 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00688832853593027
rmse: 0.08299595493715504
mae: 0.03830330494652326
r2: 0.6894082981001456
pearson: 0.8331316886546793

=== Experiment 2579 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006572472103353842
rmse: 0.08107078452410488
mae: 0.04323438180805159
r2: 0.7036501256259695
pearson: 0.8456130794395947

=== Experiment 2778 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007026407832577619
rmse: 0.08382367107552388
mae: 0.0375227486029217
r2: 0.6831823633876627
pearson: 0.8331552786790438

=== Experiment 2823 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0071736376694791085
rmse: 0.08469732976593246
mae: 0.04366029034033864
r2: 0.6765438348425813
pearson: 0.8321306369069162

=== Experiment 2900 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00708224104162103
rmse: 0.08415605172309969
mae: 0.038078867090349335
r2: 0.68066486856597
pearson: 0.8254381754015282

=== Experiment 2773 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006239553537776447
rmse: 0.07899084464529067
mae: 0.0332044597229864
r2: 0.7186612772191878
pearson: 0.8505370688587676

=== Experiment 2723 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007529681214181723
rmse: 0.08677373573946048
mae: 0.03346739950777307
r2: 0.6604899881186885
pearson: 0.8212951267781645

=== Experiment 2455 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005189112166101552
rmse: 0.07203549240549101
mae: 0.028508099758073106
r2: 0.7660252163334045
pearson: 0.8767002590566422

=== Experiment 2491 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006593553944030266
rmse: 0.08120070162277088
mae: 0.03384417341501963
r2: 0.7026995547086972
pearson: 0.8465972944356372

=== Experiment 2528 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008597535878110395
rmse: 0.09272289834830658
mae: 0.03998053414605896
r2: 0.612340891320925
pearson: 0.7840619460068718

=== Experiment 2637 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008138468559117098
rmse: 0.09021346107492549
mae: 0.0437250031163663
r2: 0.633040034683354
pearson: 0.8144441873229826

=== Experiment 2494 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007139736737665534
rmse: 0.08449696289018638
mae: 0.03156535197077138
r2: 0.6780724129371143
pearson: 0.832895512222332

=== Experiment 2509 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007338888593393733
rmse: 0.08566731344797579
mae: 0.031938546616531775
r2: 0.6690927434157647
pearson: 0.8426611979409012

=== Experiment 2505 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007042078644458441
rmse: 0.0839170938751959
mae: 0.03419763999939736
r2: 0.6824757733772083
pearson: 0.8308588746500487

=== Experiment 2233 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005410445523232802
rmse: 0.07355573073005803
mae: 0.02912232727162119
r2: 0.7560453926766167
pearson: 0.8718543583154227

=== Experiment 2892 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00722695017029156
rmse: 0.08501147081595259
mae: 0.034351363209296375
r2: 0.674139997645574
pearson: 0.8224180660229318

=== Experiment 2815 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006470094777464454
rmse: 0.08043689935262581
mae: 0.03885591291296506
r2: 0.7082662741906149
pearson: 0.8441727041166267

=== Experiment 2821 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006546940491427082
rmse: 0.08091316636634042
mae: 0.03438349281721204
r2: 0.7048013347704252
pearson: 0.8396464435704342

=== Experiment 2605 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0072047362690709875
rmse: 0.08488071788734464
mae: 0.03284040413743805
r2: 0.675141612674524
pearson: 0.8458520517343294

=== Experiment 2761 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007558556083223652
rmse: 0.08693995677031162
mae: 0.042280123094184406
r2: 0.6591880329823896
pearson: 0.8122064249274468

=== Experiment 2716 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006332716893001768
rmse: 0.0795783695045442
mae: 0.030681651985193688
r2: 0.7144605825364067
pearson: 0.8454376188851682

=== Experiment 2253 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00626901332244562
rmse: 0.07917710099798818
mae: 0.03522274174058748
r2: 0.7173329484946978
pearson: 0.8490050269183307

=== Experiment 2827 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007451287982203035
rmse: 0.08632084326628786
mae: 0.04070396240884493
r2: 0.6640247044451075
pearson: 0.8246309665585201

=== Experiment 2003 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006230862878670228
rmse: 0.07893581493004445
mae: 0.029933152635283816
r2: 0.7190531352132357
pearson: 0.8539285129804144

=== Experiment 2672 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006895024117571012
rmse: 0.08303628193489285
mae: 0.03799634901182258
r2: 0.6891063972709153
pearson: 0.8315342341482145

=== Experiment 2771 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0075171178381793135
rmse: 0.08670131393571445
mae: 0.03951174597396805
r2: 0.6610564652130725
pearson: 0.8288660745540142

=== Experiment 2076 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005873492649499961
rmse: 0.07663871508252185
mae: 0.030009209005961747
r2: 0.7351668015558566
pearson: 0.8577027029867145

=== Experiment 2175 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008045073641648538
rmse: 0.08969433450139723
mae: 0.039102747831187196
r2: 0.6372511704057647
pearson: 0.8162576491814031

=== Experiment 2641 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00693276779867423
rmse: 0.08326324398361037
mae: 0.03396177456522021
r2: 0.6874045513022358
pearson: 0.8314683677675668

=== Experiment 2909 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007653979470272107
rmse: 0.08748702458234653
mae: 0.04671043853802236
r2: 0.6548854344594193
pearson: 0.8214983264796337

=== Experiment 2739 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006060483753368696
rmse: 0.0778491088797341
mae: 0.03221369136548657
r2: 0.7267354549834941
pearson: 0.8524971692561198

=== Experiment 2291 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005725188559517221
rmse: 0.07566497577821076
mae: 0.031083972100060225
r2: 0.7418537676995566
pearson: 0.8615723931922001

=== Experiment 2408 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006091910958732377
rmse: 0.07805069479980545
mae: 0.03487838257938051
r2: 0.7253184161258165
pearson: 0.8617993853411628

=== Experiment 2973 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007329462841630592
rmse: 0.08561228207232062
mae: 0.040714795537096594
r2: 0.6695177464141742
pearson: 0.8219624573222938

=== Experiment 2516 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006971943349988612
rmse: 0.08349816375219644
mae: 0.040211204587906994
r2: 0.685638142935949
pearson: 0.8300780140249924

=== Experiment 2250 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0056512419473332045
rmse: 0.07517474274869987
mae: 0.030505581996458305
r2: 0.7451879878965408
pearson: 0.8662367529878131

=== Experiment 2997 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006612723196089954
rmse: 0.08131865220286151
mae: 0.03619224107545354
r2: 0.7018352215703589
pearson: 0.8396840948539935

=== Experiment 2569 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006363816334392691
rmse: 0.07977353154018375
mae: 0.03598803357764397
r2: 0.7130583224120008
pearson: 0.8471232736180005

=== Experiment 2521 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006783480103443593
rmse: 0.0823618850163326
mae: 0.03875914806097696
r2: 0.6941358677736458
pearson: 0.8374633075983997

=== Experiment 2550 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0064637467415983646
rmse: 0.08039742994398742
mae: 0.035828463070911754
r2: 0.7085525043338325
pearson: 0.8442334303797063

=== Experiment 2766 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007227267855132804
rmse: 0.08501333927762633
mae: 0.039486481795051154
r2: 0.674125673375906
pearson: 0.8220450904603246

=== Experiment 2695 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008095464855896859
rmse: 0.08997480122732619
mae: 0.038784905470809636
r2: 0.6349790527341774
pearson: 0.8291192649841299

=== Experiment 2503 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007777751821440713
rmse: 0.08819156321009801
mae: 0.04118845603008908
r2: 0.6493045936216046
pearson: 0.8125248633450851

=== Experiment 2530 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007142296773864711
rmse: 0.08451211022016142
mae: 0.044599490651054105
r2: 0.6779569820316514
pearson: 0.8292251827559378

=== Experiment 2601 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005883610136591526
rmse: 0.07670469435824333
mae: 0.028886308939139398
r2: 0.7347106085159445
pearson: 0.8623039284558578

=== Experiment 2878 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006289400744813024
rmse: 0.07930574219319193
mae: 0.03714803475125784
r2: 0.7164136885933421
pearson: 0.8505204368551654

=== Experiment 2769 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00548383251148896
rmse: 0.07405290346427316
mae: 0.030228229387493945
r2: 0.7527364056762303
pearson: 0.8679325215239687

=== Experiment 2749 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006965864308864628
rmse: 0.08346175356931237
mae: 0.04420139636197651
r2: 0.6859122442246388
pearson: 0.8336078716718116

=== Experiment 2533 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006888593054296567
rmse: 0.08299754848365443
mae: 0.03791380545566666
r2: 0.6893963710834328
pearson: 0.8318107380247475

=== Experiment 2407 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005573350692691616
rmse: 0.07465487721972099
mae: 0.031434217461648616
r2: 0.7487000702857671
pearson: 0.8654114900700198

=== Experiment 2028 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006758533932217948
rmse: 0.08221030307825138
mae: 0.03148014062343876
r2: 0.6952606796545604
pearson: 0.8492699077992049

=== Experiment 2714 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006506068119903887
rmse: 0.08066020158606031
mae: 0.03509121132842397
r2: 0.7066442520130998
pearson: 0.8421044448304847

=== Experiment 2543 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00711126721513739
rmse: 0.08432832984909276
mae: 0.03529088759434181
r2: 0.6793560911775133
pearson: 0.844399847129746

=== Experiment 2179 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0061144468153775085
rmse: 0.07819492832260612
mae: 0.03113822575896781
r2: 0.7243022842684765
pearson: 0.8541109292549862

=== Experiment 2397 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005979558369429295
rmse: 0.07732760418782736
mae: 0.027762820058472144
r2: 0.7303843449273362
pearson: 0.8712190358720474

=== Experiment 2316 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00590065262464655
rmse: 0.07681570558581462
mae: 0.03022250421861903
r2: 0.7339421702305161
pearson: 0.8599233663326236

=== Experiment 2737 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005997794002628645
rmse: 0.07744542596324618
mae: 0.03148881557350831
r2: 0.7295621082524935
pearson: 0.8541479887787642

=== Experiment 2689 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006582356034027058
rmse: 0.08113172027035455
mae: 0.03350846254929493
r2: 0.7032044635421648
pearson: 0.8470926690626147

=== Experiment 2876 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006500763999444114
rmse: 0.08062731546717969
mae: 0.03953005839928134
r2: 0.706883412470109
pearson: 0.846969639546533

=== Experiment 2512 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0065204754560956426
rmse: 0.08074946102665728
mae: 0.03811388397117433
r2: 0.7059946315653671
pearson: 0.8488363818036326

=== Experiment 2215 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006425396271489138
rmse: 0.08015856954492849
mae: 0.029787338872097367
r2: 0.7102817101517249
pearson: 0.8465197728200448

=== Experiment 2030 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005976593636267703
rmse: 0.07730843185751282
mae: 0.030244444577788774
r2: 0.7305180234407136
pearson: 0.8563814724555158

=== Experiment 2634 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007301677271449929
rmse: 0.08544985237816347
mae: 0.04555708175863868
r2: 0.6707705855442556
pearson: 0.8239843254327577

=== Experiment 2318 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006309143854561104
rmse: 0.07943011931604474
mae: 0.03509760648254098
r2: 0.7155234804644086
pearson: 0.8469476065336741

=== Experiment 2628 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006593872724826081
rmse: 0.08120266451801986
mae: 0.02989799936146225
r2: 0.70268518102292
pearson: 0.8420860580825605

=== Experiment 2809 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005234488826626496
rmse: 0.0723497672879913
mae: 0.027876973196362488
r2: 0.7639792026821304
pearson: 0.8742039562744646

=== Experiment 2152 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0064035457769529005
rmse: 0.08002215803733927
mae: 0.031083601385584605
r2: 0.7112669393332258
pearson: 0.8462500404394506

=== Experiment 2765 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006179742616094638
rmse: 0.07861133897914879
mae: 0.02913613773528517
r2: 0.7213581253530208
pearson: 0.8507856815269877

=== Experiment 2444 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005927605605751476
rmse: 0.07699094495946569
mae: 0.03017229672574903
r2: 0.7327268721754092
pearson: 0.8597802588302642

=== Experiment 2980 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007776728208414759
rmse: 0.08818575966909147
mae: 0.04797820590871502
r2: 0.6493507478824183
pearson: 0.8214393699329118

=== Experiment 2454 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005938657325228168
rmse: 0.07706268438893216
mae: 0.03179849803926115
r2: 0.7322285550084389
pearson: 0.8620652914484377

=== Experiment 2667 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006206358812065706
rmse: 0.07878044688922313
mae: 0.03625335533396889
r2: 0.720158012791369
pearson: 0.8516433769678396

=== Experiment 2146 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005719869882491591
rmse: 0.07562982138344365
mae: 0.029588910836703793
r2: 0.7420935845057135
pearson: 0.8633972089147673

=== Experiment 2600 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007299642888070184
rmse: 0.08543794758811908
mae: 0.034820489085057256
r2: 0.6708623149954477
pearson: 0.8205938125791143

=== Experiment 2492 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007423502073407911
rmse: 0.08615974740798578
mae: 0.046340652201441554
r2: 0.6652775588431661
pearson: 0.8217334489804986

=== Experiment 2858 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007798890839673658
rmse: 0.08831132905620692
mae: 0.04225113611591176
r2: 0.6483514445934746
pearson: 0.8068285998480049

=== Experiment 2417 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005945882820253086
rmse: 0.07710955077195747
mae: 0.02905902713904709
r2: 0.7319027606179489
pearson: 0.8561372560836576

=== Experiment 2652 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008542291465789949
rmse: 0.09242451766598486
mae: 0.04462068669256365
r2: 0.6148318375575286
pearson: 0.7971502343303353

=== Experiment 2556 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006090786411064466
rmse: 0.07804349051051257
mae: 0.03120042390868109
r2: 0.7253691214852114
pearson: 0.8580301406582365

=== Experiment 2329 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005656397482335816
rmse: 0.0752090252718104
mae: 0.03164321861601608
r2: 0.7449555270923977
pearson: 0.8640295598097121

=== Experiment 2367 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005991976463840604
rmse: 0.0774078578946647
mae: 0.02968246345360355
r2: 0.7298244185159515
pearson: 0.8573286540925352

=== Experiment 2340 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006307235091161021
rmse: 0.07941810304433758
mae: 0.03404276984019799
r2: 0.7156095457660071
pearson: 0.8469162619118138

=== Experiment 2981 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006734660920769089
rmse: 0.0820649798682062
mae: 0.037885395825589585
r2: 0.6963371032334735
pearson: 0.8355068618625616

=== Experiment 2180 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00516098376679926
rmse: 0.07183998724108502
mae: 0.03055995612288408
r2: 0.7672935134776896
pearson: 0.8766029499766491

=== Experiment 2538 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.011256965733123072
rmse: 0.1060988488774646
mae: 0.0497310563040513
r2: 0.49242836966299597
pearson: 0.70496422229803

=== Experiment 2239 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005975046047448683
rmse: 0.07729842202431227
mae: 0.0337268214335444
r2: 0.7305878035394173
pearson: 0.8631313322752717

=== Experiment 2970 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006767113203029782
rmse: 0.08226246533522918
mae: 0.039996662670219486
r2: 0.6948738441096807
pearson: 0.8360575272654183

=== Experiment 2308 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006410736388564149
rmse: 0.08006707431000679
mae: 0.031879813347190755
r2: 0.7109427178205048
pearson: 0.8432804103718078

=== Experiment 2041 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009212459218320743
rmse: 0.09598155665710337
mae: 0.043601527949322295
r2: 0.5846142685592999
pearson: 0.7772601763956847

=== Experiment 2804 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00899315228743255
rmse: 0.09483223232336435
mae: 0.04018909602447259
r2: 0.5945027215486867
pearson: 0.7795790742845674

=== Experiment 2732 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005908682557117685
rmse: 0.07686795533327061
mae: 0.03051596276635419
r2: 0.7335801041096361
pearson: 0.8573231377498027

=== Experiment 2567 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006126078134100094
rmse: 0.07826926685551676
mae: 0.036333224206790894
r2: 0.72377783322661
pearson: 0.8519049648765861

=== Experiment 2478 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008856816513874922
rmse: 0.09411066100009563
mae: 0.04474921733215461
r2: 0.6006500415724372
pearson: 0.7815627535613958

=== Experiment 2710 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006713619485291702
rmse: 0.08193667973070243
mae: 0.04379951769124148
r2: 0.697285852298105
pearson: 0.843254918144006

=== Experiment 2764 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006181368707195827
rmse: 0.07862168089780215
mae: 0.032105886218959934
r2: 0.7212848056209656
pearson: 0.8502255579167878

=== Experiment 2724 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006439434161538778
rmse: 0.08024608502312607
mae: 0.03830803446144351
r2: 0.7096487478679346
pearson: 0.8464905119479885

=== Experiment 2816 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005432175345519328
rmse: 0.07370329263689193
mae: 0.02980785827143076
r2: 0.7550656045537436
pearson: 0.8718168535004611

=== Experiment 2811 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006611241941558771
rmse: 0.08130954397583823
mae: 0.04213891708810336
r2: 0.70190201068522
pearson: 0.8441432285538638

=== Experiment 2994 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006530482418586099
rmse: 0.08081140030086163
mae: 0.03741628473979442
r2: 0.7055434220310735
pearson: 0.8419044739868708

=== Experiment 2542 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006618477284611221
rmse: 0.08135402439099876
mae: 0.03625372358833109
r2: 0.701575772251503
pearson: 0.8414995005217106

=== Experiment 2702 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00595254378878514
rmse: 0.07715273027434051
mae: 0.03026124484793618
r2: 0.731602420478555
pearson: 0.8565553234921798

=== Experiment 2392 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006625874127078398
rmse: 0.08139947252334254
mae: 0.03760568242390681
r2: 0.7012422518802575
pearson: 0.8426898615839751

=== Experiment 2034 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006814007347895718
rmse: 0.08254700084131293
mae: 0.0349082670206752
r2: 0.6927594077573671
pearson: 0.8464224458271898

=== Experiment 2256 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006788973592975482
rmse: 0.08239522797453432
mae: 0.03083155020037475
r2: 0.6938881687485223
pearson: 0.8508023854734208

=== Experiment 2539 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00747894574089412
rmse: 0.08648089812724033
mae: 0.03801864507143984
r2: 0.662777628278837
pearson: 0.8232826235628211

=== Experiment 2102 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006363268570340737
rmse: 0.07977009822195744
mae: 0.032146408478419285
r2: 0.713083020852014
pearson: 0.8471929253661453

=== Experiment 2768 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006619292388508584
rmse: 0.08135903384694648
mae: 0.034689376239689956
r2: 0.7015390195755256
pearson: 0.8382014556672206

=== Experiment 2960 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005586351974209614
rmse: 0.07474190239891954
mae: 0.028720626996127367
r2: 0.7481138482244223
pearson: 0.8710333450692567

=== Experiment 2626 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007694572789213466
rmse: 0.08771871401937825
mae: 0.046336932325042754
r2: 0.6530550995748394
pearson: 0.8142355357975852

=== Experiment 2416 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005900307564896087
rmse: 0.07681345952953875
mae: 0.028600146332690795
r2: 0.7339577288227921
pearson: 0.8697704063608845

=== Experiment 2937 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006779827078678718
rmse: 0.0823397053594359
mae: 0.03960131875040881
r2: 0.6943005810524747
pearson: 0.8353824379314772

=== Experiment 2113 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007939601092987643
rmse: 0.08910443924399974
mae: 0.039570297883861696
r2: 0.6420068812028656
pearson: 0.8181721881287413

=== Experiment 2866 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00620478746780953
rmse: 0.0787704733247778
mae: 0.03151052684570184
r2: 0.7202288640122785
pearson: 0.8609808204195134

=== Experiment 2727 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007579119537524428
rmse: 0.08705813883563344
mae: 0.03942043304814575
r2: 0.6582608358786362
pearson: 0.8365427367260196

=== Experiment 2735 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005571091775194269
rmse: 0.07463974661796668
mae: 0.03282596943036334
r2: 0.7488019238814969
pearson: 0.8653919035551102

=== Experiment 2717 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006500399483945053
rmse: 0.0806250549391754
mae: 0.038863699987521746
r2: 0.7068998483135261
pearson: 0.842907965897851

=== Experiment 2713 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006853048600608994
rmse: 0.0827831420073495
mae: 0.031735123002802304
r2: 0.6909990548558951
pearson: 0.846972643758317

=== Experiment 2496 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0072231210084444525
rmse: 0.08498894638977737
mae: 0.033340883214091746
r2: 0.674312652867917
pearson: 0.8424497617516775

=== Experiment 2978 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007922818820059801
rmse: 0.08901021750372146
mae: 0.04171518582403479
r2: 0.6427635865027388
pearson: 0.8166712140008733

=== Experiment 2896 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007155082432071723
rmse: 0.08458772033854396
mae: 0.03692128243638267
r2: 0.6773804823304959
pearson: 0.8267752205595674

=== Experiment 2272 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006457390461516205
rmse: 0.08035788985231136
mae: 0.039490184614922284
r2: 0.7088391062051245
pearson: 0.8478966960307406

=== Experiment 2632 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.01524415303652568
rmse: 0.12346721441956031
mae: 0.05377445789097306
r2: 0.3126478490479083
pearson: 0.5643793219067871

=== Experiment 2648 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0064698984307811494
rmse: 0.08043567884204839
mae: 0.031705191099765904
r2: 0.7082751273761465
pearson: 0.8444537800177137

=== Experiment 2810 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006672502281168511
rmse: 0.081685385970616
mae: 0.042476656926038595
r2: 0.6991398089349542
pearson: 0.8404953731444111

=== Experiment 2615 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0062412621418707894
rmse: 0.07900165910834271
mae: 0.03699119316248269
r2: 0.7185842370125881
pearson: 0.8507947048849612

=== Experiment 2590 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006064544336747437
rmse: 0.07787518434486969
mae: 0.03219563627587131
r2: 0.726552365066146
pearson: 0.8531733435265604

=== Experiment 2801 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009866282842983201
rmse: 0.09932916411096593
mae: 0.04986392110877662
r2: 0.555133649093054
pearson: 0.7499397085423365

=== Experiment 2544 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006848558074094316
rmse: 0.082756015334804
mae: 0.040161964152664625
r2: 0.6912015307200099
pearson: 0.8346075511828394

=== Experiment 2664 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0060092537607456835
rmse: 0.07751937667929021
mae: 0.029236726246208777
r2: 0.7290453928028214
pearson: 0.8542414487520135

=== Experiment 2580 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006831618310152364
rmse: 0.0826536043385427
mae: 0.04842015989905022
r2: 0.6919653372203922
pearson: 0.8498723313517722

=== Experiment 2168 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006847032332610152
rmse: 0.0827467965096544
mae: 0.03998560481481465
r2: 0.6912703257320008
pearson: 0.8360495942252879

=== Experiment 2354 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006375533115432285
rmse: 0.0798469355418997
mae: 0.03100113969317156
r2: 0.712530017911875
pearson: 0.8446416268994604

=== Experiment 2989 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00665634031055555
rmse: 0.08158639782804208
mae: 0.036578880596527384
r2: 0.6998685450764652
pearson: 0.8370116915321381

=== Experiment 2426 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0070815373341364726
rmse: 0.08415187065143871
mae: 0.03615969978324945
r2: 0.6806965984267233
pearson: 0.8315139104246749

=== Experiment 2915 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006221001654612372
rmse: 0.07887332663589366
mae: 0.03672619333842461
r2: 0.719497773465748
pearson: 0.8510170133043861

=== Experiment 2187 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005484482856028344
rmse: 0.07405729441471882
mae: 0.02835119701469098
r2: 0.7527070819271879
pearson: 0.868587562508662

=== Experiment 2138 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007738135224552342
rmse: 0.08796667110077738
mae: 0.04153742165844393
r2: 0.6510908885387037
pearson: 0.8185420606043019

=== Experiment 2786 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006512920966673234
rmse: 0.08070267013347969
mae: 0.04223646117055517
r2: 0.7063352601684383
pearson: 0.847571433420983

=== Experiment 2734 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008240274647492562
rmse: 0.0907759585324912
mae: 0.04124614958168761
r2: 0.6284496429667905
pearson: 0.7978901909635593

=== Experiment 2588 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009189604858366654
rmse: 0.09586242672896746
mae: 0.04174606964983363
r2: 0.5856447615906566
pearson: 0.8151903172408042

=== Experiment 2213 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.009418965498347599
rmse: 0.09705135495369242
mae: 0.039089450086211334
r2: 0.5753029912832537
pearson: 0.7864329345118098

=== Experiment 2932 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010830807904791444
rmse: 0.10407116749989617
mae: 0.04661034131838851
r2: 0.5116436385760648
pearson: 0.7153291085374666

=== Experiment 2742 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00903390541616173
rmse: 0.09504685905468802
mae: 0.03693964061726554
r2: 0.5926651809111116
pearson: 0.8001678190466063

=== Experiment 2519 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006216247939315903
rmse: 0.07884318575067793
mae: 0.03392116304326368
r2: 0.7197121163961953
pearson: 0.8543393402484726

=== Experiment 2390 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0062498690156603965
rmse: 0.07905611308216713
mae: 0.02984018127824072
r2: 0.7181961568615896
pearson: 0.8493951938574756

=== Experiment 2475 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005735843978631267
rmse: 0.07573535487888908
mae: 0.032241782725264276
r2: 0.7413733195415128
pearson: 0.8642031779220997

=== Experiment 2356 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011483146905841674
rmse: 0.1071594461811075
mae: 0.06131779430972915
r2: 0.48222996013506103
pearson: 0.7237070789564105

=== Experiment 2554 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006971523399988795
rmse: 0.08349564898836823
mae: 0.045835933306471684
r2: 0.6856570782966067
pearson: 0.8377777303334709

=== Experiment 2770 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.011988855670707042
rmse: 0.10949363301446821
mae: 0.03867632761635369
r2: 0.45942777450673034
pearson: 0.7017957091926808

=== Experiment 2546 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005868396490326243
rmse: 0.07660545992503565
mae: 0.030029983521375165
r2: 0.7353965851300084
pearson: 0.8597530907697354

=== Experiment 2599 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006755931485239947
rmse: 0.0821944735687257
mae: 0.040775553964321636
r2: 0.6953780228433739
pearson: 0.8410021276616483

=== Experiment 2526 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007496851821010768
rmse: 0.08658436245079575
mae: 0.03650226446618805
r2: 0.6619702510074459
pearson: 0.8255512729930583

=== Experiment 2745 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006474845285906904
rmse: 0.08046642334481448
mae: 0.038464350067204355
r2: 0.7080520758558035
pearson: 0.8437258959155259

=== Experiment 2681 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006612819977906622
rmse: 0.08131924727828352
mae: 0.03379763371919397
r2: 0.7018308577208424
pearson: 0.8411713074491296

=== Experiment 2755 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059592361832865185
rmse: 0.07719608917093222
mae: 0.029635492647671534
r2: 0.7313006633560359
pearson: 0.8558805983802503

=== Experiment 2518 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006542182798369258
rmse: 0.0808837610300687
mae: 0.030581449841572312
r2: 0.7050158570563817
pearson: 0.8410874021416673

=== Experiment 2990 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005787716553356284
rmse: 0.07607704353716885
mae: 0.03562394981171654
r2: 0.7390344079780279
pearson: 0.8614373914342541

=== Experiment 2677 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006421883999079124
rmse: 0.08013665827247306
mae: 0.027985518772286343
r2: 0.7104400769688233
pearson: 0.8475394056362181

=== Experiment 2241 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008687716552418314
rmse: 0.09320792108194621
mae: 0.04514323843367199
r2: 0.6082746844079311
pearson: 0.7814162410972677

=== Experiment 2376 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007191911014334338
rmse: 0.08480513554222019
mae: 0.038837183725902204
r2: 0.6757198977657732
pearson: 0.8223448539125863

=== Experiment 2694 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006553457941169548
rmse: 0.08095343069425501
mae: 0.03922861807116639
r2: 0.7045074658300865
pearson: 0.8409071664892578

=== Experiment 2750 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00639183646836353
rmse: 0.07994896164656255
mae: 0.035045574625562216
r2: 0.7117949069038597
pearson: 0.8448130958584177

=== Experiment 2725 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006793458924137517
rmse: 0.08242244187196542
mae: 0.03472099172823695
r2: 0.6936859271405715
pearson: 0.8348505714057762

=== Experiment 2719 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006265204797999941
rmse: 0.07915304667541194
mae: 0.03019567681390082
r2: 0.7175046731856934
pearson: 0.8534050837533481

=== Experiment 2650 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006145204748548027
rmse: 0.0783913563382343
mae: 0.030657605205154665
r2: 0.7229154225994194
pearson: 0.8526819448114723

=== Experiment 2988 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006113003650766422
rmse: 0.07818569978433666
mae: 0.032084655620540345
r2: 0.7243673559256052
pearson: 0.8511139470590928

=== Experiment 2536 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006895298499630641
rmse: 0.08303793410020893
mae: 0.04045913042736332
r2: 0.6890940255046116
pearson: 0.8407591362225243

=== Experiment 2807 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0061044332906211645
rmse: 0.07813087283923792
mae: 0.031364734185489844
r2: 0.7247537896924509
pearson: 0.8523011100531621

=== Experiment 2592 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006266276955310069
rmse: 0.07915981907072596
mae: 0.035458459255339414
r2: 0.7174563300844725
pearson: 0.8473108114852044

=== Experiment 2733 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007934810729562265
rmse: 0.08907755457780746
mae: 0.04090437867267412
r2: 0.6422228765813158
pearson: 0.8161160612016947

=== Experiment 2548 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006682985117311039
rmse: 0.08174952671001245
mae: 0.04391894520627414
r2: 0.6986671424671367
pearson: 0.8474474126198314

=== Experiment 2559 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008196000542191832
rmse: 0.09053176537653418
mae: 0.03609775203865914
r2: 0.6304459428882764
pearson: 0.7979549193337937

=== Experiment 2662 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005908113104968872
rmse: 0.07686425115077146
mae: 0.040165220992772595
r2: 0.7336057804563911
pearson: 0.8686097768824244

=== Experiment 2057 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006600395306188792
rmse: 0.0812428169513391
mae: 0.03416852279144434
r2: 0.7023910807000829
pearson: 0.8452649019833173

=== Experiment 2513 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006350286225174899
rmse: 0.07968868316878439
mae: 0.029911059985639553
r2: 0.7136683890815746
pearson: 0.8466919248565914

=== Experiment 2678 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006046152814355137
rmse: 0.0777570113517433
mae: 0.03333742277245884
r2: 0.7273816307160867
pearson: 0.8535541856913457

=== Experiment 2218 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00555476950726434
rmse: 0.07453032609122504
mae: 0.03179603211291919
r2: 0.749537887758478
pearson: 0.8692267938592625

=== Experiment 2887 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005043772474689627
rmse: 0.07101952178584159
mae: 0.02801802959918192
r2: 0.7725785190502781
pearson: 0.8792769173042696

=== Experiment 2282 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005821765691021842
rmse: 0.07630049600770523
mae: 0.02954700590888314
r2: 0.7374991473468594
pearson: 0.8591631283265748

=== Experiment 2715 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006565585628278539
rmse: 0.08102830140314271
mae: 0.02970563623976944
r2: 0.7039606337561455
pearson: 0.846936109340228

=== Experiment 2654 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0162282685208363
rmse: 0.12739022144904333
mae: 0.05092602254083069
r2: 0.2682745150026913
pearson: 0.5405270873268484

=== Experiment 2949 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006425513700428452
rmse: 0.0801593020205918
mae: 0.0343652212650288
r2: 0.7102764153325356
pearson: 0.8445893793003953

=== Experiment 2782 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006494263640293869
rmse: 0.08058699423786614
mae: 0.03985018413346048
r2: 0.7071765108031667
pearson: 0.84299522719547

=== Experiment 2791 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00583095883715584
rmse: 0.07636071527399307
mae: 0.03382703973896822
r2: 0.7370846324338904
pearson: 0.8589377862246017

=== Experiment 2287 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005658344024640989
rmse: 0.07522196504107685
mae: 0.033384737609026355
r2: 0.744867758356596
pearson: 0.8643737717350055

=== Experiment 2861 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006608410388143423
rmse: 0.08129212992746238
mae: 0.03202440563952988
r2: 0.702029684182456
pearson: 0.8395925530264667

=== Experiment 2730 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006807226830018272
rmse: 0.08250591996952869
mae: 0.031174554440534297
r2: 0.6930651383241271
pearson: 0.8446218936987212

=== Experiment 2972 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00605931033415812
rmse: 0.07784157201751593
mae: 0.03332523133837798
r2: 0.7267883639392376
pearson: 0.8600020055941279

=== Experiment 2793 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005789077165089563
rmse: 0.07608598533954569
mae: 0.029229972064846864
r2: 0.7389730585938254
pearson: 0.8619894454491998

=== Experiment 2772 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006900712057565772
rmse: 0.08307052460148408
mae: 0.0346777729953315
r2: 0.6888499305600054
pearson: 0.8349884328917064

=== Experiment 2251 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005525272577404386
rmse: 0.07433217726802024
mae: 0.029002980570970404
r2: 0.7508678913432716
pearson: 0.8681156615453218

=== Experiment 2534 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.014504797181611085
rmse: 0.12043586335311872
mae: 0.04859798506456133
r2: 0.34598507913060994
pearson: 0.6002685430326721

=== Experiment 2226 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005519140173926661
rmse: 0.07429091582371737
mae: 0.03266164197482133
r2: 0.751144398717005
pearson: 0.8686030380472909

=== Experiment 2507 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076413617669982056
rmse: 0.08741488298338107
mae: 0.039178938083280136
r2: 0.6554543611465636
pearson: 0.8182936862723758

=== Experiment 2666 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005888909623278117
rmse: 0.07673923131800395
mae: 0.03045457886112987
r2: 0.7344716569937284
pearson: 0.8671351522456388

=== Experiment 2946 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008342987172573518
rmse: 0.09133995386780923
mae: 0.03944904323589955
r2: 0.6238183804181298
pearson: 0.8001058205254213

=== Experiment 2709 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005786493732094525
rmse: 0.07606900638298442
mae: 0.029823846581521915
r2: 0.7390895444505158
pearson: 0.8599954112935516

=== Experiment 2540 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007295837520731198
rmse: 0.0854156749123438
mae: 0.03810012624746964
r2: 0.6710338973338923
pearson: 0.8200220704796493

=== Experiment 2673 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006200245750674422
rmse: 0.07874163924299787
mae: 0.03839147188812333
r2: 0.720433648038938
pearson: 0.8561046782626501

=== Experiment 2196 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010293984506071907
rmse: 0.10145927511111001
mae: 0.04563633545699154
r2: 0.535848769350282
pearson: 0.7361480524891242

=== Experiment 2670 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011660240903594295
rmse: 0.10798259537348737
mae: 0.050485448693261985
r2: 0.47424486972142454
pearson: 0.6972217022138821

=== Experiment 2265 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006579801676439606
rmse: 0.08111597670274091
mae: 0.03298655543496486
r2: 0.7033196384015242
pearson: 0.8440574024486982

=== Experiment 2696 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005819200952198645
rmse: 0.07628368732696818
mae: 0.03143046538618475
r2: 0.7376147902915695
pearson: 0.8592049266513248

=== Experiment 2575 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005743398608942987
rmse: 0.07578521365637883
mae: 0.030916356783544843
r2: 0.7410326845858044
pearson: 0.8626430113079686

=== Experiment 2618 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006175608702850505
rmse: 0.07858504121555518
mae: 0.030797072414625794
r2: 0.7215445216817244
pearson: 0.8506401235669498

=== Experiment 2314 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006738290579666959
rmse: 0.08208709143139961
mae: 0.03403358809424423
r2: 0.6961734435113036
pearson: 0.8345267138844624

=== Experiment 2722 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0060485143639152986
rmse: 0.07777219531371928
mae: 0.03100948419258889
r2: 0.7272751494858165
pearson: 0.8548470620974068

=== Experiment 2048 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00656254073011665
rmse: 0.08100951012144593
mae: 0.03368850007207391
r2: 0.7040979268741029
pearson: 0.839928726764408

=== Experiment 2174 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005449717598597306
rmse: 0.07382220261274589
mae: 0.02842577734328516
r2: 0.7542746320833936
pearson: 0.8686657424587182

=== Experiment 2756 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0070758950356506495
rmse: 0.08411833947273716
mae: 0.04243903056076449
r2: 0.6809510071820835
pearson: 0.8294200592420795

=== Experiment 2578 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.013276019559223308
rmse: 0.11522161064324396
mae: 0.046360290836577556
r2: 0.40139012129768403
pearson: 0.6433953995847199

=== Experiment 2703 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005663595395207469
rmse: 0.07525686277813784
mae: 0.028110093035647498
r2: 0.7446309763690586
pearson: 0.8642196192467495

=== Experiment 2385 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00888558925885551
rmse: 0.09426340360317735
mae: 0.046013473702955894
r2: 0.5993526911651161
pearson: 0.7755169486621956

=== Experiment 2498 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006456797090446921
rmse: 0.08035419771515935
mae: 0.03346574206993415
r2: 0.7088658610454759
pearson: 0.8432266704866012

=== Experiment 2425 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005963126769965903
rmse: 0.07722128443613135
mae: 0.032378197204528364
r2: 0.7311252385150405
pearson: 0.8638056255066692

=== Experiment 2616 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006808837404998752
rmse: 0.08251567975238859
mae: 0.03606290613286385
r2: 0.6929925182071254
pearson: 0.8334680586911201

=== Experiment 2643 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005973884620310346
rmse: 0.07729090904052265
mae: 0.02913856637454217
r2: 0.7306401717779016
pearson: 0.8566173335132794

=== Experiment 2707 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0071795274582660855
rmse: 0.08473209225710224
mae: 0.038428685786775586
r2: 0.6762782668584708
pearson: 0.8237660140344722

=== Experiment 2721 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0061841514648097685
rmse: 0.0786393760453996
mae: 0.030514574155924044
r2: 0.7211593323049382
pearson: 0.8523122817257596

=== Experiment 2591 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009577734285378706
rmse: 0.09786589950222041
mae: 0.04323743245230451
r2: 0.568144176555508
pearson: 0.7559154018696386

=== Experiment 2743 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006029523365305744
rmse: 0.07765000557183331
mae: 0.03025883895923095
r2: 0.7281314452545435
pearson: 0.8557223385468244

=== Experiment 2736 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006407450350555899
rmse: 0.08004655114716623
mae: 0.030520073119295856
r2: 0.711090883828001
pearson: 0.8456012166338724

=== Experiment 2640 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006246456385317886
rmse: 0.07903452653946809
mae: 0.03647999284570945
r2: 0.7183500308617199
pearson: 0.8494784552910202

=== Experiment 2604 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0068964173911169775
rmse: 0.08304467105791302
mae: 0.031455476966188806
r2: 0.6890435751799544
pearson: 0.8318035305832088

=== Experiment 2905 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.01120420413390585
rmse: 0.10584991324467796
mae: 0.04464204491164768
r2: 0.4948073669495445
pearson: 0.7062008785501653

=== Experiment 2574 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006041446988088771
rmse: 0.07772674564195244
mae: 0.02994598231305136
r2: 0.7275938143512447
pearson: 0.8532118623610145

=== Experiment 2871 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.012532564712615765
rmse: 0.11194893797002169
mae: 0.0450970733134465
r2: 0.43491217311171637
pearson: 0.7142638039073823

=== Experiment 2522 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005483653646030789
rmse: 0.07405169576742175
mae: 0.027998730279771306
r2: 0.7527444706410145
pearson: 0.867718641814823

=== Experiment 2938 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007553318185187708
rmse: 0.08690982789758421
mae: 0.03979292455259538
r2: 0.6594242074994552
pearson: 0.8131948290550469

=== Experiment 2967 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066189647609868575
rmse: 0.08135702035464953
mae: 0.04368469126842044
r2: 0.7015537921562803
pearson: 0.8499942362740831

=== Experiment 2593 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006550695454611437
rmse: 0.08093636670009
mae: 0.03612616459983974
r2: 0.7046320251331286
pearson: 0.8411021182607639

=== Experiment 2754 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007175071960127106
rmse: 0.0847057964966218
mae: 0.03682893748673342
r2: 0.6764791633085427
pearson: 0.8262230673769997

=== Experiment 2270 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006531751429967761
rmse: 0.08081925160484821
mae: 0.03559815850581869
r2: 0.705486202866409
pearson: 0.8541933864501773

=== Experiment 2489 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006361985113682642
rmse: 0.07976205309345191
mae: 0.030188208154031698
r2: 0.713140891347835
pearson: 0.846155426563618

=== Experiment 2987 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005913159554641362
rmse: 0.07689707117076282
mae: 0.03622149611592574
r2: 0.7333782382617708
pearson: 0.8582197282351525

=== Experiment 2808 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00676270205688609
rmse: 0.0822356495498521
mae: 0.042256800338458446
r2: 0.6950727407478059
pearson: 0.8338870099421722

=== Experiment 2785 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009609898766511285
rmse: 0.09803009112773121
mae: 0.039188912124625004
r2: 0.566693894257911
pearson: 0.7533321867857463

=== Experiment 2090 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005999239353710215
rmse: 0.07745475681783666
mae: 0.03498991087943039
r2: 0.7294969380083732
pearson: 0.8573817786500803

=== Experiment 2504 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007200467096179397
rmse: 0.08485556608837982
mae: 0.03598661300274587
r2: 0.6753341078011992
pearson: 0.8434391386732969

=== Experiment 2831 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008195243987318756
rmse: 0.09052758688553869
mae: 0.0427731455023665
r2: 0.6304800556145181
pearson: 0.7995189581810762

=== Experiment 2651 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005822452061323068
rmse: 0.07630499368536156
mae: 0.03105819802827397
r2: 0.737468199212072
pearson: 0.8603949905578618

=== Experiment 2619 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006099452330062392
rmse: 0.07809899058286472
mae: 0.030631268472642033
r2: 0.7249783790117574
pearson: 0.8555383930637988

=== Experiment 2276 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006027687024379635
rmse: 0.07763818019750099
mae: 0.028684765190109904
r2: 0.7282142450586663
pearson: 0.8556262834791974

=== Experiment 2873 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011712639076490429
rmse: 0.10822494664581928
mae: 0.0453107988122736
r2: 0.4718822591677375
pearson: 0.6961930361853259

=== Experiment 2705 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0069816563467018315
rmse: 0.08355630644482696
mae: 0.03290492235665352
r2: 0.6852001881891667
pearson: 0.8388917987944474

=== Experiment 2982 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007247703923626366
rmse: 0.08513344773722233
mae: 0.04450067510167371
r2: 0.6732042200421329
pearson: 0.8286487475075932

=== Experiment 2649 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0065133077737975454
rmse: 0.08070506659310521
mae: 0.03522250748560784
r2: 0.70631781920545
pearson: 0.8510650991455533

=== Experiment 2884 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005692440912693649
rmse: 0.07544826646579528
mae: 0.03049948455945235
r2: 0.7433303446814897
pearson: 0.8632333150661273

=== Experiment 2998 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006598111950813663
rmse: 0.08122876307573361
mae: 0.04265628033445757
r2: 0.7024940361889661
pearson: 0.8482919663594725

=== Experiment 2901 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0060739052504673995
rmse: 0.07793526320265685
mae: 0.030788189075935928
r2: 0.7261302855865661
pearson: 0.8543391156325248

=== Experiment 2001 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005564909416064401
rmse: 0.07459832046409892
mae: 0.028957362292183986
r2: 0.7490806837335984
pearson: 0.8690229119884167

=== Experiment 2913 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006752664990879585
rmse: 0.08217460064326194
mae: 0.033051655205311815
r2: 0.6955253076363885
pearson: 0.8483525274766719

=== Experiment 2653 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008719823321296496
rmse: 0.093379994224119
mae: 0.04064776532054165
r2: 0.6068270043304835
pearson: 0.7818179964897141

=== Experiment 2822 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0065713925659472375
rmse: 0.08106412625784132
mae: 0.03312913300148877
r2: 0.7036988014924925
pearson: 0.8391843905073845

=== Experiment 2726 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005442565172232099
rmse: 0.07377374310845355
mae: 0.028073246864995308
r2: 0.7545971318401771
pearson: 0.8705147527913749

=== Experiment 2883 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011793371426974784
rmse: 0.10859729014563294
mae: 0.04685920802844892
r2: 0.4682420730174226
pearson: 0.7942537425567624

=== Experiment 2189 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005523270279927464
rmse: 0.07431870746943507
mae: 0.034297640817338215
r2: 0.7509581740552276
pearson: 0.8682132926107191

=== Experiment 2833 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00573937436741054
rmse: 0.07575865869595726
mae: 0.03469376678611999
r2: 0.7412141358652983
pearson: 0.8645131640661288

=== Experiment 2874 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007927937418159576
rmse: 0.0890389657293905
mae: 0.04073743565602244
r2: 0.6425327911672873
pearson: 0.8016255249965699

=== Experiment 2129 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006201226410977144
rmse: 0.07874786607253013
mae: 0.030537657946148104
r2: 0.7203894304975099
pearson: 0.8490945688258315

=== Experiment 2914 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005464794162526628
rmse: 0.0739242461072592
mae: 0.034578264570803624
r2: 0.7535948364515236
pearson: 0.8696358602139582

=== Experiment 2995 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006290325413056809
rmse: 0.0793115717474872
mae: 0.035453303900631006
r2: 0.7163719957092075
pearson: 0.8516657088714221

=== Experiment 2627 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005823183660317224
rmse: 0.07630978744772668
mae: 0.029969589877280153
r2: 0.7374352117354268
pearson: 0.859845635161892

=== Experiment 2671 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006272274110602949
rmse: 0.07919769005850454
mae: 0.03513686374600615
r2: 0.7171859209918663
pearson: 0.8474702668790517

=== Experiment 2646 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006232121422695783
rmse: 0.07894378647300739
mae: 0.03028240754516305
r2: 0.7189963880170511
pearson: 0.8492851724009031

=== Experiment 2712 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006012403690404182
rmse: 0.07753969106466818
mae: 0.029706645230186877
r2: 0.7289033638609099
pearson: 0.8569010608877331

=== Experiment 2642 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006162312253705518
rmse: 0.07850039651941586
mae: 0.032157446646957695
r2: 0.7221440527214571
pearson: 0.8502333815402773

=== Experiment 2620 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006515719206411872
rmse: 0.08072000499511799
mae: 0.04150888741317175
r2: 0.7062090887702228
pearson: 0.8464224159414798

=== Experiment 2555 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007111750842950296
rmse: 0.08433119732904482
mae: 0.037718137298672266
r2: 0.6793342846122901
pearson: 0.826395339727455

=== Experiment 2758 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006761828118599156
rmse: 0.08223033575633239
mae: 0.03720001506300174
r2: 0.6951121462405141
pearson: 0.8395114194826123

=== Experiment 2676 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00600744514899681
rmse: 0.07750771025515339
mae: 0.03422438333859117
r2: 0.7291269423105473
pearson: 0.8549420947376792

=== Experiment 2594 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006485416549393097
rmse: 0.08053208397522751
mae: 0.034269871126639914
r2: 0.70757542223798
pearson: 0.8443673952102954

=== Experiment 2942 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005701001809130068
rmse: 0.07550497870425545
mae: 0.031076348026883405
r2: 0.7429443376291103
pearson: 0.8619758357643262

=== Experiment 2680 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006237557421282747
rmse: 0.07897820852160896
mae: 0.03603649056763106
r2: 0.7187512812333308
pearson: 0.8546699452963521

=== Experiment 2910 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007378095586555385
rmse: 0.08589584149745193
mae: 0.03277056192319863
r2: 0.6673249173504232
pearson: 0.8548743082334783

=== Experiment 2986 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006329826235277994
rmse: 0.07956020509826502
mae: 0.030541345892169537
r2: 0.7145909210208957
pearson: 0.8514640233928745

=== Experiment 2930 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006132659770974926
rmse: 0.07831130040405998
mae: 0.03283056699084843
r2: 0.7234810701167238
pearson: 0.8526854169893959

=== Experiment 2848 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008089791622072757
rmse: 0.08994326890920051
mae: 0.03830863179875656
r2: 0.635234856350325
pearson: 0.8042631972228731

=== Experiment 2794 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006278626616436884
rmse: 0.07923778528225586
mae: 0.035177372908890975
r2: 0.7168994892997658
pearson: 0.847680466212145

=== Experiment 2611 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006623330336441449
rmse: 0.08138384567247636
mae: 0.03483511966170027
r2: 0.7013569502804694
pearson: 0.8376796488487357

=== Experiment 2812 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006250584788304806
rmse: 0.07906063994368377
mae: 0.03145117379251452
r2: 0.7181638829880889
pearson: 0.8477437967159395

=== Experiment 2497 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0066644962203220165
rmse: 0.08163636579565517
mae: 0.030241506522926568
r2: 0.6995007986947858
pearson: 0.8413874214072143

=== Experiment 2420 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006104344732597242
rmse: 0.07813030610843172
mae: 0.031082027495980526
r2: 0.7247577827347722
pearson: 0.8521538904678064

=== Experiment 2720 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005648353474617693
rmse: 0.07515552856987763
mae: 0.037510136647338106
r2: 0.7453182278599693
pearson: 0.8662528464329542

=== Experiment 2639 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005540803526852203
rmse: 0.07443657385218776
mae: 0.02687459908658036
r2: 0.7501676076683631
pearson: 0.8668764386498534

=== Experiment 2246 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006510358699512022
rmse: 0.08068679383586896
mae: 0.036487529103359555
r2: 0.7064507916669975
pearson: 0.8410952400690872

=== Experiment 2774 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006356684154647127
rmse: 0.07972881633792846
mae: 0.03352469231297975
r2: 0.7133799092576157
pearson: 0.84661600686205

=== Experiment 2428 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063091488265323206
rmse: 0.0794301506138086
mae: 0.03102660133284152
r2: 0.7155232562804146
pearson: 0.8541998489016392

=== Experiment 2242 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0063521627202762455
rmse: 0.0797004562112178
mae: 0.03521724962819964
r2: 0.7135837787433637
pearson: 0.8475847932890442

=== Experiment 2202 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006836389252900885
rmse: 0.08268246037038862
mae: 0.03748877874995897
r2: 0.691750217511714
pearson: 0.8347239941492423

=== Experiment 2663 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009077081815741657
rmse: 0.09527372048860933
mae: 0.0418628506589248
r2: 0.5907183760574396
pearson: 0.7700277353085132

=== Experiment 2485 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005996983837359967
rmse: 0.07744019523064212
mae: 0.029621446968481745
r2: 0.7295986382478772
pearson: 0.8569596133451788

=== Experiment 2553 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005644840060924542
rmse: 0.07513215064753931
mae: 0.02879923596955801
r2: 0.7454766461370219
pearson: 0.8641612719336141

=== Experiment 2659 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006563730957475557
rmse: 0.08101685600833666
mae: 0.03070920089433299
r2: 0.7040442600463488
pearson: 0.8676855805971441

=== Experiment 2524 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005637931089652957
rmse: 0.07508615777660325
mae: 0.02906393065081306
r2: 0.7457881686107868
pearson: 0.8637861461198476

=== Experiment 2453 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006037227009781146
rmse: 0.07769959465647903
mae: 0.028013986818987268
r2: 0.7277840913157809
pearson: 0.8539528610870688

=== Experiment 2860 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006759240817919258
rmse: 0.08221460221833624
mae: 0.03667750256535112
r2: 0.6952288064894125
pearson: 0.8391365801759163

=== Experiment 2869 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005530530830866305
rmse: 0.07436753882485493
mae: 0.03234569369503978
r2: 0.750630799009008
pearson: 0.8671585249570172

=== Experiment 2515 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007581577410221722
rmse: 0.08707225396314099
mae: 0.034730567746964355
r2: 0.6581500114805077
pearson: 0.8179051834570713

=== Experiment 2792 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005683739389606842
rmse: 0.07539057891810383
mae: 0.03307085046542201
r2: 0.7437226925276408
pearson: 0.8637202869403272

=== Experiment 2857 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006829285488882028
rmse: 0.0826394910976709
mae: 0.03509669925306564
r2: 0.6920705231047186
pearson: 0.8571710610505432

=== Experiment 2311 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00812366010053509
rmse: 0.09013134915519178
mae: 0.0408621665460412
r2: 0.6337077415631157
pearson: 0.7965904492758206

=== Experiment 2729 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005897624971130149
rmse: 0.07679599580140978
mae: 0.02889500484417768
r2: 0.7340786857949985
pearson: 0.8583051642275474

=== Experiment 2589 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0056868614675201065
rmse: 0.07541128209704505
mae: 0.034962702026009984
r2: 0.7435819194086631
pearson: 0.8645919323521459

=== Experiment 2846 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007005378872407854
rmse: 0.08369814139159755
mae: 0.03779662888866091
r2: 0.6841305499461643
pearson: 0.8302746168839104

=== Experiment 2508 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00609559074275416
rmse: 0.07807426427930116
mae: 0.03528025338880409
r2: 0.7251524962838709
pearson: 0.8546440461525667

=== Experiment 2211 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005885724580168552
rmse: 0.07671847613299258
mae: 0.03083227572055011
r2: 0.7346152691856942
pearson: 0.8604272438734498

=== Experiment 2841 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006976454564638591
rmse: 0.0835251732392013
mae: 0.03312304572767123
r2: 0.685434734252919
pearson: 0.8514135177985279

=== Experiment 2268 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006021419575984937
rmse: 0.07759780651529356
mae: 0.032882254823264324
r2: 0.7284968415482683
pearson: 0.8539964329046333

=== Experiment 2985 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007378206887455244
rmse: 0.08589648937794399
mae: 0.05077250525878343
r2: 0.6673198988418358
pearson: 0.8308805329296518

=== Experiment 2595 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00680332276805611
rmse: 0.08248225729243902
mae: 0.029959019261169156
r2: 0.6932411707596919
pearson: 0.8560481090062191

=== Experiment 2603 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.011673218903230784
rmse: 0.10804267167758665
mae: 0.044967266941015153
r2: 0.4736596974298699
pearson: 0.7116031034302758

=== Experiment 2748 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007075149847672486
rmse: 0.0841139099535415
mae: 0.0356025144357387
r2: 0.6809846073800365
pearson: 0.8320566196209757

=== Experiment 2959 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005601915252265094
rmse: 0.07484594345898175
mae: 0.027063018931244687
r2: 0.7474121068668231
pearson: 0.8674964844367806

=== Experiment 2598 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00557397765481139
rmse: 0.07465907617169791
mae: 0.02993144993316048
r2: 0.7486718008397337
pearson: 0.8656082461327196

=== Experiment 2488 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006774325291533159
rmse: 0.08230628950167271
mae: 0.03530390601712964
r2: 0.6945486542133463
pearson: 0.8334549967132467

=== Experiment 2936 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005695974815539688
rmse: 0.07547168220955253
mae: 0.028808290696041315
r2: 0.7431710025575515
pearson: 0.8634972864667335

=== Experiment 2547 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005897413179308323
rmse: 0.07679461686412872
mae: 0.03141245031266839
r2: 0.7340882353950278
pearson: 0.8578862690396317

=== Experiment 2541 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005643152592565988
rmse: 0.07512091980644266
mae: 0.03174684047488059
r2: 0.7455527333425973
pearson: 0.8643921547866171

=== Experiment 2835 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005687607645042327
rmse: 0.07541622932129613
mae: 0.031657411296598194
r2: 0.7435482745926041
pearson: 0.8647647670927288

=== Experiment 2685 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063043983036448065
rmse: 0.0794002412064649
mae: 0.03022067376055094
r2: 0.7157374552665476
pearson: 0.8571708740557722

=== Experiment 2370 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005802373382684573
rmse: 0.0761733114856153
mae: 0.029392310388923513
r2: 0.7383735379945793
pearson: 0.85933687576241

=== Experiment 2939 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.02051637022411741
rmse: 0.14323536652697688
mae: 0.06551779383114466
r2: 0.07492589653963833
pearson: 0.2805646828191344

=== Experiment 2617 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005758077857681171
rmse: 0.07588199956301343
mae: 0.029647208371574182
r2: 0.740370803721031
pearson: 0.8631586154762579

=== Experiment 2480 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005883136320731385
rmse: 0.07670160572459604
mae: 0.028905883814576327
r2: 0.734731972664536
pearson: 0.8578748835845367

=== Experiment 2510 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006879650247260428
rmse: 0.08294365706466304
mae: 0.03395068536001343
r2: 0.6897995983166619
pearson: 0.8394356355010493

=== Experiment 2636 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006585534692058828
rmse: 0.08115130739586904
mae: 0.03424890967227548
r2: 0.7030611392505466
pearson: 0.840189871627568

=== Experiment 2763 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.010313570072394779
rmse: 0.10155574859354236
mae: 0.03475239450942472
r2: 0.5349656647850507
pearson: 0.7380388378900111

=== Experiment 2566 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006596884680154986
rmse: 0.08122120831503916
mae: 0.03569664319385904
r2: 0.7025493732828025
pearson: 0.8450625310216687

=== Experiment 2746 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005623081240883162
rmse: 0.07498720718151304
mae: 0.03034448493494463
r2: 0.7464577417558984
pearson: 0.8652017503706932

=== Experiment 2692 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00653396304087748
rmse: 0.08083293289790665
mae: 0.034491925573833404
r2: 0.7053864823038942
pearson: 0.8409342787874023

=== Experiment 2576 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006297636875558677
rmse: 0.07935765165098245
mae: 0.029275306812344973
r2: 0.7160423250830191
pearson: 0.847017256874848

=== Experiment 2249 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0061007507264768624
rmse: 0.07810730264499513
mae: 0.030515901169385247
r2: 0.724919834888897
pearson: 0.8541815538736133

=== Experiment 2572 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007260248567932032
rmse: 0.0852070922396254
mae: 0.04930028695653841
r2: 0.6726385875517107
pearson: 0.8350301550446881

=== Experiment 2114 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005561161511149271
rmse: 0.074573195661372
mae: 0.02622071266632932
r2: 0.7492496751166424
pearson: 0.8726793289273493

=== Experiment 2674 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005855209705097772
rmse: 0.07651934203257221
mae: 0.028553217749129542
r2: 0.7359911714720121
pearson: 0.8617943121740568

=== Experiment 2947 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.012560491963670501
rmse: 0.11207360065452747
mae: 0.04984344707132431
r2: 0.4336529456533815
pearson: 0.6631110302849832

=== Experiment 2585 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.012466480981283366
rmse: 0.11165339664015317
mae: 0.04831706992123795
r2: 0.43789185947181974
pearson: 0.6871429494080604

=== Experiment 2645 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006512738343233367
rmse: 0.08070153866707479
mae: 0.03130372433289265
r2: 0.7063434945789635
pearson: 0.8511290298950573

=== Experiment 2529 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005962880405671764
rmse: 0.07721968923578859
mae: 0.030347990800587782
r2: 0.73113634697263
pearson: 0.855207323655428

=== Experiment 2460 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007520213233935412
rmse: 0.08671916301449993
mae: 0.03199202238322293
r2: 0.660916895180818
pearson: 0.8352078467472747

=== Experiment 2134 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006047993988286216
rmse: 0.07776884972973572
mae: 0.03420691394167748
r2: 0.727298612993897
pearson: 0.8555453620854333

=== Experiment 2587 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005800033276403043
rmse: 0.07615794952861482
mae: 0.03227682865805145
r2: 0.7384790523568538
pearson: 0.8617687980367408

=== Experiment 2789 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005591467974369374
rmse: 0.07477611901114803
mae: 0.028460438454226942
r2: 0.7478831700289424
pearson: 0.866387393157794

=== Experiment 2687 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005755385252464856
rmse: 0.07586425543340458
mae: 0.026669861438336145
r2: 0.7404922121051285
pearson: 0.8655641296897233

=== Experiment 2583 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005490406416387292
rmse: 0.07409727671370447
mae: 0.028288010653447188
r2: 0.7524399911977613
pearson: 0.8686139402076686

=== Experiment 2443 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008691978892680797
rmse: 0.09323078296721957
mae: 0.042565914460152794
r2: 0.6080824973614951
pearson: 0.7865622217204512

=== Experiment 2568 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006049880818121351
rmse: 0.07778097979661448
mae: 0.031401291671336345
r2: 0.7272135366670882
pearson: 0.8533564359257192

=== Experiment 2701 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009306715051837477
rmse: 0.09647131724941604
mae: 0.04205479174479373
r2: 0.5803643145111953
pearson: 0.7736163539804206

=== Experiment 2830 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005953266980905533
rmse: 0.07715741688849836
mae: 0.03220099816445323
r2: 0.7315698120641476
pearson: 0.8596155613818711

=== Experiment 2738 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006413377600989921
rmse: 0.08008356636033338
mae: 0.030361286170870753
r2: 0.7108236267147133
pearson: 0.8450560590693895

=== Experiment 2381 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0073243957234911455
rmse: 0.08558268354925046
mae: 0.033577579752143386
r2: 0.6697462205408724
pearson: 0.844450283438807

=== Experiment 2609 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005662230223308157
rmse: 0.07524779214906015
mae: 0.027572452274415255
r2: 0.7446925313691404
pearson: 0.8646938547642431

=== Experiment 2731 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006080060568404623
rmse: 0.07797474314420422
mae: 0.031295309377678965
r2: 0.7258527450099392
pearson: 0.8520356398127685

=== Experiment 2931 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006020839750525556
rmse: 0.07759407033095735
mae: 0.029468836765212624
r2: 0.7285229856230311
pearson: 0.861718459067726

=== Experiment 2853 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006286884128163696
rmse: 0.07928987405819041
mae: 0.031351731265543845
r2: 0.7165271617304128
pearson: 0.8581191667493027

=== Experiment 2683 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005589300179717872
rmse: 0.07476162237216279
mae: 0.02929504264872866
r2: 0.7479809149356595
pearson: 0.8660688395346278

=== Experiment 2977 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006033206618914804
rmse: 0.07767371897182987
mae: 0.030109219773792027
r2: 0.727965368970435
pearson: 0.8534684255253928

=== Experiment 2330 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006430506502379723
rmse: 0.08019043897111253
mae: 0.038630512619178776
r2: 0.7100512920900532
pearson: 0.8439343631782917

=== Experiment 2557 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006702317954433077
rmse: 0.08186768565455528
mae: 0.0348184971758339
r2: 0.6977954333503362
pearson: 0.8418357849032996

=== Experiment 2297 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005438536242551143
rmse: 0.07374643206658302
mae: 0.02750914282227239
r2: 0.7547787945062239
pearson: 0.8711167888830784

=== Experiment 2944 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006324621568815705
rmse: 0.07952748939087481
mae: 0.029114636229100654
r2: 0.7148255971409316
pearson: 0.8471036729535009

=== Experiment 2535 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006063879127929836
rmse: 0.07787091323420983
mae: 0.03244161592956367
r2: 0.7265823590389509
pearson: 0.8574649933777554

=== Experiment 2514 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005649373672045613
rmse: 0.07516231550481672
mae: 0.028291891714098162
r2: 0.7452722276069674
pearson: 0.8636242949676849

=== Experiment 2435 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006015169892190857
rmse: 0.07755752634136069
mae: 0.03436410858982412
r2: 0.7287786370398472
pearson: 0.8542059614286474

=== Experiment 2638 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0056788471400603165
rmse: 0.07535812590597192
mae: 0.027862598115082043
r2: 0.7439432819064498
pearson: 0.8626861803755257

=== Experiment 2300 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005907492797291024
rmse: 0.07686021595917503
mae: 0.03125078165311522
r2: 0.7336337498565668
pearson: 0.8627746430364647

=== Experiment 2032 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0058441681626590514
rmse: 0.07644715928443026
mae: 0.028672017726612882
r2: 0.7364890297608195
pearson: 0.8583571060888989

=== Experiment 2953 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005903199945405357
rmse: 0.0768322845254868
mae: 0.03013252408452555
r2: 0.733827312658663
pearson: 0.8568311128605729

=== Experiment 2156 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006592387165030311
rmse: 0.08119351676722909
mae: 0.028958722414981517
r2: 0.7027521642602664
pearson: 0.868400056006348

=== Experiment 2631 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006413942997057723
rmse: 0.08008709632055418
mae: 0.033790994460937904
r2: 0.710798133254866
pearson: 0.8450348874254868

=== Experiment 2693 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007442284045487829
rmse: 0.08626867360454679
mae: 0.03686331871615591
r2: 0.6644306879886606
pearson: 0.8331285471051999

=== Experiment 2405 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006264689223497727
rmse: 0.07914978978808299
mae: 0.034446975159722844
r2: 0.7175279202130767
pearson: 0.8524275385045367

=== Experiment 2401 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005713729950465881
rmse: 0.07558921848032218
mae: 0.03223536786303475
r2: 0.7423704313383619
pearson: 0.8632593025873218

=== Experiment 2976 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008268532998254375
rmse: 0.09093147418938272
mae: 0.0379004412300554
r2: 0.6271754863683916
pearson: 0.7986588289270771

=== Experiment 2706 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0068395053800883385
rmse: 0.08270130216682406
mae: 0.031042238389365453
r2: 0.6916097127083443
pearson: 0.8383203051376429

=== Experiment 2665 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007100284306038518
rmse: 0.08426318476083443
mae: 0.03516626011995128
r2: 0.6798513057148334
pearson: 0.8457835854751758

=== Experiment 2779 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0063721683791198806
rmse: 0.07982586284607189
mae: 0.032339090897656714
r2: 0.7126817323912678
pearson: 0.8450228771721474

=== Experiment 2469 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008709043131391545
rmse: 0.09332225421297723
mae: 0.04282268374189356
r2: 0.6073130783485732
pearson: 0.7987479106015833

=== Experiment 2814 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006476468154366563
rmse: 0.08047650684744315
mae: 0.03205548302064489
r2: 0.7079789014312367
pearson: 0.8465917690287861

=== Experiment 2500 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00900084697748766
rmse: 0.09487279366334513
mae: 0.04239011744511745
r2: 0.5941557713607939
pearson: 0.7724252473575522

=== Experiment 2697 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.009331100537956985
rmse: 0.09659762180280106
mae: 0.04368379997809898
r2: 0.5792647836749398
pearson: 0.7743341433084249

=== Experiment 2752 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00611957068531143
rmse: 0.07822768490318137
mae: 0.0325741847878228
r2: 0.7240712512283436
pearson: 0.8617965929117656

=== Experiment 2940 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006356308628671369
rmse: 0.07972646128275962
mae: 0.03449658933380861
r2: 0.7133968415585812
pearson: 0.8486819939495353

=== Experiment 2564 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008193717779041425
rmse: 0.09051915697266201
mae: 0.03556333561005337
r2: 0.6305488716740073
pearson: 0.8316461870611092

=== Experiment 2996 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00587421152771076
rmse: 0.07664340498510462
mae: 0.028341980689698128
r2: 0.7351343876537341
pearson: 0.8608468825029345

=== Experiment 2991 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006482861536037543
rmse: 0.08051621908682463
mae: 0.03257214787777986
r2: 0.7076906266656344
pearson: 0.846398674216847

=== Experiment 2629 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006604251907268058
rmse: 0.08126654851332163
mae: 0.032878257052237736
r2: 0.7022171882548389
pearson: 0.8410400317759734

=== Experiment 2820 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005929046528510365
rmse: 0.07700030213259143
mae: 0.030060390588807738
r2: 0.7326619016024094
pearson: 0.8588110155736611

=== Experiment 2675 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005736924389828307
rmse: 0.07574248734909823
mae: 0.027564480460017655
r2: 0.7413246042761634
pearson: 0.8626305788145474

=== Experiment 2879 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006131008585809196
rmse: 0.07830075724927056
mae: 0.02872342899305095
r2: 0.7235555213291045
pearson: 0.8542291918772653

=== Experiment 2951 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005729651672639441
rmse: 0.07569446262864571
mae: 0.029336347577384684
r2: 0.741652527893242
pearson: 0.8616181031355725

=== Experiment 2704 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007363839602804571
rmse: 0.0858128172408095
mae: 0.039051241188658256
r2: 0.6679677133832091
pearson: 0.8179758314112926

=== Experiment 2532 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005597342708715383
rmse: 0.07481539085452527
mae: 0.03132668352163947
r2: 0.7476182808429492
pearson: 0.866313991095647

=== Experiment 2145 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006960012454692186
rmse: 0.08342668910302138
mae: 0.03836196214035965
r2: 0.6861761017536763
pearson: 0.8284270399954679

=== Experiment 2199 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00849433183468746
rmse: 0.09216469950413478
mae: 0.04273425006501785
r2: 0.6169943162152918
pearson: 0.7943115325316011

=== Experiment 2669 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005640782193327331
rmse: 0.07510514092475515
mae: 0.02650140874981218
r2: 0.7456596136009759
pearson: 0.8637933369150375

=== Experiment 2700 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005617078657433157
rmse: 0.07494717244455028
mae: 0.03490696983746932
r2: 0.7467283956017203
pearson: 0.8650264473823092

=== Experiment 2655 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.013510330477959237
rmse: 0.11623394718394123
mae: 0.04295475543115443
r2: 0.3908251451000029
pearson: 0.6322758804795184

=== Experiment 2999 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007857371752204695
rmse: 0.08864181717566881
mae: 0.04285148070038232
r2: 0.6457145659868683
pearson: 0.803596274764218

=== Experiment 2688 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005842797953063261
rmse: 0.07643819695062974
mae: 0.0316087619671738
r2: 0.7365508119084183
pearson: 0.8619392032980943

=== Experiment 2487 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005594354102156618
rmse: 0.0747954149808437
mae: 0.03034809378791828
r2: 0.7477530357972977
pearson: 0.8713107482990571

=== Experiment 2062 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006554583468612971
rmse: 0.08096038209280494
mae: 0.030842301983661984
r2: 0.7044567162930452
pearson: 0.8467873479364735

=== Experiment 2327 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0061977589919745245
rmse: 0.07872584703878724
mae: 0.0280975211278186
r2: 0.7205457748942101
pearson: 0.854658984463294

=== Experiment 2762 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011907908563497278
rmse: 0.10912336396710504
mae: 0.0523266492654025
r2: 0.4630776439432722
pearson: 0.6817492498816656

=== Experiment 2974 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005319572910717614
rmse: 0.07293540231408623
mae: 0.028271231538130322
r2: 0.7601427987788312
pearson: 0.8726610117913097

=== Experiment 2943 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005614609574490617
rmse: 0.07493069847859832
mae: 0.02838595604365563
r2: 0.7468397254648729
pearson: 0.8682533474283761

=== Experiment 2082 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006313784810829081
rmse: 0.07945932802905573
mae: 0.033610982061230955
r2: 0.7153142217889263
pearson: 0.8471911797142957

=== Experiment 2389 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006243239365485195
rmse: 0.0790141719281117
mae: 0.0303801457782888
r2: 0.7184950848700572
pearson: 0.8530731609275366

=== Experiment 2612 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005473911907484997
rmse: 0.07398588992155866
mae: 0.028182500815969982
r2: 0.7531837213443773
pearson: 0.8818042402455945

=== Experiment 2661 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.020512134507606906
rmse: 0.14322057990249482
mae: 0.06558592485547124
r2: 0.07511688313281495
pearson: 0.28057192940357867

=== Experiment 2818 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0053567868054409384
rmse: 0.0731900731345511
mae: 0.028320336334345206
r2: 0.7584648406448442
pearson: 0.8729176635878423

=== Experiment 2596 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0058838612689334205
rmse: 0.07670633134841882
mae: 0.029891671207327244
r2: 0.7346992850691803
pearson: 0.8575997036761674

=== Experiment 2623 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0057190801039162025
rmse: 0.0756245998595444
mae: 0.029328510541606873
r2: 0.7421291952740695
pearson: 0.866795401252834

=== Experiment 2157 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.020515886376549077
rmse: 0.1432336775222541
mae: 0.06552673980323798
r2: 0.07494771301353631
pearson: 0.28058576028141174

=== Experiment 2079 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0059718055554935775
rmse: 0.07727745826237803
mae: 0.02907877293292655
r2: 0.7307339158954234
pearson: 0.8549279840057327

=== Experiment 2570 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006096783569354157
rmse: 0.07808190295679375
mae: 0.035343414971275224
r2: 0.7250987122574803
pearson: 0.8551154865879027

=== Experiment 2442 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006378445651466805
rmse: 0.07986517170498543
mae: 0.029411040008124226
r2: 0.7123986929439837
pearson: 0.8459657194612704

=== Experiment 2805 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005658639060570919
rmse: 0.07522392611776468
mae: 0.030261641699681832
r2: 0.7448544553163707
pearson: 0.865266344962152

=== Experiment 2621 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006133477316526113
rmse: 0.07831652007415876
mae: 0.03085578030409109
r2: 0.7234442073476501
pearson: 0.85110852926164

=== Experiment 2393 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006377103763266482
rmse: 0.07985677030325282
mae: 0.03283688247062495
r2: 0.7124591980923263
pearson: 0.8491004919951477

=== Experiment 2837 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005442905560286522
rmse: 0.0737760500452994
mae: 0.03014246617816017
r2: 0.7545817838926195
pearson: 0.8690933679380569

=== Experiment 3000 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005339633422130978
rmse: 0.07307279536278175
mae: 0.028024338899964592
r2: 0.7592382791485089
pearson: 0.8744447339747242

=== Experiment 2205 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0068863034863488794
rmse: 0.08298375435197469
mae: 0.03066410288592764
r2: 0.6894996066944782
pearson: 0.8373242084597742

=== Experiment 2219 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064908899228876375
rmse: 0.08056605937296199
mae: 0.039078739816579965
r2: 0.707328630236127
pearson: 0.8430103168812991

=== Experiment 2493 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00617322391456107
rmse: 0.07856986645375612
mae: 0.02979874003398317
r2: 0.7216520507360695
pearson: 0.8525660304070252

=== Experiment 2506 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005787142881937959
rmse: 0.07607327311177006
mae: 0.03328274869383762
r2: 0.7390602745697881
pearson: 0.8598390910519516

=== Experiment 3013 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007493841877928042
rmse: 0.08656697914290439
mae: 0.03879828528349065
r2: 0.6621059680162686
pearson: 0.8147019452914326

=== Experiment 3041 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00824106488477594
rmse: 0.09078031110750799
mae: 0.03925554438442108
r2: 0.628414011515493
pearson: 0.8252068865034933

=== Experiment 3009 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00714592452754684
rmse: 0.08453357041759706
mae: 0.034436807895195855
r2: 0.6777934082148209
pearson: 0.8276798951865008

=== Experiment 3033 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008306218735027605
rmse: 0.09113845914336935
mae: 0.04732058356713414
r2: 0.625476253084048
pearson: 0.8197443970719577

=== Experiment 3037 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007592131737936811
rmse: 0.08713283960675683
mae: 0.0452175961422156
r2: 0.657674121489149
pearson: 0.8277038920587709

=== Experiment 3048 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0070733065889606065
rmse: 0.08410295232012135
mae: 0.03689904854872386
r2: 0.681067719104075
pearson: 0.8256311300044052

=== Experiment 3050 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00863078265928728
rmse: 0.09290200567957227
mae: 0.048287064907344454
r2: 0.6108418085906867
pearson: 0.8069363814109795

=== Experiment 3017 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007024107335997275
rmse: 0.08380994771503723
mae: 0.0435782321808353
r2: 0.6832860917659445
pearson: 0.8327971523523298

=== Experiment 3001 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005988936032544762
rmse: 0.07738821636751142
mae: 0.0329153709593233
r2: 0.729961510224885
pearson: 0.8549056325275993

=== Experiment 3052 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007796655444489487
rmse: 0.0882986718161122
mae: 0.044582844940762464
r2: 0.6484522375784562
pearson: 0.8087543916454121

=== Experiment 3024 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006413689232058292
rmse: 0.08008551199847755
mae: 0.030578110417764266
r2: 0.7108095754069996
pearson: 0.8435484220436963

=== Experiment 3051 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008477500788430873
rmse: 0.09207334461412202
mae: 0.0412843789222289
r2: 0.6177532206830914
pearson: 0.8087902840654523

=== Experiment 3016 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007035426939156817
rmse: 0.08387745191144529
mae: 0.03825465540411498
r2: 0.6827756958416191
pearson: 0.835867300233816

=== Experiment 3066 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007258336370111149
rmse: 0.08519587061654543
mae: 0.036840124922562965
r2: 0.6727248077098318
pearson: 0.8228125944049802

=== Experiment 3079 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007128636685739158
rmse: 0.08443125419972843
mae: 0.035542558023132456
r2: 0.6785729093929681
pearson: 0.8323337983476199

=== Experiment 3036 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006405297433474519
rmse: 0.08003310211077988
mae: 0.03963377773313197
r2: 0.7111879579116289
pearson: 0.8500202560739646

=== Experiment 3053 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008644477534048674
rmse: 0.09297568248767349
mae: 0.04353156270488822
r2: 0.6102243127152707
pearson: 0.8123725385222373

=== Experiment 3071 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007381823272760119
rmse: 0.08591753763208138
mae: 0.04375463962393624
r2: 0.6671568376201318
pearson: 0.8233205838390969

=== Experiment 3073 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007723491808795933
rmse: 0.08788339893743262
mae: 0.045225568658601306
r2: 0.6517511537101
pearson: 0.8210124664814645

=== Experiment 3043 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0066299445143649886
rmse: 0.08142447122557805
mae: 0.03340598736726765
r2: 0.7010587199090194
pearson: 0.8467576770226705

=== Experiment 3070 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007590412713931262
rmse: 0.08712297466186093
mae: 0.04704818020322416
r2: 0.657751631524959
pearson: 0.8219875967857494

=== Experiment 3078 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007491446904107478
rmse: 0.08655314496947802
mae: 0.038244799527219056
r2: 0.6622139563317295
pearson: 0.814120752196019

=== Experiment 3068 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00722268441908181
rmse: 0.08498637784422754
mae: 0.03312438541524328
r2: 0.674332338490121
pearson: 0.823054204107686

=== Experiment 3076 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006825030415764482
rmse: 0.08261374229366736
mae: 0.03696106169150889
r2: 0.6922623824787879
pearson: 0.8331828777477527

=== Experiment 3072 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006987336301373277
rmse: 0.08359028831971617
mae: 0.043796132198317766
r2: 0.6849440815329706
pearson: 0.833043275117075

=== Experiment 3056 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006714916772570067
rmse: 0.08194459574962872
mae: 0.04453369532730442
r2: 0.6972273581857034
pearson: 0.8428765475435294

=== Experiment 3018 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00617777159665561
rmse: 0.07859880149630534
mae: 0.032095895125269766
r2: 0.7214469977520146
pearson: 0.8519737605929364

=== Experiment 3062 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007035779714572876
rmse: 0.08387955480671602
mae: 0.03545715333390582
r2: 0.6827597893533772
pearson: 0.827462239593838

=== Experiment 3047 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006527650466867538
rmse: 0.08079387641936447
mae: 0.03383739343502172
r2: 0.705671113487626
pearson: 0.8429450213218248

=== Experiment 3032 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.015679983228519257
rmse: 0.12521973977180778
mae: 0.05441114619601023
r2: 0.292996457514455
pearson: 0.5824129933331041

=== Experiment 3026 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006163664875247162
rmse: 0.07850901142701494
mae: 0.039625033503229025
r2: 0.7220830636114786
pearson: 0.8532051092005045

=== Experiment 3077 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00625741564841505
rmse: 0.07910382827913609
mae: 0.030853486149919798
r2: 0.7178558825122092
pearson: 0.8481698027191278

=== Experiment 3022 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006735788053270617
rmse: 0.0820718468981332
mae: 0.037539253388847504
r2: 0.6962862813250686
pearson: 0.8352932233053489

=== Experiment 3027 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006817742336554913
rmse: 0.08256962114818568
mae: 0.033575505867671517
r2: 0.6925909987626183
pearson: 0.8387934416008952

=== Experiment 3055 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006103241924000369
rmse: 0.07812324829396412
mae: 0.036774491250172486
r2: 0.7248075078890219
pearson: 0.8541683928146225

=== Experiment 3007 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00551859755588178
rmse: 0.07428726375282495
mae: 0.028235514084057396
r2: 0.7511688651258243
pearson: 0.8669685486204793

=== Experiment 3083 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007043206641340585
rmse: 0.08392381450661418
mae: 0.039441987608294385
r2: 0.6824249124942618
pearson: 0.8271242148909412

=== Experiment 3014 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006601478114193745
rmse: 0.08124948070107123
mae: 0.03419109283295164
r2: 0.7023422573637199
pearson: 0.8451922396488445

=== Experiment 3012 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005640556491302049
rmse: 0.07510363833598242
mae: 0.029800121103382093
r2: 0.7456697904059568
pearson: 0.8636257602304392

=== Experiment 3064 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007812005176404301
rmse: 0.0883855484590343
mae: 0.04738229541516619
r2: 0.6477601249223508
pearson: 0.8119519705252655

=== Experiment 3005 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007848140148099949
rmse: 0.08858972936012362
mae: 0.03798482624298293
r2: 0.6461308149528115
pearson: 0.8103688543089261

=== Experiment 3046 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.01293044768630764
rmse: 0.1137121263819635
mae: 0.056022870980802414
r2: 0.4169718049496386
pearson: 0.6505185777182213

=== Experiment 3061 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00835680493884384
rmse: 0.09141556179799937
mae: 0.04134808640708336
r2: 0.6231953434186649
pearson: 0.796897137690113

=== Experiment 3015 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006560887141729793
rmse: 0.08099930334101518
mae: 0.03074050688940029
r2: 0.7041724864466663
pearson: 0.8399967752343361

=== Experiment 3010 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006491577805410362
rmse: 0.08057032831886911
mae: 0.03762087566208268
r2: 0.7072976139159377
pearson: 0.8422689480986132

=== Experiment 3059 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008576800632485107
rmse: 0.09261101787846361
mae: 0.03734350138904995
r2: 0.6132758344198896
pearson: 0.8293461543695713

=== Experiment 3103 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0074006401936535455
rmse: 0.08602697363997844
mae: 0.039437653915639866
r2: 0.6663083909390076
pearson: 0.822143350858495

=== Experiment 3002 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00631522838081976
rmse: 0.07946841121363733
mae: 0.032305649403296034
r2: 0.7152491318534104
pearson: 0.8478082954475752

=== Experiment 3102 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007168822786958901
rmse: 0.08466890094337413
mae: 0.03946080888601385
r2: 0.6767609357762256
pearson: 0.8233497406404655

=== Experiment 3004 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00630489105361649
rmse: 0.07940334409592892
mae: 0.03430684417502438
r2: 0.7157152373872002
pearson: 0.8595415482963209

=== Experiment 3111 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006567597003055166
rmse: 0.08104071200979891
mae: 0.037310993391365156
r2: 0.7038699417527411
pearson: 0.8397407909184659

=== Experiment 3119 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007179431371971772
rmse: 0.08473152525460505
mae: 0.0346255066132911
r2: 0.6762825993471908
pearson: 0.8231320129167793

=== Experiment 3085 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007660809045025414
rmse: 0.08752604780878327
mae: 0.03867867331749058
r2: 0.6545774919397185
pearson: 0.8131827456942423

=== Experiment 3058 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005761990359031075
rmse: 0.07590777535293124
mae: 0.04076373067184141
r2: 0.7401943907571875
pearson: 0.8663317106036571

=== Experiment 3086 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006311835016736712
rmse: 0.07944705794890528
mae: 0.03206830008026368
r2: 0.7154021371463813
pearson: 0.8467002948002222

=== Experiment 3125 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0076857543429490836
rmse: 0.0876684341308152
mae: 0.04778212581832616
r2: 0.6534527194356055
pearson: 0.8220368168574836

=== Experiment 3081 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006130849708871062
rmse: 0.07829974271267474
mae: 0.03980091195372756
r2: 0.7235626850203176
pearson: 0.855108119551185

=== Experiment 3098 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0066148545534185045
rmse: 0.08133175611911073
mae: 0.03915377033096304
r2: 0.7017391196064966
pearson: 0.8396481458740243

=== Experiment 3112 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006951981341488621
rmse: 0.0833785424524117
mae: 0.03544016300565422
r2: 0.6865382211132618
pearson: 0.8350355248346795

=== Experiment 3049 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007046445249105411
rmse: 0.08394310721616999
mae: 0.030405738873168643
r2: 0.6822788850955659
pearson: 0.845669895474848

=== Experiment 3031 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00720746948138594
rmse: 0.08489681667404227
mae: 0.04094520445108738
r2: 0.6750183733342214
pearson: 0.8246458199783581

=== Experiment 3088 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009252406433108379
rmse: 0.09618942994481451
mae: 0.04229831339503685
r2: 0.5828130662266393
pearson: 0.764209255497567

=== Experiment 3029 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005765527687394981
rmse: 0.075931071949466
mae: 0.02953578062104021
r2: 0.7400348941781562
pearson: 0.8607623927185735

=== Experiment 3094 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007329893373244533
rmse: 0.0856147964620867
mae: 0.040255992674456724
r2: 0.669498333933193
pearson: 0.823038043005673

=== Experiment 3107 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007028245826759638
rmse: 0.08383463381419184
mae: 0.04087634655891847
r2: 0.6830994890389586
pearson: 0.8287276551538174

=== Experiment 3113 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006633887132365577
rmse: 0.08144867790434401
mae: 0.033335829371888705
r2: 0.7008809489986517
pearson: 0.8497485303958248

=== Experiment 3145 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007049615385300397
rmse: 0.08396198774028875
mae: 0.03416579960498588
r2: 0.6821359450498177
pearson: 0.8386692231729954

=== Experiment 3134 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006664216109345427
rmse: 0.08163465017592363
mae: 0.04019530623419399
r2: 0.6995134287754339
pearson: 0.84578023813533

=== Experiment 3158 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006608299823114183
rmse: 0.08129144987705769
mae: 0.03594931315149352
r2: 0.702034669510961
pearson: 0.8394223264952928

=== Experiment 3157 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007179901329570795
rmse: 0.08473429842496363
mae: 0.035249956956682
r2: 0.6762614091658949
pearson: 0.8234946199707897

=== Experiment 3152 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006832696667516128
rmse: 0.08266012743467148
mae: 0.04103785503581197
r2: 0.6919167145614815
pearson: 0.8339215547140973

=== Experiment 3116 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005820185790885606
rmse: 0.07629014216060688
mae: 0.029073354138390815
r2: 0.737570384348635
pearson: 0.8618444153267847

=== Experiment 3148 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006869480248609522
rmse: 0.08288232772195482
mae: 0.03569243365567776
r2: 0.6902581590796726
pearson: 0.8308898367493515

=== Experiment 3164 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0071668421365632005
rmse: 0.08465720368972271
mae: 0.0343693689187647
r2: 0.6768502424308169
pearson: 0.8342046518734955

=== Experiment 3151 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006823543609907624
rmse: 0.082604743265188
mae: 0.03733700658295873
r2: 0.6923294219004802
pearson: 0.8350846198343955

=== Experiment 3092 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067771356011343915
rmse: 0.08232335999662788
mae: 0.03854672412692874
r2: 0.6944219385903385
pearson: 0.8365577783574762

=== Experiment 3095 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006824171639859441
rmse: 0.08260854459351914
mae: 0.032287475976454386
r2: 0.69230110430637
pearson: 0.8389054851886688

=== Experiment 3166 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007270458888036264
rmse: 0.08526698592090766
mae: 0.03954097691329002
r2: 0.6721782087121164
pearson: 0.8260731995286672

=== Experiment 3038 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006156134301114954
rmse: 0.07846103683431002
mae: 0.03668511813830851
r2: 0.7224226138846407
pearson: 0.8537796150734804

=== Experiment 3110 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006711287113979037
rmse: 0.08192244572752352
mae: 0.039699653449218435
r2: 0.6973910178940379
pearson: 0.8404109745202192

=== Experiment 3008 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005767987690787512
rmse: 0.07594726914634595
mae: 0.032605590302260616
r2: 0.7399239737079173
pearson: 0.860360320540138

=== Experiment 3137 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007033656472103436
rmse: 0.08386689735589029
mae: 0.038459536141685166
r2: 0.6828555254218138
pearson: 0.831992381920756

=== Experiment 3173 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006680385824904376
rmse: 0.08173362725894634
mae: 0.037626930915574765
r2: 0.698784343417717
pearson: 0.8367901469576948

=== Experiment 3115 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00676669670844638
rmse: 0.08225993379796984
mae: 0.03296841813130392
r2: 0.694892623667133
pearson: 0.836116949167309

=== Experiment 3154 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005457872999562342
rmse: 0.0738774187391678
mae: 0.027882935559739138
r2: 0.7539069086433466
pearson: 0.8684824969526441

=== Experiment 3082 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006053429011182355
rmse: 0.07780378532682299
mae: 0.030512014844121065
r2: 0.7270535502036466
pearson: 0.8557987360168543

=== Experiment 3035 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006245797269257534
rmse: 0.0790303566312182
mae: 0.035290705839837816
r2: 0.718379750114782
pearson: 0.8491566902812739

=== Experiment 3114 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007425262369317964
rmse: 0.08616996210581715
mae: 0.03633128390754781
r2: 0.6651981878753472
pearson: 0.8191939578034724

=== Experiment 3045 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006978110802407566
rmse: 0.08353508725324686
mae: 0.038377957665518575
r2: 0.6853600552208818
pearson: 0.8391286120644268

=== Experiment 3142 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.014171536268859352
rmse: 0.1190442618056803
mae: 0.054529980548562645
r2: 0.36101166700724063
pearson: 0.6374668532188128

=== Experiment 3067 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005696961299005398
rmse: 0.07547821738094639
mae: 0.028359967414050332
r2: 0.7431265224522672
pearson: 0.8624727134797948

=== Experiment 3177 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00695945503678027
rmse: 0.08342334827121403
mae: 0.036426276536368624
r2: 0.6862012354819862
pearson: 0.832783328124142

=== Experiment 3175 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006766026430880411
rmse: 0.0822558595534714
mae: 0.035093145078798765
r2: 0.6949228461875709
pearson: 0.8370370874084077

=== Experiment 3170 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006857226556470894
rmse: 0.08280837250224699
mae: 0.041020115288729585
r2: 0.6908106726649409
pearson: 0.835185672301493

=== Experiment 3162 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005855211221289172
rmse: 0.07651935193981436
mae: 0.030880041538215774
r2: 0.7359911031076091
pearson: 0.8598676585226243

=== Experiment 3101 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0061190483717353085
rmse: 0.07822434641296346
mae: 0.03055106494639288
r2: 0.7240948021176032
pearson: 0.8539436222742991

=== Experiment 3069 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007390692758245462
rmse: 0.08596913840585738
mae: 0.03808379714722889
r2: 0.6667569164233568
pearson: 0.8279408408591242

=== Experiment 3057 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007353630874137242
rmse: 0.08575331407086983
mae: 0.03596995832394716
r2: 0.6684280204656139
pearson: 0.8182074972108072

=== Experiment 3182 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0069489094770254075
rmse: 0.08336011922391551
mae: 0.03673370177113131
r2: 0.6866767301298772
pearson: 0.8295270615349805

=== Experiment 3003 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007542877790270078
rmse: 0.0868497426033611
mae: 0.03722635710068727
r2: 0.6598949603111219
pearson: 0.8133214506864277

=== Experiment 3172 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006533844109429287
rmse: 0.0808321972324722
mae: 0.032537833491241376
r2: 0.7053918448705474
pearson: 0.8423666405310734

=== Experiment 3128 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.01336326765758212
rmse: 0.1155996005943884
mae: 0.042962074654944775
r2: 0.39745614294350995
pearson: 0.687173280206528

=== Experiment 3044 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007177938337980876
rmse: 0.08472271441579805
mae: 0.03089956465310753
r2: 0.6763499195927056
pearson: 0.8427360458233784

=== Experiment 3054 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005823067120521326
rmse: 0.07630902384725757
mae: 0.03437531877456499
r2: 0.7374404664635307
pearson: 0.8599220028347786

=== Experiment 3155 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005946929317686166
rmse: 0.07711633625689285
mae: 0.031204344091239542
r2: 0.73185557450929
pearson: 0.8584670832122119

=== Experiment 3186 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007186301586327619
rmse: 0.08477205663617947
mae: 0.04251371599564948
r2: 0.6759728244056982
pearson: 0.8298387294727854

=== Experiment 3129 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006965228304680953
rmse: 0.0834579433288465
mae: 0.04680900120132538
r2: 0.6859409213733539
pearson: 0.8369495826698267

=== Experiment 3180 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007210703542912255
rmse: 0.084915861550786
mae: 0.04198601044720929
r2: 0.6748725509234195
pearson: 0.8240307157040937

=== Experiment 3167 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006552614548319107
rmse: 0.08094822140306177
mae: 0.03867250446180193
r2: 0.7045454940425073
pearson: 0.843094423796532

=== Experiment 3122 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006494884672558948
rmse: 0.08059084732498442
mae: 0.03109923507514865
r2: 0.7071485087316715
pearson: 0.8519718572910563

=== Experiment 3091 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008499596720700777
rmse: 0.09219325745791163
mae: 0.040154260630259064
r2: 0.6167569248222051
pearson: 0.7922129059758487

=== Experiment 3021 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006531884265876772
rmse: 0.08082007340925132
mae: 0.03815770027048492
r2: 0.7054802133537372
pearson: 0.8447258241888981

=== Experiment 3025 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005890816305827398
rmse: 0.07675165344034875
mae: 0.030670164277206983
r2: 0.7343856855167767
pearson: 0.8574608753947504

=== Experiment 3087 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005915920288557924
rmse: 0.07691501991521503
mae: 0.030490901116648636
r2: 0.7332537579845649
pearson: 0.8568769871603004

=== Experiment 3075 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005942981294840097
rmse: 0.0770907341698086
mae: 0.029644932171608376
r2: 0.7320335891217615
pearson: 0.8560480226374666

=== Experiment 3028 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006573105626788994
rmse: 0.08107469165398654
mae: 0.032451856664228625
r2: 0.7036215603331135
pearson: 0.839131754496401

=== Experiment 3192 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00761617823058344
rmse: 0.0872707180592863
mae: 0.04384639696637995
r2: 0.6565898757193833
pearson: 0.8168228526167265

=== Experiment 3133 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005549245773638348
rmse: 0.07449325992087034
mae: 0.02852330288469911
r2: 0.7497869504764927
pearson: 0.8662300668495948

=== Experiment 3156 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.02056219127718433
rmse: 0.14339522752582923
mae: 0.06488708032547426
r2: 0.07285984542423818
pearson: 0.28033967944762583

=== Experiment 3206 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007548857025947698
rmse: 0.08688415865937645
mae: 0.03738176749034504
r2: 0.659625359206081
pearson: 0.8122282557143702

=== Experiment 3183 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00749879109984015
rmse: 0.08659556050883989
mae: 0.037971631876575124
r2: 0.6618828097785711
pearson: 0.8197249804510583

=== Experiment 3084 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006732427199949656
rmse: 0.08205136927528787
mae: 0.038870393999753616
r2: 0.6964378207220862
pearson: 0.841579976318243

=== Experiment 3220 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007563024658473871
rmse: 0.08696565217644188
mae: 0.038955767489758035
r2: 0.6589865468911276
pearson: 0.8122608168984109

=== Experiment 3171 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006796246105195203
rmse: 0.08243934803960548
mae: 0.0417008239262458
r2: 0.6935602543734127
pearson: 0.838727355201097

=== Experiment 3136 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.012484799754060045
rmse: 0.11173540063050763
mae: 0.046565808979952904
r2: 0.4370658740700276
pearson: 0.6769457775617818

=== Experiment 3205 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007532284275150513
rmse: 0.08678873357268507
mae: 0.03718221805275816
r2: 0.6603726172453005
pearson: 0.8191101009622594

=== Experiment 3096 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006485933293354512
rmse: 0.08053529222244439
mae: 0.03326612043265672
r2: 0.7075521224801962
pearson: 0.8413722618457465

=== Experiment 3202 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006249624577726206
rmse: 0.07905456708961353
mae: 0.03522741505072093
r2: 0.7182071784604569
pearson: 0.8535320100650237

=== Experiment 3230 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006666971418103845
rmse: 0.08165152428524433
mae: 0.03397981274746893
r2: 0.699389193116221
pearson: 0.8366753382993912

=== Experiment 3221 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006036726640291565
rmse: 0.07769637469207662
mae: 0.033540370118042895
r2: 0.7278066527558368
pearson: 0.8584174100796764

=== Experiment 3194 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006845553918863807
rmse: 0.08273786266796966
mae: 0.03678516255116282
r2: 0.6913369867571237
pearson: 0.832647048228919

=== Experiment 3131 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009384855820515497
rmse: 0.09687546552412275
mae: 0.04514759600614157
r2: 0.5768409816438858
pearson: 0.8040364291514714

=== Experiment 3231 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007515982185728388
rmse: 0.08669476446549923
mae: 0.03686231264379472
r2: 0.6611076712822459
pearson: 0.8160877744346869

=== Experiment 3120 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009498989871924136
rmse: 0.09746276146264345
mae: 0.040675966868044235
r2: 0.571694727500108
pearson: 0.7952147918182111

=== Experiment 3179 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006905355843196134
rmse: 0.0830984707632826
mae: 0.03662588384953501
r2: 0.6886405443098189
pearson: 0.8457020601956613

=== Experiment 3225 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006764417910262216
rmse: 0.08224608142800614
mae: 0.036540162954190276
r2: 0.6949953736742812
pearson: 0.8337970473484851

=== Experiment 3236 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006677874581644581
rmse: 0.08171826345220866
mae: 0.037293063442059925
r2: 0.6988975742710185
pearson: 0.8374030179737711

=== Experiment 3020 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00816672539416805
rmse: 0.09036993634040057
mae: 0.03972241429381886
r2: 0.6317659464276908
pearson: 0.7995464151867056

=== Experiment 3235 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007240646900983473
rmse: 0.08509199081572526
mae: 0.04203375848034003
r2: 0.6735224180870678
pearson: 0.8222223282233077

=== Experiment 3163 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00705561778819811
rmse: 0.08399772489894063
mae: 0.037385960466072905
r2: 0.6818652993450194
pearson: 0.8335762687560763

=== Experiment 3019 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0058706184382392584
rmse: 0.07661996109526067
mae: 0.0316398264671574
r2: 0.7352963984765645
pearson: 0.8576756346605258

=== Experiment 3074 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005851783212436166
rmse: 0.07649694904005104
mae: 0.027855653264098494
r2: 0.7361456705180089
pearson: 0.8588699732111766

=== Experiment 3214 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006254131875945704
rmse: 0.07908306946461868
mae: 0.030118966502006513
r2: 0.7180039463675534
pearson: 0.851307369898341

=== Experiment 3204 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006255171020036103
rmse: 0.07908963914468256
mae: 0.03953820361328314
r2: 0.7179570918178795
pearson: 0.852316354901779

=== Experiment 3207 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006887693350031331
rmse: 0.08299212824136594
mae: 0.03326638534414567
r2: 0.6894369383527621
pearson: 0.8304369043734603

=== Experiment 3208 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006050825700896525
rmse: 0.07778705355582331
mae: 0.03325538602497159
r2: 0.7271709323186302
pearson: 0.8549776218804396

=== Experiment 3197 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005931616672663446
rmse: 0.0770169895066241
mae: 0.03196503899524095
r2: 0.7325460149337539
pearson: 0.8642943753201054

=== Experiment 3168 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007871956949631869
rmse: 0.08872404944338298
mae: 0.054013898267216046
r2: 0.6450569258543132
pearson: 0.830356851176147

=== Experiment 3147 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005749106556096763
rmse: 0.0758228630170133
mae: 0.032673892723685975
r2: 0.7407753157609003
pearson: 0.8613562623793755

=== Experiment 3127 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070597401852498486
rmse: 0.08402226005797421
mae: 0.03242942762809673
r2: 0.6816794222763622
pearson: 0.8354361967707463

=== Experiment 3121 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008318223757446844
rmse: 0.09120429681460651
mae: 0.04204557394185898
r2: 0.6249349519069789
pearson: 0.7907449327095484

=== Experiment 3228 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006290355430471607
rmse: 0.0793117609845577
mae: 0.035016037118328645
r2: 0.716370642237186
pearson: 0.8477814599853473

=== Experiment 3216 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006507229747403467
rmse: 0.08066740201223457
mae: 0.03196070195493093
r2: 0.7065918747404151
pearson: 0.8430607759567905

=== Experiment 3239 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0064790027987891045
rmse: 0.08049225303586119
mae: 0.03349748991159312
r2: 0.7078646154298065
pearson: 0.8470993691154013

=== Experiment 3223 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006515655067347508
rmse: 0.08071960770065417
mae: 0.03416996948527322
r2: 0.7062119807724051
pearson: 0.8406601074261116

=== Experiment 3187 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006855502653563865
rmse: 0.08279796285877972
mae: 0.03200998677813491
r2: 0.6908884026882711
pearson: 0.8373459205223571

=== Experiment 3188 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006760387600750328
rmse: 0.08222157624827152
mae: 0.0461568545629558
r2: 0.6951770985562973
pearson: 0.8456297960660785

=== Experiment 3271 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007523013975050939
rmse: 0.08673530985158777
mae: 0.04024397089159491
r2: 0.6607906109966193
pearson: 0.8172581656788204

=== Experiment 3189 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0063297606091337446
rmse: 0.07955979266648289
mae: 0.03434180615371194
r2: 0.7145938800748557
pearson: 0.8490824228996458

=== Experiment 3174 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00621006467099828
rmse: 0.07880396354878529
mae: 0.036607638837152685
r2: 0.7199909172431727
pearson: 0.8495350528582197

=== Experiment 3130 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0058818114208066445
rmse: 0.0766929685225878
mae: 0.034701143734169895
r2: 0.7347917118190095
pearson: 0.8634642728590747

=== Experiment 3040 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006481513355654191
rmse: 0.08050784654711733
mae: 0.03278059053992792
r2: 0.7077514155257403
pearson: 0.8462083210494109

=== Experiment 3135 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0061173457008018165
rmse: 0.07821346240131437
mae: 0.033634290857649234
r2: 0.7241715748005915
pearson: 0.8521047761492014

=== Experiment 3252 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006636915884113185
rmse: 0.08146726879006798
mae: 0.03391507238182859
r2: 0.7007443839154077
pearson: 0.8407705419098747

=== Experiment 3138 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0056896596656590874
rmse: 0.07542983272988936
mae: 0.03128095730986671
r2: 0.7434557498861578
pearson: 0.8668795381764732

=== Experiment 3198 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.016250304014936472
rmse: 0.12747668027892972
mae: 0.055577276213695806
r2: 0.2672809442722771
pearson: 0.5289399569353908

=== Experiment 3219 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006580467856587451
rmse: 0.08112008294243449
mae: 0.03766724748864766
r2: 0.7032896006318661
pearson: 0.8392216211739234

=== Experiment 3132 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005335390689404073
rmse: 0.07304375872998371
mae: 0.028688100945449622
r2: 0.7594295820997956
pearson: 0.8721489420219022

=== Experiment 3233 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005714027088018648
rmse: 0.07559118393052623
mae: 0.031322788787690514
r2: 0.7423570335368879
pearson: 0.861969169684374

=== Experiment 3150 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006493978014181246
rmse: 0.08058522205827347
mae: 0.03317941467831704
r2: 0.7071893895588712
pearson: 0.8440702470329104

=== Experiment 3272 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006351683765294605
rmse: 0.07969745143537907
mae: 0.035711779174490095
r2: 0.7136053746126814
pearson: 0.8464311388044373

=== Experiment 3093 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006778375849509294
rmse: 0.08233089243722125
mae: 0.02866221790532393
r2: 0.6943660163369894
pearson: 0.8419942619306863

=== Experiment 3295 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006707031859982244
rmse: 0.08189647037560437
mae: 0.034718035163627974
r2: 0.697582885423873
pearson: 0.8366472979423129

=== Experiment 3285 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007396767821137134
rmse: 0.08600446396052436
mae: 0.03406660514007818
r2: 0.6664829945114109
pearson: 0.8281307445012992

=== Experiment 3289 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006467333626397908
rmse: 0.08041973406072608
mae: 0.035137984742169164
r2: 0.70839077327693
pearson: 0.8423552521149533

=== Experiment 3227 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011362243484340237
rmse: 0.10659382479459228
mae: 0.04515668581722623
r2: 0.4876814421968957
pearson: 0.6989125857497951

=== Experiment 3165 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008283033556533476
rmse: 0.0910111727016715
mae: 0.04004704370251069
r2: 0.6265216625777708
pearson: 0.793688261104417

=== Experiment 3259 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006501008496941639
rmse: 0.08062883167292975
mae: 0.034551510831623694
r2: 0.7068723881855572
pearson: 0.8436252972460313

=== Experiment 3109 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006495984058389211
rmse: 0.08059766782227146
mae: 0.04077393975940158
r2: 0.7070989379084616
pearson: 0.847717718249963

=== Experiment 3149 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006680446437261751
rmse: 0.08173399805014894
mae: 0.03564739592340733
r2: 0.6987816104332006
pearson: 0.8495192096062861

=== Experiment 3286 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006766758862920942
rmse: 0.08226031159022522
mae: 0.04043126753370211
r2: 0.6948898211492315
pearson: 0.8367331396555227

=== Experiment 3256 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006554208855138587
rmse: 0.08095806849930763
mae: 0.03593597164118009
r2: 0.7044736074497252
pearson: 0.8535555317209974

=== Experiment 3039 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006131668081334695
rmse: 0.07830496843326543
mae: 0.029996547329111972
r2: 0.723525784966126
pearson: 0.8514454705043281

=== Experiment 3265 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006791344532410085
rmse: 0.08240961432023623
mae: 0.034510378399493315
r2: 0.6937812641329466
pearson: 0.8364050657683232

=== Experiment 3274 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006277698894129332
rmse: 0.07923193102612944
mae: 0.03406841692654659
r2: 0.7169413198902915
pearson: 0.846976593969087

=== Experiment 3244 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006036408310344287
rmse: 0.07769432611422977
mae: 0.03276455944263646
r2: 0.7278210061130518
pearson: 0.853202952943341

=== Experiment 3178 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008854057781393096
rmse: 0.09409600300434177
mae: 0.03673510192208629
r2: 0.6007744316058319
pearson: 0.7821214158397881

=== Experiment 3034 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005604766800958196
rmse: 0.07486499048926805
mae: 0.029203630381913603
r2: 0.7472835317913138
pearson: 0.864894993898012

=== Experiment 3257 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007135866428773652
rmse: 0.0844740577264621
mae: 0.04011975899523201
r2: 0.6782469234615961
pearson: 0.8273539477641072

=== Experiment 3030 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.014151160846721746
rmse: 0.1189586518363492
mae: 0.05434565659017652
r2: 0.361930385823516
pearson: 0.6315390792380629

=== Experiment 3326 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006828741359716632
rmse: 0.08263619884600593
mae: 0.03837993553005218
r2: 0.6920950576492968
pearson: 0.8344315191153558

=== Experiment 3277 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007992484249398283
rmse: 0.08940069490444849
mae: 0.03781352862016651
r2: 0.6396224029559672
pearson: 0.8091874118107751

=== Experiment 3011 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005459677877663745
rmse: 0.07388963308654162
mae: 0.030770178338884978
r2: 0.7538255274841418
pearson: 0.8687101050160592

=== Experiment 3281 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005857671181603879
rmse: 0.07653542435763898
mae: 0.0320750608351111
r2: 0.7358801845797303
pearson: 0.8628704505456428

=== Experiment 3117 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005382795414608624
rmse: 0.07336753651724054
mae: 0.027991988458748163
r2: 0.7572921239047374
pearson: 0.8705230906841772

=== Experiment 3313 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006480232783252168
rmse: 0.08049989306360704
mae: 0.03256681733005525
r2: 0.7078091559717246
pearson: 0.8413331410887955

=== Experiment 3123 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006155522524090393
rmse: 0.07845713813344452
mae: 0.03219895851218537
r2: 0.7224501986414145
pearson: 0.8500072569987539

=== Experiment 3276 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007462247928520386
rmse: 0.08638430371612882
mae: 0.04164582766196732
r2: 0.6635305252894994
pearson: 0.8180068573702679

=== Experiment 3153 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005881836992382756
rmse: 0.076693135236361
mae: 0.03499840312005212
r2: 0.7347905588078975
pearson: 0.8590997783398573

=== Experiment 3139 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006448213160183457
rmse: 0.08030076687170215
mae: 0.03415269757444628
r2: 0.7092529066829787
pearson: 0.8422700826144395

=== Experiment 3190 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006329023976489745
rmse: 0.07955516310391014
mae: 0.03601010181867429
r2: 0.7146270945165567
pearson: 0.8534105882556622

=== Experiment 3090 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0070159117359496785
rmse: 0.08376103948704122
mae: 0.03657817288090259
r2: 0.683655627764931
pearson: 0.8272225710080628

=== Experiment 3089 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006150111946610368
rmse: 0.07842264944906138
mae: 0.03492899140934253
r2: 0.7226941591986116
pearson: 0.8558367677561566

=== Experiment 3161 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005770678808914486
rmse: 0.07596498409737533
mae: 0.03323393605483174
r2: 0.7398026323760236
pearson: 0.8620366842834422

=== Experiment 3144 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006065915047270013
rmse: 0.07788398453642452
mae: 0.029023037494958194
r2: 0.7264905603319787
pearson: 0.861267958943026

=== Experiment 3215 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007375106684158157
rmse: 0.08587844132352518
mae: 0.03646253767780268
r2: 0.6674596856439976
pearson: 0.8229355969232882

=== Experiment 3290 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007058906551598297
rmse: 0.08401729912106373
mae: 0.03827971559202499
r2: 0.6817170104508006
pearson: 0.8263358660899169

=== Experiment 3321 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007585540797044088
rmse: 0.08709501017305232
mae: 0.03622436081206034
r2: 0.6579713041131072
pearson: 0.8306334547035117

=== Experiment 3143 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005817171164369908
rmse: 0.07627038196029903
mae: 0.03164178504753599
r2: 0.7377063125313892
pearson: 0.8657656713748485

=== Experiment 3348 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006610518088980302
rmse: 0.0813050926386552
mae: 0.040791666115099004
r2: 0.7019346488793912
pearson: 0.8401193391825016

=== Experiment 3342 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007643898381320636
rmse: 0.08742939083237762
mae: 0.03452344249614101
r2: 0.6553399863232146
pearson: 0.8273796384316541

=== Experiment 3023 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006488363146985629
rmse: 0.08055037645464873
mae: 0.03447994873870047
r2: 0.7074425614494295
pearson: 0.8514509002123082

=== Experiment 3080 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005778761684137921
rmse: 0.07601816680332354
mae: 0.02956040516748607
r2: 0.7394381790897444
pearson: 0.8609834205599961

=== Experiment 3328 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008220684695008466
rmse: 0.090667991568185
mae: 0.050896788582670084
r2: 0.6293329453021008
pearson: 0.8312427590642819

=== Experiment 3360 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007126721792697902
rmse: 0.08441991348430714
mae: 0.03998644244221988
r2: 0.6786592510773894
pearson: 0.8245793265861985

=== Experiment 3124 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0058917292596351156
rmse: 0.07675760066361582
mae: 0.03248938959575145
r2: 0.7343445208314083
pearson: 0.8581597919767021

=== Experiment 3106 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00698735559572146
rmse: 0.08359040372986279
mae: 0.03349658229674059
r2: 0.684943211559303
pearson: 0.8340548985576745

=== Experiment 3229 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007601771408783108
rmse: 0.087188138005024
mae: 0.03540545456426185
r2: 0.6572394729734331
pearson: 0.8247094955380694

=== Experiment 3232 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0075156572068933785
rmse: 0.08669289017499289
mae: 0.04138895169125658
r2: 0.6611223244348823
pearson: 0.8177814698928271

=== Experiment 3309 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006391611620431345
rmse: 0.07994755543749506
mae: 0.03793597282543888
r2: 0.7118050451981587
pearson: 0.844882203764087

=== Experiment 3332 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007130810743949678
rmse: 0.08444412794238376
mae: 0.041178981613048185
r2: 0.6784748820651318
pearson: 0.8258012678614032

=== Experiment 3299 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008068044043181784
rmse: 0.08982229146031505
mae: 0.039511047005421414
r2: 0.6362154451106773
pearson: 0.7999943491620802

=== Experiment 3355 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007260588806154055
rmse: 0.0852090887532196
mae: 0.04169981042585772
r2: 0.6726232463600299
pearson: 0.8250921313822585

=== Experiment 3200 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006568020475205088
rmse: 0.08104332468010605
mae: 0.034152722240668495
r2: 0.7038508475798853
pearson: 0.841612011779356

=== Experiment 3104 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0056077165500860405
rmse: 0.0748846883554044
mae: 0.03156055997887674
r2: 0.7471505289014231
pearson: 0.8662670335097317

=== Experiment 3160 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0064445774515747775
rmse: 0.0802781256107464
mae: 0.039611718946927256
r2: 0.7094168391839466
pearson: 0.8431542018066402

=== Experiment 3246 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006650960714780345
rmse: 0.08155342245902587
mae: 0.0358672191941245
r2: 0.7001111086822281
pearson: 0.8377552012183257

=== Experiment 3195 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006153940315559818
rmse: 0.07844705421849707
mae: 0.036670211372959685
r2: 0.7225215397276749
pearson: 0.8533153583602499

=== Experiment 3379 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006250541221360508
rmse: 0.07906036441454409
mae: 0.03164318915727258
r2: 0.718165847402432
pearson: 0.8527643004856438

=== Experiment 3358 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007583831446171332
rmse: 0.08708519648121218
mae: 0.049799018851842555
r2: 0.6580483779916168
pearson: 0.8249170666829079

=== Experiment 3369 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008364028887442589
rmse: 0.09145506485396306
mae: 0.03842075903753801
r2: 0.6228696187558502
pearson: 0.8261808969585598

=== Experiment 3338 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006930647544616828
rmse: 0.08325051077691252
mae: 0.03656974368762117
r2: 0.6875001526244888
pearson: 0.8325422650718837

=== Experiment 3334 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067300676337444364
rmse: 0.08203698942394483
mae: 0.030857108122911935
r2: 0.696544212523755
pearson: 0.8358591250780047

=== Experiment 3383 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007163721753231124
rmse: 0.0846387721628281
mae: 0.040023865998131035
r2: 0.6769909391418745
pearson: 0.8243504890718016

=== Experiment 3006 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006019546149009478
rmse: 0.07758573418489689
mae: 0.028645141268010706
r2: 0.7285813135460334
pearson: 0.8541438707088411

=== Experiment 3251 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00681495579797607
rmse: 0.0825527455508057
mae: 0.036747552039414
r2: 0.6927166425606888
pearson: 0.8404782615898421

=== Experiment 3354 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007066985148354348
rmse: 0.08406536235783647
mae: 0.03979836252415147
r2: 0.6813527500787324
pearson: 0.8272941998275755

=== Experiment 3361 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0065772372153375695
rmse: 0.08110016778859074
mae: 0.03820881794816905
r2: 0.7034352688239094
pearson: 0.8400405111732041

=== Experiment 3362 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006442061598752735
rmse: 0.08026245447749984
mae: 0.0329722383020417
r2: 0.7095302778803809
pearson: 0.8424054312140986

=== Experiment 3063 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005427902285906634
rmse: 0.07367429867943524
mae: 0.028256007098806567
r2: 0.7552582749309625
pearson: 0.8691228656398046

=== Experiment 3258 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007504599842751551
rmse: 0.08662909351223497
mae: 0.0435471395369474
r2: 0.6616208961173193
pearson: 0.8198781634488677

=== Experiment 3240 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005735800919801576
rmse: 0.07573507060669829
mae: 0.029323967952098372
r2: 0.741375261045192
pearson: 0.8618500984814498

=== Experiment 3279 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006514913594093541
rmse: 0.08071501467566948
mae: 0.03848138332803392
r2: 0.7062454134750792
pearson: 0.8433435640337951

=== Experiment 3308 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010813111239073418
rmse: 0.10398611079886302
mae: 0.043651802670866914
r2: 0.5124415734443923
pearson: 0.7175074873582465

=== Experiment 3318 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0071303033269803015
rmse: 0.08444112343509116
mae: 0.03994171668762613
r2: 0.6784977612729499
pearson: 0.8271060822252962

=== Experiment 3298 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007178560850332582
rmse: 0.08472638815819179
mae: 0.03414552592123624
r2: 0.6763218507847557
pearson: 0.8359335693346687

=== Experiment 3294 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.012751941327010113
rmse: 0.11292449391965462
mae: 0.06479280959007881
r2: 0.4250205781237072
pearson: 0.6889887423147726

=== Experiment 3278 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.010938471085523795
rmse: 0.10458714589051464
mae: 0.04380151809559321
r2: 0.5067891531430325
pearson: 0.7616378435184306

=== Experiment 3097 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0061241171989543895
rmse: 0.07825673900025729
mae: 0.030147094777808053
r2: 0.7238662509292564
pearson: 0.8508705643126683

=== Experiment 3108 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.01058898489805349
rmse: 0.10290279344144886
mae: 0.050224745653491144
r2: 0.5225473315154343
pearson: 0.732039011458061

=== Experiment 3370 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066618893601757875
rmse: 0.08162039794178773
mae: 0.038197638492947335
r2: 0.6996183408714125
pearson: 0.8379476418739163

=== Experiment 3394 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007374918636984512
rmse: 0.08587734647149102
mae: 0.03280271385251048
r2: 0.6674681646082915
pearson: 0.8238753118527594

=== Experiment 3387 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006859616632630737
rmse: 0.08282280261275114
mae: 0.03457128628172187
r2: 0.6907029051828466
pearson: 0.8313018769248312

=== Experiment 3262 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.01153187625245313
rmse: 0.10738657389289002
mae: 0.05074513433803983
r2: 0.4800327753437762
pearson: 0.6929628213251456

=== Experiment 3393 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007013459807804991
rmse: 0.08374640176034426
mae: 0.03582130711495924
r2: 0.6837661841257989
pearson: 0.827121340372828

=== Experiment 3331 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007178840288427675
rmse: 0.08472803720391306
mae: 0.0430538580125991
r2: 0.6763092510440383
pearson: 0.8257689495929895

=== Experiment 3403 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007782035095863133
rmse: 0.08821584379159525
mae: 0.04011492852751552
r2: 0.6491114626631109
pearson: 0.8081749390171827

=== Experiment 3213 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006928105753310078
rmse: 0.08323524345678385
mae: 0.03627845304926966
r2: 0.6876147608757823
pearson: 0.8377128399586831

=== Experiment 3305 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.016041250610861015
rmse: 0.12665405880137048
mae: 0.05303086868994451
r2: 0.2767070702505998
pearson: 0.5354256173088314

=== Experiment 3359 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006334167263774833
rmse: 0.07958748182833048
mae: 0.036128889323474335
r2: 0.714395185956606
pearson: 0.8464893605522824

=== Experiment 3184 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007076497107820202
rmse: 0.08412191811781398
mae: 0.03286568353389234
r2: 0.6809238600129506
pearson: 0.8406695532484195

=== Experiment 3196 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006346495072208328
rmse: 0.07966489234417083
mae: 0.03835526992316631
r2: 0.7138393305002232
pearson: 0.8471992199634809

=== Experiment 3210 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006887986327038756
rmse: 0.0829938933117296
mae: 0.03324449984262099
r2: 0.6894237281484463
pearson: 0.8348009144647585

=== Experiment 3302 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006240310545581058
rmse: 0.07899563624391577
mae: 0.03694436144371118
r2: 0.7186271440704626
pearson: 0.8501795510318996

=== Experiment 3319 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0064393466041656735
rmse: 0.08024553946585239
mae: 0.03132645276101324
r2: 0.7096526957913509
pearson: 0.8443724450518864

=== Experiment 3283 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006283821808161304
rmse: 0.07927056079126288
mae: 0.03142413872545623
r2: 0.7166652403914906
pearson: 0.8614554557838691

=== Experiment 3284 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0064285319610169435
rmse: 0.08017812644990492
mae: 0.03724265233255989
r2: 0.7101403232910387
pearson: 0.847091384519171

=== Experiment 3382 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.011092259640636976
rmse: 0.10531979700244858
mae: 0.050681678876910684
r2: 0.4998548948804987
pearson: 0.7075529456230853

=== Experiment 3399 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076078152427722584
rmse: 0.08722279084489477
mae: 0.0467344077328826
r2: 0.6569669591589571
pearson: 0.8166915119612125

=== Experiment 3412 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008905687685778301
rmse: 0.0943699511803323
mae: 0.03860418768395437
r2: 0.5984464619411616
pearson: 0.8131883043517955

=== Experiment 3248 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007818109775181516
rmse: 0.08842007563433497
mae: 0.03749039319622592
r2: 0.6474848712503256
pearson: 0.8049241857214396

=== Experiment 3372 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006324276806054704
rmse: 0.07952532179158224
mae: 0.04034423004233557
r2: 0.7148411423420833
pearson: 0.8487799213649936

=== Experiment 3397 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006413573027766033
rmse: 0.08008478649385309
mae: 0.02876071771540475
r2: 0.7108148150073923
pearson: 0.8523500899575863

=== Experiment 3297 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006327644417472255
rmse: 0.07954649217578519
mae: 0.03409393830426237
r2: 0.7146892982254651
pearson: 0.8456778121909778

=== Experiment 3268 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005378701879452161
rmse: 0.07333963375591782
mae: 0.028768352203881877
r2: 0.7574766996032398
pearson: 0.8731193424091754

=== Experiment 3126 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0058232224498658966
rmse: 0.07631004160571463
mae: 0.03002702059862221
r2: 0.7374334627317496
pearson: 0.8588761958381879

=== Experiment 3420 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007298417137910693
rmse: 0.08543077395125655
mae: 0.04515673836267616
r2: 0.6709175835306452
pearson: 0.8260535967163571

=== Experiment 3201 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.014507464544555083
rmse: 0.12044693663416718
mae: 0.054212914900933266
r2: 0.34586480890946103
pearson: 0.5911649113791987

=== Experiment 3255 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008847416540474308
rmse: 0.09406070667645607
mae: 0.04100065409427936
r2: 0.6010738822358264
pearson: 0.8161833936805172

=== Experiment 3238 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006689294741048397
rmse: 0.0817881087998029
mae: 0.03639234343040725
r2: 0.6983826443098969
pearson: 0.8396206033379962

=== Experiment 3311 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007937904272035484
rmse: 0.08909491720651343
mae: 0.038829429030477224
r2: 0.6420833901127705
pearson: 0.8034491781763392

=== Experiment 3099 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00556291966692585
rmse: 0.07458498285128079
mae: 0.028062909526411165
r2: 0.7491704006465001
pearson: 0.8673665824027836

=== Experiment 3416 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00728863438858898
rmse: 0.08537349933433079
mae: 0.036648192068112866
r2: 0.6713586833918441
pearson: 0.8211919688081386

=== Experiment 3346 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006908214633176532
rmse: 0.08311567020229418
mae: 0.0381253290612637
r2: 0.6885116427278524
pearson: 0.8331870380383081

=== Experiment 3222 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006392017128280643
rmse: 0.07995009148387913
mae: 0.036021738748944175
r2: 0.7117867610277115
pearson: 0.8498625066918348

=== Experiment 3421 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006933536026456814
rmse: 0.08326785710258679
mae: 0.034757640006191665
r2: 0.687369912249642
pearson: 0.8300912601285048

=== Experiment 3347 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.012561443120841672
rmse: 0.11207784402298998
mae: 0.04756846887486629
r2: 0.43361005839516953
pearson: 0.685314944432612

=== Experiment 3105 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007112862874658348
rmse: 0.08433779031168856
mae: 0.035560121695100165
r2: 0.6792841435920165
pearson: 0.8253970194447761

=== Experiment 3388 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067873021957474
rmse: 0.082385084789344
mae: 0.038224258504977675
r2: 0.6939635313138985
pearson: 0.8368892025402797

=== Experiment 3329 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005562631782720161
rmse: 0.07458305291901211
mae: 0.029257516329043574
r2: 0.749183381218627
pearson: 0.866098826677893

=== Experiment 3423 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007019721766885019
rmse: 0.08378377985556046
mae: 0.03632053459730358
r2: 0.6834838351469796
pearson: 0.8285278038044105

=== Experiment 3169 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006508632103966241
rmse: 0.08067609375748333
mae: 0.035664772561844194
r2: 0.7065286431002238
pearson: 0.8405676628245947

=== Experiment 3275 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00939331579394628
rmse: 0.09691911985746816
mae: 0.042196234457440175
r2: 0.576459525165411
pearson: 0.7778310040143651

=== Experiment 3322 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00638441628454856
rmse: 0.07990254241604931
mae: 0.039349032093643044
r2: 0.7121294797262041
pearson: 0.8472134308306019

=== Experiment 3226 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.018417335589923464
rmse: 0.13571048445099393
mae: 0.055739825268167266
r2: 0.16957044433964918
pearson: 0.4309794130803643

=== Experiment 3417 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007246217935937535
rmse: 0.08512471988757164
mae: 0.035579734453432495
r2: 0.6732712225729892
pearson: 0.8225667901977551

=== Experiment 3263 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005708551427806497
rmse: 0.07555495634176819
mae: 0.03021348669615356
r2: 0.7426039286458315
pearson: 0.8641694726138566

=== Experiment 3307 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0064246649503925555
rmse: 0.08015400770013036
mae: 0.03134949955704279
r2: 0.7103146850980908
pearson: 0.8447288051021121

=== Experiment 3247 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006140447056632519
rmse: 0.07836100469386874
mae: 0.029626090306119436
r2: 0.7231299448338694
pearson: 0.8507744107151983

=== Experiment 3306 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0062870130512035945
rmse: 0.07929068703954831
mae: 0.028838416189943133
r2: 0.7165213486472871
pearson: 0.8549580192173479

=== Experiment 3185 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006114918330733619
rmse: 0.07819794326408859
mae: 0.0326912961355291
r2: 0.7242810238485993
pearson: 0.8577021785326447

=== Experiment 3327 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006897745630112487
rmse: 0.08305266780852068
mae: 0.035531001070649104
r2: 0.6889836854682495
pearson: 0.8303806718187007

=== Experiment 3386 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006829850688272213
rmse: 0.08264291069578934
mae: 0.03940271190857936
r2: 0.6920450385129784
pearson: 0.8348156781294098

=== Experiment 3191 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007871346290989597
rmse: 0.08872060804001287
mae: 0.04304604549210959
r2: 0.6450844601837218
pearson: 0.8035192744644994

=== Experiment 3303 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006451222943521923
rmse: 0.08031950537398698
mae: 0.03538506164886665
r2: 0.7091171968769543
pearson: 0.8425600089586943

=== Experiment 3431 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006904448937681996
rmse: 0.08309301377180874
mae: 0.0338980257579341
r2: 0.6886814362802918
pearson: 0.8408058013451242

=== Experiment 3141 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005971337199664883
rmse: 0.07727442785077664
mae: 0.028871817437879727
r2: 0.7307550338536001
pearson: 0.8548678012650666

=== Experiment 3146 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00547259589916863
rmse: 0.07397699574305941
mae: 0.03421771410095632
r2: 0.7532430595801427
pearson: 0.8698092466791436

=== Experiment 3140 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006175442634326405
rmse: 0.07858398459181365
mae: 0.029141601854796705
r2: 0.7215520096383836
pearson: 0.8519836895260914

=== Experiment 3273 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006153079767971495
rmse: 0.07844156913251733
mae: 0.03850075724566329
r2: 0.7225603414396764
pearson: 0.854337277603199

=== Experiment 3243 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005534039087907513
rmse: 0.07439112237295195
mae: 0.028626720343989682
r2: 0.7504726132431219
pearson: 0.8695987662245729

=== Experiment 3374 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006961438027055299
rmse: 0.08343523252832283
mae: 0.03363869116995123
r2: 0.6861118233232659
pearson: 0.8394813205771772

=== Experiment 3323 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005814592993584403
rmse: 0.07625347856710803
mae: 0.03609475911014222
r2: 0.7378225611173679
pearson: 0.8612968691877259

=== Experiment 3199 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006830723277765154
rmse: 0.08264818980331726
mae: 0.033308556880843804
r2: 0.6920056938367977
pearson: 0.84148787804171

=== Experiment 3324 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0062349686383139785
rmse: 0.07896181759758307
mae: 0.030834238721379064
r2: 0.7188680083179819
pearson: 0.8556300186362648

=== Experiment 3310 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0073175117803079325
rmse: 0.08554245601049769
mae: 0.03515609588647983
r2: 0.6700566145091486
pearson: 0.8253562926877727

=== Experiment 3433 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0075488166944191635
rmse: 0.08688392655962991
mae: 0.04368036685888616
r2: 0.6596271777369513
pearson: 0.8172773542989342

=== Experiment 3373 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006686449182137877
rmse: 0.08177071102869216
mae: 0.03612329861868951
r2: 0.698510949308746
pearson: 0.841071162292913

=== Experiment 3464 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007993720041009738
rmse: 0.08940760616977583
mae: 0.039468121572911435
r2: 0.6395666816561201
pearson: 0.818960564674761

=== Experiment 3443 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00747461333751712
rmse: 0.08645584617316009
mae: 0.03966041246519219
r2: 0.6629729744402695
pearson: 0.8243800895646204

=== Experiment 3159 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005970127981745208
rmse: 0.07726660327557572
mae: 0.029156322103627487
r2: 0.7308095569573807
pearson: 0.8562733976534667

=== Experiment 3450 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006932520575636259
rmse: 0.08326175938350246
mae: 0.03697340471702692
r2: 0.6874156984802067
pearson: 0.8299562789897421

=== Experiment 3459 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007428482426282304
rmse: 0.08618864441608479
mae: 0.038130414882339043
r2: 0.6650529969240805
pearson: 0.8162878896838766

=== Experiment 3065 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006139413389009565
rmse: 0.07835440886771826
mae: 0.032022524383563775
r2: 0.7231765524520366
pearson: 0.8514575164597642

=== Experiment 3430 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005678486529124864
rmse: 0.07535573322000698
mae: 0.030197513597796162
r2: 0.7439595416948124
pearson: 0.8626290042909653

=== Experiment 3249 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006306410222504485
rmse: 0.07941290967156715
mae: 0.03033959040906463
r2: 0.715646738730678
pearson: 0.8539078396730679

=== Experiment 3266 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008833991577909673
rmse: 0.09398931629663912
mae: 0.04031442007258245
r2: 0.6016792078890878
pearson: 0.8076164377453303

=== Experiment 3100 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005861587685669417
rmse: 0.07656100630000508
mae: 0.03139481829154112
r2: 0.7357035911352003
pearson: 0.8595087001853852

=== Experiment 3212 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007783982447060199
rmse: 0.08822688052436287
mae: 0.03804081195091412
r2: 0.6490236574547301
pearson: 0.8174164451390677

=== Experiment 3340 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007011865631119162
rmse: 0.08373688333774527
mae: 0.030711148246171435
r2: 0.6838380648509123
pearson: 0.8476328685989543

=== Experiment 3280 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008872490858127711
rmse: 0.09419390032336336
mae: 0.0414371898270302
r2: 0.5999432922888855
pearson: 0.7843547637600982

=== Experiment 3320 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00588387402143358
rmse: 0.07670641447384684
mae: 0.032835374588035746
r2: 0.7346987100645608
pearson: 0.8628284454686124

=== Experiment 3377 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0059061210261707395
rmse: 0.07685129163632021
mae: 0.029223572741752326
r2: 0.733695602412618
pearson: 0.859995366800469

=== Experiment 3462 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007584157964375637
rmse: 0.08708707116659531
mae: 0.03455659637476152
r2: 0.6580336554294963
pearson: 0.8156976639620052

=== Experiment 3365 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0072022213520149736
rmse: 0.08486590217522567
mae: 0.03404467625633875
r2: 0.6752550091776799
pearson: 0.8410061098277211

=== Experiment 3242 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00787687597167243
rmse: 0.0887517660200203
mae: 0.04082978861979301
r2: 0.6448351293155314
pearson: 0.8054923767857637

=== Experiment 3442 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007256569041983598
rmse: 0.0851854978384443
mae: 0.04703097954470144
r2: 0.6728044957572424
pearson: 0.8298472794248608

=== Experiment 3402 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006496963232090297
rmse: 0.08060374204768844
mae: 0.03139371954194667
r2: 0.7070547873972468
pearson: 0.8444396328423649

=== Experiment 3300 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006338757438855343
rmse: 0.079616313898945
mae: 0.037340129897907165
r2: 0.7141882169825164
pearson: 0.8523558100583739

=== Experiment 3176 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006739636601594502
rmse: 0.0820952897649707
mae: 0.03571571371190859
r2: 0.6961127519750203
pearson: 0.8349347927693966

=== Experiment 3463 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006376588166124532
rmse: 0.0798535419760735
mae: 0.033099318412156416
r2: 0.712482446140525
pearson: 0.859938747776217

=== Experiment 3444 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007301850906758133
rmse: 0.0854508683791928
mae: 0.046492275250212114
r2: 0.6707627564046313
pearson: 0.8276493143583482

=== Experiment 3482 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008300355806287994
rmse: 0.09110628851121087
mae: 0.045877130703947895
r2: 0.6257406099604448
pearson: 0.8016477094060788

=== Experiment 3345 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008406656692226752
rmse: 0.09168782194068496
mae: 0.037918714758102594
r2: 0.6209475498000641
pearson: 0.797947623908178

=== Experiment 3375 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006122541864268822
rmse: 0.07824667318339369
mae: 0.03806484864386772
r2: 0.7239372820768708
pearson: 0.8523764550626567

=== Experiment 3339 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006043647395704754
rmse: 0.07774089911819103
mae: 0.02955876174355507
r2: 0.7274945989403134
pearson: 0.8577883553232483

=== Experiment 3288 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005934441091850715
rmse: 0.07703532366291918
mae: 0.028100130377055027
r2: 0.732418663115722
pearson: 0.8651691054918941

=== Experiment 3408 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006094582775255193
rmse: 0.07806780882832048
mae: 0.036817629495563324
r2: 0.7251979450947592
pearson: 0.8524184055739695

=== Experiment 3254 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006399968926565309
rmse: 0.07999980579079745
mae: 0.02947119171573932
r2: 0.7114282179429101
pearson: 0.8447497163444689

=== Experiment 3446 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007589389760304831
rmse: 0.08711710371852838
mae: 0.033452446608738896
r2: 0.6577977560537384
pearson: 0.81407095299017

=== Experiment 3485 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007391812481243077
rmse: 0.08597565051363716
mae: 0.03872477721232838
r2: 0.6667064286062223
pearson: 0.8182076219652146

=== Experiment 3343 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007445092137859046
rmse: 0.08628494734227428
mae: 0.04164018525378225
r2: 0.6643040723395783
pearson: 0.8192795450875383

=== Experiment 3436 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007261030399876095
rmse: 0.0852116799498525
mae: 0.03449068084189162
r2: 0.6726033350934633
pearson: 0.8366267212176267

=== Experiment 3282 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006800717372867838
rmse: 0.08246646210956207
mae: 0.03437177772561172
r2: 0.693358646882009
pearson: 0.8333004887489466

=== Experiment 3250 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006767891223636557
rmse: 0.0822671940911841
mae: 0.032871909842771885
r2: 0.6948387635029541
pearson: 0.8421878645017166

=== Experiment 3344 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0058489813548643806
rmse: 0.07647863332241484
mae: 0.034641789258919135
r2: 0.7362720050427289
pearson: 0.8622568373858703

=== Experiment 3363 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006840403369452935
rmse: 0.0827067311012407
mae: 0.03455025347495598
r2: 0.6915692227631298
pearson: 0.8317953388511437

=== Experiment 3400 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006877697967057638
rmse: 0.08293188751654962
mae: 0.03436683877604257
r2: 0.6898876257717415
pearson: 0.8312177022530428

=== Experiment 3496 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006900451588912354
rmse: 0.08306895683052938
mae: 0.03567102681735081
r2: 0.6888616749769472
pearson: 0.8302823143415744

=== Experiment 3312 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005990480290747607
rmse: 0.07739819307159312
mae: 0.03150813138647679
r2: 0.7298918803021988
pearson: 0.8569361339580204

=== Experiment 3449 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007348752246452094
rmse: 0.08572486364207349
mae: 0.0369675776444977
r2: 0.6686479956406889
pearson: 0.8215206426986358

=== Experiment 3270 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006535076538415746
rmse: 0.08083982025224788
mae: 0.03511015934028902
r2: 0.7053362751899814
pearson: 0.8422166757011371

=== Experiment 3516 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0073074642530738445
rmse: 0.0854837075299957
mae: 0.03780680675898974
r2: 0.6705096530898786
pearson: 0.821737558088458

=== Experiment 3428 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00672253407467413
rmse: 0.08199106094370368
mae: 0.03448193052661112
r2: 0.6968838973864623
pearson: 0.8527368191760843

=== Experiment 3511 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007692379689013045
rmse: 0.08770621237411319
mae: 0.03466460454517521
r2: 0.6531539854976193
pearson: 0.8195834679026641

=== Experiment 3368 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007288745103496145
rmse: 0.08537414774682173
mae: 0.038364761692497605
r2: 0.6713536913054092
pearson: 0.8226527714055688

=== Experiment 3336 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006372921976529083
rmse: 0.07983058296498331
mae: 0.03663130324513177
r2: 0.7126477530157704
pearson: 0.8474349635434045

=== Experiment 3234 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0071925090231869234
rmse: 0.08480866125100033
mae: 0.03369434040154081
r2: 0.6756929338098141
pearson: 0.8421368734403387

=== Experiment 3503 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0064849909076294945
rmse: 0.08052944124746858
mae: 0.03635975009663026
r2: 0.7075946142377615
pearson: 0.8424804420479226

=== Experiment 3118 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.004895783516916831
rmse: 0.06996987578177362
mae: 0.0261314041549612
r2: 0.7792512760213319
pearson: 0.8897707108332005

=== Experiment 3217 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006595503956115547
rmse: 0.08121270809494993
mae: 0.028975551695536544
r2: 0.7026116295220378
pearson: 0.8383133860666262

=== Experiment 3287 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009867565397867512
rmse: 0.09933561998531801
mae: 0.04350027623242275
r2: 0.555075819257816
pearson: 0.7674071709891286

=== Experiment 3435 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00696369756221138
rmse: 0.08344877208330498
mae: 0.04282758220135479
r2: 0.6860099418775757
pearson: 0.8416231080608363

=== Experiment 3413 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007839024826036349
rmse: 0.08853826757982307
mae: 0.03407399035685218
r2: 0.6465418208126992
pearson: 0.8229516850500361

=== Experiment 3203 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006935133849883397
rmse: 0.08327745102897541
mae: 0.03443663490514702
r2: 0.6872978670945933
pearson: 0.8325098791623778

=== Experiment 3414 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006347763807547973
rmse: 0.07967285489768754
mae: 0.03592646513007306
r2: 0.7137821237821717
pearson: 0.8470712841030392

=== Experiment 3325 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008180311552650494
rmse: 0.09044507478381834
mae: 0.04030246823511209
r2: 0.631153352521443
pearson: 0.7985342821536856

=== Experiment 3410 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006804752266186622
rmse: 0.08249092232595429
mae: 0.03265759473477815
r2: 0.6931767153181574
pearson: 0.8429416068933926

=== Experiment 3181 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00720849180853907
rmse: 0.08490283745870376
mae: 0.03162268908006051
r2: 0.6749722770528475
pearson: 0.8363702484903054

=== Experiment 3509 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006297174684474301
rmse: 0.07935473952117984
mae: 0.03319938408910183
r2: 0.7160631650755878
pearson: 0.8463067402732289

=== Experiment 3523 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00726793945518922
rmse: 0.08525221085220734
mae: 0.042710669822487404
r2: 0.6722918088303023
pearson: 0.824893713987728

=== Experiment 3472 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006393901600740658
rmse: 0.07996187592059517
mae: 0.03755704565209621
r2: 0.7117017909939092
pearson: 0.846530428352357

=== Experiment 3491 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006081194365701691
rmse: 0.07798201309085122
mae: 0.03544176091456879
r2: 0.7258016225888368
pearson: 0.8521954567651124

=== Experiment 3314 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005903172801628463
rmse: 0.07683210788224193
mae: 0.033848329539954175
r2: 0.7338285365596164
pearson: 0.8591150831824033

=== Experiment 3337 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005353458774425088
rmse: 0.07316733406667957
mae: 0.02607963786058661
r2: 0.7586149000985705
pearson: 0.8716285057247251

=== Experiment 3426 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007000204538028017
rmse: 0.08366722499299244
mae: 0.03668198134423317
r2: 0.6843638584059524
pearson: 0.8433928908892676

=== Experiment 3515 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006583043942860791
rmse: 0.08113595961631803
mae: 0.0395934989241273
r2: 0.7031734460356236
pearson: 0.8405051533878256

=== Experiment 3389 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007113360012273171
rmse: 0.08434073756064249
mae: 0.03787089553643117
r2: 0.6792617278757713
pearson: 0.8354543617350229

=== Experiment 3341 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006376386343734952
rmse: 0.0798522782626454
mae: 0.03359524936515623
r2: 0.7124915462232424
pearson: 0.8442056454710789

=== Experiment 3396 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006769312826864845
rmse: 0.0822758337962299
mae: 0.03139962996336471
r2: 0.6947746640390862
pearson: 0.836198700726933

=== Experiment 3569 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006912316111004365
rmse: 0.08314033985379399
mae: 0.034188399469102466
r2: 0.6883267088977939
pearson: 0.836706749779204

=== Experiment 3495 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007044004367302428
rmse: 0.08392856705140644
mae: 0.03143574409087583
r2: 0.6823889433817483
pearson: 0.8295662168320377

=== Experiment 3570 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007594437928940883
rmse: 0.08714607236669294
mae: 0.03604537236192762
r2: 0.6575701363518078
pearson: 0.8345012312954712

=== Experiment 3476 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006822963642383252
rmse: 0.08260123269287967
mae: 0.040964552048491365
r2: 0.6923555723808908
pearson: 0.8349691086578327

=== Experiment 3533 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00823141741520667
rmse: 0.0907271591928606
mae: 0.049267497028228364
r2: 0.6288490116721953
pearson: 0.8074980745320636

=== Experiment 3580 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007566003746941462
rmse: 0.08698277845034304
mae: 0.043663556831688224
r2: 0.6588522211033212
pearson: 0.8211205285443578

=== Experiment 3565 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006627245743382848
rmse: 0.08140789730353468
mae: 0.034504447751630724
r2: 0.7011804063047842
pearson: 0.8392546176020073

=== Experiment 3543 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007824414882369048
rmse: 0.08845572272255225
mae: 0.04087684294554799
r2: 0.6472005767423298
pearson: 0.8200676446325027

=== Experiment 3579 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0070108702253500865
rmse: 0.0837309394749043
mae: 0.03677700462924095
r2: 0.6838829472589283
pearson: 0.8273640027527344

=== Experiment 3510 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006317977670666803
rmse: 0.07948570733576447
mae: 0.03610044594272102
r2: 0.7151251675842623
pearson: 0.8472837726613784

=== Experiment 3536 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006984538410383517
rmse: 0.08357355090208575
mae: 0.033840486544097215
r2: 0.6850702372062527
pearson: 0.8282212761223661

=== Experiment 3532 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0072264353230490725
rmse: 0.08500844265747416
mae: 0.042779643703533085
r2: 0.674163211881136
pearson: 0.8340324139147851

=== Experiment 3429 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007013041170641288
rmse: 0.08374390228930872
mae: 0.035103713485174534
r2: 0.6837850602912542
pearson: 0.8377365871724087

=== Experiment 3518 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007483907239733455
rmse: 0.0865095788900481
mae: 0.04458298392566659
r2: 0.662553916479894
pearson: 0.822616125096196

=== Experiment 3520 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007077518440745201
rmse: 0.08412798845060543
mae: 0.04253935419242386
r2: 0.6808778085608855
pearson: 0.8305645748181768

=== Experiment 3241 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009166980813223952
rmse: 0.09574435133846775
mae: 0.03973732713656594
r2: 0.5866648698285375
pearson: 0.7995418479581025

=== Experiment 3315 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006028956166948229
rmse: 0.07764635321087675
mae: 0.029617114583974664
r2: 0.7281570199788409
pearson: 0.8554202480192313

=== Experiment 3304 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007298688704816328
rmse: 0.08543236333390485
mae: 0.038757401554044946
r2: 0.6709053386983959
pearson: 0.8219619134464874

=== Experiment 3624 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006984173332259563
rmse: 0.08357136670091954
mae: 0.03543435922988265
r2: 0.6850866984181787
pearson: 0.8333295874723566

=== Experiment 3591 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009703035931316526
rmse: 0.09850398941827954
mae: 0.04203345834891506
r2: 0.5624943804896436
pearson: 0.7570401705973046

=== Experiment 3529 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006443964126947523
rmse: 0.08027430552142772
mae: 0.0355945797221167
r2: 0.7094444937214446
pearson: 0.8426281799408466

=== Experiment 3610 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007402159175918742
rmse: 0.08603580171021098
mae: 0.04635128747466158
r2: 0.666239900697225
pearson: 0.82528680048507

=== Experiment 3528 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006767491468551809
rmse: 0.082264764441113
mae: 0.0386233674465419
r2: 0.69485678828378
pearson: 0.8343884056517782

=== Experiment 3557 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0072921540376928735
rmse: 0.0853941100878326
mae: 0.035208007857648174
r2: 0.6711999839628655
pearson: 0.8297032584350568

=== Experiment 3553 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006611202098629847
rmse: 0.08130929896776781
mae: 0.03345015329055663
r2: 0.701903807185349
pearson: 0.8399871368341704

=== Experiment 3649 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006253691195240341
rmse: 0.07908028322685966
mae: 0.03121193527958289
r2: 0.7180238164665997
pearson: 0.8475505203583978

=== Experiment 3506 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0068075805655632565
rmse: 0.08250806363964226
mae: 0.03935958469837718
r2: 0.6930491885440946
pearson: 0.8360171393929031

=== Experiment 3497 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006502574535944175
rmse: 0.08063854249640288
mae: 0.039507333612600726
r2: 0.7068017761761993
pearson: 0.841899357459502

=== Experiment 3661 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008191620487778392
rmse: 0.0905075714389597
mae: 0.03545702322890642
r2: 0.6306434376139691
pearson: 0.83372392799756

=== Experiment 3666 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008362443096252389
rmse: 0.09144639465967146
mae: 0.04434201972832852
r2: 0.6229411213826563
pearson: 0.8114016024743941

=== Experiment 3542 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00727605598344424
rmse: 0.08529980060612241
mae: 0.04443009672207105
r2: 0.6719258381436408
pearson: 0.8241131156069896

=== Experiment 3550 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0067353704997076295
rmse: 0.08206930302925466
mae: 0.03649688697120076
r2: 0.6963051086314445
pearson: 0.8400787202880572

=== Experiment 3646 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006714035380331198
rmse: 0.08193921759652821
mae: 0.03416031283809014
r2: 0.6972670997738235
pearson: 0.8385486605923784

=== Experiment 3333 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005809377338383622
rmse: 0.07621927143697729
mae: 0.03226222648462702
r2: 0.7380577327147859
pearson: 0.8592979698696719

=== Experiment 3566 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007063912663782506
rmse: 0.08404708599221335
mae: 0.040756108017130976
r2: 0.681491287055773
pearson: 0.8315617411850436

=== Experiment 3608 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007272627078994578
rmse: 0.08527969910239235
mae: 0.04485137378030359
r2: 0.6720804459360998
pearson: 0.8279056145091759

=== Experiment 3589 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006511023834918798
rmse: 0.0806909154423148
mae: 0.03291105224340766
r2: 0.7064208010042543
pearson: 0.8405615665760675

=== Experiment 3574 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007166354827039745
rmse: 0.0846543255069683
mae: 0.04658362497102322
r2: 0.6768722150027482
pearson: 0.8314929413168705

=== Experiment 3264 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006075692287246091
rmse: 0.0779467272388398
mae: 0.030944855284803258
r2: 0.72604970888475
pearson: 0.8541101349096318

=== Experiment 3669 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006421914695133658
rmse: 0.0801368497954197
mae: 0.03443275897507865
r2: 0.7104386928972346
pearson: 0.8450436859665846

=== Experiment 3609 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007129041264327614
rmse: 0.08443365007109199
mae: 0.0369989404772802
r2: 0.6785546671224832
pearson: 0.8237611449776634

=== Experiment 3621 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00681103086325844
rmse: 0.08252896984246465
mae: 0.03720847822907815
r2: 0.6928936161396111
pearson: 0.8334419081343072

=== Experiment 3501 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006763224583973211
rmse: 0.08223882649924676
mae: 0.042067103288323984
r2: 0.6950491802314295
pearson: 0.8418603060815326

=== Experiment 3643 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0072857676122545
rmse: 0.08535670806828541
mae: 0.04119569896442845
r2: 0.671487945075001
pearson: 0.8224956518124737

=== Experiment 3623 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00759709194700292
rmse: 0.08716129844720603
mae: 0.03454316599370927
r2: 0.6574504678455133
pearson: 0.8375127035492643

=== Experiment 3659 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067750360064171376
rmse: 0.08231060688888849
mae: 0.04010524494280556
r2: 0.6945166083920373
pearson: 0.8347810065586239

=== Experiment 3480 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00714581714363049
rmse: 0.08453293525975832
mae: 0.034813326696976364
r2: 0.6777982501083457
pearson: 0.8318610028509473

=== Experiment 3409 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006006343674924467
rmse: 0.077500604351995
mae: 0.0319657049132874
r2: 0.7291766072916734
pearson: 0.8583624119420626

=== Experiment 3335 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005796080281297667
rmse: 0.07613199249525568
mae: 0.02917847912290568
r2: 0.7386572911663091
pearson: 0.8687908608257785

=== Experiment 3504 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00578833291561572
rmse: 0.07608109433765868
mae: 0.028767241730842014
r2: 0.7390066164750317
pearson: 0.8681031859138088

=== Experiment 3687 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0072111728600214
rmse: 0.08491862493011412
mae: 0.03920525304485624
r2: 0.6748513896215302
pearson: 0.8220240893852832

=== Experiment 3481 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006474502224804506
rmse: 0.08046429161314046
mae: 0.032538590111390917
r2: 0.708067544329918
pearson: 0.844325816339878

=== Experiment 3678 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007009342544835479
rmse: 0.08372181642102301
mae: 0.03588905817662758
r2: 0.6839518297009484
pearson: 0.8274858291433543

=== Experiment 3626 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006916976995762388
rmse: 0.08316836535463702
mae: 0.03829142872232543
r2: 0.688116551655467
pearson: 0.8313439529083552

=== Experiment 3619 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006886886566542165
rmse: 0.08298726749653929
mae: 0.030945642866590035
r2: 0.689473315865196
pearson: 0.8338018596859588

=== Experiment 3381 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005661522197628861
rmse: 0.07524308737438185
mae: 0.029446901506668427
r2: 0.7447244559353938
pearson: 0.8663905043311783

=== Experiment 3261 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006262480041595789
rmse: 0.07913583285462907
mae: 0.036294668003866375
r2: 0.7176275312526996
pearson: 0.8513692109898601

=== Experiment 3385 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006693310370591005
rmse: 0.08181265409819562
mae: 0.03883777936998653
r2: 0.6982015813412346
pearson: 0.8395889136000074

=== Experiment 3698 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00798470430693489
rmse: 0.08935717266641156
mae: 0.035533113081587755
r2: 0.639973197137427
pearson: 0.801653448620948

=== Experiment 3499 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006618608185040255
rmse: 0.08135482889810693
mae: 0.03824459691546015
r2: 0.7015698700087716
pearson: 0.8388591195280822

=== Experiment 3655 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00571672120797074
rmse: 0.07560900216224745
mae: 0.03267425400870345
r2: 0.7422355568540209
pearson: 0.8622179451669111

=== Experiment 3689 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007459342021376481
rmse: 0.08636748243046384
mae: 0.03832132509427816
r2: 0.6636615513636306
pearson: 0.8152496638807637

=== Experiment 3706 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007137018780229668
rmse: 0.08448087819281751
mae: 0.04000451930807616
r2: 0.6781949644416329
pearson: 0.8248333747823564

=== Experiment 3366 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00659057089721412
rmse: 0.08118233118859128
mae: 0.03420808014430842
r2: 0.7028340589766979
pearson: 0.8387878563336649

=== Experiment 3537 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00516500408529651
rmse: 0.07186796285756616
mae: 0.027883883341446626
r2: 0.7671122390861261
pearson: 0.8761762861181768

=== Experiment 3596 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006835204341172829
rmse: 0.08267529462404612
mae: 0.04002063419383947
r2: 0.6918036446600125
pearson: 0.8340239050368005

=== Experiment 3664 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006270805889058249
rmse: 0.07918842017023858
mae: 0.0346399692900799
r2: 0.7172521224551032
pearson: 0.8469900941787136

=== Experiment 3654 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00591687585650607
rmse: 0.07692123150669176
mae: 0.029699147061177624
r2: 0.7332106718463607
pearson: 0.8584743420480916

=== Experiment 3662 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006796861064393075
rmse: 0.08244307772271166
mae: 0.039318546311596765
r2: 0.693532526133845
pearson: 0.8351301327165542

=== Experiment 3753 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00886588708423887
rmse: 0.09415883965002367
mae: 0.040203089693026314
r2: 0.6002410535469902
pearson: 0.8110686434086494

=== Experiment 3317 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006036677163247113
rmse: 0.07769605629146896
mae: 0.029346018763698488
r2: 0.7278088836539945
pearson: 0.8536989208175539

=== Experiment 3351 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007758679861787171
rmse: 0.08808336881493108
mae: 0.03869231218400436
r2: 0.6501645398881716
pearson: 0.816402891357081

=== Experiment 3584 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0058732447660567845
rmse: 0.07663709784469128
mae: 0.029623230114123704
r2: 0.7351779785112076
pearson: 0.8576128407794884

=== Experiment 3471 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007612089900019402
rmse: 0.08724729164862026
mae: 0.03708902195516786
r2: 0.6567742167451041
pearson: 0.8188447870731311

=== Experiment 3748 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007590384616322433
rmse: 0.08712281340913201
mae: 0.03530294185996155
r2: 0.6577528984337744
pearson: 0.8147749472235113

=== Experiment 3602 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007133540654294083
rmse: 0.08446029039906318
mae: 0.03725656246976115
r2: 0.6783517916092281
pearson: 0.8290413470532443

=== Experiment 3720 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007094669155624198
rmse: 0.08422985905024535
mae: 0.03199656012451114
r2: 0.6801044903755984
pearson: 0.8262190900052581

=== Experiment 3398 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005893787382231799
rmse: 0.0767710061301257
mae: 0.03052113799743409
r2: 0.7342517209893729
pearson: 0.8585985603491577

=== Experiment 3600 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006599604339239682
rmse: 0.08123794888621746
mae: 0.038789963298102935
r2: 0.7024267450516872
pearson: 0.8394717130568226

=== Experiment 3269 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006347772437950575
rmse: 0.07967290905916875
mae: 0.027694304508313325
r2: 0.7137817346411169
pearson: 0.8622054907079129

=== Experiment 3770 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007845187271831037
rmse: 0.08857306177292866
mae: 0.043681601647188456
r2: 0.6462639588441165
pearson: 0.8071108821433842

=== Experiment 3734 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00669142353529117
rmse: 0.08180112184616523
mae: 0.03993419329852262
r2: 0.6982866579144443
pearson: 0.8382382681282534

=== Experiment 3576 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067675435194806685
rmse: 0.08226508080273591
mae: 0.03619406902992831
r2: 0.694854441330307
pearson: 0.8405361465411479

=== Experiment 3422 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005681346003821895
rmse: 0.07537470400487085
mae: 0.03217648568946696
r2: 0.7438306092392742
pearson: 0.8632453033209572

=== Experiment 3692 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006540432594924831
rmse: 0.08087294105524313
mae: 0.0392932321813047
r2: 0.7050947729593675
pearson: 0.8425709584795462

=== Experiment 3771 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008468952834651572
rmse: 0.09202691364297497
mae: 0.040824274035052475
r2: 0.618138644156758
pearson: 0.7983348277285174

=== Experiment 3745 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006315796012528684
rmse: 0.07947198256321962
mae: 0.03589636153912611
r2: 0.7152235375894889
pearson: 0.8470311831349037

=== Experiment 3739 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007109367880705094
rmse: 0.08431706755280982
mae: 0.04059686355364773
r2: 0.6794417313311043
pearson: 0.827824045377469

=== Experiment 3749 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007939971197509011
rmse: 0.0891065160216076
mae: 0.0446808326447231
r2: 0.6419901933528926
pearson: 0.8038760656191248

=== Experiment 3349 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007303072104353654
rmse: 0.08545801369300397
mae: 0.03298798001337785
r2: 0.6707076931425394
pearson: 0.8231985249245117

=== Experiment 3411 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0061828852139021884
rmse: 0.0786313246353016
mae: 0.035385382145843254
r2: 0.7212164270010415
pearson: 0.8567590292986572

=== Experiment 3378 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0060762926530473545
rmse: 0.07795057827269375
mae: 0.040263201057841654
r2: 0.726022638655012
pearson: 0.857673316168916

=== Experiment 3728 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0069124312431217
rmse: 0.08314103224715039
mae: 0.03610360849654573
r2: 0.6883215176412967
pearson: 0.8309587541757595

=== Experiment 3489 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006047434155980361
rmse: 0.07776525031130782
mae: 0.03068396441696034
r2: 0.727323855586167
pearson: 0.8555481253838552

=== Experiment 3558 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007343370670175566
rmse: 0.08569346923876735
mae: 0.03658537888233019
r2: 0.6688906485463847
pearson: 0.8297109489495226

=== Experiment 3761 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007034348933622916
rmse: 0.08387102559062287
mae: 0.04313708994358146
r2: 0.6828243026366762
pearson: 0.8308175233674661

=== Experiment 3525 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005506352203267358
rmse: 0.07420479905819675
mae: 0.03216677742968616
r2: 0.7517210026856167
pearson: 0.8682938648935806

=== Experiment 3376 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006222068913612952
rmse: 0.07888009199800006
mae: 0.030331011139934666
r2: 0.7194496512271482
pearson: 0.8516251015462137

=== Experiment 3552 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007854891792002602
rmse: 0.08862782741330513
mae: 0.04256080838350689
r2: 0.6458263863008665
pearson: 0.826364571767041

=== Experiment 3390 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006099877306866505
rmse: 0.07810171129281679
mae: 0.028708035276759636
r2: 0.7249592169947066
pearson: 0.8547396604548927

=== Experiment 3556 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007691008630985742
rmse: 0.08769839582903294
mae: 0.04361045212818518
r2: 0.6532158059006214
pearson: 0.8133806294514581

=== Experiment 3668 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0067523861212175785
rmse: 0.08217290381395548
mae: 0.03687618769438797
r2: 0.6955378817467086
pearson: 0.8410102178756821

=== Experiment 3747 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006813106811102044
rmse: 0.08254154596990564
mae: 0.03164629676473008
r2: 0.6928000125650373
pearson: 0.8383728715501074

=== Experiment 3560 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006819857922710967
rmse: 0.0825824310777478
mae: 0.04379601632585484
r2: 0.6924956079139825
pearson: 0.8416814468911542

=== Experiment 3735 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006033695057456207
rmse: 0.0776768630768275
mae: 0.032617691845433436
r2: 0.7279433454915812
pearson: 0.8583256878829714

=== Experiment 3676 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008443475724529343
rmse: 0.09188838732140936
mae: 0.03742547286478807
r2: 0.6192873958388354
pearson: 0.821411203476865

=== Experiment 3573 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006143469218186316
rmse: 0.07838028590268292
mae: 0.029463809710240676
r2: 0.7229936768995637
pearson: 0.8513819054874345

=== Experiment 3620 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006640229977326631
rmse: 0.0814876062805052
mae: 0.038374693235848634
r2: 0.7005949529110573
pearson: 0.8420559495915277

=== Experiment 3559 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00577523164306792
rmse: 0.07599494485206183
mae: 0.03151868432904471
r2: 0.7395973470879009
pearson: 0.8629352477621403

=== Experiment 3507 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006086297173653301
rmse: 0.07801472408240191
mae: 0.03637367841983875
r2: 0.7255715392242161
pearson: 0.8539958042429161

=== Experiment 3726 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007373535870825474
rmse: 0.0858692952738374
mae: 0.04217428882747049
r2: 0.6675305129258005
pearson: 0.8204366694229328

=== Experiment 3702 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005923416611291128
rmse: 0.0769637356895514
mae: 0.029022895001503145
r2: 0.7329157520919088
pearson: 0.8612116899671606

=== Experiment 3717 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006605454052985836
rmse: 0.08127394449013679
mae: 0.034115082820132765
r2: 0.7021629840335291
pearson: 0.8413052026095873

=== Experiment 3582 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0090470429611027
rmse: 0.0951159448310466
mae: 0.043890315872265925
r2: 0.5920728147919991
pearson: 0.7704743353095136

=== Experiment 3694 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006440807814424627
rmse: 0.08025464356923297
mae: 0.03928018872367878
r2: 0.7095868104639026
pearson: 0.8478944812389039

=== Experiment 3777 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006740064559548481
rmse: 0.0820978961944122
mae: 0.03866428350311627
r2: 0.6960934555392342
pearson: 0.8353513044791518

=== Experiment 3634 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00751525509332591
rmse: 0.08669057095974111
mae: 0.038248102710353914
r2: 0.6611404555586557
pearson: 0.8360551253141535

=== Experiment 3524 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00582056017882415
rmse: 0.07629259583225721
mae: 0.03148519645455187
r2: 0.7375535033612668
pearson: 0.8594406510754771

=== Experiment 3681 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006925752677512497
rmse: 0.08322110716346243
mae: 0.03996922662288604
r2: 0.6877208600278274
pearson: 0.8307495455778615

=== Experiment 3773 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0070080792236866295
rmse: 0.08371427132625972
mae: 0.03742005943920121
r2: 0.6840087922955198
pearson: 0.8285930205946686

=== Experiment 3688 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006594819354061039
rmse: 0.08120849311532038
mae: 0.03349885159788993
r2: 0.7026424979273592
pearson: 0.8410370962481498

=== Experiment 3744 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068765295499918526
rmse: 0.08292484277942197
mae: 0.03958576012919851
r2: 0.6899403091829781
pearson: 0.8322403318506103

=== Experiment 3743 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007726714179890562
rmse: 0.0879017302440092
mae: 0.04858751223795006
r2: 0.651605858415717
pearson: 0.8181948309558281

=== Experiment 3458 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006350348776893674
rmse: 0.07968907564336328
mae: 0.039246209434135856
r2: 0.7136655686521061
pearson: 0.8478208672493436

=== Experiment 3642 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005643770873157619
rmse: 0.0751250349294935
mae: 0.030154853786050186
r2: 0.7455248553428468
pearson: 0.8647772818692836

=== Experiment 3685 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062648444647378
rmse: 0.07915077046206057
mae: 0.031559229573728434
r2: 0.7175209204538886
pearson: 0.8471957760929576

=== Experiment 3804 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007039497311600788
rmse: 0.08390171220899362
mae: 0.03081957205139083
r2: 0.6825921645396282
pearson: 0.8273167781444335

=== Experiment 3732 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006343600905818005
rmse: 0.07964672564404644
mae: 0.033322911508736154
r2: 0.7139698271889429
pearson: 0.8458187870366316

=== Experiment 3821 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006754863221771756
rmse: 0.08218797492195411
mae: 0.034719391060673795
r2: 0.6954261903729759
pearson: 0.8344866150226155

=== Experiment 3705 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007514651140978484
rmse: 0.0866870875100697
mae: 0.03717707156150787
r2: 0.6611676875041814
pearson: 0.8145901335193909

=== Experiment 3554 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008474317062019978
rmse: 0.09205605391292838
mae: 0.04288203545627292
r2: 0.6178967735056935
pearson: 0.8148297033991156

=== Experiment 3615 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005816762894917052
rmse: 0.07626770545202637
mae: 0.02967062023709541
r2: 0.7377247212213254
pearson: 0.864745213179048

=== Experiment 3475 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006881306778426008
rmse: 0.08295364234574638
mae: 0.035975991202835816
r2: 0.6897249060555011
pearson: 0.8383413791542287

=== Experiment 3561 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006262086938422699
rmse: 0.07913334909140834
mae: 0.03396703868734506
r2: 0.7176452561017564
pearson: 0.847356113059274

=== Experiment 3193 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0060319675774312695
rmse: 0.07766574262460425
mae: 0.03247468630111654
r2: 0.7280212368055836
pearson: 0.8553302636037736

=== Experiment 3425 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005920262536956901
rmse: 0.0769432423085803
mae: 0.02901549009362592
r2: 0.7330579679154249
pearson: 0.8649081185106486

=== Experiment 3703 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0061655167441989935
rmse: 0.07852080453102218
mae: 0.030940437490042196
r2: 0.7219995636555104
pearson: 0.8599737766467556

=== Experiment 3822 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0072159601226243625
rmse: 0.08494680760702171
mae: 0.04071466878785434
r2: 0.6746355340577981
pearson: 0.8251176235160831

=== Experiment 3716 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006997387186186748
rmse: 0.08365038664696504
mae: 0.042522257734046824
r2: 0.6844908915604638
pearson: 0.8365958273207004

=== Experiment 3799 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006161908431646018
rmse: 0.07849782437523997
mae: 0.03273449114336898
r2: 0.7221622608803843
pearson: 0.8539948230534701

=== Experiment 3724 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006670312911802603
rmse: 0.0816719836406745
mae: 0.03712406491269803
r2: 0.6992385266360478
pearson: 0.8406676828006243

=== Experiment 3568 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007263331345399556
rmse: 0.08522518023095965
mae: 0.04337017298550663
r2: 0.6724995864725342
pearson: 0.8332742305882431

=== Experiment 3364 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005913866760558104
rmse: 0.07690166942634018
mae: 0.029352679796526126
r2: 0.73334635065825
pearson: 0.8570060712537682

=== Experiment 3260 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006091316160863543
rmse: 0.07804688437640252
mae: 0.030953979012207065
r2: 0.7253452352999316
pearson: 0.8585317849451163

=== Experiment 3709 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006294286918056616
rmse: 0.07933654213574358
mae: 0.04023779527124423
r2: 0.716193373192358
pearson: 0.8500251732168996

=== Experiment 3583 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006828683628466112
rmse: 0.08263584953557453
mae: 0.035310981785529466
r2: 0.6920976607259732
pearson: 0.8432963727236197

=== Experiment 3824 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007781261642215737
rmse: 0.08821145981229274
mae: 0.037767523715403824
r2: 0.6491463373476491
pearson: 0.8173405389814138

=== Experiment 3791 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00664130254565813
rmse: 0.08149418718938259
mae: 0.0364407581423884
r2: 0.7005465912770636
pearson: 0.8374072700913787

=== Experiment 3697 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0066447336654499155
rmse: 0.08151523578724358
mae: 0.036405741209903274
r2: 0.7003918835958032
pearson: 0.8404605315537951

=== Experiment 3713 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005899110579890886
rmse: 0.07680566762870358
mae: 0.03300713887681809
r2: 0.734011700349843
pearson: 0.8589020926369932

=== Experiment 3722 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00677143981997414
rmse: 0.08228875877040642
mae: 0.03857984843883802
r2: 0.6946787588559493
pearson: 0.8342473951129393

=== Experiment 3800 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006332795308375189
rmse: 0.07957886219578154
mae: 0.03614328183244845
r2: 0.7144570468217322
pearson: 0.8469634224523245

=== Experiment 3486 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006738046115150937
rmse: 0.08208560236211303
mae: 0.03622874183833461
r2: 0.6961844663087341
pearson: 0.840650647726034

=== Experiment 3820 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008421283297199333
rmse: 0.0917675503497796
mae: 0.047211122189919824
r2: 0.6202880426195115
pearson: 0.79416537525978

=== Experiment 3301 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0059451887545242
rmse: 0.07710505012334924
mae: 0.03241495363098486
r2: 0.7319340557361826
pearson: 0.8610816602607556

=== Experiment 3825 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007178240544814427
rmse: 0.0847244979024038
mae: 0.039499098171448976
r2: 0.6763362932195922
pearson: 0.8249110314317437

=== Experiment 3517 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00598366725635957
rmse: 0.07735416767285115
mae: 0.03270480755150132
r2: 0.7301990770241096
pearson: 0.8547837881973485

=== Experiment 3810 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008447218706821072
rmse: 0.09190875206867447
mae: 0.05367348437560376
r2: 0.619118626414714
pearson: 0.8018114063009646

=== Experiment 3695 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007826646259262018
rmse: 0.0884683347829155
mae: 0.04492191560170501
r2: 0.6470999649403297
pearson: 0.8200464992256737

=== Experiment 3494 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008287714347047183
rmse: 0.09103688454163611
mae: 0.04399218165598269
r2: 0.6263106077936813
pearson: 0.812550905469071

=== Experiment 3682 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006415446292042714
rmse: 0.08009648114644434
mae: 0.04015193045692651
r2: 0.7107303503456435
pearson: 0.8514411837825024

=== Experiment 3267 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006336653534122198
rmse: 0.07960310002834185
mae: 0.029917903448729206
r2: 0.7142830811209346
pearson: 0.847607465225018

=== Experiment 3667 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006068279772047293
rmse: 0.07789916412932357
mae: 0.033179904209601555
r2: 0.7263839359325663
pearson: 0.8524730241001212

=== Experiment 3780 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00653286881116617
rmse: 0.08082616414977374
mae: 0.03686346167117868
r2: 0.7054358206399697
pearson: 0.8442635229771797

=== Experiment 3641 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006786638805973659
rmse: 0.08238105853880283
mae: 0.039902466394666956
r2: 0.6939934432668162
pearson: 0.8424141887667493

=== Experiment 3293 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005964559372278935
rmse: 0.07723055983403807
mae: 0.029478172257498592
r2: 0.7310606431072826
pearson: 0.855669415177478

=== Experiment 3849 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007742395562510995
rmse: 0.08799088340567444
mae: 0.03962988331853272
r2: 0.6508987917752661
pearson: 0.8069039326332467

=== Experiment 3789 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007268821957780942
rmse: 0.08525738652915033
mae: 0.03239024843237588
r2: 0.6722520171768609
pearson: 0.8426440516147435

=== Experiment 3245 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006212844414080006
rmse: 0.07882159865214614
mae: 0.03196902776329562
r2: 0.719865579851076
pearson: 0.8499927707592275

=== Experiment 3469 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007081739268775461
rmse: 0.08415307046552407
mae: 0.03428205838827182
r2: 0.6806874932827295
pearson: 0.8301516496160533

=== Experiment 3434 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005741296461535951
rmse: 0.07577134327393141
mae: 0.029333286670362994
r2: 0.7411274694871619
pearson: 0.8613535349775986

=== Experiment 3787 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007059857104384939
rmse: 0.08402295581794858
mae: 0.047549622999198474
r2: 0.6816741504440211
pearson: 0.8368233380062312

=== Experiment 3224 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006835407491834385
rmse: 0.08267652322052726
mae: 0.03731351451665192
r2: 0.6917944846860964
pearson: 0.8324388895737277

=== Experiment 3779 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007695431871529511
rmse: 0.08772361068452159
mae: 0.04492815562049385
r2: 0.65301636393132
pearson: 0.8118154882435931

=== Experiment 3042 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005796413920583207
rmse: 0.0761341836534891
mae: 0.03045335925359733
r2: 0.7386422475177699
pearson: 0.8600852715028517

=== Experiment 3737 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005720978510718081
rmse: 0.0756371503344625
mae: 0.029580354754225265
r2: 0.7420435969469257
pearson: 0.8665860328141232

=== Experiment 3424 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008695274953038354
rmse: 0.09324845818048873
mae: 0.042893442209973855
r2: 0.60793387945068
pearson: 0.7799493778002743

=== Experiment 3405 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005502301062069743
rmse: 0.07417749700596363
mae: 0.02906644848824973
r2: 0.7519036668591734
pearson: 0.8675489539883606

=== Experiment 3868 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007017302783030814
rmse: 0.08376934273963724
mae: 0.030822708811553315
r2: 0.6835929060642894
pearson: 0.8302713415379223

=== Experiment 3512 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006908134498186627
rmse: 0.08311518813181275
mae: 0.04940118482541953
r2: 0.6885152559792771
pearson: 0.8487952917853622

=== Experiment 3801 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006924342954439599
rmse: 0.0832126369876571
mae: 0.03562199100911306
r2: 0.6877844238207178
pearson: 0.8296640467398552

=== Experiment 3404 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006637827045333231
rmse: 0.08147286079016271
mae: 0.03593196898574982
r2: 0.7007033000570285
pearson: 0.8432504794182611

=== Experiment 3540 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0076295892127735306
rmse: 0.08734751978604505
mae: 0.04010481420373033
r2: 0.6559851804350569
pearson: 0.8360667803968759

=== Experiment 3769 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007482573598673517
rmse: 0.08650187049233975
mae: 0.03902714414882483
r2: 0.6626140497683063
pearson: 0.8277224280929815

=== Experiment 3563 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00600383613803205
rmse: 0.0774844251319712
mae: 0.02914128153857038
r2: 0.7292896710265011
pearson: 0.8594510697746818

=== Experiment 3846 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0071243579154642266
rmse: 0.08440591161443745
mae: 0.033458562923226756
r2: 0.678765837261434
pearson: 0.8241782744103403

=== Experiment 3352 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0058107874461450415
rmse: 0.07622852121184721
mae: 0.028902151256487832
r2: 0.7379941515764594
pearson: 0.8678513336595608

=== Experiment 3847 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007114623259313738
rmse: 0.08434822617763658
mae: 0.034412108632099564
r2: 0.6792047686227096
pearson: 0.8246045185154587

=== Experiment 3645 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006024038167739706
rmse: 0.07761467752776988
mae: 0.03424659937485865
r2: 0.7283787703985762
pearson: 0.8553293742761785

=== Experiment 3673 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006191605929808413
rmse: 0.07868675828758237
mae: 0.03372612798044036
r2: 0.7208232137591104
pearson: 0.8503280980855202

=== Experiment 3633 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007401400305801271
rmse: 0.08603139139756645
mae: 0.03288576604351556
r2: 0.6662741178168176
pearson: 0.8257891625036874

=== Experiment 3452 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008323013316629447
rmse: 0.09123055034707095
mae: 0.04371046857005921
r2: 0.6247189927914815
pearson: 0.7915973644174168

=== Experiment 3418 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006168815365021671
rmse: 0.07854180647923545
mae: 0.03305617779978051
r2: 0.72185083029445
pearson: 0.8515429747541939

=== Experiment 3371 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006284948431153776
rmse: 0.07927766666062881
mae: 0.0324754237074398
r2: 0.7166144414566247
pearson: 0.8599234554388192

=== Experiment 3353 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006278879793982824
rmse: 0.07923938284705923
mae: 0.033050888377894855
r2: 0.7168880736356513
pearson: 0.8467802652006458

=== Experiment 3535 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0056606374880527185
rmse: 0.07523720813568721
mae: 0.0326403810689565
r2: 0.7447643471007916
pearson: 0.8638040047750976

=== Experiment 3530 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007691459433813315
rmse: 0.08770096597993271
mae: 0.037745872723011144
r2: 0.6531954793995398
pearson: 0.8103179386962013

=== Experiment 3870 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006955340911378992
rmse: 0.08339868650871543
mae: 0.03549934012825176
r2: 0.6863867395855647
pearson: 0.8285043961079781

=== Experiment 3733 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006811531431386615
rmse: 0.08253200246805245
mae: 0.03397918069491277
r2: 0.6928710457430285
pearson: 0.8400799958583269

=== Experiment 3684 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006536867953466838
rmse: 0.08085089952169264
mae: 0.03094358734975674
r2: 0.7052555010737898
pearson: 0.8402240308770351

=== Experiment 3547 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007113893139733743
rmse: 0.08434389805868438
mae: 0.03342467357616506
r2: 0.6792376893932215
pearson: 0.8353148433149957

=== Experiment 3406 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006931192022167927
rmse: 0.08325378082806767
mae: 0.03258560094635908
r2: 0.6874756023713529
pearson: 0.8483893786660401

=== Experiment 3855 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006845441819526548
rmse: 0.08273718522844821
mae: 0.035181675080588726
r2: 0.691342041266902
pearson: 0.8320097654686871

=== Experiment 3465 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005573097582023838
rmse: 0.07465318199530303
mae: 0.028186368717666943
r2: 0.7487114829343748
pearson: 0.8654036425046615

=== Experiment 3555 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006416332504966834
rmse: 0.08010201311432089
mae: 0.035469046912751136
r2: 0.7106903913949469
pearson: 0.8495675327982966

=== Experiment 3585 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005666395670730033
rmse: 0.07527546526412197
mae: 0.029026147077829856
r2: 0.7445047131782415
pearson: 0.8716886291944517

=== Experiment 3858 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0071923094369112485
rmse: 0.08480748455715008
mae: 0.03399606411615907
r2: 0.6757019330671394
pearson: 0.8388780919383764

=== Experiment 3392 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008556791767578844
rmse: 0.0925029284270441
mae: 0.04297079121314136
r2: 0.6141780253319382
pearson: 0.7848249222656069

=== Experiment 3625 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006528875223888714
rmse: 0.0808014555802599
mae: 0.03957740999863816
r2: 0.7056158897325981
pearson: 0.8429683147089627

=== Experiment 3526 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006181846113723777
rmse: 0.07862471693891036
mae: 0.03206969838972998
r2: 0.721263279570743
pearson: 0.8512808730894135

=== Experiment 3467 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009026809043039158
rmse: 0.09500952080207098
mae: 0.044238297247104
r2: 0.5929851532518557
pearson: 0.7702281186794302

=== Experiment 3843 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0065377332675548025
rmse: 0.08085625063997713
mae: 0.03562921491459986
r2: 0.7052164844424174
pearson: 0.8399260571559128

=== Experiment 3812 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006473137104576447
rmse: 0.08045580839551889
mae: 0.039094042375515455
r2: 0.7081290970001658
pearson: 0.8441422453669931

=== Experiment 3892 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007381208878395956
rmse: 0.08591396206901389
mae: 0.04367165037658229
r2: 0.6671845403915964
pearson: 0.8207712094896316

=== Experiment 3923 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006516667185638574
rmse: 0.08072587680315757
mae: 0.03136108078415235
r2: 0.7061663448041283
pearson: 0.8406088196331741

=== Experiment 3913 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006955589937040596
rmse: 0.08340017947846752
mae: 0.044217947899709655
r2: 0.6863755111280876
pearson: 0.8355897632246777

=== Experiment 3834 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006622807649321229
rmse: 0.08138063436298115
mae: 0.032848208758485306
r2: 0.701380518012668
pearson: 0.8390917119059395

=== Experiment 3380 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008449711553375955
rmse: 0.09192231259806269
mae: 0.04225729615921398
r2: 0.61900622506074
pearson: 0.8068579495974971

=== Experiment 3611 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005724892176552784
rmse: 0.07566301723135804
mae: 0.034076363206265836
r2: 0.7418671314769749
pearson: 0.8662139695604283

=== Experiment 3912 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007858758016133952
rmse: 0.08864963630006585
mae: 0.03511142086316387
r2: 0.6456520599564393
pearson: 0.8136697460368837

=== Experiment 3933 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007181402333741439
rmse: 0.08474315508488836
mae: 0.03827290622822216
r2: 0.676193729548483
pearson: 0.8259810910403017

=== Experiment 3401 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007065834730148442
rmse: 0.08405851967616633
mae: 0.037762570357120065
r2: 0.681404621929299
pearson: 0.8396212480355293

=== Experiment 3841 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008055998565131313
rmse: 0.08975521469603487
mae: 0.04480982104400693
r2: 0.6367585704143528
pearson: 0.8204958846236553

=== Experiment 3590 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005880196555963731
rmse: 0.07668243968447881
mae: 0.03254419355043076
r2: 0.73486452536402
pearson: 0.8576136899165449

=== Experiment 3806 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005815124707156822
rmse: 0.07625696497472753
mae: 0.03345071528174508
r2: 0.7377985863864127
pearson: 0.866641336167495

=== Experiment 3522 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006165511157146333
rmse: 0.07852076895412023
mae: 0.03271645621006478
r2: 0.7219998155732559
pearson: 0.8601585745856252

=== Experiment 3693 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006410459034527131
rmse: 0.08006534228070926
mae: 0.032994136634625186
r2: 0.7109552235919612
pearson: 0.8449805254175569

=== Experiment 3772 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006324028191302813
rmse: 0.07952375865930139
mae: 0.03196721942251456
r2: 0.7148523522718222
pearson: 0.8547858155233434

=== Experiment 3683 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00935976279244551
rmse: 0.09674586705614617
mae: 0.04640489201199742
r2: 0.5779724152352754
pearson: 0.7634923754800873

=== Experiment 3741 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00739931418784013
rmse: 0.08601926637585403
mae: 0.03551762194057013
r2: 0.6663681799575146
pearson: 0.8175274231556384

=== Experiment 3865 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006929107933812708
rmse: 0.08324126340831636
mae: 0.04597824610698056
r2: 0.687569572998012
pearson: 0.8367182447768534

=== Experiment 3549 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008059947110109931
rmse: 0.08977720818843685
mae: 0.034937878158894556
r2: 0.6365805322591545
pearson: 0.8129537266783861

=== Experiment 3531 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00903666422054276
rmse: 0.09506137081140141
mae: 0.04212188302360065
r2: 0.592540787635814
pearson: 0.7950510311612057

=== Experiment 3783 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006796179337393699
rmse: 0.08243894308755843
mae: 0.039666899562815924
r2: 0.6935632649041957
pearson: 0.8336074807866503

=== Experiment 3907 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006828330419038247
rmse: 0.08263371236389085
mae: 0.03572230551432244
r2: 0.6921135867836161
pearson: 0.844608585039233

=== Experiment 3598 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006599671549979183
rmse: 0.08123836255107056
mae: 0.03497875676721249
r2: 0.7024237145490249
pearson: 0.8418939971940116

=== Experiment 3934 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006726059050359274
rmse: 0.08201255422408983
mae: 0.03511560884799262
r2: 0.6967249577842795
pearson: 0.8383951760400239

=== Experiment 3754 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005906086586500826
rmse: 0.07685106756903788
mae: 0.03127686943080089
r2: 0.7336971552821763
pearson: 0.8571682230901069

=== Experiment 3864 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006756182266892748
rmse: 0.08219599909297744
mae: 0.03819103574892341
r2: 0.6953667152090266
pearson: 0.8367350018209971

=== Experiment 3478 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0062838387530067515
rmse: 0.07927066767100395
mae: 0.032249274065333566
r2: 0.7166644763558673
pearson: 0.8492083240809034

=== Experiment 3811 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0070868719132125765
rmse: 0.08418356082521443
mae: 0.03787766162935706
r2: 0.6804560646041095
pearson: 0.8256148314403038

=== Experiment 3874 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0067476593426292
rmse: 0.08214413760353931
mae: 0.033854416159523566
r2: 0.6957510101128442
pearson: 0.8437274480619747

=== Experiment 3750 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006627206134161269
rmse: 0.08140765402688661
mae: 0.0342403450597202
r2: 0.7011821922671518
pearson: 0.8377101995828213

=== Experiment 3672 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005987844785071458
rmse: 0.07738116557064424
mae: 0.033143881398825834
r2: 0.7300107140931613
pearson: 0.8657449856805969

=== Experiment 3454 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0066418562954792225
rmse: 0.08149758459904945
mae: 0.031230480658380567
r2: 0.7005216229413544
pearson: 0.8390708383491924

=== Experiment 3807 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0062327508645202935
rmse: 0.07894777301811808
mae: 0.03412042883465511
r2: 0.7189680067622225
pearson: 0.8530357563950572

=== Experiment 3660 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0060272725267546306
rmse: 0.0776355107328768
mae: 0.02982933976756675
r2: 0.7282329345741434
pearson: 0.8545014064152869

=== Experiment 3816 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00621059405246594
rmse: 0.07880732232772499
mae: 0.03223188277339679
r2: 0.7199670476657944
pearson: 0.8536444183173505

=== Experiment 3950 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006828667731643483
rmse: 0.08263575334952471
mae: 0.040042525363046096
r2: 0.6920983775067087
pearson: 0.8333648681017295

=== Experiment 3900 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007207157534742203
rmse: 0.08489497944367619
mae: 0.03768369042377882
r2: 0.6750324388710893
pearson: 0.8219346874282444

=== Experiment 3946 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006806754051361992
rmse: 0.08250305480018295
mae: 0.03513069284709618
r2: 0.693086455705653
pearson: 0.8356669307240354

=== Experiment 3218 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005409054902624539
rmse: 0.07354627728596831
mae: 0.030212936012009233
r2: 0.7561080951477845
pearson: 0.871591966831885

=== Experiment 3909 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00818714426208793
rmse: 0.09048283960004753
mae: 0.03792817758499783
r2: 0.6308452686602144
pearson: 0.835492529675154

=== Experiment 3867 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007325848517688884
rmse: 0.08559117079283869
mae: 0.036323682658621896
r2: 0.6696807146899202
pearson: 0.8273876430837627

=== Experiment 3419 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007279535268510685
rmse: 0.08532019261880909
mae: 0.03259296097577961
r2: 0.6717689587113453
pearson: 0.8484351697037626

=== Experiment 3731 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007550129355044748
rmse: 0.0868914803363641
mae: 0.04234495630053433
r2: 0.6595679904470897
pearson: 0.812439168631238

=== Experiment 3957 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007597567734817382
rmse: 0.0871640277569674
mae: 0.043289836719608954
r2: 0.6574290147823694
pearson: 0.8172198482086205

=== Experiment 3637 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006136237531223133
rmse: 0.07833414026606236
mae: 0.032364540378485
r2: 0.7233197504818247
pearson: 0.8579358269252729

=== Experiment 3862 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006712912434762366
rmse: 0.08193236500164246
mae: 0.036246364722569535
r2: 0.697317732895276
pearson: 0.8357940991437229

=== Experiment 3902 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006781968034333043
rmse: 0.08235270508206177
mae: 0.03503659391874087
r2: 0.694204046304328
pearson: 0.833328048142658

=== Experiment 3448 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006629721025294128
rmse: 0.08142309884359676
mae: 0.03692518210931423
r2: 0.7010687969328595
pearson: 0.8384318230567748

=== Experiment 3581 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007667262975778556
rmse: 0.0875629086758689
mae: 0.0375088010560688
r2: 0.6542864870426515
pearson: 0.8389308740592193

=== Experiment 3774 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006446158038750651
rmse: 0.08028796945215796
mae: 0.029792514774992267
r2: 0.7093455712038559
pearson: 0.8637767778425376

=== Experiment 3350 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006207500344817676
rmse: 0.07878769158198301
mae: 0.03517005189443526
r2: 0.7201065415820098
pearson: 0.8530519238327613

=== Experiment 3711 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009318095727290142
rmse: 0.09653028399051844
mae: 0.04570943270580967
r2: 0.579851164864053
pearson: 0.7623130176314205

=== Experiment 3505 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006960833242799693
rmse: 0.08343160817579685
mae: 0.03178959270561816
r2: 0.6861390927791653
pearson: 0.8305984323213739

=== Experiment 3357 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007279829647875437
rmse: 0.08532191774611865
mae: 0.035261505409669505
r2: 0.6717556852753548
pearson: 0.8311009979595888

=== Experiment 3962 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007494524773422434
rmse: 0.0865709233716635
mae: 0.04368739399925049
r2: 0.6620751765589921
pearson: 0.8192239783181733

=== Experiment 3628 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007349742849854425
rmse: 0.08573064125418883
mae: 0.03443288709590646
r2: 0.6686033297692757
pearson: 0.8230314225050466

=== Experiment 3928 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0076188882598879695
rmse: 0.08728624324535894
mae: 0.04105079445048615
r2: 0.6564676816908173
pearson: 0.8180921903046207

=== Experiment 3875 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00619404035060146
rmse: 0.07870222583003265
mae: 0.03245720771508569
r2: 0.7207134467970224
pearson: 0.8622423337766251

=== Experiment 3803 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007412840188082809
rmse: 0.08609785240110701
mae: 0.036821392054782326
r2: 0.6657582985598202
pearson: 0.8305947697992486

=== Experiment 3292 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005718056569921808
rmse: 0.0756178323540275
mae: 0.027843212161907067
r2: 0.742175345971385
pearson: 0.8633574987275816

=== Experiment 3604 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006263852509005585
rmse: 0.07914450397220002
mae: 0.03747037402427687
r2: 0.7175656473012612
pearson: 0.8506844839087979

=== Experiment 3872 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0065363418660189755
rmse: 0.08084764601408612
mae: 0.0390338280036666
r2: 0.7052792221252651
pearson: 0.8552113124405467

=== Experiment 3671 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0060860350741229275
rmse: 0.07801304425621992
mae: 0.030749310215753122
r2: 0.725583357176682
pearson: 0.8553728484482164

=== Experiment 3437 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007180434432971321
rmse: 0.08473744410218732
mae: 0.03546029673340045
r2: 0.6762373717682019
pearson: 0.8267235886544383

=== Experiment 3977 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007239716397831708
rmse: 0.085086523009415
mae: 0.040662790420834025
r2: 0.6735643740646351
pearson: 0.8225869203342238

=== Experiment 3869 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006207547219341788
rmse: 0.07878798905506973
mae: 0.030992999996522445
r2: 0.7201044280303486
pearson: 0.848816429884853

=== Experiment 3848 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006626003253868073
rmse: 0.08140026568671674
mae: 0.036968126970362725
r2: 0.7012364296101439
pearson: 0.8374299215200586

=== Experiment 3736 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005433044647839431
rmse: 0.07370918971091346
mae: 0.028505580404129247
r2: 0.7550264080947393
pearson: 0.8702242421934289

=== Experiment 3765 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006377239392101785
rmse: 0.07985761949934261
mae: 0.029799159312557676
r2: 0.7124530826478372
pearson: 0.8482634564560763

=== Experiment 3817 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006696824185560922
rmse: 0.08183412604507316
mae: 0.04279741378995157
r2: 0.6980431449707892
pearson: 0.8410309619735037

=== Experiment 3597 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057421995874594985
rmse: 0.07577730258764492
mae: 0.03362681819880746
r2: 0.7410867479367652
pearson: 0.8620378567759495

=== Experiment 3979 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007957581270536059
rmse: 0.08920527602410105
mae: 0.03823504656894448
r2: 0.6411961629108889
pearson: 0.8104016255213083

=== Experiment 3594 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005882303034206408
rmse: 0.07669617353040768
mae: 0.03290716626736654
r2: 0.7347695451871896
pearson: 0.8573732808854382

=== Experiment 3881 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0071905622894767145
rmse: 0.08479718326381316
mae: 0.034464197357929445
r2: 0.6757807111759844
pearson: 0.8267725924232864

=== Experiment 3291 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007732785056737893
rmse: 0.08793625564428982
mae: 0.035100035118759476
r2: 0.6513321252506681
pearson: 0.8089487105172545

=== Experiment 3911 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006274196507015558
rmse: 0.07920982582366634
mae: 0.03185001494473911
r2: 0.7170992409837331
pearson: 0.8472543555339787

=== Experiment 3784 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01186509009023334
rmse: 0.10892699431377577
mae: 0.04388217327086872
r2: 0.465008310056892
pearson: 0.6939608761183905

=== Experiment 3696 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005800010179639487
rmse: 0.0761577978912172
mae: 0.029698199553787292
r2: 0.7384800937797569
pearson: 0.8607755808679208

=== Experiment 3831 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005900798923279111
rmse: 0.07681665785022876
mae: 0.03035125993506155
r2: 0.7339355736895649
pearson: 0.8590750893438774

=== Experiment 3513 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007555977657797677
rmse: 0.08692512673443552
mae: 0.03876121047341753
r2: 0.6593042930499953
pearson: 0.8158839762008367

=== Experiment 3498 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0056676866064126336
rmse: 0.075284039519759
mae: 0.03222397385511508
r2: 0.7444465054564974
pearson: 0.8647072286839822

=== Experiment 3866 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007021279159779948
rmse: 0.083793073459445
mae: 0.036547725299613934
r2: 0.6834136129868082
pearson: 0.8293294767184652

=== Experiment 3727 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006081195911715631
rmse: 0.07798202300348223
mae: 0.032805182859334356
r2: 0.7258015528797486
pearson: 0.8550163015058242

=== Experiment 3809 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005756601712153078
rmse: 0.07587227235395733
mae: 0.029819308640420386
r2: 0.7404373624731916
pearson: 0.8610894919195629

=== Experiment 3829 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006173609634698828
rmse: 0.07857232104691084
mae: 0.033395482792651546
r2: 0.7216346587848264
pearson: 0.8501580045698597

=== Experiment 3367 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007084765182125319
rmse: 0.08417104717255999
mae: 0.030897193240532226
r2: 0.6805510561815908
pearson: 0.848105969459385

=== Experiment 3508 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008218239826456605
rmse: 0.09065450803162854
mae: 0.04658549387896528
r2: 0.6294431833490355
pearson: 0.8167479747504874

=== Experiment 3758 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.010993533927834448
rmse: 0.1048500544960967
mae: 0.04143413972134332
r2: 0.5043063938182554
pearson: 0.7103681849667858

=== Experiment 3987 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007417443181721537
rmse: 0.086124579428416
mae: 0.03661302876867409
r2: 0.6655507516025825
pearson: 0.8159880965466718

=== Experiment 3890 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005581322628418381
rmse: 0.07470825006930883
mae: 0.032218436528993186
r2: 0.7483406192125646
pearson: 0.8675046046127136

=== Experiment 3631 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007478496232049797
rmse: 0.08647829919725408
mae: 0.03478436836897277
r2: 0.6627978964347836
pearson: 0.82987866481095

=== Experiment 3889 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00805471841311938
rmse: 0.0897480830609734
mae: 0.03934301463787164
r2: 0.636816291905133
pearson: 0.8037365845848481

=== Experiment 3999 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0071368772198022795
rmse: 0.08448004036340347
mae: 0.03771811813480335
r2: 0.6782013473389985
pearson: 0.8250690597055591

=== Experiment 3627 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006252671925751468
rmse: 0.07907383844073505
mae: 0.0313654354677535
r2: 0.7180697748792383
pearson: 0.8493615377714232

=== Experiment 3915 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006223808942367695
rmse: 0.07889112080815999
mae: 0.03356575910595277
r2: 0.7193711940964421
pearson: 0.8483867412103259

=== Experiment 3788 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005716580358743413
rmse: 0.07560807072491278
mae: 0.030244078976395215
r2: 0.7422419076836886
pearson: 0.8652248801752527

=== Experiment 3592 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008179428606851785
rmse: 0.09044019353612522
mae: 0.0405956963705664
r2: 0.6311931641588926
pearson: 0.79738376468133

=== Experiment 3830 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005948528233167678
rmse: 0.07712670246527903
mae: 0.02998815523367342
r2: 0.7317834801139652
pearson: 0.8571120931790592

=== Experiment 3710 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006337626975161638
rmse: 0.07960921413480752
mae: 0.03548022192633763
r2: 0.7142391890929104
pearson: 0.8465816106713115

=== Experiment 3905 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005560130112423784
rmse: 0.07456627999587873
mae: 0.030742011241800194
r2: 0.7492961804312215
pearson: 0.8720148051744153

=== Experiment 3670 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006797977718061199
rmse: 0.08244984971521294
mae: 0.03879770750336016
r2: 0.6934821767114261
pearson: 0.8387629434151853

=== Experiment 3546 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006457225477291556
rmse: 0.08035686328678812
mae: 0.03535427355510388
r2: 0.7088465452711987
pearson: 0.8433492726200785

=== Experiment 3906 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006905008405148189
rmse: 0.08309638021687941
mae: 0.03733168142158835
r2: 0.6886562101384817
pearson: 0.8354781220663214

=== Experiment 3983 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007138010542045974
rmse: 0.08448674773031552
mae: 0.04063869463008975
r2: 0.6781502463378435
pearson: 0.8251866098495905

=== Experiment 3939 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007060538425434065
rmse: 0.08402701009457653
mae: 0.04371633933550832
r2: 0.6816434299777883
pearson: 0.8352549414166215

=== Experiment 3595 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006176341003286377
rmse: 0.07858970036389233
mae: 0.035836657580103376
r2: 0.7215115025773811
pearson: 0.8531210035377345

=== Experiment 3969 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007999905699995754
rmse: 0.08944219194538869
mae: 0.04966083707685888
r2: 0.6392877730149572
pearson: 0.8180136507761439

=== Experiment 3794 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006488642583261147
rmse: 0.08055211097954632
mae: 0.0382167351389469
r2: 0.7074299617907558
pearson: 0.8448427873911811

=== Experiment 3768 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005810236377114019
rmse: 0.07622490654053975
mae: 0.027843709301949293
r2: 0.7380189990365241
pearson: 0.8681712274443816

=== Experiment 3603 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006682844621108167
rmse: 0.08174866739652804
mae: 0.04216738243519218
r2: 0.698673477379087
pearson: 0.8444992753444077

=== Experiment 3605 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00654673392079743
rmse: 0.08091188986049844
mae: 0.03496022577720741
r2: 0.7048106489491976
pearson: 0.848897492739339

=== Experiment 3792 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012044251600836744
rmse: 0.10974630563639372
mae: 0.04926202663482266
r2: 0.4569299963987957
pearson: 0.6789624822819659

=== Experiment 3927 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006289926416103193
rmse: 0.07930905633093356
mae: 0.03479494393974566
r2: 0.7163899863062342
pearson: 0.8470614283933653

=== Experiment 3908 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007116836848670719
rmse: 0.08436134688748585
mae: 0.03843927693771825
r2: 0.6791049588528786
pearson: 0.8260310892562864

=== Experiment 3997 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009541318390607777
rmse: 0.0976796723510464
mae: 0.06448707308589813
r2: 0.5697861532228685
pearson: 0.7969221336780216

=== Experiment 3861 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0065796855154468085
rmse: 0.08111526068161778
mae: 0.03593682058868137
r2: 0.7033248760495632
pearson: 0.8388757979410884

=== Experiment 3484 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00623380108644764
rmse: 0.07895442410940402
mae: 0.02830840704672266
r2: 0.7189206527177563
pearson: 0.8550834396811992

=== Experiment 3652 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005999322690015051
rmse: 0.07745529478360437
mae: 0.031085799173738795
r2: 0.7294931804110674
pearson: 0.8556920667001068

=== Experiment 3994 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00683004014213595
rmse: 0.0826440569075354
mae: 0.03975327830812345
r2: 0.6920364961216475
pearson: 0.8335920853765806

=== Experiment 3725 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006036589720372703
rmse: 0.07769549356541024
mae: 0.0295668188108975
r2: 0.7278128264147152
pearson: 0.8592579444077492

=== Experiment 3966 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006936037690676245
rmse: 0.08328287753599924
mae: 0.03505426641058612
r2: 0.6872571133110545
pearson: 0.830532011654022

=== Experiment 4000 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007233538235704328
rmse: 0.08505021008618573
mae: 0.04461728443200213
r2: 0.6738429446757415
pearson: 0.8248075960723519

=== Experiment 3606 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0060288500100620125
rmse: 0.07764566961564574
mae: 0.03179331180259159
r2: 0.7281618065461171
pearson: 0.8550618933519543

=== Experiment 3882 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008852359557336629
rmse: 0.09408697868109396
mae: 0.04406685670524565
r2: 0.6008510037810925
pearson: 0.7882343140676511

=== Experiment 3447 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006912744969875863
rmse: 0.08314291894007489
mae: 0.0391124474721744
r2: 0.6883073718400334
pearson: 0.8327537675324722

=== Experiment 3473 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008218564506350178
rmse: 0.09065629876820572
mae: 0.04325787038368369
r2: 0.6294285436755367
pearson: 0.8040287482030538

=== Experiment 3674 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007438509800438993
rmse: 0.08624679588505879
mae: 0.032711581851981246
r2: 0.6646008670367403
pearson: 0.8454352367208041

=== Experiment 3470 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.020352103057449115
rmse: 0.14266079719898214
mae: 0.06633358118209162
r2: 0.08233263078520248
pearson: 0.29072491830105457

=== Experiment 3944 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007421657720115665
rmse: 0.08614904364016855
mae: 0.03426950707598635
r2: 0.6653607199213514
pearson: 0.8568994864917165

=== Experiment 3767 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006313697511856073
rmse: 0.0794587786959759
mae: 0.0314145040039173
r2: 0.7153181580611961
pearson: 0.8585541231602505

=== Experiment 3764 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.012838577729998416
rmse: 0.11330744781345319
mae: 0.04457014270192302
r2: 0.42111418084456165
pearson: 0.6913614799645863

=== Experiment 3978 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007611105638438889
rmse: 0.08724165082366844
mae: 0.04558875870487036
r2: 0.6568185966665626
pearson: 0.8188079904983397

=== Experiment 3813 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00666711452509247
rmse: 0.08165240060826424
mae: 0.03992819309547708
r2: 0.6993827404850902
pearson: 0.8460037925158165

=== Experiment 3534 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00804444090784122
rmse: 0.08969080726496569
mae: 0.0424408789669119
r2: 0.6372797000946492
pearson: 0.8156335504923333

=== Experiment 3729 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006172983337001206
rmse: 0.07856833546029346
mae: 0.03217681189982084
r2: 0.7216628982723574
pearson: 0.853318595221299

=== Experiment 3548 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00645835728932217
rmse: 0.08036390538868908
mae: 0.03351891507949247
r2: 0.708795512364887
pearson: 0.8447931157037714

=== Experiment 3952 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006551984639981498
rmse: 0.08094433049930982
mae: 0.03942541669388552
r2: 0.7045738963322064
pearson: 0.8420013878978968

=== Experiment 3438 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.01100836433256899
rmse: 0.10492075263058777
mae: 0.03871252009918645
r2: 0.5036376973961353
pearson: 0.7708064787260713

=== Experiment 3632 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0059650260440233605
rmse: 0.07723358106434895
mae: 0.03304756294770029
r2: 0.7310396010837241
pearson: 0.8559769539566953

=== Experiment 3719 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.013066796427752141
rmse: 0.11431008891498659
mae: 0.04605960356208664
r2: 0.4108239002095766
pearson: 0.6965908511606671

=== Experiment 3842 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007285123794380639
rmse: 0.0853529366476669
mae: 0.04245243098631491
r2: 0.6715169745395675
pearson: 0.8300614951637554

=== Experiment 3980 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00719188526768337
rmse: 0.08480498374319383
mae: 0.04112278697268509
r2: 0.6757210586709335
pearson: 0.8245957135271275

=== Experiment 3878 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006319184629242879
rmse: 0.07949329927259831
mae: 0.03395521707926513
r2: 0.71507074635329
pearson: 0.8481995333766121

=== Experiment 3527 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007340765794879946
rmse: 0.08567826909362691
mae: 0.03828728826747921
r2: 0.6690081012269711
pearson: 0.8230734738979331

=== Experiment 3805 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006574785520407537
rmse: 0.08108505115252464
mae: 0.041088097543284394
r2: 0.703545814669231
pearson: 0.8438641203792563

=== Experiment 3984 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00672626916482186
rmse: 0.0820138352037134
mae: 0.044006020756008894
r2: 0.69671548381564
pearson: 0.8457052837802923

=== Experiment 3948 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0067582338674526754
rmse: 0.0822084780752732
mae: 0.0429235845195029
r2: 0.695274209442758
pearson: 0.8402444308283494

=== Experiment 3407 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007673093667560785
rmse: 0.08759619665008742
mae: 0.035562971804840214
r2: 0.6540235837164744
pearson: 0.8241185968068508

=== Experiment 3514 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061279216268007985
rmse: 0.07828104257609755
mae: 0.02975703259037663
r2: 0.7236947109521185
pearson: 0.8546483663947412

=== Experiment 3850 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.012544981938158256
rmse: 0.11200438356670803
mae: 0.044060289268243014
r2: 0.4343522858772443
pearson: 0.6748982208868848

=== Experiment 3818 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.01252514082189318
rmse: 0.11191577557204874
mae: 0.043632857989159855
r2: 0.4352469130768932
pearson: 0.6666108165965239

=== Experiment 3487 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005629448796564216
rmse: 0.07502965278184497
mae: 0.026917497205615478
r2: 0.7461706314728154
pearson: 0.8654180051386261

=== Experiment 3899 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007046908581847679
rmse: 0.08394586697299444
mae: 0.03365927805022977
r2: 0.6822579936261455
pearson: 0.8411152760425541

=== Experiment 3925 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006538970536149147
rmse: 0.08086390131665147
mae: 0.033107545193952675
r2: 0.7051606965460617
pearson: 0.8419941577457841

=== Experiment 3835 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006713721120356791
rmse: 0.08193729993328308
mae: 0.03604540356468983
r2: 0.697281269617753
pearson: 0.855193808141912

=== Experiment 3776 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006772494437841885
rmse: 0.08229516655212434
mae: 0.03845818123892912
r2: 0.6946312066004656
pearson: 0.8388543285246403

=== Experiment 3211 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006455670605092219
rmse: 0.08034718791029477
mae: 0.029594528410276953
r2: 0.7089166537743152
pearson: 0.8534708836970726

=== Experiment 3968 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007500016476020761
rmse: 0.08660263550274183
mae: 0.038630509872334295
r2: 0.6618275581059165
pearson: 0.8241185270735297

=== Experiment 3699 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006415820135526158
rmse: 0.08009881481973474
mae: 0.028645614056567556
r2: 0.7107134939075159
pearson: 0.8554095609599569

=== Experiment 3636 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007778695187867369
rmse: 0.08819691144176971
mae: 0.036219314816968946
r2: 0.6492620576446342
pearson: 0.8130716457032116

=== Experiment 3839 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005923281817390634
rmse: 0.0769628599870784
mae: 0.03696104799310044
r2: 0.7329218298895546
pearson: 0.8619013562080647

=== Experiment 3577 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006462191795779921
rmse: 0.08038775899214955
mae: 0.03917856646164053
r2: 0.7086226161564022
pearson: 0.8627472143328327

=== Experiment 3815 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005901063443742911
rmse: 0.07681837959592035
mae: 0.029406032351644837
r2: 0.7339236465782764
pearson: 0.8573554639754217

=== Experiment 3949 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006495262210142772
rmse: 0.08059318960149656
mae: 0.03585817991130681
r2: 0.7071314857281839
pearson: 0.8412055607987182

=== Experiment 3857 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006099804363568323
rmse: 0.07810124431510886
mae: 0.02986218179103785
r2: 0.7249625059759168
pearson: 0.8518620964231769

=== Experiment 3607 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009503335594566857
rmse: 0.09748505318543381
mae: 0.05783264193303433
r2: 0.5714987807788464
pearson: 0.7747921138060362

=== Experiment 3599 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006353082550248173
rmse: 0.07970622654628792
mae: 0.037369096333927376
r2: 0.7135423040147775
pearson: 0.8452805081324203

=== Experiment 3640 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006826850444986676
rmse: 0.08262475685281426
mae: 0.03763547611070875
r2: 0.6921803181622151
pearson: 0.8386992420588745

=== Experiment 3658 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006076444741615218
rmse: 0.07795155381142327
mae: 0.02996066435897428
r2: 0.7260157810484265
pearson: 0.852664350068109

=== Experiment 3819 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0065834653041960175
rmse: 0.08113855621217336
mae: 0.03161364221672213
r2: 0.7031544470384734
pearson: 0.8447897924483656

=== Experiment 3562 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00558160771289198
rmse: 0.07471015803016334
mae: 0.03034483817034771
r2: 0.7483277648791254
pearson: 0.8651175313410635

=== Experiment 3991 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0072405229214711885
rmse: 0.08509126230977648
mae: 0.04170807883319344
r2: 0.6735280082687092
pearson: 0.8319698406219672

=== Experiment 3921 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006266035768353954
rmse: 0.07915829563825862
mae: 0.03304522389802599
r2: 0.717467205098169
pearson: 0.8476903838627878

=== Experiment 3880 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0059415797691060685
rmse: 0.07708164352883291
mae: 0.034064288089111765
r2: 0.7320967833002503
pearson: 0.866269330505175

=== Experiment 3588 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006459429717044037
rmse: 0.08037057743380993
mae: 0.033696654637540555
r2: 0.7087471570709195
pearson: 0.8561344266387559

=== Experiment 3738 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011155710587928424
rmse: 0.10562059736589462
mae: 0.042593582826453556
r2: 0.4969939195940247
pearson: 0.7599764990430555

=== Experiment 3967 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006030891056752148
rmse: 0.0776588118422639
mae: 0.03168431493660779
r2: 0.7280697766491921
pearson: 0.8546365892979736

=== Experiment 3742 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006504900005023115
rmse: 0.08065296029919246
mae: 0.03424966844262731
r2: 0.7066969217989469
pearson: 0.8411458653729148

=== Experiment 3647 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.009443603418957933
rmse: 0.09717820444399007
mae: 0.04076159336681552
r2: 0.5741920782869119
pearson: 0.7912031078888707

=== Experiment 3445 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007639266453878608
rmse: 0.08740289728537955
mae: 0.03827624751358564
r2: 0.6555488378929085
pearson: 0.8096955860565004

=== Experiment 3686 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006049299732944819
rmse: 0.07777724431313325
mae: 0.034206204073458014
r2: 0.7272397375419427
pearson: 0.8570536581917487

=== Experiment 3691 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005678203498296213
rmse: 0.07535385523180757
mae: 0.02646216177801715
r2: 0.7439723034303047
pearson: 0.8637026490430922

=== Experiment 3740 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005773455974379107
rmse: 0.07598326114598601
mae: 0.028676738757706457
r2: 0.739677411207548
pearson: 0.8670680566199516

=== Experiment 3656 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006344982887524974
rmse: 0.07965540086852224
mae: 0.029972292898706464
r2: 0.7139075142420321
pearson: 0.8529218746753159

=== Experiment 3752 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006077131337200485
rmse: 0.07795595767611661
mae: 0.03425606283098614
r2: 0.7259848227556805
pearson: 0.8526341365852032

=== Experiment 3679 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0063476366611096065
rmse: 0.07967205696547319
mae: 0.02860352154398958
r2: 0.7137878567591163
pearson: 0.8519244600734004

=== Experiment 3718 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0063179251818124395
rmse: 0.07948537715713777
mae: 0.035148490420208074
r2: 0.7151275342836034
pearson: 0.8530070860660249

=== Experiment 3956 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006570159692333822
rmse: 0.08105652159039285
mae: 0.04156250230475609
r2: 0.7037543912210922
pearson: 0.8432153487238742

=== Experiment 3253 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006318408062745384
rmse: 0.07948841464481088
mae: 0.027395369343542027
r2: 0.715105761394869
pearson: 0.8477325499765501

=== Experiment 3763 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005785351130023885
rmse: 0.07606149571250809
mae: 0.030592835320912844
r2: 0.7391410638749834
pearson: 0.8611142540626507

=== Experiment 3432 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008123688519960124
rmse: 0.09013150681066041
mae: 0.042301776215297716
r2: 0.6337064601437492
pearson: 0.7963019883652075

=== Experiment 3490 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0060397297654035224
rmse: 0.07771569832024623
mae: 0.03315295513391477
r2: 0.727671243166315
pearson: 0.853966492644866

=== Experiment 3989 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006483015591075723
rmse: 0.08051717575198303
mae: 0.042531981648520296
r2: 0.7076836803917677
pearson: 0.8464640899856266

=== Experiment 3613 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006219581167813715
rmse: 0.07886432126008386
mae: 0.033241558791293216
r2: 0.7195618225902951
pearson: 0.8552830838329913

=== Experiment 3860 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00810099838167462
rmse: 0.0900055463939563
mae: 0.03817252703429181
r2: 0.634729548492349
pearson: 0.7990849836287826

=== Experiment 3571 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008380391847632597
rmse: 0.09154448015927884
mae: 0.04148046314613384
r2: 0.6221318200827722
pearson: 0.8205875379357074

=== Experiment 3993 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006824187388381065
rmse: 0.08260863991363776
mae: 0.031412586085727584
r2: 0.6923003942124614
pearson: 0.8385195279811769

=== Experiment 3930 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009570074143407468
rmse: 0.0978267557645017
mae: 0.04726420819236126
r2: 0.568489568985505
pearson: 0.7560568767507629

=== Experiment 3996 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006431412816918807
rmse: 0.08019608978571716
mae: 0.03676061130666869
r2: 0.7100104267663854
pearson: 0.8431186571399704

=== Experiment 3828 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057787571986577385
rmse: 0.076018137300632
mae: 0.03101379682573274
r2: 0.7394383813380717
pearson: 0.8716426695312459

=== Experiment 3951 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007234086092516743
rmse: 0.08505343080979592
mae: 0.0458251463119323
r2: 0.6738182420532
pearson: 0.8286108866007647

=== Experiment 3917 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.011823285518900434
rmse: 0.10873493237640071
mae: 0.047298381252953055
r2: 0.466893259778694
pearson: 0.6888010309937811

=== Experiment 3840 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009448859377049998
rmse: 0.09720524356766973
mae: 0.044309288465489184
r2: 0.5739550894498647
pearson: 0.7703321774043033

=== Experiment 3971 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007113870493173381
rmse: 0.08434376380725123
mae: 0.03251372418005274
r2: 0.6792387105166607
pearson: 0.8267543561420081

=== Experiment 3798 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006131132371770072
rmse: 0.07830154769715648
mae: 0.030539364256062244
r2: 0.7235499398746108
pearson: 0.8570621436526845

=== Experiment 3578 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.010096459088408588
rmse: 0.1004811379732962
mae: 0.03925881126446209
r2: 0.5447551034951362
pearson: 0.7763568872012174

=== Experiment 3986 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006459758936327911
rmse: 0.08037262554083892
mae: 0.02952204437856638
r2: 0.7087323127183102
pearson: 0.8423323056159012

=== Experiment 3712 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007134120305025547
rmse: 0.08446372182792768
mae: 0.03625540045418314
r2: 0.6783256554128703
pearson: 0.8263518567244379

=== Experiment 3601 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006336291713659701
rmse: 0.07960082734280907
mae: 0.03388586470359664
r2: 0.7142993954463392
pearson: 0.8538880930048514

=== Experiment 3690 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006001518057772878
rmse: 0.07746946532520331
mae: 0.03423550665586862
r2: 0.7293941922451221
pearson: 0.8603654061402525

=== Experiment 3723 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006788477001204227
rmse: 0.08239221444532382
mae: 0.031675134306614616
r2: 0.6939105598529216
pearson: 0.8439036014876413

=== Experiment 3856 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006073104088951859
rmse: 0.07793012311649365
mae: 0.030718318033332248
r2: 0.7261664096066837
pearson: 0.8523375558208006

=== Experiment 3356 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006034320471887848
rmse: 0.07768088871716033
mae: 0.032904120665354336
r2: 0.7279151458301245
pearson: 0.8535966470976672

=== Experiment 3477 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005344822714700334
rmse: 0.07310829443161927
mae: 0.029202875217117133
r2: 0.759004296230539
pearson: 0.8720098253465535

=== Experiment 3958 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005833079924207107
rmse: 0.07637460261243333
mae: 0.030142372715329196
r2: 0.7369889935523106
pearson: 0.8633851217009316

=== Experiment 3746 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006357350232746693
rmse: 0.07973299337630999
mae: 0.030671035194311635
r2: 0.7133498760892698
pearson: 0.8452656320114961

=== Experiment 3721 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007428759917138553
rmse: 0.08619025418884987
mae: 0.03932773147368982
r2: 0.6650404849835052
pearson: 0.8156490450156041

=== Experiment 3940 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.01067808677664889
rmse: 0.10333482847834456
mae: 0.04278203114317997
r2: 0.5185297670263035
pearson: 0.7854346698553921

=== Experiment 3781 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005927346809322465
rmse: 0.07698926424718232
mae: 0.035782264602712624
r2: 0.7327385411924883
pearson: 0.8641284458180293

=== Experiment 3795 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007678568171130107
rmse: 0.08762743960158888
mae: 0.03565099844661528
r2: 0.653776740760047
pearson: 0.8101754736466039

=== Experiment 3945 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0055127674600286905
rmse: 0.07424801317226401
mae: 0.031239161453385806
r2: 0.7514317415818187
pearson: 0.8670267725090601

=== Experiment 3851 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006722116909500693
rmse: 0.08198851693682899
mae: 0.0333931928247929
r2: 0.696902707180524
pearson: 0.8442341574551104

=== Experiment 3814 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006099085910993408
rmse: 0.07809664468460478
mae: 0.03313162787594958
r2: 0.7249949006863046
pearson: 0.8587384430483398

=== Experiment 3650 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006772561055014232
rmse: 0.08229557129648127
mae: 0.03274790928896599
r2: 0.694628202861485
pearson: 0.8421746954434914

=== Experiment 3457 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009830405343131106
rmse: 0.09914840060803354
mae: 0.04407130581096635
r2: 0.5567513497704897
pearson: 0.7709091064916218

=== Experiment 3827 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0064419250746736234
rmse: 0.08026160398767036
mae: 0.03738236273032016
r2: 0.7095364336910193
pearson: 0.8454977027736852

=== Experiment 3885 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006641521711253871
rmse: 0.08149553184840179
mae: 0.04411298981550135
r2: 0.7005367091968138
pearson: 0.8498983844745833

=== Experiment 3551 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0070849121202842015
rmse: 0.08417192002255978
mae: 0.03609613805258334
r2: 0.6805444308046791
pearson: 0.8346317285647411

=== Experiment 3897 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.016031078238017752
rmse: 0.1266138943324063
mae: 0.0555544745659922
r2: 0.2771657380648992
pearson: 0.536445614408881

=== Experiment 3887 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006697883152363831
rmse: 0.08184059599223255
mae: 0.03715736050439757
r2: 0.697995396623733
pearson: 0.8438648615663896

=== Experiment 3630 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00768935949473276
rmse: 0.08768899300786137
mae: 0.038564574743052756
r2: 0.653290164728428
pearson: 0.8210834370251159

=== Experiment 3877 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0066112791657280175
rmse: 0.08130977287957468
mae: 0.04342212130045424
r2: 0.7019003322638122
pearson: 0.8450280058438073

=== Experiment 3894 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006119021670842069
rmse: 0.07822417574408866
mae: 0.03324135320067043
r2: 0.7240960060491274
pearson: 0.8510501280000969

=== Experiment 3961 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005918162103726401
rmse: 0.07692959185987146
mae: 0.03062494018878754
r2: 0.7331526755253166
pearson: 0.8600505959077143

=== Experiment 3965 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.010436410029679107
rmse: 0.10215874915874365
mae: 0.046660783088155314
r2: 0.5294268651771001
pearson: 0.7318593663677557

=== Experiment 3918 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006021229457536258
rmse: 0.07759658148099217
mae: 0.03009883571665614
r2: 0.7285054139054417
pearson: 0.8578463644154105

=== Experiment 3593 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006358746358071085
rmse: 0.07974174789952303
mae: 0.03132369967245397
r2: 0.7132869254128748
pearson: 0.8458780956168112

=== Experiment 3541 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005812385099330585
rmse: 0.07623899985788497
mae: 0.03001453819340992
r2: 0.7379221140975041
pearson: 0.8591006727281139

=== Experiment 3441 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0064902355788892365
rmse: 0.08056199835461653
mae: 0.03522471098452255
r2: 0.7073581343190161
pearson: 0.8517504282530687

=== Experiment 3942 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.010806147143394815
rmse: 0.10395261970433846
mae: 0.03977730026176002
r2: 0.5127555814534075
pearson: 0.7665690421666765

=== Experiment 3922 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006455849605280155
rmse: 0.08034830181951673
mae: 0.039031388229746306
r2: 0.7089085827346253
pearson: 0.8473033787083685

=== Experiment 3704 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006269256461359783
rmse: 0.07917863639492526
mae: 0.031003404239145355
r2: 0.7173219854680717
pearson: 0.8469561488008578

=== Experiment 3762 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007595885576671855
rmse: 0.08715437783996771
mae: 0.03619890982548136
r2: 0.6575048625527767
pearson: 0.8421686211980158

=== Experiment 3937 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007797202945856239
rmse: 0.08830177204256005
mae: 0.043088804539662524
r2: 0.6484275509828032
pearson: 0.8064219692633195

=== Experiment 3715 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006821920257202245
rmse: 0.08259491665473273
mae: 0.02958852378145946
r2: 0.6924026181594736
pearson: 0.8345226816049389

=== Experiment 3888 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0059433693701069065
rmse: 0.07709325113203429
mae: 0.03143114047347803
r2: 0.7320160909787861
pearson: 0.8569862571547402

=== Experiment 3730 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005627619272406422
rmse: 0.07501745978374916
mae: 0.027949686320215536
r2: 0.7462531239118552
pearson: 0.8651084507560325

=== Experiment 3468 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0053325286376804506
rmse: 0.07302416475167964
mae: 0.03053731186280256
r2: 0.7595586307523284
pearson: 0.8718372689604597

=== Experiment 3916 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006227710091893798
rmse: 0.07891584183098979
mae: 0.031599002006482316
r2: 0.7191952929813372
pearson: 0.8517301746885204

=== Experiment 3663 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005981974561473228
rmse: 0.07734322569865591
mae: 0.02872392949543523
r2: 0.7302753998915227
pearson: 0.8613002999220855

=== Experiment 3955 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005753677807024267
rmse: 0.07585300130531597
mae: 0.031327375381725385
r2: 0.7405692000685407
pearson: 0.8642039521400812

=== Experiment 3521 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007868615449685114
rmse: 0.08870521658665353
mae: 0.045116007849569414
r2: 0.6452075926162057
pearson: 0.8197029436964708

=== Experiment 3972 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006479161326055382
rmse: 0.08049323776600978
mae: 0.03207922395316597
r2: 0.7078574675051432
pearson: 0.8443280754141589

=== Experiment 3648 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0063251999104006655
rmse: 0.07953112541892429
mae: 0.03244907529804499
r2: 0.7147995199734131
pearson: 0.8461266625913669

=== Experiment 3790 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007122238724254132
rmse: 0.08439335710975202
mae: 0.03957154027109037
r2: 0.6788613906603698
pearson: 0.8255816707646023

=== Experiment 3992 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006644843036110255
rmse: 0.08151590664471722
mae: 0.032369853531202454
r2: 0.7003869521208662
pearson: 0.8389541008267872

=== Experiment 3586 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005682952560744685
rmse: 0.07538536038744316
mae: 0.03445404936269984
r2: 0.7437581702947256
pearson: 0.8697474913587938

=== Experiment 3883 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0062876224234851355
rmse: 0.07929452959369351
mae: 0.03636627871891916
r2: 0.7164938723193176
pearson: 0.8466729015524317

=== Experiment 3618 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006066394007935682
rmse: 0.07788705930984736
mae: 0.0307360482579169
r2: 0.7264689642063706
pearson: 0.855327762480446

=== Experiment 3914 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0063555456162765304
rmse: 0.07972167594999825
mae: 0.04044916992453613
r2: 0.713431245451637
pearson: 0.8501347472662939

=== Experiment 3802 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007356307159748228
rmse: 0.08576891721217092
mae: 0.03587469977026608
r2: 0.6683073479253379
pearson: 0.8253658258269292

=== Experiment 3629 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0067556199036417665
rmse: 0.08219257815424558
mae: 0.030876296397744973
r2: 0.6953920719204983
pearson: 0.8347630458368166

=== Experiment 3391 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006291453359951874
rmse: 0.07931868228829746
mae: 0.031592811130463236
r2: 0.7163211370801565
pearson: 0.8473755285914969

=== Experiment 3614 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.010292899446272175
rmse: 0.10145392770253982
mae: 0.056192095837413886
r2: 0.5358976942190805
pearson: 0.7739246783803295

=== Experiment 3985 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006599851425006579
rmse: 0.08123946962534023
mae: 0.04073526955219123
r2: 0.7024156040632087
pearson: 0.8440468443119303

=== Experiment 3964 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.011690415056130866
rmse: 0.10812222276725014
mae: 0.04862727502690839
r2: 0.47288433046421097
pearson: 0.6877626078284448

=== Experiment 3296 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005506647658338863
rmse: 0.07420678983987154
mae: 0.027734654853471263
r2: 0.7517076807464822
pearson: 0.8691436817444441

=== Experiment 3932 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005941860826941422
rmse: 0.07708346662509037
mae: 0.029892060971487555
r2: 0.7320841105261571
pearson: 0.8556561859486719

=== Experiment 3545 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005812248874728304
rmse: 0.07623810644768339
mae: 0.036277866184623454
r2: 0.7379282564048638
pearson: 0.8635884154249011

=== Experiment 3910 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006425542366364506
rmse: 0.08015948082644064
mae: 0.02894570256357827
r2: 0.7102751227980959
pearson: 0.8448482386416519

=== Experiment 3622 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006423539538785146
rmse: 0.08014698708488764
mae: 0.029054517562963768
r2: 0.7103654294121375
pearson: 0.8586886645277356

=== Experiment 3859 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008196522480123283
rmse: 0.0905346479538264
mae: 0.04359004908817962
r2: 0.6304224089366699
pearson: 0.8049127084838085

=== Experiment 3954 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006229765735808724
rmse: 0.07892886503560484
mae: 0.033030333319325665
r2: 0.7191026049019714
pearson: 0.8486401365486195

=== Experiment 3936 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0065961877163393965
rmse: 0.08121691767322493
mae: 0.03337627552896505
r2: 0.7025807990744903
pearson: 0.8452081745938766

=== Experiment 3845 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0058249719561574344
rmse: 0.07632150389082644
mae: 0.029765917186259153
r2: 0.7373545782630808
pearson: 0.8616884691998229

=== Experiment 3760 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0173410644412002
rmse: 0.13168547543749917
mae: 0.05537552853640346
r2: 0.21809903673242304
pearson: 0.5183251508646982

=== Experiment 3237 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005940952998323323
rmse: 0.07707757779226929
mae: 0.029517879741605885
r2: 0.7321250441189813
pearson: 0.8606055038941229

=== Experiment 3793 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006507687414724268
rmse: 0.08067023871741219
mae: 0.033426472410065314
r2: 0.706571238722355
pearson: 0.8410125230577107

=== Experiment 3456 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005665479636695217
rmse: 0.07526938047237547
mae: 0.03404053092801001
r2: 0.7445460167496941
pearson: 0.8636895006486351

=== Experiment 3427 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005981870236267859
rmse: 0.07734255126557346
mae: 0.030402525929589988
r2: 0.7302801038691158
pearson: 0.8568949648927352

=== Experiment 3970 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006987820090484739
rmse: 0.08359318208134404
mae: 0.03159959186897284
r2: 0.6849222676948665
pearson: 0.8341782630330662

=== Experiment 3973 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005666023527404819
rmse: 0.07527299334691573
mae: 0.034705628057247716
r2: 0.7445214929569827
pearson: 0.8642393333517625

=== Experiment 3616 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006141368946796733
rmse: 0.07836688680046397
mae: 0.028098983897157693
r2: 0.7230883772121199
pearson: 0.8528043632416341

=== Experiment 3572 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006004598554402502
rmse: 0.07748934477979862
mae: 0.028986521843944103
r2: 0.7292552940079227
pearson: 0.8560518550965525

=== Experiment 3924 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005853866757853101
rmse: 0.07651056631507246
mae: 0.032740027100866924
r2: 0.736051724372198
pearson: 0.8580918413660127

=== Experiment 3539 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007404736314600473
rmse: 0.08605077753629232
mae: 0.03919077164439963
r2: 0.6661236986483514
pearson: 0.8238366789603518

=== Experiment 3651 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007732063982029444
rmse: 0.08793215556341971
mae: 0.04026099948772794
r2: 0.6513646381919027
pearson: 0.83506901877908

=== Experiment 3474 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008406289838543846
rmse: 0.09168582136046907
mae: 0.0433435519671967
r2: 0.6209640910711616
pearson: 0.789482477266389

=== Experiment 3893 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00923860927579199
rmse: 0.09611768451118655
mae: 0.04328177242814388
r2: 0.5834351739774422
pearson: 0.7663204326279215

=== Experiment 3759 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006270940201622411
rmse: 0.07918926822254649
mae: 0.03481899294456867
r2: 0.7172460663607005
pearson: 0.8472352815967729

=== Experiment 3943 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006782944257149247
rmse: 0.08235863195287575
mae: 0.04391903928105957
r2: 0.6941600288472138
pearson: 0.8442083687349892

=== Experiment 3990 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00651429652844869
rmse: 0.08071119208913155
mae: 0.0314213892956374
r2: 0.7062732366934141
pearson: 0.84200427945308

=== Experiment 3926 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0074733221260032835
rmse: 0.08644837838851162
mae: 0.03522980773427421
r2: 0.6630311945991227
pearson: 0.8250940070799755

=== Experiment 3538 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006339159485120842
rmse: 0.0796188387576762
mae: 0.03137940807824734
r2: 0.7141700888933591
pearson: 0.8560971731715521

=== Experiment 3898 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006761145819993935
rmse: 0.08222618694791785
mae: 0.04223407694237145
r2: 0.6951429107843208
pearson: 0.8428990378267065

=== Experiment 3775 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005650125354989802
rmse: 0.07516731573622808
mae: 0.028489499362061927
r2: 0.7452383345538549
pearson: 0.8639116744473063

=== Experiment 3808 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007038986891771272
rmse: 0.08389867038142662
mae: 0.03363821020678461
r2: 0.6826151791450883
pearson: 0.8262758139667932

=== Experiment 3479 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006306084552091925
rmse: 0.07941085915724577
mae: 0.02811208353259211
r2: 0.7156614230662418
pearson: 0.8491675265970224

=== Experiment 3488 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005949399138708672
rmse: 0.07713234819910951
mae: 0.03027279694929104
r2: 0.7317442113664365
pearson: 0.8556617681824256

=== Experiment 3981 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005285628824786251
rmse: 0.0727023302569199
mae: 0.029939149358451305
r2: 0.7616733226735417
pearson: 0.87484328606932

=== Experiment 3935 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0078033588305832105
rmse: 0.08833662225024913
mae: 0.033061660357038766
r2: 0.648149984849876
pearson: 0.8302216117459033

=== Experiment 3451 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005388228019243341
rmse: 0.07340455039875485
mae: 0.02702588784530778
r2: 0.7570471701528304
pearson: 0.8701923419662769

=== Experiment 3832 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008074335805690223
rmse: 0.08985730802605998
mae: 0.042458901498908214
r2: 0.6359317523083885
pearson: 0.8145521440672095

=== Experiment 3873 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006798341967404008
rmse: 0.08245205860015872
mae: 0.036293172591629654
r2: 0.6934657528688772
pearson: 0.8557814495661403

=== Experiment 3896 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008095815467181692
rmse: 0.0899767495922235
mae: 0.04183413525624914
r2: 0.63496324382566
pearson: 0.8026024994467308

=== Experiment 3440 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006140042065080762
rmse: 0.07835842051165122
mae: 0.02816850890002096
r2: 0.7231482057246876
pearson: 0.8533402176189233

=== Experiment 3879 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007289296113781362
rmse: 0.0853773747182552
mae: 0.03864925865659312
r2: 0.671328846494167
pearson: 0.8201546366365613

=== Experiment 3657 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005658000461442556
rmse: 0.07521968134366534
mae: 0.03103381756203524
r2: 0.7448832494699997
pearson: 0.8641401693146853

=== Experiment 3998 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00684016791372012
rmse: 0.08270530765144471
mae: 0.03658549992595545
r2: 0.6915798393584904
pearson: 0.8457152625989681

=== Experiment 3714 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006680634606283753
rmse: 0.08173514914823214
mae: 0.032603676679182056
r2: 0.6987731259748179
pearson: 0.8514768412815261

=== Experiment 3960 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007963640787454004
rmse: 0.08923923345398034
mae: 0.04274260768134788
r2: 0.6409229419600562
pearson: 0.8122722626145507

=== Experiment 3931 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.010562617471533921
rmse: 0.10277459545789476
mae: 0.046246106007201526
r2: 0.5237362271720096
pearson: 0.7830239552091481

=== Experiment 3941 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0066480166565790885
rmse: 0.08153537058589412
mae: 0.034678553894450674
r2: 0.7002438549707437
pearson: 0.8574254563031354

=== Experiment 3838 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005379974518233576
rmse: 0.07334830957993221
mae: 0.030985855029195108
r2: 0.757419316880717
pearson: 0.8720644579426777

=== Experiment 3786 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006656071297815099
rmse: 0.08158474917418757
mae: 0.031917073185406215
r2: 0.6998806747425307
pearson: 0.8449288715911868

=== Experiment 3707 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005671896279314074
rmse: 0.07531199293149846
mae: 0.028801484568600336
r2: 0.7442566931581902
pearson: 0.8637860074483611

=== Experiment 3853 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006532995712388529
rmse: 0.08082694917160074
mae: 0.03821292617698589
r2: 0.7054300987197071
pearson: 0.8477670411076378

=== Experiment 3785 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006056325400694878
rmse: 0.07782239652371853
mae: 0.027331868185989143
r2: 0.7269229532753254
pearson: 0.8534356338567872

=== Experiment 3976 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006277556961975178
rmse: 0.07923103534584902
mae: 0.028495379371107583
r2: 0.7169477195486532
pearson: 0.8473946924871986

=== Experiment 3904 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006649023664529634
rmse: 0.08154154563490708
mae: 0.034390726404708395
r2: 0.7001984494254718
pearson: 0.8488611288043967

=== Experiment 3384 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007820340400010917
rmse: 0.08843268852642057
mae: 0.03713945412122399
r2: 0.6473842933585412
pearson: 0.8083266415571779

=== Experiment 3439 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005814514242009258
rmse: 0.07625296218514568
mae: 0.029829143141731317
r2: 0.73782611199123
pearson: 0.8634650792274065

=== Experiment 3891 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006989781316227609
rmse: 0.0836049120340881
mae: 0.030359930991540034
r2: 0.6848338368893221
pearson: 0.8331160922665601

=== Experiment 3502 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006398237395092207
rmse: 0.07998898296073158
mae: 0.032716360199890084
r2: 0.7115062919349269
pearson: 0.849438739230957

=== Experiment 3575 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006712450438082366
rmse: 0.0819295455747337
mae: 0.03274584187893666
r2: 0.6973385641222369
pearson: 0.8427432645349088

=== Experiment 3453 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0056296676559502296
rmse: 0.07503111125360086
mae: 0.028457769562627976
r2: 0.7461607631994274
pearson: 0.8708223241495123

=== Experiment 3974 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009254223568480305
rmse: 0.09619887508947443
mae: 0.043871717771530934
r2: 0.5827311323925015
pearson: 0.7662003379725493

=== Experiment 3919 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005730739294389036
rmse: 0.07570164657647174
mae: 0.031480422981608205
r2: 0.7416034875072516
pearson: 0.8613926436832361

=== Experiment 3755 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00619402075059841
rmse: 0.07870210130992952
mae: 0.03119697755854364
r2: 0.7207143305525334
pearson: 0.8499004083703137

=== Experiment 3677 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005791511168267529
rmse: 0.07610197874081547
mae: 0.030901934742348584
r2: 0.7388633104618176
pearson: 0.8625267625492256

=== Experiment 3929 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.015441060963436191
rmse: 0.12426206566541613
mae: 0.04562539698207245
r2: 0.30376935728931975
pearson: 0.5801707085506127

=== Experiment 3493 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006919681809254036
rmse: 0.08318462483688939
mae: 0.03212990902430636
r2: 0.6879945928056079
pearson: 0.8435044888933209

=== Experiment 3519 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005680929864345396
rmse: 0.07537194348260762
mae: 0.030103643569938037
r2: 0.7438493727851125
pearson: 0.8653482289842864

=== Experiment 3680 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005958898938327493
rmse: 0.07719390480036291
mae: 0.030449617304955413
r2: 0.7313158695828055
pearson: 0.8557772059954772

=== Experiment 3567 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0074323170269816686
rmse: 0.08621088694000119
mae: 0.036073162145867796
r2: 0.664880096466283
pearson: 0.8166158789836093

=== Experiment 3612 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00612663181812611
rmse: 0.07827280382180077
mae: 0.0399003424066426
r2: 0.723752867857572
pearson: 0.8586078073547673

=== Experiment 3823 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005487712079178546
rmse: 0.0740790934014351
mae: 0.031383784638169814
r2: 0.7525614776766352
pearson: 0.8691360253223003

=== Experiment 3492 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005636715514612753
rmse: 0.07507806280540777
mae: 0.029960953709037882
r2: 0.7458429783543341
pearson: 0.8638912164879735

=== Experiment 3895 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007283324536093119
rmse: 0.0853423958891073
mae: 0.04299113518187015
r2: 0.6715981023038375
pearson: 0.8254566125679108

=== Experiment 3852 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006218301980335984
rmse: 0.07885621079113543
mae: 0.03460921500579555
r2: 0.7196195005906525
pearson: 0.8526028362676291

=== Experiment 3886 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008350763019750445
rmse: 0.09138250937542941
mae: 0.040767915779730456
r2: 0.6234677708913341
pearson: 0.7920727442744488

=== Experiment 3638 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005921132821578081
rmse: 0.07694889746824239
mae: 0.029666974863137706
r2: 0.7330187271648974
pearson: 0.8578314940958905

=== Experiment 3766 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008896968148569655
rmse: 0.09432374117140209
mae: 0.04320747656852037
r2: 0.5988396220361377
pearson: 0.7778878339826695

=== Experiment 3635 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00570137926717407
rmse: 0.07550747822020061
mae: 0.028461438021496036
r2: 0.7429273182120382
pearson: 0.8621518281499233

=== Experiment 3455 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005985618278431459
rmse: 0.0773667776143705
mae: 0.028758363504650317
r2: 0.7301111062975985
pearson: 0.8584046974643803

=== Experiment 3708 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00792544978827203
rmse: 0.089024995300601
mae: 0.041930456153058196
r2: 0.6426449573040272
pearson: 0.8026820449474551

=== Experiment 3675 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005673570104954253
rmse: 0.0753231047219527
mae: 0.03382432325389581
r2: 0.7441812210967815
pearson: 0.8633329573364558

=== Experiment 3947 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006148906716373486
rmse: 0.07841496487516579
mae: 0.02893225606100591
r2: 0.7227485024995303
pearson: 0.8555019049258717

=== Experiment 3988 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005831254964237202
rmse: 0.07636265425086534
mae: 0.02921488455155662
r2: 0.7370712801941273
pearson: 0.860661142407848

=== Experiment 3587 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005972044234068185
rmse: 0.07727900254317589
mae: 0.032636440833246926
r2: 0.7307231539835448
pearson: 0.856085789862507

=== Experiment 3836 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00549913560255964
rmse: 0.07415615687560703
mae: 0.029064610768149684
r2: 0.7520463960352588
pearson: 0.8673131478253923

=== Experiment 3876 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005468162318253309
rmse: 0.0739470237281617
mae: 0.026872569157852532
r2: 0.7534429677922463
pearson: 0.8686075241267242

=== Experiment 3963 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0064301616721307514
rmse: 0.08018828887144776
mae: 0.03575583201453249
r2: 0.7100668403342079
pearson: 0.8512108125940644

=== Experiment 3701 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007295042888744627
rmse: 0.08541102322735998
mae: 0.03687897246699613
r2: 0.6710697269404224
pearson: 0.8214461980852212

=== Experiment 3863 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00651692692655219
rmse: 0.0807274855706047
mae: 0.03279678718172194
r2: 0.7061546332006541
pearson: 0.8462621445494749

=== Experiment 3757 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006014929688606538
rmse: 0.07755597777480816
mae: 0.030115881471520955
r2: 0.7287894677137425
pearson: 0.8594934731847937

=== Experiment 3544 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012331845284467954
rmse: 0.11104884188710819
mae: 0.05036080654998645
r2: 0.44396252378352485
pearson: 0.6694061652805094

=== Experiment 3871 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067138561594993085
rmse: 0.08193812396863445
mae: 0.032097974700016144
r2: 0.6972751807622523
pearson: 0.838229431573181

=== Experiment 3959 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006925030421243223
rmse: 0.08321676766880112
mae: 0.03361653856512549
r2: 0.6877534262451197
pearson: 0.8440509356674125

=== Experiment 3316 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005181741704494803
rmse: 0.07198431568400719
mae: 0.03163678290942304
r2: 0.7663575472032645
pearson: 0.8761182886781791

=== Experiment 3982 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00598191240974172
rmse: 0.07734282390591722
mae: 0.029276893954824062
r2: 0.7302782022857414
pearson: 0.8664796878986958

=== Experiment 3700 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0052922297442458514
rmse: 0.07274771298292373
mae: 0.029479027418605443
r2: 0.7613756901203951
pearson: 0.8729670063905056

=== Experiment 3060 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005672179230517158
rmse: 0.07531387143492996
mae: 0.02849032885775882
r2: 0.7442439350129795
pearson: 0.8634035890698594

=== Experiment 3833 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005564807068769378
rmse: 0.07459763447167328
mae: 0.0326184761396578
r2: 0.7490852985280845
pearson: 0.8659891474388587

=== Experiment 3395 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060452514824444895
rmse: 0.07775121531168815
mae: 0.03545378898964291
r2: 0.7274222713752314
pearson: 0.8549893509803594

=== Experiment 3460 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005653834162338295
rmse: 0.07519198203491044
mae: 0.02726948419488879
r2: 0.7450711060628824
pearson: 0.8639277857905946

=== Experiment 3995 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0060499135756925
rmse: 0.07778119037204625
mae: 0.03437743442904773
r2: 0.7272120596426226
pearson: 0.8558902948409453

=== Experiment 3756 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005542497175246747
rmse: 0.07444794943614463
mae: 0.02995491929694566
r2: 0.7500912418076847
pearson: 0.8664399669684998

=== Experiment 3938 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00552314764938918
rmse: 0.07431788243343039
mae: 0.027618657004257263
r2: 0.750963703412225
pearson: 0.8704593690235534

=== Experiment 3854 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006129743811612369
rmse: 0.07829268044723191
mae: 0.02930119513202504
r2: 0.7236125494409695
pearson: 0.8513644616209262

=== Experiment 3796 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007030177170331956
rmse: 0.08384615179202894
mae: 0.03779991144513063
r2: 0.6830124056073321
pearson: 0.8312456304216146

=== Experiment 3500 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00973324315465146
rmse: 0.09865720021697079
mae: 0.04447599630407314
r2: 0.5611323500846873
pearson: 0.7590325924809326

=== Experiment 3653 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007916589264744705
rmse: 0.08897521713794637
mae: 0.03811045334353632
r2: 0.6430444744100594
pearson: 0.8048883572549806

=== Experiment 3903 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005658024387211427
rmse: 0.0752198403827835
mae: 0.027925398996358812
r2: 0.7448821706676119
pearson: 0.8634478820706214

=== Experiment 3844 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00735481312099094
rmse: 0.08576020709507959
mae: 0.032705511044811224
r2: 0.6683747134753273
pearson: 0.8249579542793892

=== Experiment 3782 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006950823909031311
rmse: 0.08337160133421519
mae: 0.0347364186710321
r2: 0.6865904092333369
pearson: 0.8359858053376462

=== Experiment 3901 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006317153996922852
rmse: 0.0794805258973722
mae: 0.033132638174031954
r2: 0.7151623066708505
pearson: 0.8518786507559064

=== Experiment 3639 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008671283280586972
rmse: 0.09311972551821109
mae: 0.043954204302928186
r2: 0.6090156534019702
pearson: 0.7976659913595113

=== Experiment 3837 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005951558171042673
rmse: 0.07714634256426336
mae: 0.03219576121022777
r2: 0.7316468615487592
pearson: 0.8558437480128634

=== Experiment 3461 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006177099200326093
rmse: 0.07859452398434698
mae: 0.027852636690236465
r2: 0.7214773158065682
pearson: 0.8533210442835691

=== Experiment 3644 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005519957319243627
rmse: 0.07429641525163665
mae: 0.03158113121401523
r2: 0.7511075539943162
pearson: 0.868077782323769

=== Experiment 3975 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.02049543590494911
rmse: 0.14316227123425052
mae: 0.06526386979517082
r2: 0.07586981577703811
pearson: 0.2836598148335395

=== Experiment 3483 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005677784626943136
rmse: 0.07535107581808727
mae: 0.03251461315412485
r2: 0.7439911901552558
pearson: 0.8629166140632601

=== Experiment 3617 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006368129949628352
rmse: 0.0798005635921724
mae: 0.029992488248068275
r2: 0.712863823399592
pearson: 0.844450419029464

=== Experiment 3826 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00576565750174768
rmse: 0.0759319267617231
mae: 0.02818123245700401
r2: 0.7400290409061286
pearson: 0.8624043849016609

=== Experiment 3564 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007160605638830611
rmse: 0.08462036184530654
mae: 0.034489105630551804
r2: 0.6771314433686868
pearson: 0.8231820832007469

=== Experiment 3953 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006607897358717608
rmse: 0.08128897439823933
mae: 0.036855521542502864
r2: 0.7020528164534646
pearson: 0.8393350482894869

=== Experiment 3330 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005426504206788417
rmse: 0.07366480982659507
mae: 0.02851037917204063
r2: 0.7553213137030613
pearson: 0.871479494494375

=== Experiment 3920 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006567039971010167
rmse: 0.08103727519487662
mae: 0.034438300385615635
r2: 0.7038950580824814
pearson: 0.8401536759661636

=== Experiment 3415 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005877401274238049
rmse: 0.07666421116947626
mae: 0.028030846361407173
r2: 0.7349905633867334
pearson: 0.8590648788451477

=== Experiment 3751 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062640712798925375
rmse: 0.07914588605791546
mae: 0.032984033428190035
r2: 0.717555783018257
pearson: 0.8471335570815204

=== Experiment 3797 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006326442499044844
rmse: 0.07953893699971634
mae: 0.033717171571487854
r2: 0.7147434921983524
pearson: 0.8472313524737388

=== Experiment 3884 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006219371958861294
rmse: 0.0788629948636323
mae: 0.030628672165143516
r2: 0.7195712557298799
pearson: 0.8523457767372329

=== Experiment 3665 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005514268902463878
rmse: 0.07425812347793255
mae: 0.02844340605579585
r2: 0.7513640422032515
pearson: 0.866927713962646

=== Experiment 3466 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0058484134034475395
rmse: 0.07647492009441749
mae: 0.03237153725331508
r2: 0.7362976137221398
pearson: 0.8585854824967006

=== Experiment 3209 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006126972188179463
rmse: 0.0782749780464962
mae: 0.030325997467330553
r2: 0.7237375207216745
pearson: 0.8635907582701613

=== Experiment 3778 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005625813947367661
rmse: 0.07500542611949926
mae: 0.027170516741992258
r2: 0.7463345252232685
pearson: 0.8653857188042777

=== Experiment 4021 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067291865072400875
rmse: 0.08203161894806227
mae: 0.037300301561038626
r2: 0.6965839421300226
pearson: 0.8365939245685327

=== Experiment 4022 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006389576814836397
rmse: 0.07993482854698819
mae: 0.039185105585564046
r2: 0.7118967936868486
pearson: 0.8466129897899337

=== Experiment 4028 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0085398198099665
rmse: 0.09241114548563123
mae: 0.03582589084083805
r2: 0.6149432834307489
pearson: 0.8032994410306044

=== Experiment 4037 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006805973704076655
rmse: 0.08249832546225805
mae: 0.03907728902178062
r2: 0.6931216412212916
pearson: 0.8339434775061839

=== Experiment 4016 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067239063822741
rmse: 0.08199942915822098
mae: 0.04057360638472212
r2: 0.6968220206407783
pearson: 0.838441383725636

=== Experiment 4013 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007253816447821022
rmse: 0.08516933983436188
mae: 0.040086904155988534
r2: 0.6729286090165236
pearson: 0.8258206536005939

=== Experiment 4040 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006870534074080331
rmse: 0.08288868483743925
mae: 0.039393412447801285
r2: 0.6902106425530179
pearson: 0.8318089788256885

=== Experiment 4046 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0066622009443538036
rmse: 0.08162230665911006
mae: 0.03337005668428417
r2: 0.6996042916779643
pearson: 0.8365023588639787

=== Experiment 4045 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00751020184904018
rmse: 0.08666142076518352
mae: 0.04349491451688803
r2: 0.6613683041193965
pearson: 0.8174375771131777

=== Experiment 4034 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006625307094601726
rmse: 0.08139598942578022
mae: 0.03609304153798182
r2: 0.7012678191250603
pearson: 0.8383141411242869

=== Experiment 4048 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006551708249241094
rmse: 0.0809426231922409
mae: 0.03209595694483898
r2: 0.7045863586690422
pearson: 0.8409917248808239

=== Experiment 4063 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007266336076580571
rmse: 0.08524280659727583
mae: 0.03449803660637031
r2: 0.6723641044660655
pearson: 0.8314536237044414

=== Experiment 4025 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006682930265438959
rmse: 0.08174919122192562
mae: 0.03250565603802666
r2: 0.6986696157139065
pearson: 0.8375480278831746

=== Experiment 4039 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005945952316467389
rmse: 0.0771100014036272
mae: 0.03227371852554099
r2: 0.7318996270642335
pearson: 0.8559746540260355

=== Experiment 4009 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007448314309752338
rmse: 0.08630361701430792
mae: 0.03796067279162662
r2: 0.6641587860270963
pearson: 0.8274841611906621

=== Experiment 4051 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00772556859901609
rmse: 0.08789521374350306
mae: 0.04717145472330744
r2: 0.6516575121531383
pearson: 0.8182431640606384

=== Experiment 4080 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0072428279265557
rmse: 0.08510480554325765
mae: 0.03801820396761
r2: 0.6734240766039588
pearson: 0.8392197263245801

=== Experiment 4041 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006329782552070502
rmse: 0.07955993056853747
mae: 0.03746231507084711
r2: 0.7145928906774952
pearson: 0.8483592144901266

=== Experiment 4023 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006707798015109098
rmse: 0.08190114782534551
mae: 0.03788449362792131
r2: 0.6975483398264137
pearson: 0.837344420442958

=== Experiment 4055 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006919632292894008
rmse: 0.08318432720707675
mae: 0.03693635535085946
r2: 0.6879968254764879
pearson: 0.8401591901680572

=== Experiment 4068 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006810555803791887
rmse: 0.08252609165465118
mae: 0.03703470156070571
r2: 0.6929150363618681
pearson: 0.8348942489237804

=== Experiment 4073 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006213503705828441
rmse: 0.07882578071816632
mae: 0.03336843135445523
r2: 0.7198358526763156
pearson: 0.8500110115111966

=== Experiment 4084 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007380218089744942
rmse: 0.08590819570765611
mae: 0.04131493364627788
r2: 0.6672292146157912
pearson: 0.8199054649101585

=== Experiment 4004 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006179209622269891
rmse: 0.0786079488491456
mae: 0.035382686592450543
r2: 0.7213821578099902
pearson: 0.8505612876827177

=== Experiment 4078 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006721495063679483
rmse: 0.08198472457524927
mae: 0.033776797099298005
r2: 0.6969307459349072
pearson: 0.8352635105007545

=== Experiment 4031 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067649250190373105
rmse: 0.08224916424522082
mae: 0.03577497621162339
r2: 0.6949725083628078
pearson: 0.8340208146663203

=== Experiment 4108 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0070545878190850335
rmse: 0.08399159374059427
mae: 0.03721796663958497
r2: 0.681911740199004
pearson: 0.8282112335296777

=== Experiment 4089 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007924485317609097
rmse: 0.08901957828258399
mae: 0.04765007442359087
r2: 0.6426884448617227
pearson: 0.8124739886199799

=== Experiment 4069 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007716299388860729
rmse: 0.08784246916418463
mae: 0.04299912225504838
r2: 0.6520754567593522
pearson: 0.8123189183008258

=== Experiment 4112 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006495407945748623
rmse: 0.08059409373985554
mae: 0.036807270429699064
r2: 0.7071249145738591
pearson: 0.8429438835310954

=== Experiment 4091 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007142116910730992
rmse: 0.08451104608707072
mae: 0.036087250140359704
r2: 0.677965091981187
pearson: 0.824561125825948

=== Experiment 4019 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006787684143997639
rmse: 0.08238740282347562
mae: 0.04123004256061006
r2: 0.6939463094354035
pearson: 0.8382345179363102

=== Experiment 4072 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006924451635249057
rmse: 0.08321329001577246
mae: 0.044744758369312604
r2: 0.6877795234508534
pearson: 0.8346574912803281

=== Experiment 4065 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006606283801743905
rmse: 0.08127904897169937
mae: 0.03670384503957399
r2: 0.7021255710272281
pearson: 0.840557494713196

=== Experiment 4120 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007358462088294983
rmse: 0.08578147870196097
mae: 0.03933861543987892
r2: 0.6682101831456213
pearson: 0.8183619668609661

=== Experiment 4098 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006796700780108523
rmse: 0.08244210562636378
mae: 0.03609764078259151
r2: 0.6935397532816888
pearson: 0.8337648049142137

=== Experiment 4095 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007070328147005301
rmse: 0.08408524333677879
mae: 0.036075278461747406
r2: 0.6812020157409273
pearson: 0.8258299289049624

=== Experiment 4132 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007667924459142393
rmse: 0.08756668578370654
mae: 0.05061923210615693
r2: 0.6542566610489171
pearson: 0.8255921296201829

=== Experiment 4122 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006617153466379721
rmse: 0.0813458878271036
mae: 0.03523065505797959
r2: 0.7016354626329042
pearson: 0.838009163683631

=== Experiment 4115 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007913778178120598
rmse: 0.08895941871505568
mae: 0.04476318369904218
r2: 0.6431712250686867
pearson: 0.8187995885323003

=== Experiment 4111 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008074047788988006
rmse: 0.08985570537805602
mae: 0.05155550392697711
r2: 0.6359447388547255
pearson: 0.8124734149451115

=== Experiment 4085 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006804117962620671
rmse: 0.0824870775492784
mae: 0.03840976556989852
r2: 0.6932053157867679
pearson: 0.834745478213777

=== Experiment 4094 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007870478143211236
rmse: 0.08871571531138796
mae: 0.038276724576095406
r2: 0.645123604584947
pearson: 0.8225078447243706

=== Experiment 4001 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005971893854022802
rmse: 0.07727802956871249
mae: 0.032247319738443914
r2: 0.7307299345536038
pearson: 0.8550923594202297

=== Experiment 4050 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006999573000325501
rmse: 0.08366345080335559
mae: 0.03436861535700791
r2: 0.6843923341629978
pearson: 0.8361256488332638

=== Experiment 4152 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00753536923237698
rmse: 0.08680650455108177
mae: 0.04784184777971483
r2: 0.6602335178817527
pearson: 0.8232241290065103

=== Experiment 4058 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00637327366450201
rmse: 0.07983278564914299
mae: 0.031848590547126784
r2: 0.7126318955598607
pearson: 0.8470714964798471

=== Experiment 4096 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008856646296490656
rmse: 0.09410975664876972
mae: 0.04091299961166986
r2: 0.6006577165993756
pearson: 0.8001019046273303

=== Experiment 4099 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0069445619130285765
rmse: 0.0833340381418576
mae: 0.03990335689236443
r2: 0.686872759876985
pearson: 0.8470606252703773

=== Experiment 4062 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006888098403406365
rmse: 0.08299456851750218
mae: 0.031337672821039775
r2: 0.6894186746743596
pearson: 0.8328357642067489

=== Experiment 4082 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007242253020356408
rmse: 0.08510142783970435
mae: 0.04357307027935895
r2: 0.6734499988714494
pearson: 0.8385534615559666

=== Experiment 4165 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007302347996148058
rmse: 0.08545377695659834
mae: 0.03698308042514152
r2: 0.6707403428628255
pearson: 0.8200762084223675

=== Experiment 4104 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006749392303265518
rmse: 0.08215468521798083
mae: 0.031556796588578566
r2: 0.695672871680487
pearson: 0.8471380101586672

=== Experiment 4088 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0060675555232504105
rmse: 0.07789451536052079
mae: 0.035275909849086316
r2: 0.7264165919920513
pearson: 0.8542631684899122

=== Experiment 4049 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0058506201981566325
rmse: 0.07648934695862315
mae: 0.03567485432120711
r2: 0.7361981103199915
pearson: 0.859912363060302

=== Experiment 4142 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006388999303180645
rmse: 0.07993121607470166
mae: 0.04027335623877671
r2: 0.7119228334332239
pearson: 0.8472224420157128

=== Experiment 4093 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006146927061003151
rmse: 0.07840234091532695
mae: 0.03109033348248436
r2: 0.7228377642888657
pearson: 0.8505256159689565

=== Experiment 4102 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0074770268788335245
rmse: 0.08646980327740733
mae: 0.04171087780990536
r2: 0.6628641489245934
pearson: 0.8400504183598412

=== Experiment 4146 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007433281742960388
rmse: 0.08621648185214001
mae: 0.034091566577060646
r2: 0.6648365978474066
pearson: 0.8279544395911403

=== Experiment 4114 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006834290469721627
rmse: 0.08266976756784566
mae: 0.03672440570295206
r2: 0.6918448507215216
pearson: 0.8333323769816084

=== Experiment 4053 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0064335159239258715
rmse: 0.08020920099294016
mae: 0.03610961274612325
r2: 0.7099155985970849
pearson: 0.8430747467963416

=== Experiment 4007 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008793739032383644
rmse: 0.09377493818917527
mae: 0.04403347725032296
r2: 0.6034941774503564
pearson: 0.7847016280885986

=== Experiment 4038 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057764325177472675
rmse: 0.07600284545822786
mae: 0.03369700421381137
r2: 0.7395432001771545
pearson: 0.8600578710482017

=== Experiment 4144 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007471831484611517
rmse: 0.08643975638912639
mae: 0.049476044894775015
r2: 0.6630984069633961
pearson: 0.8315961625146894

=== Experiment 4158 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006744007974577854
rmse: 0.08212190922389624
mae: 0.03836974533273549
r2: 0.6959156486911894
pearson: 0.8409620261559365

=== Experiment 4172 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007235176886225177
rmse: 0.08505984297084716
mae: 0.04319420478697968
r2: 0.6737690586449816
pearson: 0.8234202947108173

=== Experiment 4153 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006625984768670078
rmse: 0.08140015214156592
mae: 0.03602599813312675
r2: 0.7012372630995856
pearson: 0.8400875865837781

=== Experiment 4150 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0063418114583500855
rmse: 0.07963549119802103
mae: 0.03389750582083298
r2: 0.7140505125876752
pearson: 0.8477220268303223

=== Experiment 4042 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006109695881336839
rmse: 0.07816454363288279
mae: 0.033351929666718426
r2: 0.7245165017933244
pearson: 0.8576590569916821

=== Experiment 4066 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006530071994997089
rmse: 0.08080886086931982
mae: 0.03205497148203797
r2: 0.7055619278500607
pearson: 0.8412438559473556

=== Experiment 4201 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007164360377017192
rmse: 0.0846425447220084
mae: 0.0344971469463735
r2: 0.6769621438764406
pearson: 0.8227778476146294

=== Experiment 4136 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00782084494126243
rmse: 0.08843554116565595
mae: 0.039968445277241575
r2: 0.647361543815576
pearson: 0.8113056257782617

=== Experiment 4077 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005937629524593412
rmse: 0.07705601549907322
mae: 0.030008622441293528
r2: 0.7322748980866902
pearson: 0.8562677835904838

=== Experiment 4190 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007149344112830374
rmse: 0.08455379419535455
mae: 0.03812381289000629
r2: 0.6776392206194608
pearson: 0.8258140020352941

=== Experiment 4228 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007362514094558896
rmse: 0.08580509363994014
mae: 0.047748074091829594
r2: 0.6680274799666057
pearson: 0.8261811191220777

=== Experiment 4101 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006238245098231985
rmse: 0.07898256198827679
mae: 0.03642514387421781
r2: 0.7187202741823594
pearson: 0.8524068417452477

=== Experiment 4006 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00933551126221227
rmse: 0.09662044950326132
mae: 0.044416632174585205
r2: 0.5790659060595775
pearson: 0.7686843770867787

=== Experiment 4200 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007259420361961911
rmse: 0.0852022321418982
mae: 0.04087837099854943
r2: 0.6726759309943822
pearson: 0.8214187397111696

=== Experiment 4116 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006477705368580427
rmse: 0.08048419328402583
mae: 0.03286164736576443
r2: 0.7079231159868731
pearson: 0.8429854845166677

=== Experiment 4100 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0058802447270394406
rmse: 0.07668275377840471
mae: 0.029046208784773396
r2: 0.734862353351418
pearson: 0.859267980790539

=== Experiment 4216 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007966880677357933
rmse: 0.08925738444161319
mae: 0.037091705030694914
r2: 0.6407768567502725
pearson: 0.828687953924633

=== Experiment 4026 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007698728049588583
rmse: 0.08774239596448563
mae: 0.03676761456909246
r2: 0.6528677407133954
pearson: 0.812109725337452

=== Experiment 4221 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064826002033818025
rmse: 0.08051459621324449
mae: 0.035833271204242366
r2: 0.7077024100400606
pearson: 0.8419320219330921

=== Experiment 4033 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005583651589544333
rmse: 0.07472383548469881
mae: 0.031740359956850636
r2: 0.7482356073804496
pearson: 0.8668150322877392

=== Experiment 4189 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0077083802699692595
rmse: 0.08779738190839895
mae: 0.050815332873124154
r2: 0.6524325263446977
pearson: 0.8223582702766224

=== Experiment 4219 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0074977022952845964
rmse: 0.0865892735578986
mae: 0.04033851967360991
r2: 0.6619319034967615
pearson: 0.8162287471846386

=== Experiment 4168 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007277838981000624
rmse: 0.0853102513242144
mae: 0.04339740024500415
r2: 0.6718454435685806
pearson: 0.8293753730024658

=== Experiment 4187 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006008318325078237
rmse: 0.07751334288416567
mae: 0.03443481525165636
r2: 0.72908757118535
pearson: 0.8541159786924049

=== Experiment 4222 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006976966391302326
rmse: 0.0835282370896353
mae: 0.04388675543393814
r2: 0.6854116562139232
pearson: 0.8327128649529775

=== Experiment 4054 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007348334001767343
rmse: 0.08572242414775344
mae: 0.05185866168190987
r2: 0.6686668541094407
pearson: 0.8358936961807821

=== Experiment 4246 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006739858571749766
rmse: 0.08209664165938681
mae: 0.037878381169183
r2: 0.696102743438416
pearson: 0.8351771414883389

=== Experiment 4124 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00924673632700465
rmse: 0.09615995178349794
mae: 0.04358026831989276
r2: 0.5830687288152518
pearson: 0.7645952515266932

=== Experiment 4133 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008814211922308897
rmse: 0.09388403443775142
mae: 0.04081112481335136
r2: 0.6025710638544347
pearson: 0.7969178195895651

=== Experiment 4107 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005692619522697922
rmse: 0.07544945011527865
mae: 0.0329533315640717
r2: 0.7433222912350094
pearson: 0.8635767538629037

=== Experiment 4262 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007504479105177454
rmse: 0.08662839664438823
mae: 0.04215732731900955
r2: 0.6616263401213947
pearson: 0.814371276877152

=== Experiment 4233 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005946503377275682
rmse: 0.07711357453312408
mae: 0.03126201145095636
r2: 0.7318747799749279
pearson: 0.8592329658471857

=== Experiment 4213 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007726422640486639
rmse: 0.08790007190262497
mae: 0.04724291216097986
r2: 0.6516190037991236
pearson: 0.8183375823759218

=== Experiment 4234 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007471193855210438
rmse: 0.0864360680226168
mae: 0.03591583829457993
r2: 0.6631271573924484
pearson: 0.8280177020931077

=== Experiment 4036 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005477998277198994
rmse: 0.07401350064143024
mae: 0.03022218490519628
r2: 0.752999468732522
pearson: 0.8699898905520376

=== Experiment 4191 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006779001828028142
rmse: 0.08233469395114154
mae: 0.04251216401412735
r2: 0.6943377912410886
pearson: 0.8370771986574566

=== Experiment 4270 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007352661749350442
rmse: 0.08574766322967899
mae: 0.038849397016955944
r2: 0.6684717178757063
pearson: 0.8203938787159633

=== Experiment 4127 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00870859199490205
rmse: 0.09331983709213197
mae: 0.044011870059295424
r2: 0.6073334198943241
pearson: 0.7809610854845718

=== Experiment 4258 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007340494455591348
rmse: 0.0856766856011094
mae: 0.04273474306701838
r2: 0.6690203357960682
pearson: 0.8276383168169446

=== Experiment 4121 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006499567190093201
rmse: 0.08061989326520595
mae: 0.0364378572648661
r2: 0.7069373760769855
pearson: 0.8419827845776564

=== Experiment 4229 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006721666076147037
rmse: 0.08198576752185123
mae: 0.04075761982418005
r2: 0.6969230350580133
pearson: 0.8442101200567385

=== Experiment 4257 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007076378591074877
rmse: 0.08412121368046753
mae: 0.040405579158896504
r2: 0.6809292038808341
pearson: 0.83155785246414

=== Experiment 4183 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0058586785437606415
rmse: 0.07654200509367809
mae: 0.032260798266976524
r2: 0.7358347630634559
pearson: 0.8611937639169202

=== Experiment 4281 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007299471528770902
rmse: 0.0854369447532559
mae: 0.04451503806895397
r2: 0.6708700415108313
pearson: 0.824978096953295

=== Experiment 4139 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006223717881884529
rmse: 0.07889054367846966
mae: 0.03277323487614172
r2: 0.7193752999735492
pearson: 0.8484842697910604

=== Experiment 4203 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006046290742502032
rmse: 0.07775789826443377
mae: 0.035121620763078565
r2: 0.72737541159665
pearson: 0.8540696734928992

=== Experiment 4180 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006463716112633987
rmse: 0.08039723945903857
mae: 0.038544685960604085
r2: 0.7085538853803557
pearson: 0.843676574967411

=== Experiment 4176 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0077111060805427
rmse: 0.08781290383846044
mae: 0.04886148322828296
r2: 0.6523096207456622
pearson: 0.8152043911991209

=== Experiment 4018 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062200083803334565
rmse: 0.07886702974205036
mae: 0.03752646583355791
r2: 0.7195425597658107
pearson: 0.8513611370730952

=== Experiment 4003 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005959999304483588
rmse: 0.07720103175789549
mae: 0.03440923669187148
r2: 0.7312662545571352
pearson: 0.8579831499138856

=== Experiment 4109 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0063534702453236745
rmse: 0.07970865853421243
mae: 0.03679716958579754
r2: 0.7135248230144606
pearson: 0.8453831197610806

=== Experiment 4217 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006611752668263607
rmse: 0.08131268454714558
mae: 0.03798792693720433
r2: 0.7018789822428868
pearson: 0.8388040645923109

=== Experiment 4161 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006293418775016513
rmse: 0.07933107067862197
mae: 0.030872846727102818
r2: 0.7162325173799372
pearson: 0.8516825058892126

=== Experiment 4210 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007786750358323045
rmse: 0.08824256545637738
mae: 0.03785911588036782
r2: 0.6488988535541393
pearson: 0.8225406147952811

=== Experiment 4252 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006471992231940098
rmse: 0.0804486931648992
mae: 0.03605207391951737
r2: 0.7081807188034417
pearson: 0.8419758188891447

=== Experiment 4074 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006455513476854343
rmse: 0.08034621009639685
mae: 0.03626501428461302
r2: 0.7089237386174037
pearson: 0.850096500176792

=== Experiment 4196 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0074191209074367616
rmse: 0.08613431898747886
mae: 0.03898157995698977
r2: 0.6654751036885065
pearson: 0.8372671586207809

=== Experiment 4268 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007246006317500896
rmse: 0.08512347688799428
mae: 0.036093033191678635
r2: 0.6732807643551568
pearson: 0.829393840200153

=== Experiment 4209 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0065051447992831276
rmse: 0.08065447786256587
mae: 0.03273091159663595
r2: 0.7066858841335053
pearson: 0.850167019688491

=== Experiment 4061 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008890612326816134
rmse: 0.0942900436250622
mae: 0.041889367554747294
r2: 0.5991262032415929
pearson: 0.7809666264653771

=== Experiment 4154 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007111736067927086
rmse: 0.08433110972782872
mae: 0.04060867217488037
r2: 0.6793349508115838
pearson: 0.8358030966701879

=== Experiment 4250 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006675036573123521
rmse: 0.08170089701541546
mae: 0.04217161659867087
r2: 0.6990255388261299
pearson: 0.8424066614886792

=== Experiment 4300 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00756255914294433
rmse: 0.0869629757019867
mae: 0.04062898245230229
r2: 0.6590075367814656
pearson: 0.8227066100872008

=== Experiment 4067 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006809346914051157
rmse: 0.08251876704151097
mae: 0.0396108349277075
r2: 0.6929695446682047
pearson: 0.8345414087606685

=== Experiment 4214 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006183358178668064
rmse: 0.07863433206092657
mae: 0.032236460293640155
r2: 0.7211951012279157
pearson: 0.8527719817694973

=== Experiment 4329 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007357151140388252
rmse: 0.0857738371555584
mae: 0.03974442060460139
r2: 0.6682692932097434
pearson: 0.8198685911148672

=== Experiment 4260 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00881604189767997
rmse: 0.09389377986682595
mae: 0.05571415531220562
r2: 0.6024885510703866
pearson: 0.8191534435058005

=== Experiment 4223 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0066076932247571656
rmse: 0.08128771878185022
mae: 0.03764746663854547
r2: 0.7020620207638946
pearson: 0.8433566677329561

=== Experiment 4195 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007197222433690745
rmse: 0.08483644519716008
mae: 0.04314330143505087
r2: 0.6754804082047272
pearson: 0.8287222097281276

=== Experiment 4010 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005840989774418003
rmse: 0.07642636831891204
mae: 0.030935636604291467
r2: 0.7366323418876928
pearson: 0.8597446701407209

=== Experiment 4202 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007153334308398291
rmse: 0.08457738650725909
mae: 0.046203829535112755
r2: 0.6774593044575228
pearson: 0.8322053935416917

=== Experiment 4317 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0071496727010888815
rmse: 0.08455573724525664
mae: 0.03813396472469168
r2: 0.6776244047195075
pearson: 0.8294741857436051

=== Experiment 4020 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0062908392775611445
rmse: 0.07931481121178531
mae: 0.030057975655169183
r2: 0.7163488257848762
pearson: 0.8517464758339917

=== Experiment 4076 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0064532523928774645
rmse: 0.0803321379827368
mae: 0.03924270627804792
r2: 0.7090256898987419
pearson: 0.8454195562832655

=== Experiment 4242 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007029550518363131
rmse: 0.08384241479324847
mae: 0.039760229585534636
r2: 0.6830406610687958
pearson: 0.8308100602068847

=== Experiment 4235 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007656069066029172
rmse: 0.08749896608548682
mae: 0.039877362389950384
r2: 0.6547912155064346
pearson: 0.8249150668713157

=== Experiment 4159 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007071672607939249
rmse: 0.08409323758744962
mae: 0.04358333088966283
r2: 0.681141394589158
pearson: 0.8349357433620491

=== Experiment 4306 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006939121220865562
rmse: 0.08330138786878381
mae: 0.03497920593692912
r2: 0.6871180782919837
pearson: 0.8380862316049257

=== Experiment 4369 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007122695457954383
rmse: 0.08439606304771795
mae: 0.037355511775799655
r2: 0.6788407967388492
pearson: 0.8281557581131809

=== Experiment 4350 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007148603972038276
rmse: 0.08454941733707144
mae: 0.04045379842971083
r2: 0.6776725932420155
pearson: 0.8251699393157345

=== Experiment 4335 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006703760128174948
rmse: 0.081876493135545
mae: 0.0360810033492243
r2: 0.6977304063710645
pearson: 0.8379520121135797

=== Experiment 4264 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00721795313590952
rmse: 0.0849585377458294
mae: 0.03445733218678862
r2: 0.6745456699659625
pearson: 0.822279804412178

=== Experiment 4378 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00722161437956706
rmse: 0.08498008225206104
mae: 0.04274120932711414
r2: 0.6743805861008806
pearson: 0.8257570605087196

=== Experiment 4376 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007538363541015665
rmse: 0.08682374986727805
mae: 0.03717251848348013
r2: 0.6600985058231303
pearson: 0.8155802874697659

=== Experiment 4151 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005940069785758798
rmse: 0.07707184820515724
mae: 0.03315961872893865
r2: 0.7321648677847841
pearson: 0.856538645780377

=== Experiment 4371 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007645617740841694
rmse: 0.08743922312579003
mae: 0.03681971487126551
r2: 0.6552624611591581
pearson: 0.8111814688564026

=== Experiment 4128 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008732948113992967
rmse: 0.09345024405528841
mae: 0.04410002975723406
r2: 0.6062352132044627
pearson: 0.7922668611108518

=== Experiment 4301 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00639394610597396
rmse: 0.07996215421043858
mae: 0.03408131950597862
r2: 0.7116997842725279
pearson: 0.845377121979021

=== Experiment 4368 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00685200019466805
rmse: 0.08277680952216056
mae: 0.039597323678192506
r2: 0.6910463270183334
pearson: 0.8339345553561613

=== Experiment 4188 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00720684961513377
rmse: 0.08489316589180645
mae: 0.04162603905747773
r2: 0.6750463228307078
pearson: 0.8273659356233758

=== Experiment 4057 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006534465476934272
rmse: 0.08083604070545682
mae: 0.03915956221055653
r2: 0.7053638276832324
pearson: 0.842668070652907

=== Experiment 4231 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006775130116947036
rmse: 0.08231117856614031
mae: 0.030136720721727104
r2: 0.6945123649896654
pearson: 0.8492351389098282

=== Experiment 4090 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0070135115197353225
rmse: 0.0837467105009822
mae: 0.03685752416894223
r2: 0.6837638524576205
pearson: 0.8429099928610432

=== Experiment 4265 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005813428900441354
rmse: 0.07624584513559643
mae: 0.03006265741012874
r2: 0.7378750495648316
pearson: 0.8601400038593329

=== Experiment 4380 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007669970902166664
rmse: 0.08757837005886022
mae: 0.03452135539143515
r2: 0.6541643878336612
pearson: 0.8154206032295015

=== Experiment 4047 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008000925878408201
rmse: 0.08944789476789379
mae: 0.03802009809579462
r2: 0.6392417736193547
pearson: 0.8071116801291995

=== Experiment 4316 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006705782078486868
rmse: 0.08188883976761954
mae: 0.04555402734657723
r2: 0.6976392375214284
pearson: 0.8437287936362566

=== Experiment 4175 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005970724038138913
rmse: 0.07727046032048025
mae: 0.03540081142912456
r2: 0.7307826810369329
pearson: 0.8565585372224164

=== Experiment 4390 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006980631612624203
rmse: 0.08355017422258437
mae: 0.039266466089094273
r2: 0.6852463929977071
pearson: 0.8330383287648856

=== Experiment 4321 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0075404696685993245
rmse: 0.08683587777295353
mae: 0.03678635361268153
r2: 0.6600035414573606
pearson: 0.8185187022440553

=== Experiment 4322 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006465854170510042
rmse: 0.08041053519601796
mae: 0.03421396901718695
r2: 0.7084574812917557
pearson: 0.8435186896301469

=== Experiment 4043 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007029768539456763
rmse: 0.08384371496693573
mae: 0.034660757978627534
r2: 0.6830308305936423
pearson: 0.8513492853935483

=== Experiment 4323 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007039478581695544
rmse: 0.08390160059078458
mae: 0.03894280651262694
r2: 0.6825930090628122
pearson: 0.8268508998812786

=== Experiment 4314 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008743366397229743
rmse: 0.09350596984807838
mae: 0.042283803993320705
r2: 0.6057654573987535
pearson: 0.7970036954926962

=== Experiment 4059 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007184676327597215
rmse: 0.08476247004186001
mae: 0.03954325345298353
r2: 0.6760461066065209
pearson: 0.825577758593743

=== Experiment 4298 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006423817851538163
rmse: 0.08014872333068171
mae: 0.03638721070006173
r2: 0.7103528804125986
pearson: 0.8493220243662606

=== Experiment 4360 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006611792421465598
rmse: 0.08131292899327633
mae: 0.04322164888860147
r2: 0.7018771897885058
pearson: 0.8437528386579514

=== Experiment 4289 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007333788577806603
rmse: 0.08563754187157992
mae: 0.034975229774416396
r2: 0.6693227008739
pearson: 0.8202987002320195

=== Experiment 4349 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006542584503447931
rmse: 0.0808862442164793
mae: 0.03307186435254376
r2: 0.7049977443511883
pearson: 0.8434903773404842

=== Experiment 4296 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007177730848230985
rmse: 0.08472148988439111
mae: 0.042366033884662126
r2: 0.6763592752142049
pearson: 0.8309275819816831

=== Experiment 4377 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007292173872881897
rmse: 0.08539422622684684
mae: 0.04277497684880027
r2: 0.6711990896029225
pearson: 0.8253658633184832

=== Experiment 4014 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005879900779949977
rmse: 0.0766805110829993
mae: 0.029408700782629277
r2: 0.734877861774299
pearson: 0.8587505158301292

=== Experiment 4071 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007456916689641349
rmse: 0.08635344052000099
mae: 0.04821227454590761
r2: 0.6637709085040976
pearson: 0.8279463936800385

=== Experiment 4414 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0069006069469884305
rmse: 0.08306989194039216
mae: 0.0386688226056204
r2: 0.6888546699496753
pearson: 0.8347753403981999

=== Experiment 4313 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0065230330719272134
rmse: 0.08076529621023631
mae: 0.03310712666097727
r2: 0.7058793097932144
pearson: 0.8403467658852548

=== Experiment 4163 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0073520026962313224
rmse: 0.08574382016350404
mae: 0.036302510088329035
r2: 0.668501434290776
pearson: 0.8412615378083563

=== Experiment 4450 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007020100026040566
rmse: 0.08378603717828267
mae: 0.036718681169582074
r2: 0.6834667796081404
pearson: 0.8309239344162561

=== Experiment 4463 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007638634660816165
rmse: 0.08739928295367282
mae: 0.03996305712545802
r2: 0.6555773251640206
pearson: 0.8192254149821847

=== Experiment 4402 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007006971274759291
rmse: 0.08370765362115516
mae: 0.0371538804120218
r2: 0.6840587492250079
pearson: 0.8320741109266723

=== Experiment 4379 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0069737144514257785
rmse: 0.08350876870979346
mae: 0.042133697279999355
r2: 0.6855582847516686
pearson: 0.8327048337752254

=== Experiment 4126 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005990797329110083
rmse: 0.07740024114374633
mae: 0.028768905316452487
r2: 0.7298775851819739
pearson: 0.8546263423045461

=== Experiment 4206 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006778978180120176
rmse: 0.0823345503426124
mae: 0.03618416877124442
r2: 0.6943388575148506
pearson: 0.8336693518708393

=== Experiment 4471 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007022611378428528
rmse: 0.08380102253808439
mae: 0.03821181852282586
r2: 0.6833535438342979
pearson: 0.8272864614265897

=== Experiment 4420 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007559394153162398
rmse: 0.08694477645702701
mae: 0.03822810035810003
r2: 0.6591502447777142
pearson: 0.8124714719587998

=== Experiment 4435 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007987065056176149
rmse: 0.08937038131381195
mae: 0.03441898865356033
r2: 0.6398667519931837
pearson: 0.8329572789737539

=== Experiment 4297 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007352881253347373
rmse: 0.08574894316169368
mae: 0.0390349221022721
r2: 0.6684618205370958
pearson: 0.839499386700634

=== Experiment 4448 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007132894712745362
rmse: 0.08445646637614766
mae: 0.03566673396495097
r2: 0.6783809168293594
pearson: 0.8326757867990747

=== Experiment 4383 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00779511882689167
rmse: 0.08828997013756246
mae: 0.051952217437992956
r2: 0.6485215229896246
pearson: 0.8207881687532509

=== Experiment 4266 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005833165811812569
rmse: 0.07637516488893867
mae: 0.03062530812006545
r2: 0.7369851209179823
pearson: 0.8587115557352232

=== Experiment 4075 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009358254059457229
rmse: 0.09673806933910367
mae: 0.04395957103752367
r2: 0.5780404433416728
pearson: 0.7632987466843237

=== Experiment 4184 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006466385534532931
rmse: 0.08041383919782048
mae: 0.032653307364076144
r2: 0.7084335223218348
pearson: 0.8432385442611484

=== Experiment 4347 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006127069523097075
rmse: 0.07827559979391455
mae: 0.039578830677307476
r2: 0.7237331319330798
pearson: 0.8543227569052668

=== Experiment 4211 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007960332149956091
rmse: 0.08922069350748228
mae: 0.04332362173389387
r2: 0.6410721269685942
pearson: 0.8227946445703748

=== Experiment 4309 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006207336049882341
rmse: 0.07878664893167078
mae: 0.03796796768714028
r2: 0.7201139495683322
pearson: 0.8576743761504327

=== Experiment 4465 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007252071545362209
rmse: 0.08515909549403522
mae: 0.03666414689802026
r2: 0.673007285900401
pearson: 0.821642009858512

=== Experiment 4479 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008438122703733248
rmse: 0.09185925486162648
mae: 0.04096257945862412
r2: 0.6195287611904869
pearson: 0.8197396394343313

=== Experiment 4333 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00649891141397783
rmse: 0.08061582607638422
mae: 0.03249399126433654
r2: 0.7069669447333986
pearson: 0.841748381424079

=== Experiment 4387 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005863153515251396
rmse: 0.07657123164251307
mae: 0.030673686348573457
r2: 0.7356329885685233
pearson: 0.8577732283771449

=== Experiment 4446 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006719638744719512
rmse: 0.08197340266647171
mae: 0.03666903809214249
r2: 0.6970144465397836
pearson: 0.8389082706217861

=== Experiment 4432 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006751162771831779
rmse: 0.08216545972506804
mae: 0.035399576111209245
r2: 0.6955930420320771
pearson: 0.83583581289625

=== Experiment 4205 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006249946922623346
rmse: 0.07905660581269187
mae: 0.039342358064579364
r2: 0.7181926440709198
pearson: 0.8492370534217843

=== Experiment 4241 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00637081128803168
rmse: 0.0798173620713669
mae: 0.03300527113612538
r2: 0.7127429230311333
pearson: 0.8504467190868015

=== Experiment 4243 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006109935588806543
rmse: 0.07816607696953035
mae: 0.03260172640113292
r2: 0.7245056934890186
pearson: 0.851520267971849

=== Experiment 4441 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007361334630064495
rmse: 0.08579822043646648
mae: 0.0471277587068509
r2: 0.6680806615015364
pearson: 0.8250064709214537

=== Experiment 4052 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005672832305738759
rmse: 0.0753182070002915
mae: 0.028497054764273545
r2: 0.7442144881386779
pearson: 0.862861492482926

=== Experiment 4462 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006966436946901877
rmse: 0.08346518404042416
mae: 0.039827198781421734
r2: 0.6858864242275766
pearson: 0.8309206956601638

=== Experiment 4401 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006506210671311624
rmse: 0.08066108523514684
mae: 0.03621294524533333
r2: 0.7066378244328666
pearson: 0.8433247657900377

=== Experiment 4342 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007349820659909595
rmse: 0.08573109505838354
mae: 0.03921831947400862
r2: 0.668599821348135
pearson: 0.8210498923990386

=== Experiment 4407 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007027379690230454
rmse: 0.08382946791093483
mae: 0.04051101087142765
r2: 0.6831385427538436
pearson: 0.8280639641419694

=== Experiment 4428 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008161745813835112
rmse: 0.0903423810502862
mae: 0.05055464229336628
r2: 0.6319904735132221
pearson: 0.8032256729735109

=== Experiment 4464 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007194026539400548
rmse: 0.08481760748453442
mae: 0.036801267498181266
r2: 0.6756245096716516
pearson: 0.8248430952209954

=== Experiment 4079 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006317109095854451
rmse: 0.07948024343102159
mae: 0.03613317383131834
r2: 0.7151643312402622
pearson: 0.8465297260719215

=== Experiment 4413 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005902031721940586
rmse: 0.07682468172365302
mae: 0.030967791286889198
r2: 0.7338799873405152
pearson: 0.8579709749459392

=== Experiment 4417 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006839399161150312
rmse: 0.08270065998013748
mae: 0.03803513821461891
r2: 0.6916145020735092
pearson: 0.8327837343311573

=== Experiment 4437 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006168139425188377
rmse: 0.07853750330376168
mae: 0.034626218577083256
r2: 0.7218813081240293
pearson: 0.8510567098496296

=== Experiment 4452 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005964548407208061
rmse: 0.0772304888448083
mae: 0.033950393687737376
r2: 0.731061137517502
pearson: 0.8553935990570458

=== Experiment 4287 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005602870728540579
rmse: 0.07485232613981063
mae: 0.031041548638006327
r2: 0.7473690248620988
pearson: 0.8664621019218389

=== Experiment 4381 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007228839850578031
rmse: 0.08502258435602879
mae: 0.04719054810252977
r2: 0.6740547927931688
pearson: 0.8319806184426751

=== Experiment 4519 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0065193299828551
rmse: 0.08074236795422277
mae: 0.03869948071792539
r2: 0.7060462804496218
pearson: 0.8417602119929782

=== Experiment 4427 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007583072495472642
rmse: 0.08708083885374923
mae: 0.04572738625403764
r2: 0.6580825987445809
pearson: 0.8184636933392058

=== Experiment 4353 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007008976743618231
rmse: 0.08371963176948541
mae: 0.04439470832186612
r2: 0.6839683235168325
pearson: 0.8375983527840295

=== Experiment 4468 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006729049154692583
rmse: 0.08203078175107552
mae: 0.04462272106328377
r2: 0.696590135295936
pearson: 0.8412239313500504

=== Experiment 4197 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007513524267548975
rmse: 0.08668058760500516
mae: 0.04064974383872783
r2: 0.661218497731143
pearson: 0.8148977031528047

=== Experiment 4259 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005915670291534783
rmse: 0.07691339474717511
mae: 0.03805954525484074
r2: 0.7332650302403063
pearson: 0.866041584158863

=== Experiment 4345 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006783805601089098
rmse: 0.0823638610137304
mae: 0.03116363324599911
r2: 0.6941211912280725
pearson: 0.857237293464798

=== Experiment 4386 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009368603008000506
rmse: 0.09679154409348219
mae: 0.04301564899093224
r2: 0.577573813806777
pearson: 0.8169044368358545

=== Experiment 4275 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007713926646455618
rmse: 0.08782896245803896
mae: 0.04020710004519441
r2: 0.6521824426700749
pearson: 0.8266668282702619

=== Experiment 4409 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007057443942738858
rmse: 0.08400859445758427
mae: 0.04591854587553191
r2: 0.6817829588405291
pearson: 0.8296564013835067

=== Experiment 4513 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007233745440185695
rmse: 0.08505142820779493
mae: 0.03991541029403278
r2: 0.6738336019168745
pearson: 0.8224880592475878

=== Experiment 4405 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006272606674416453
rmse: 0.07919978961093554
mae: 0.029874149740279327
r2: 0.7171709258358879
pearson: 0.8508694096880619

=== Experiment 4227 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007375372899530031
rmse: 0.08587999126414739
mae: 0.03646849231262544
r2: 0.6674476821100503
pearson: 0.8187528283876188

=== Experiment 4370 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.011840151009373533
rmse: 0.10881245796954286
mae: 0.051410018216114316
r2: 0.466132802236332
pearson: 0.6836931997860083

=== Experiment 4070 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006141561954162948
rmse: 0.07836811822522567
mae: 0.03168121569667563
r2: 0.7230796745949264
pearson: 0.8527195868617653

=== Experiment 4346 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006920301817852839
rmse: 0.08318835145531397
mae: 0.049904502948599924
r2: 0.6879666368907745
pearson: 0.8449556228648178

=== Experiment 4384 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006546729383594072
rmse: 0.08091186182256636
mae: 0.03332086001373969
r2: 0.7048108535297001
pearson: 0.841191624600667

=== Experiment 4567 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007709800314385958
rmse: 0.08780546859043552
mae: 0.03568571919533378
r2: 0.6523684971669581
pearson: 0.8076959430166117

=== Experiment 4408 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060340892184259165
rmse: 0.07767940021927253
mae: 0.03593765541540001
r2: 0.7279255729469406
pearson: 0.8547085809245643

=== Experiment 4456 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007424103716425842
rmse: 0.0861632387763241
mae: 0.04559979702112006
r2: 0.6652504310242908
pearson: 0.8350818057968381

=== Experiment 4354 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0061600084698230665
rmse: 0.078485721439145
mae: 0.031454115174401276
r2: 0.7222479293227441
pearson: 0.8500335871482314

=== Experiment 4186 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005739533110417406
rmse: 0.07575970637758178
mae: 0.03191462265971518
r2: 0.7412069782129869
pearson: 0.8625043794906866

=== Experiment 4560 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006978954259845185
rmse: 0.08354013562261665
mae: 0.03802931582101256
r2: 0.6853220240962536
pearson: 0.8328172507403268

=== Experiment 4143 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.01105546963811288
rmse: 0.10514499340488295
mae: 0.04588798363548243
r2: 0.5015137399017916
pearson: 0.7093382081358445

=== Experiment 4482 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0060872640404197485
rmse: 0.07802092053045612
mae: 0.0315723784734819
r2: 0.7255279436272678
pearson: 0.8525093108402926

=== Experiment 4131 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00659140988932204
rmse: 0.08118749835610184
mae: 0.03345154121476733
r2: 0.7027962291918204
pearson: 0.845720352605265

=== Experiment 4304 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008979522061397363
rmse: 0.09476034012917726
mae: 0.036137957071586006
r2: 0.5951173024414921
pearson: 0.7813750593013254

=== Experiment 4398 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008473360044936554
rmse: 0.09205085575341793
mae: 0.04066300799377972
r2: 0.6179399249847697
pearson: 0.7977665296544351

=== Experiment 4416 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006422760013940564
rmse: 0.08014212384221274
mae: 0.03410018020653738
r2: 0.7104005778442845
pearson: 0.8429527671234778

=== Experiment 4528 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007496696833747915
rmse: 0.08658346743892806
mae: 0.04826812498357367
r2: 0.6619772393149129
pearson: 0.821911319157222

=== Experiment 4555 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008272614319993444
rmse: 0.09095391316481904
mae: 0.03300191630778126
r2: 0.626991461367504
pearson: 0.8008683659255277

=== Experiment 4212 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006896925646719923
rmse: 0.08304773113529305
mae: 0.03586108098776301
r2: 0.6890206581585181
pearson: 0.8311477467764484

=== Experiment 4518 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005718700556343976
rmse: 0.07562209039919471
mae: 0.031783868220149586
r2: 0.7421463089070497
pearson: 0.8629108458016105

=== Experiment 4517 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072361050842478
rmse: 0.08506529894291678
mae: 0.03507236365760482
r2: 0.6737272066046729
pearson: 0.8315881106731147

=== Experiment 4572 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006897679717065746
rmse: 0.08305227099282562
mae: 0.035978277201087434
r2: 0.6889866574585166
pearson: 0.8312278478307408

=== Experiment 4239 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060793235899876535
rmse: 0.07797001725014337
mae: 0.0301464679661405
r2: 0.7258859750423914
pearson: 0.8547914737475717

=== Experiment 4524 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007538628149767938
rmse: 0.08682527368092736
mae: 0.04377204661151928
r2: 0.6600865747309536
pearson: 0.8163705296150582

=== Experiment 4533 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007167576022636375
rmse: 0.08466153803609036
mae: 0.040794169742538244
r2: 0.6768171518307855
pearson: 0.8241648785113029

=== Experiment 4240 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005630898985757578
rmse: 0.07503931626659173
mae: 0.03250374037065637
r2: 0.7461052430803639
pearson: 0.8640853346171364

=== Experiment 4012 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006292806651843097
rmse: 0.07932721255561105
mae: 0.03050570828359635
r2: 0.7162601177444106
pearson: 0.8533212851723424

=== Experiment 4561 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007152469686634336
rmse: 0.08457227492881066
mae: 0.03495045970229755
r2: 0.6774982898723105
pearson: 0.8258967910882568

=== Experiment 4568 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007406559545306105
rmse: 0.08606137080773292
mae: 0.04539081324787271
r2: 0.6660414899782996
pearson: 0.8237197204639702

=== Experiment 4253 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008679746244230856
rmse: 0.09316515574092525
mae: 0.03822164519612232
r2: 0.6086340620961026
pearson: 0.7842901513355267

=== Experiment 4149 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.010081147394583328
rmse: 0.10040491718329002
mae: 0.04529100766892973
r2: 0.5454455010305255
pearson: 0.7470941512288883

=== Experiment 4208 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006394963957219089
rmse: 0.07996851853835413
mae: 0.03250984120059494
r2: 0.7116538898078757
pearson: 0.8449289492628645

=== Experiment 4609 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008415774665969468
rmse: 0.09173753139238851
mae: 0.041323474221063236
r2: 0.620536424377132
pearson: 0.8012300180090871

=== Experiment 4597 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006868323457432568
rmse: 0.08287534891288584
mae: 0.036874174679005396
r2: 0.6903103182846986
pearson: 0.8313588310500128

=== Experiment 4504 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007744194085024361
rmse: 0.08800110274891083
mae: 0.04875040181794272
r2: 0.6508176971867285
pearson: 0.8167587105342571

=== Experiment 4363 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005764754612004138
rmse: 0.0759259811395555
mae: 0.029907135378667203
r2: 0.7400697518072729
pearson: 0.8603357243371885

=== Experiment 4466 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00736369176187445
rmse: 0.08581195582128665
mae: 0.03678804718196741
r2: 0.6679743794656785
pearson: 0.8178624973378912

=== Experiment 4453 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006709316962635745
rmse: 0.08191042035440757
mae: 0.03771288644380733
r2: 0.697479851150977
pearson: 0.8360356988195274

=== Experiment 4315 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.014201841259739173
rmse: 0.1191714783819483
mae: 0.05313448571124242
r2: 0.3596452283067162
pearson: 0.616408146423002

=== Experiment 4541 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005981746579948209
rmse: 0.07734175185466262
mae: 0.03272743777326518
r2: 0.7302856794781436
pearson: 0.8547489062381978

=== Experiment 4591 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006490044297650445
rmse: 0.08056081117795702
mae: 0.04197904665290489
r2: 0.7073667591058841
pearson: 0.8456867301654226

=== Experiment 4598 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007140413778103644
rmse: 0.08450096909564792
mae: 0.039071131224820144
r2: 0.6780418854817482
pearson: 0.8272191837045622

=== Experiment 4485 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005899739701731716
rmse: 0.07680976306259327
mae: 0.029712368757094123
r2: 0.7339833335229378
pearson: 0.8580124938562779

=== Experiment 4389 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007563795089130875
rmse: 0.08697008157482017
mae: 0.03970849309606104
r2: 0.6589518085118957
pearson: 0.8360039821276126

=== Experiment 4199 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006225624772802162
rmse: 0.07890262842771564
mae: 0.035587944526569694
r2: 0.7192893191013601
pearson: 0.8497818365540661

=== Experiment 4554 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0074623201249087315
rmse: 0.08638472159420746
mae: 0.048112084029462834
r2: 0.663527269986125
pearson: 0.8227810749918952

=== Experiment 4489 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007012554943719926
rmse: 0.08374099918032937
mae: 0.034345117517173195
r2: 0.6838069840491334
pearson: 0.8271393911897374

=== Experiment 4359 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00628571366245353
rmse: 0.07928249278657634
mae: 0.028862219579921227
r2: 0.7165799375141229
pearson: 0.8572565963163656

=== Experiment 4274 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061974436363747155
rmse: 0.07872384414124298
mae: 0.034281205152725605
r2: 0.720559994139407
pearson: 0.853616674079543

=== Experiment 4123 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005919457608158283
rmse: 0.07693801146480381
mae: 0.02989586999387707
r2: 0.7330942618006782
pearson: 0.8607200592932199

=== Experiment 4207 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006669237125282941
rmse: 0.08166539735581368
mae: 0.03959963801218206
r2: 0.6992870333767295
pearson: 0.8372540597041398

=== Experiment 4605 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008332552837466454
rmse: 0.09128281786550223
mae: 0.03799683021529893
r2: 0.6242888599956051
pearson: 0.8254827081452878

=== Experiment 4267 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0059504155478956105
rmse: 0.0771389366526115
mae: 0.03265920526362215
r2: 0.7316983819235534
pearson: 0.8558815016741326

=== Experiment 4224 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008752315830270573
rmse: 0.09355381248388851
mae: 0.04177110510151764
r2: 0.6053619314018892
pearson: 0.7998652515905891

=== Experiment 4247 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005819165428219633
rmse: 0.07628345448535766
mae: 0.029824360431688013
r2: 0.7376163920521479
pearson: 0.8601491021237211

=== Experiment 4419 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00732047389252365
rmse: 0.08555976795505964
mae: 0.041790163367969924
r2: 0.6699230541730672
pearson: 0.8243823375961787

=== Experiment 4305 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006078557068421902
rmse: 0.07796510160592303
mae: 0.03784513499614625
r2: 0.7259205371624193
pearson: 0.8588413759229787

=== Experiment 4364 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00619506327108198
rmse: 0.07870872423741844
mae: 0.02993130867321541
r2: 0.7206673237627774
pearson: 0.8494225438793982

=== Experiment 4529 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00920724432511476
rmse: 0.09595438669031635
mae: 0.062006543896972695
r2: 0.5848494057987077
pearson: 0.7996664179290897

=== Experiment 4495 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006983839795525145
rmse: 0.0835693711566932
mae: 0.04332097108874022
r2: 0.6851017374427328
pearson: 0.838606222064946

=== Experiment 4549 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00641826585689875
rmse: 0.0801140802661976
mae: 0.031705422689769995
r2: 0.7106032174072703
pearson: 0.843048076997548

=== Experiment 4553 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006926014080158489
rmse: 0.08322267767957535
mae: 0.03642389493515101
r2: 0.6877090734975718
pearson: 0.8350125322244811

=== Experiment 4573 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007121111455963318
rmse: 0.08438667819012263
mae: 0.04496575920944522
r2: 0.6789122186914539
pearson: 0.8333504972926423

=== Experiment 4628 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007109650076912801
rmse: 0.08431874095901101
mae: 0.0375158323196281
r2: 0.6794290072283029
pearson: 0.8268247250447379

=== Experiment 4256 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006394484888692253
rmse: 0.0799655231252335
mae: 0.032437839752976504
r2: 0.7116754907968965
pearson: 0.8650939347694782

=== Experiment 4461 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006572400075656924
rmse: 0.08107034029567733
mae: 0.03374096759024556
r2: 0.7036533733231216
pearson: 0.8417936648662242

=== Experiment 4148 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006899680178084139
rmse: 0.08306431350516381
mae: 0.031040013961945633
r2: 0.6888964575516676
pearson: 0.838334991143374

=== Experiment 4574 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0070651012816985385
rmse: 0.08405415683771111
mae: 0.039583888073019886
r2: 0.6814376927970898
pearson: 0.8276405279417622

=== Experiment 4418 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008095148191524664
rmse: 0.08997304147090207
mae: 0.03973049849927347
r2: 0.634993330991345
pearson: 0.8015254870103362

=== Experiment 4430 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006984493722037586
rmse: 0.08357328354227556
mae: 0.04307209291019815
r2: 0.685072252184102
pearson: 0.835627625324092

=== Experiment 4627 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007538972742093319
rmse: 0.08682725805928297
mae: 0.04860128624099841
r2: 0.6600710372146688
pearson: 0.8246979919033761

=== Experiment 4571 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00787121384138275
rmse: 0.08871986159470015
mae: 0.04323616458136617
r2: 0.645090432278199
pearson: 0.8080467733385527

=== Experiment 4498 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007392381991716171
rmse: 0.08597896249499741
mae: 0.03745190446316589
r2: 0.6666807496296512
pearson: 0.8358327956366686

=== Experiment 4294 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006296937323487875
rmse: 0.07935324393802609
mae: 0.037669143716735354
r2: 0.7160738675779952
pearson: 0.848347832709762

=== Experiment 4538 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006829726102054165
rmse: 0.08264215693006909
mae: 0.03834710413855778
r2: 0.6920506560507156
pearson: 0.8336736058278099

=== Experiment 4494 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0071520553013857974
rmse: 0.08456982500505601
mae: 0.04713673329170506
r2: 0.6775169743207821
pearson: 0.8315665530026493

=== Experiment 4338 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006570977302044363
rmse: 0.08106156488770966
mae: 0.035315155708018096
r2: 0.7037175255591015
pearson: 0.8399161770084356

=== Experiment 4645 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006674879307122233
rmse: 0.08169993456008538
mae: 0.03716415831021463
r2: 0.69903262988091
pearson: 0.8373584522713902

=== Experiment 4164 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.011017012042816908
rmse: 0.10496195521624446
mae: 0.038481222539369316
r2: 0.5032477759472049
pearson: 0.793311428789536

=== Experiment 4373 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.01207247277952969
rmse: 0.10987480502612822
mae: 0.04414613597842926
r2: 0.4556575158726217
pearson: 0.6898947481764892

=== Experiment 4565 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007688191267357606
rmse: 0.08768233155749
mae: 0.0387073001591167
r2: 0.6533428395865977
pearson: 0.8296903191563719

=== Experiment 4170 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006854567930316735
rmse: 0.08279231806343348
mae: 0.030803082928109713
r2: 0.6909305489480811
pearson: 0.8439419217745464

=== Experiment 4276 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0064928202723389585
rmse: 0.08057803839967165
mae: 0.039789734377205836
r2: 0.7072415916289787
pearson: 0.8490619219937805

=== Experiment 4230 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005334111470182368
rmse: 0.07303500167852649
mae: 0.02905148479579035
r2: 0.7594872615314749
pearson: 0.8718575050104456

=== Experiment 4269 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0055953586523803936
rmse: 0.07480212999895386
mae: 0.02954303236156666
r2: 0.7477077410698441
pearson: 0.8647808951416386

=== Experiment 4618 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007405466629048598
rmse: 0.08605502094037627
mae: 0.05016667869989815
r2: 0.6660907690913213
pearson: 0.8293042788850571

=== Experiment 4632 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007670610861294868
rmse: 0.08758202361954688
mae: 0.046200349071872836
r2: 0.654135532358238
pearson: 0.8138001500271638

=== Experiment 4642 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007179124714526001
rmse: 0.08472971565233771
mae: 0.040073241244997136
r2: 0.6762964263964502
pearson: 0.8244199833127174

=== Experiment 4491 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00737240049823851
rmse: 0.08586268396829037
mae: 0.03740821406048051
r2: 0.667581706376031
pearson: 0.8206147761530189

=== Experiment 4288 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.013354317063342063
rmse: 0.11556088033301781
mae: 0.04836583806008308
r2: 0.39785972129833624
pearson: 0.6310124357676844

=== Experiment 4654 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068908693852675566
rmse: 0.08301126059317228
mae: 0.03407781967678346
r2: 0.6892937323218408
pearson: 0.8310224394177135

=== Experiment 4035 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005682377491901147
rmse: 0.07538154609651587
mae: 0.02773405865527576
r2: 0.7437840998957734
pearson: 0.8698234402041809

=== Experiment 4272 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010566740722654403
rmse: 0.10279465318125454
mae: 0.04412241188989013
r2: 0.5235503115937687
pearson: 0.7740233566490692

=== Experiment 4653 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007479492280669843
rmse: 0.08648405795676938
mae: 0.03724838762718404
r2: 0.6627529850409002
pearson: 0.820626880381615

=== Experiment 4615 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006279904762589222
rmse: 0.0792458501285034
mae: 0.030627600579802603
r2: 0.7168418582523121
pearson: 0.8541583224807889

=== Experiment 4483 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006412361946394464
rmse: 0.0800772248919408
mae: 0.03113475915009012
r2: 0.7108694221333987
pearson: 0.8490014966279366

=== Experiment 4484 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006770793381826345
rmse: 0.0822848308124064
mae: 0.03732649482631907
r2: 0.6947079064675141
pearson: 0.8413712210480206

=== Experiment 4515 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006226568902871402
rmse: 0.07890861108188005
mae: 0.03472875655833887
r2: 0.7192467486920815
pearson: 0.8489957672758627

=== Experiment 4643 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008259364388967952
rmse: 0.09088104526780022
mae: 0.03828558690441098
r2: 0.6275888949257029
pearson: 0.798651573245669

=== Experiment 4394 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006686718547967663
rmse: 0.08177235809225306
mae: 0.03665320187775692
r2: 0.6984988037220379
pearson: 0.8366351195587275

=== Experiment 4141 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005857224735951161
rmse: 0.07653250770719042
mae: 0.029774004979653365
r2: 0.7359003146177157
pearson: 0.8608634570189073

=== Experiment 4671 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0075620932007180374
rmse: 0.0869602966917549
mae: 0.03848664020064496
r2: 0.6590285459113723
pearson: 0.8152339215409677

=== Experiment 4087 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005680597604802365
rmse: 0.07536973931759593
mae: 0.029051939543978663
r2: 0.7438643542216694
pearson: 0.8629987719290857

=== Experiment 4307 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009428189062160943
rmse: 0.09709886231136255
mae: 0.04055915142171058
r2: 0.5748871048505109
pearson: 0.7677819121050998

=== Experiment 4537 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00714195399715234
rmse: 0.08451008222190025
mae: 0.040417138958983204
r2: 0.6779724376827447
pearson: 0.8255266870083507

=== Experiment 4155 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006413707254494234
rmse: 0.08008562451835056
mae: 0.031178425160730983
r2: 0.7108087627832954
pearson: 0.8542920427093567

=== Experiment 4310 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006071016820830914
rmse: 0.07791673004452197
mae: 0.03285607032809615
r2: 0.7262605236075801
pearson: 0.8561042583030922

=== Experiment 4182 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005677799609288996
rmse: 0.07535117523495566
mae: 0.033619582975516794
r2: 0.7439905146078751
pearson: 0.8639614720015277

=== Experiment 4312 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005990578563278369
rmse: 0.07739882791928034
mae: 0.034039679068348955
r2: 0.7298874492370397
pearson: 0.8544783451449552

=== Experiment 4355 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006564143609690779
rmse: 0.0810194026742408
mae: 0.039735413918872926
r2: 0.7040256537395866
pearson: 0.8463367050778218

=== Experiment 4585 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006560846415337382
rmse: 0.0809990519409788
mae: 0.042805590527577334
r2: 0.704174322781775
pearson: 0.8460551218564034

=== Experiment 4673 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007173165243172237
rmse: 0.0846945408109179
mae: 0.042159547585492896
r2: 0.6765651363368275
pearson: 0.8248653159439726

=== Experiment 4254 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005883050995901484
rmse: 0.07670104950977844
mae: 0.03424709464372943
r2: 0.7347358199235619
pearson: 0.8585595717077852

=== Experiment 4579 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0058420129903864184
rmse: 0.07643306215497596
mae: 0.0319622082971579
r2: 0.7365862055300287
pearson: 0.8624893185440954

=== Experiment 4613 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007825765141756645
rmse: 0.08846335479596422
mae: 0.049157928589130886
r2: 0.6471396941408376
pearson: 0.8166175306900149

=== Experiment 4279 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006317685077925178
rmse: 0.0794838667776372
mae: 0.032733400179583184
r2: 0.7151383604622022
pearson: 0.8524128229294202

=== Experiment 4696 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007646272751710049
rmse: 0.0874429685664322
mae: 0.03840301551150946
r2: 0.6552329270073962
pearson: 0.8165110638316255

=== Experiment 4583 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006856409757970931
rmse: 0.08280344049597776
mae: 0.03664182632553971
r2: 0.6908475017498038
pearson: 0.8325366207427505

=== Experiment 4655 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008673436789652
rmse: 0.0931312879200755
mae: 0.04647180082775566
r2: 0.6089185526260599
pearson: 0.7871515416241259

=== Experiment 4324 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0071301417364702715
rmse: 0.08444016660612573
mae: 0.037514973857734925
r2: 0.6785050473179257
pearson: 0.8326438235456171

=== Experiment 4423 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007455572944517499
rmse: 0.08634565967387996
mae: 0.0368939187193011
r2: 0.6638314973803046
pearson: 0.8315571119271896

=== Experiment 4505 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006521575224010018
rmse: 0.08075627049344229
mae: 0.037322727491180534
r2: 0.7059450435141528
pearson: 0.8454142249197367

=== Experiment 4681 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006439562653540943
rmse: 0.08024688563141216
mae: 0.03360662117412983
r2: 0.7096429542201106
pearson: 0.8424563997508739

=== Experiment 4277 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007463065139442051
rmse: 0.08638903367582051
mae: 0.033971009242273434
r2: 0.6634936776087238
pearson: 0.8294637458771451

=== Experiment 4362 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0058470137731553235
rmse: 0.07646576863639915
mae: 0.03062658224894448
r2: 0.7363607224359914
pearson: 0.8582203584176381

=== Experiment 4683 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0071105724491872975
rmse: 0.08432421033835595
mae: 0.039308867243263615
r2: 0.679387417868413
pearson: 0.8270210027254485

=== Experiment 4542 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007091307470811526
rmse: 0.08420990126351845
mae: 0.03934574967042683
r2: 0.6802560672642111
pearson: 0.8262158467603777

=== Experiment 4703 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006935996097088116
rmse: 0.08328262782290263
mae: 0.038988128803159695
r2: 0.6872589887476366
pearson: 0.830157980604219

=== Experiment 4593 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006346540868344616
rmse: 0.0796651797735034
mae: 0.03860990712568474
r2: 0.7138372655725943
pearson: 0.849088503052037

=== Experiment 4721 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006747290139070695
rmse: 0.08214189028182085
mae: 0.03758399318761827
r2: 0.6957676573387983
pearson: 0.8347927377195445

=== Experiment 4697 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007239132150431782
rmse: 0.08508308968550556
mae: 0.03587145231063538
r2: 0.6735907175227509
pearson: 0.8316177678591808

=== Experiment 4024 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006690318155664063
rmse: 0.08179436506058387
mae: 0.0358087820482752
r2: 0.6983364989953158
pearson: 0.8382406028227973

=== Experiment 4509 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006183391563771099
rmse: 0.07863454434134592
mae: 0.03619629545695158
r2: 0.7211935959083138
pearson: 0.8547135037118904

=== Experiment 4344 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006271363306270335
rmse: 0.07919193965467909
mae: 0.029198099466037002
r2: 0.7172269887583482
pearson: 0.8670219431133089

=== Experiment 4469 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.015223773243469109
rmse: 0.12338465562406498
mae: 0.05499633791127538
r2: 0.31356676494699653
pearson: 0.5652492385148975

=== Experiment 4081 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006123634933955736
rmse: 0.07825365763947227
mae: 0.0341580281131525
r2: 0.7238879960457845
pearson: 0.8583058396098503

=== Experiment 4284 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.016978421171541774
rmse: 0.1303012707978774
mae: 0.05689826480915213
r2: 0.23445046214981224
pearson: 0.510830915872244

=== Experiment 4689 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008316032270568052
rmse: 0.09119228185854356
mae: 0.05221511434061669
r2: 0.6250337650857967
pearson: 0.8100970574450472

=== Experiment 4400 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0061763806790012836
rmse: 0.07858995278660806
mae: 0.03070460332447515
r2: 0.7215097136168585
pearson: 0.8498525178097929

=== Experiment 4630 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007819986867045825
rmse: 0.0884306896221319
mae: 0.04674816502542083
r2: 0.6474002340043353
pearson: 0.8078848672211508

=== Experiment 4577 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.01671050792674261
rmse: 0.12926912982898356
mae: 0.05063887837702955
r2: 0.24653055244016964
pearson: 0.5353268272464131

=== Experiment 4527 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006364378184451393
rmse: 0.07977705299427519
mae: 0.03749783876885409
r2: 0.713032988840143
pearson: 0.8453895452293099

=== Experiment 4118 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00632163927500901
rmse: 0.07950873709856679
mae: 0.030011861323012354
r2: 0.7149600674560674
pearson: 0.8547385049639069

=== Experiment 4543 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0055726908196701815
rmse: 0.07465045759853171
mae: 0.03055837690754846
r2: 0.7487298236698713
pearson: 0.8654986063053302

=== Experiment 4652 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007332916369338441
rmse: 0.08563244927793692
mae: 0.044463203199434934
r2: 0.669362028369841
pearson: 0.825893075289873

=== Experiment 4442 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006598088940481578
rmse: 0.08122862143654525
mae: 0.03572244707658539
r2: 0.7024950737147122
pearson: 0.8426535423140511

=== Experiment 4393 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005963374375343407
rmse: 0.07722288763924468
mae: 0.03835987457032955
r2: 0.7311140740975484
pearson: 0.8582062923758892

=== Experiment 4563 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006374613466573001
rmse: 0.07984117651045106
mae: 0.03720337697699277
r2: 0.7125714844741718
pearson: 0.8456770219605213

=== Experiment 4733 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006961916076073333
rmse: 0.08343809727021184
mae: 0.04014967938584901
r2: 0.6860902683034482
pearson: 0.8298852633669467

=== Experiment 4204 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006208121453312959
rmse: 0.07879163314282145
mae: 0.032977461343128296
r2: 0.7200785360733282
pearson: 0.8502995271664151

=== Experiment 4157 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009771368814003268
rmse: 0.09885023426377536
mae: 0.054504555936803395
r2: 0.5594132808849003
pearson: 0.755002700954524

=== Experiment 4396 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0067174620357871384
rmse: 0.08196012466917763
mae: 0.03289172154469426
r2: 0.6971125933875016
pearson: 0.8455774734825328

=== Experiment 4706 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006511158215282872
rmse: 0.08069174812385013
mae: 0.0305674979975908
r2: 0.7064147418527835
pearson: 0.8426764346151453

=== Experiment 4562 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007760453884565222
rmse: 0.08809343837406519
mae: 0.048554139333817616
r2: 0.6500845499819163
pearson: 0.822553906914461

=== Experiment 4064 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006432713522410874
rmse: 0.08020419890760629
mae: 0.032220070797008005
r2: 0.7099517785282323
pearson: 0.8436959597588009

=== Experiment 4110 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006611330277979324
rmse: 0.08131008718467422
mae: 0.0327824035319667
r2: 0.7018980276348972
pearson: 0.8405177234455529

=== Experiment 4097 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006821823856292035
rmse: 0.08259433307613807
mae: 0.032454448084585946
r2: 0.6924069648340857
pearson: 0.8322757501075749

=== Experiment 4730 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006958169895945238
rmse: 0.08341564539068938
mae: 0.041199936378167976
r2: 0.686259181916604
pearson: 0.8325305676366622

=== Experiment 4712 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006077555915529902
rmse: 0.07795868082215028
mae: 0.03413475142053503
r2: 0.7259656787056783
pearson: 0.8521758440749261

=== Experiment 4586 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007071211592314001
rmse: 0.0840904964446875
mae: 0.04232066046527482
r2: 0.681162181580792
pearson: 0.8283445121667405

=== Experiment 4757 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006437864749648749
rmse: 0.08023630568295595
mae: 0.03824664631243742
r2: 0.709719511959305
pearson: 0.844673391858391

=== Experiment 4575 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006832070776848408
rmse: 0.08265634141944832
mae: 0.04073626273115054
r2: 0.6919449356962135
pearson: 0.8340465974493586

=== Experiment 4299 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00606598898213422
rmse: 0.07788445918239544
mae: 0.02992361919904207
r2: 0.726487226641493
pearson: 0.8527717740147078

=== Experiment 4760 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00661327366850894
rmse: 0.08132203679513285
mae: 0.035838751667436525
r2: 0.7018104010112682
pearson: 0.8388389240961517

=== Experiment 4743 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006956351421400225
rmse: 0.08340474459765598
mae: 0.039006127411506976
r2: 0.6863411761334669
pearson: 0.8318320759463939

=== Experiment 4740 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007037362838756621
rmse: 0.08388899116544805
mae: 0.035190645614193106
r2: 0.6826884069807185
pearson: 0.8291090966498382

=== Experiment 4742 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007332075941054738
rmse: 0.08562754195382896
mae: 0.035389767005162526
r2: 0.6693999229112524
pearson: 0.8231134257451674

=== Experiment 4729 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006806339364656016
rmse: 0.08250054160219808
mae: 0.038276712139279086
r2: 0.6931051537467079
pearson: 0.8332329026417765

=== Experiment 4663 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005165180715624983
rmse: 0.07186919170009486
mae: 0.028142789762570203
r2: 0.7671042749023563
pearson: 0.875924787739529

=== Experiment 4238 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006886640663765788
rmse: 0.08298578591400932
mae: 0.03337468024494685
r2: 0.6894844035131527
pearson: 0.8526406307966092

=== Experiment 4137 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00588418643986524
rmse: 0.07670845090252598
mae: 0.032361377697685934
r2: 0.7346846232549833
pearson: 0.8585824285824392

=== Experiment 4140 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009234782660887474
rmse: 0.09609777656578466
mae: 0.04305868741585226
r2: 0.5836077143592646
pearson: 0.7710423927175013

=== Experiment 4302 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006327098866321322
rmse: 0.07954306296793782
mae: 0.034589257676545186
r2: 0.7147138968867452
pearson: 0.8488597448657126

=== Experiment 4657 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008022921895689726
rmse: 0.08957076473766273
mae: 0.040527017042921186
r2: 0.6382499828813184
pearson: 0.7991674131454662

=== Experiment 4255 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005942699967485882
rmse: 0.07708890949731928
mae: 0.03401841553210777
r2: 0.7320462740483414
pearson: 0.8556250294012201

=== Experiment 4589 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006511945724538775
rmse: 0.08069662771478604
mae: 0.031887751435220865
r2: 0.7063792334070442
pearson: 0.8410371466085002

=== Experiment 4278 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005745401709668156
rmse: 0.07579842814774035
mae: 0.033874944696743674
r2: 0.7409423656557386
pearson: 0.8620956382004826

=== Experiment 4293 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00673521466654554
rmse: 0.08206835362394899
mae: 0.032772182737295145
r2: 0.6963121350801356
pearson: 0.8415969737336398

=== Experiment 4639 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006397458638007489
rmse: 0.07998411491044637
mae: 0.034890789766738296
r2: 0.7115414057491196
pearson: 0.8454297293030566

=== Experiment 4752 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007519475593795332
rmse: 0.0867149098701909
mae: 0.04907683619177422
r2: 0.6609501550500738
pearson: 0.8311119280959585

=== Experiment 4160 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005992620074240687
rmse: 0.07741201505089948
mae: 0.032698273542601713
r2: 0.7297953984062866
pearson: 0.8547571099318555

=== Experiment 4746 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007766807549481394
rmse: 0.08812949307400669
mae: 0.04923222128212435
r2: 0.6497980660273079
pearson: 0.8224629906691671

=== Experiment 4011 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007217384722253966
rmse: 0.08495519243844937
mae: 0.03396235258017666
r2: 0.6745712994875134
pearson: 0.8267929736448772

=== Experiment 4382 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005874144002745968
rmse: 0.07664296446997577
mae: 0.03178963529229881
r2: 0.7351374323246765
pearson: 0.8600319048024545

=== Experiment 4595 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006885740030445834
rmse: 0.0829803593053551
mae: 0.03434554553398824
r2: 0.6895250126731505
pearson: 0.8505168927319467

=== Experiment 4701 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0069652681261753585
rmse: 0.08345818190073014
mae: 0.04392206528328205
r2: 0.6859391258396978
pearson: 0.8354974207365314

=== Experiment 4719 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007217900400687293
rmse: 0.08495822738668277
mae: 0.030442205108374983
r2: 0.6745480477739222
pearson: 0.8224309330794739

=== Experiment 4745 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007199676640726139
rmse: 0.0848509083081975
mae: 0.04445767621041363
r2: 0.6753697490896804
pearson: 0.8289880490961421

=== Experiment 4785 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007109435358518908
rmse: 0.08431746769512773
mae: 0.041171067291773784
r2: 0.6794386887861787
pearson: 0.8290889084940293

=== Experiment 4440 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008624586345566268
rmse: 0.09286865103772246
mae: 0.03748796040544647
r2: 0.6111211976491608
pearson: 0.7955315673952672

=== Experiment 4715 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0068174950852297545
rmse: 0.08256812390523206
mae: 0.03515148848467015
r2: 0.6926021472160461
pearson: 0.8322383026791804

=== Experiment 4449 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006571368962114466
rmse: 0.08106398067029812
mae: 0.03632280961864258
r2: 0.7036998657789233
pearson: 0.8461793239784154

=== Experiment 4739 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006484260652138378
rmse: 0.08052490702967857
mae: 0.033442092220213
r2: 0.707627541136446
pearson: 0.8421021647307129

=== Experiment 4399 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005665724201905173
rmse: 0.07527100505443761
mae: 0.03166454963458947
r2: 0.7445349894120185
pearson: 0.8628672836091192

=== Experiment 4557 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006315895620205547
rmse: 0.07947260924498167
mae: 0.0407571792318197
r2: 0.7152190463231798
pearson: 0.8494428723617152

=== Experiment 4623 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009052962286956089
rmse: 0.09514705611292494
mae: 0.0415003188557615
r2: 0.5918059149945643
pearson: 0.8155792298457961

=== Experiment 4005 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006358356605738762
rmse: 0.0797393040209078
mae: 0.032761700247106985
r2: 0.7133044991739961
pearson: 0.8535575812334233

=== Experiment 4334 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009161788494021241
rmse: 0.09571723195966984
mae: 0.04194029336232302
r2: 0.5868989892160733
pearson: 0.8033677684496784

=== Experiment 4624 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006981301642883766
rmse: 0.08355418387420085
mae: 0.03569937901043271
r2: 0.6852161816282063
pearson: 0.8279256796098975

=== Experiment 4693 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007748948590913363
rmse: 0.08802811250341201
mae: 0.042207477702973104
r2: 0.6506033186088117
pearson: 0.8101658419381519

=== Experiment 4813 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007829051849538922
rmse: 0.08848192950845343
mae: 0.03939220816289151
r2: 0.6469914979335254
pearson: 0.8044571167990506

=== Experiment 4771 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006413130620646469
rmse: 0.0800820243290994
mae: 0.0373348742201094
r2: 0.7108347629496952
pearson: 0.8448603771973141

=== Experiment 4793 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007391315758543299
rmse: 0.08597276172453285
mae: 0.04457498235708874
r2: 0.6667288256141308
pearson: 0.8204269910230809

=== Experiment 4117 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006138543798959988
rmse: 0.078348859589403
mae: 0.03115438608743263
r2: 0.723215761884636
pearson: 0.8508616125180131

=== Experiment 4480 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006267493250503857
rmse: 0.07916750122685354
mae: 0.030485359964000616
r2: 0.7174014878695167
pearson: 0.8473330144511869

=== Experiment 4311 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00516826328826797
rmse: 0.0718906342180118
mae: 0.02867427299084192
r2: 0.766965283058628
pearson: 0.8769158315264709

=== Experiment 4388 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0061556849573902285
rmse: 0.0784581732988363
mae: 0.029414399154596302
r2: 0.7224428745954172
pearson: 0.8509601937870721

=== Experiment 4772 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007868413170948819
rmse: 0.08870407640547766
mae: 0.03946641422383423
r2: 0.6452167132753956
pearson: 0.8293806015970346

=== Experiment 4788 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008152028512169356
rmse: 0.0902885846171561
mae: 0.03898937814401303
r2: 0.6324286223684195
pearson: 0.798389008685762

=== Experiment 4581 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005843891747885699
rmse: 0.07644535138179233
mae: 0.030543201229168152
r2: 0.7365014931812903
pearson: 0.8583133614098163

=== Experiment 4599 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006612061320628344
rmse: 0.08131458245990288
mae: 0.03165492059618499
r2: 0.7018650652436091
pearson: 0.8398468102554008

=== Experiment 4308 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00583985140057402
rmse: 0.07641892043580582
mae: 0.028190703671358553
r2: 0.7366836706632812
pearson: 0.8614610969971318

=== Experiment 4392 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008215909805559375
rmse: 0.09064165601730462
mae: 0.03436814321015574
r2: 0.6295482429657708
pearson: 0.8586454184419674

=== Experiment 4358 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009636232901515215
rmse: 0.09816431582563602
mae: 0.040280642171028884
r2: 0.565506499701123
pearson: 0.7602627134576474

=== Experiment 4236 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005824448904982101
rmse: 0.07631807718347011
mae: 0.031644630810211766
r2: 0.7373781624103641
pearson: 0.858769625671058

=== Experiment 4433 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060031296257099485
rmse: 0.07747986593760955
mae: 0.030805686985249936
r2: 0.729321527356145
pearson: 0.8551219092912622

=== Experiment 4458 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005970409011817127
rmse: 0.07726842182817717
mae: 0.034165249318670536
r2: 0.7307968854351287
pearson: 0.8549158550780616

=== Experiment 4282 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006026744448450089
rmse: 0.07763210964832844
mae: 0.02730990620575614
r2: 0.7282567453924703
pearson: 0.853991615930152

=== Experiment 4539 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006489606470243608
rmse: 0.08055809375999166
mae: 0.03269250480758831
r2: 0.7073865005509556
pearson: 0.8430261470793695

=== Experiment 4753 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007518630315928416
rmse: 0.08671003584319646
mae: 0.035852748316054395
r2: 0.6609882682570578
pearson: 0.8167558613820075

=== Experiment 4806 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006864851890885572
rmse: 0.08285440175926426
mae: 0.03491852745221978
r2: 0.6904668496923494
pearson: 0.8314354580104153

=== Experiment 4351 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0060436924628837615
rmse: 0.07774118897266598
mae: 0.03343751041322079
r2: 0.7274925668810475
pearson: 0.8540017855395395

=== Experiment 4815 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007346787274421354
rmse: 0.08571340195337807
mae: 0.03894041825745602
r2: 0.6687365953647046
pearson: 0.8188499444297622

=== Experiment 4783 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0069840171064493575
rmse: 0.08357043201066605
mae: 0.04416432788843172
r2: 0.6850937425712007
pearson: 0.834640988238804

=== Experiment 4506 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005991178606860479
rmse: 0.07740270413144801
mae: 0.030282377478898306
r2: 0.7298603935360217
pearson: 0.8586491656268314

=== Experiment 4218 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006460390417082267
rmse: 0.08037655390150954
mae: 0.030917386148772028
r2: 0.7087038395290322
pearson: 0.8467553242511483

=== Experiment 4790 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007157172612794772
rmse: 0.08460007454367148
mae: 0.034886349058595674
r2: 0.6772862370016525
pearson: 0.8240525345693103

=== Experiment 4125 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0055928089262347385
rmse: 0.07478508491828259
mae: 0.028472980994889624
r2: 0.7478227070995314
pearson: 0.8667768185290524

=== Experiment 4713 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007170833937030826
rmse: 0.08468077666761699
mae: 0.03859724227857621
r2: 0.6766702539046574
pearson: 0.8243585152692655

=== Experiment 4429 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005712531064776643
rmse: 0.07558128779517218
mae: 0.02986180907995252
r2: 0.7424244885664197
pearson: 0.8630689068345749

=== Experiment 4226 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005823062248244884
rmse: 0.07630899192260951
mae: 0.030585603216494447
r2: 0.7374406861523309
pearson: 0.8603412347668178

=== Experiment 4674 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0057392392346539405
rmse: 0.07575776682726293
mae: 0.033523507303698775
r2: 0.7412202289418168
pearson: 0.8627929747984816

=== Experiment 4341 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0068400124400254765
rmse: 0.08270436772036575
mae: 0.04113222134904392
r2: 0.6915868495989528
pearson: 0.8401958235687349

=== Experiment 4800 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070116904931595845
rmse: 0.08373583756767221
mae: 0.041776126772391724
r2: 0.6838459617444256
pearson: 0.8303891375213618

=== Experiment 4147 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007180415179347813
rmse: 0.0847373304945808
mae: 0.033127303478016276
r2: 0.6762382399056117
pearson: 0.8269887943229083

=== Experiment 4365 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00664150637650949
rmse: 0.08149543776500308
mae: 0.03198042125188406
r2: 0.7005374006336884
pearson: 0.8481310973245881

=== Experiment 4475 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0063056139817110005
rmse: 0.07940789621763696
mae: 0.03481586233974423
r2: 0.7156826408776038
pearson: 0.8577040825745772

=== Experiment 4794 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007707281929028016
rmse: 0.08779112671009534
mae: 0.04883475723906201
r2: 0.6524820500543216
pearson: 0.8207559276445188

=== Experiment 4818 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007102857137589823
rmse: 0.08427845001890948
mae: 0.04403226880247072
r2: 0.6797352978725748
pearson: 0.8308261341179414

=== Experiment 4514 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007192643466348097
rmse: 0.08480945387365783
mae: 0.03525374973994494
r2: 0.6756868718268496
pearson: 0.8241992215975935

=== Experiment 4799 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006769526256553308
rmse: 0.0822771308235363
mae: 0.03546835225850872
r2: 0.694765040588371
pearson: 0.835205559700919

=== Experiment 4781 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0074709838419133485
rmse: 0.08643485316649382
mae: 0.04051999163697859
r2: 0.6631366267995802
pearson: 0.81517894972887

=== Experiment 4779 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006903335294316493
rmse: 0.0830863123162684
mae: 0.04066559466072447
r2: 0.6887316499694902
pearson: 0.832574087865549

=== Experiment 4629 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005593966534837762
rmse: 0.07479282408652425
mae: 0.034685836319351133
r2: 0.7477705110371236
pearson: 0.867370382030083

=== Experiment 4415 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005605193673569283
rmse: 0.07486784138446416
mae: 0.028436140460019115
r2: 0.7472642842931608
pearson: 0.8651454581462423

=== Experiment 4762 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00653651386561399
rmse: 0.0808487097337613
mae: 0.04075681568051436
r2: 0.7052714667392288
pearson: 0.8461280764541018

=== Experiment 4687 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007179599778699098
rmse: 0.08473251901542346
mae: 0.03997218817529306
r2: 0.6762750059619769
pearson: 0.8355136266132476

=== Experiment 4404 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.010223397111200603
rmse: 0.10111081599512786
mae: 0.045773731442904676
r2: 0.5390315239171406
pearson: 0.7809824665426979

=== Experiment 4319 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005791851502568603
rmse: 0.07610421474904398
mae: 0.027991920510376835
r2: 0.7388479649379747
pearson: 0.8640066820361345

=== Experiment 4503 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006068356850698112
rmse: 0.07789965886124349
mae: 0.0370508622000377
r2: 0.7263804604901256
pearson: 0.8539392600727141

=== Experiment 4852 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0069917074062802505
rmse: 0.08361643024119274
mae: 0.036132290679075946
r2: 0.6847469903365855
pearson: 0.8314493662606446

=== Experiment 4855 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006467004747932733
rmse: 0.08041768927252718
mae: 0.03444043655501169
r2: 0.7084056022621743
pearson: 0.8429587430772253

=== Experiment 4492 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.011761368087002027
rmse: 0.10844984134152538
mae: 0.04042864588213735
r2: 0.4696850895310475
pearson: 0.6979458623332397

=== Experiment 4610 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007565239630881239
rmse: 0.08697838599836881
mae: 0.03173853267022641
r2: 0.6588866747601634
pearson: 0.8367573756569637

=== Experiment 4496 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.009516215102731493
rmse: 0.09755108970550505
mae: 0.04538007680872394
r2: 0.5709180494244082
pearson: 0.7639483282397813

=== Experiment 4580 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007874986911839695
rmse: 0.08874112300303448
mae: 0.044410852716533764
r2: 0.6449203061919506
pearson: 0.8084794114157944

=== Experiment 4877 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007368920761003031
rmse: 0.08584241819172518
mae: 0.03639019000272591
r2: 0.6677386061964292
pearson: 0.8209090433988521

=== Experiment 4928 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007458757279189131
rmse: 0.08636409716536804
mae: 0.037236606517061864
r2: 0.6636879171314936
pearson: 0.8233313796477599

=== Experiment 4501 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005529256482231422
rmse: 0.07435897042207767
mae: 0.029054236172754693
r2: 0.7506882588280712
pearson: 0.8668898425379347

=== Experiment 4878 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068337891521115446
rmse: 0.08266673546301163
mae: 0.03461668149129538
r2: 0.6918674549119134
pearson: 0.8359219386450051

=== Experiment 4804 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006429712078915323
rmse: 0.08018548546286493
mae: 0.03936331102648759
r2: 0.7100871122944105
pearson: 0.8451462098025553

=== Experiment 4886 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0075347159371290315
rmse: 0.08680274153002907
mae: 0.04079545503838541
r2: 0.6602629746769454
pearson: 0.8146361392991918

=== Experiment 4375 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006447235415791268
rmse: 0.08029467862686336
mae: 0.03023250355110474
r2: 0.7092969927472844
pearson: 0.844927202222522

=== Experiment 4874 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007296096277090595
rmse: 0.08541718958787274
mae: 0.039091084914677464
r2: 0.6710222301235345
pearson: 0.8244138391555332

=== Experiment 4922 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0074077889686250155
rmse: 0.08606851322420421
mae: 0.039362123249229365
r2: 0.6659860558219604
pearson: 0.8184019393178066

=== Experiment 4105 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006377788527535127
rmse: 0.07986105764097498
mae: 0.03438089579075387
r2: 0.7124283223728409
pearson: 0.8460552468287504

=== Experiment 4290 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005740564169612841
rmse: 0.07576651087131334
mae: 0.03345140916968811
r2: 0.7411604882076686
pearson: 0.8660689694083598

=== Experiment 4940 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007812940039470194
rmse: 0.08839083685241471
mae: 0.03301921891812948
r2: 0.6477179723581745
pearson: 0.8135194638043699

=== Experiment 4352 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006796239335409167
rmse: 0.08243930698040328
mae: 0.03981598171386534
r2: 0.6935605596200853
pearson: 0.8381927562198005

=== Experiment 4918 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006978174164420553
rmse: 0.08353546650627237
mae: 0.03644393812245811
r2: 0.6853571982556041
pearson: 0.828507249336859

=== Experiment 4938 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008768105138801784
rmse: 0.09363816069745168
mae: 0.04142861851929377
r2: 0.604649998429623
pearson: 0.7916706582125584

=== Experiment 4923 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006226424470599259
rmse: 0.07890769588955984
mae: 0.030351165046139852
r2: 0.7192532610796629
pearson: 0.8530182344160663

=== Experiment 4027 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00681762222177538
rmse: 0.08256889379043042
mae: 0.033853104619759995
r2: 0.692596414685161
pearson: 0.8388493466841058

=== Experiment 4192 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.012929127435262778
rmse: 0.1137063209995943
mae: 0.04521592642064914
r2: 0.41703133448816454
pearson: 0.6464761195984209

=== Experiment 4263 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009643686421597757
rmse: 0.09820227299608578
mae: 0.043575003491043145
r2: 0.5651704237611462
pearson: 0.7931646577290646

=== Experiment 4171 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005020482125559137
rmse: 0.07085536059861058
mae: 0.028667220192632722
r2: 0.7736286706417055
pearson: 0.8804452813384149

=== Experiment 4621 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006444057706068352
rmse: 0.08027488839025783
mae: 0.032213130148172786
r2: 0.7094402742800738
pearson: 0.842406818976695

=== Experiment 4695 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007056072124588379
rmse: 0.08400042931192898
mae: 0.03802940978851614
r2: 0.681844813517151
pearson: 0.8259435388548966

=== Experiment 4550 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005993211383151303
rmse: 0.0774158341888228
mae: 0.03314335155718155
r2: 0.7297687365477618
pearson: 0.8553427429161095

=== Experiment 4588 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00563074279499977
rmse: 0.07503827553322218
mae: 0.029410965577267027
r2: 0.7461122856528886
pearson: 0.8642006585144769

=== Experiment 4340 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005983237294441034
rmse: 0.07735138844546383
mae: 0.03566268631439395
r2: 0.7302184638177757
pearson: 0.8555903734613187

=== Experiment 4956 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007399121594053796
rmse: 0.08601814688804797
mae: 0.03496346551600618
r2: 0.6663768639265735
pearson: 0.8228842366036584

=== Experiment 4782 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006162265545913002
rmse: 0.07850009901849171
mae: 0.030912868764826237
r2: 0.722146158755264
pearson: 0.8500740402138176

=== Experiment 4917 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007182685277252801
rmse: 0.0847507243464786
mae: 0.03920763922713496
r2: 0.6761358821902224
pearson: 0.823816028895971

=== Experiment 4426 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008932978445225158
rmse: 0.0945144351156222
mae: 0.039002784238749304
r2: 0.5972159336092847
pearson: 0.8113935886713264

=== Experiment 4607 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005980944441706629
rmse: 0.07733656600668683
mae: 0.030346369655763292
r2: 0.7303218475384082
pearson: 0.8584329290467326

=== Experiment 4840 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006683505812147188
rmse: 0.08175271134431682
mae: 0.033694259122727784
r2: 0.6986436645661501
pearson: 0.8391081473158615

=== Experiment 4497 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008219927595679547
rmse: 0.09066381635293955
mae: 0.04392359137226138
r2: 0.629367082577617
pearson: 0.7956124231844413

=== Experiment 4660 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00630283634297173
rmse: 0.07939040460264533
mae: 0.031179413302301565
r2: 0.7158078833858252
pearson: 0.8499182556420021

=== Experiment 4899 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006978402983940072
rmse: 0.08353683609007509
mae: 0.03568056066379315
r2: 0.68534688088418
pearson: 0.8280175400428704

=== Experiment 4534 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006112295195685701
rmse: 0.07818116906062292
mae: 0.03126917330716662
r2: 0.7243992998533799
pearson: 0.8568895998319219

=== Experiment 4454 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00640193005867848
rmse: 0.08001206195742289
mae: 0.03308356475875343
r2: 0.7113397913590875
pearson: 0.8435348277435423

=== Experiment 4092 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007982565082641582
rmse: 0.08934520178857722
mae: 0.05189627750625836
r2: 0.6400696538192675
pearson: 0.813095070176531

=== Experiment 4709 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.014669575947013062
rmse: 0.12111802486423341
mae: 0.05411689826527779
r2: 0.33855527712331557
pearson: 0.6102299374317713

=== Experiment 4960 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007114395471079401
rmse: 0.08434687588215346
mae: 0.04263515803900627
r2: 0.6792150394939387
pearson: 0.8259718420576316

=== Experiment 4812 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00704578899124871
rmse: 0.08393919818087799
mae: 0.045050998701770616
r2: 0.6823084754734835
pearson: 0.8320298029991603

=== Experiment 4756 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.009265286274921661
rmse: 0.0962563570623866
mae: 0.04285335316127673
r2: 0.5822323198281303
pearson: 0.7977985707146702

=== Experiment 4508 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007571059164639618
rmse: 0.08701183347476145
mae: 0.046877469476027604
r2: 0.6586242745443709
pearson: 0.8286062008226018

=== Experiment 4921 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006813043917523923
rmse: 0.0825411649876831
mae: 0.034769008559451545
r2: 0.6928028484087934
pearson: 0.8332406053821292

=== Experiment 4391 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005950883623405036
rmse: 0.07714197056988521
mae: 0.03351618601293508
r2: 0.7316772766048492
pearson: 0.8556207022774225

=== Experiment 4934 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00751142429227561
rmse: 0.08666847346224353
mae: 0.040975732574874454
r2: 0.6613131846919492
pearson: 0.8238956051963696

=== Experiment 4919 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007267470910189548
rmse: 0.0852494628146685
mae: 0.03750070511197022
r2: 0.6723129353181152
pearson: 0.8208738179921636

=== Experiment 4711 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005962913865012427
rmse: 0.077219905885804
mae: 0.03135978753067952
r2: 0.7311348383056862
pearson: 0.8552094132046676

=== Experiment 4884 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006739186457682632
rmse: 0.08209254812516562
mae: 0.03929434018426456
r2: 0.696133048765883
pearson: 0.834792133836568

=== Experiment 4546 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007804846396709706
rmse: 0.08834504172113852
mae: 0.03785253714326465
r2: 0.6480829111479602
pearson: 0.8162680448258613

=== Experiment 4948 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008072082897698642
rmse: 0.0898447711205201
mae: 0.041075491942815415
r2: 0.6360333349381481
pearson: 0.828229918580292

=== Experiment 4679 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005800065865859016
rmse: 0.07615816348796113
mae: 0.029680743942847577
r2: 0.7384775829126278
pearson: 0.8692695895766012

=== Experiment 4647 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006195280123891935
rmse: 0.0787101017906338
mae: 0.03614670629223577
r2: 0.7206575459650209
pearson: 0.8495114783197755

=== Experiment 4678 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006303351886706172
rmse: 0.07939365142570388
mae: 0.03194637319065153
r2: 0.7157846377457473
pearson: 0.84705054533973

=== Experiment 4410 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008920102660513974
rmse: 0.09444629511269341
mae: 0.04076043508364206
r2: 0.5977964970748457
pearson: 0.8054089089960743

=== Experiment 4732 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006491155294772437
rmse: 0.08056770627722026
mae: 0.031260192330021586
r2: 0.7073166647346405
pearson: 0.8507704907498287

=== Experiment 4718 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006195965557632223
rmse: 0.07871445583647405
mae: 0.0331496973045302
r2: 0.7206266400593537
pearson: 0.8571902909557267

=== Experiment 4179 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008094100819168741
rmse: 0.08996722080385022
mae: 0.034667385085518414
r2: 0.6350405565498896
pearson: 0.8253776087487488

=== Experiment 4397 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0073179741570464476
rmse: 0.08554515858332631
mae: 0.03818637911271653
r2: 0.6700357661455163
pearson: 0.8202871502841729

=== Experiment 4619 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059728057259513
rmse: 0.07728392928643897
mae: 0.03291805568792634
r2: 0.7306888186496934
pearson: 0.8549707083167055

=== Experiment 4825 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007757596593693275
rmse: 0.08807721949342677
mae: 0.04359951942896842
r2: 0.6502133839697424
pearson: 0.8175376974732619

=== Experiment 4809 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008432745769971292
rmse: 0.09182998295748122
mae: 0.0481694614902519
r2: 0.6197712047672439
pearson: 0.8237289888215961

=== Experiment 4824 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006693105680468448
rmse: 0.0818114031200324
mae: 0.0409162491235203
r2: 0.6982108107287697
pearson: 0.8398429649770858

=== Experiment 4520 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006673864633292061
rmse: 0.08169372456493865
mae: 0.032908596041379495
r2: 0.6990783810773203
pearson: 0.8406655185190669

=== Experiment 4083 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005786239457077347
rmse: 0.07606733502021315
mae: 0.031124995060842486
r2: 0.7391010095991253
pearson: 0.8599932718889919

=== Experiment 4983 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006472950898707045
rmse: 0.0804546511937442
mae: 0.03818264987492724
r2: 0.7081374929408617
pearson: 0.8566103130258211

=== Experiment 4015 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005795460909573033
rmse: 0.07612792463723829
mae: 0.030236342895586046
r2: 0.7386852183647665
pearson: 0.8637084989739446

=== Experiment 4651 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006049329787367948
rmse: 0.07777743752122429
mae: 0.03036013683806536
r2: 0.7272383824012318
pearson: 0.8548368687813698

=== Experiment 4776 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005125138218206363
rmse: 0.07159007066770058
mae: 0.0299072372145661
r2: 0.7689097734869109
pearson: 0.8769329120132784

=== Experiment 4889 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006496026204161703
rmse: 0.08059792927961427
mae: 0.03328836341692946
r2: 0.7070970375741297
pearson: 0.8415595621069004

=== Experiment 4767 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006430862718370532
rmse: 0.08019266000308589
mae: 0.034780110851657095
r2: 0.7100352304678104
pearson: 0.844954419349522

=== Experiment 4883 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008517373805525023
rmse: 0.09228961916448146
mae: 0.03635602033491748
r2: 0.6159553638917731
pearson: 0.8187625627337791

=== Experiment 4668 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00661371935937343
rmse: 0.0813247770324237
mae: 0.03305099754956748
r2: 0.7017903050063519
pearson: 0.8377965353770174

=== Experiment 4866 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006720196608319276
rmse: 0.08197680530686273
mae: 0.034839429040395255
r2: 0.696989292715605
pearson: 0.8449326667406082

=== Experiment 4525 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005715813970752498
rmse: 0.075603002392448
mae: 0.03039643293863333
r2: 0.7422764637808859
pearson: 0.8617479703044548

=== Experiment 4913 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00662350554404563
rmse: 0.08138492209276624
mae: 0.03328947282505089
r2: 0.7013490502467119
pearson: 0.8382429036659674

=== Experiment 4885 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006562422578745096
rmse: 0.08100878087432928
mae: 0.03059005581306801
r2: 0.7041032542674439
pearson: 0.840331492848119

=== Experiment 4232 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006013166997821087
rmse: 0.07754461295164924
mae: 0.030312844782502277
r2: 0.7288689466654389
pearson: 0.8556872889228013

=== Experiment 4946 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006987722898815018
rmse: 0.08359260074202153
mae: 0.039297712245628044
r2: 0.6849266500244771
pearson: 0.8302726934095871

=== Experiment 4590 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008392615954574516
rmse: 0.09161122177208705
mae: 0.040251707586942895
r2: 0.621580640481002
pearson: 0.7887114622148216

=== Experiment 4751 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007515315621097244
rmse: 0.08669092006142999
mae: 0.039315297931897905
r2: 0.6611377263880867
pearson: 0.8236598250401851

=== Experiment 4173 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006081612191561899
rmse: 0.07798469203351321
mae: 0.0364397889691401
r2: 0.7257827830046992
pearson: 0.8533993520131539

=== Experiment 4708 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00701078618819953
rmse: 0.08373043764485846
mae: 0.030038909361547413
r2: 0.6838867364570593
pearson: 0.8285849651875815

=== Experiment 4861 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007095652760526816
rmse: 0.08423569766154261
mae: 0.031864978136880016
r2: 0.6800601400634569
pearson: 0.8474741182512101

=== Experiment 4755 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007174920107022973
rmse: 0.08470490013584205
mae: 0.03826162901536533
r2: 0.6764860102981725
pearson: 0.8235769805498008

=== Experiment 4669 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006365178675533699
rmse: 0.07978206988749853
mae: 0.03178039342770282
r2: 0.7129968950495649
pearson: 0.8490713725762438

=== Experiment 4291 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006610380860201681
rmse: 0.08130424872170999
mae: 0.032546091465221805
r2: 0.7019408364646202
pearson: 0.8441119791270889

=== Experiment 4487 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005995372963491531
rmse: 0.07742979377146456
mae: 0.03507915908714151
r2: 0.7296712718416052
pearson: 0.8550664370950465

=== Experiment 4285 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005297858146847655
rmse: 0.07278638709846542
mae: 0.029746163122743634
r2: 0.7611219079243292
pearson: 0.8726019142197801

=== Experiment 4424 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005785091780301718
rmse: 0.07605979082473024
mae: 0.033697393191019984
r2: 0.7391527578398074
pearson: 0.8605521483081767

=== Experiment 4361 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006618260177808283
rmse: 0.08135269004653922
mae: 0.04304809965220003
r2: 0.7015855615016916
pearson: 0.8420200072106103

=== Experiment 4244 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006670790670488073
rmse: 0.08167490845105413
mae: 0.03346170527849356
r2: 0.6992169847071976
pearson: 0.8390496444670182

=== Experiment 4638 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005778122379275334
rmse: 0.076013961739113
mae: 0.030090800530887463
r2: 0.7394670050646188
pearson: 0.8613201597190135

=== Experiment 4447 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006998925391589649
rmse: 0.08365958039333958
mae: 0.037682617350926303
r2: 0.684421534555863
pearson: 0.8317171731155014

=== Experiment 4474 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006249640096564563
rmse: 0.07905466524225223
mae: 0.034583174583581365
r2: 0.7182064787228657
pearson: 0.8506969861323879

=== Experiment 4808 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006028753103159918
rmse: 0.07764504558025527
mae: 0.03489140699396054
r2: 0.7281661760356807
pearson: 0.8554933452455573

=== Experiment 4248 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.015121129298883642
rmse: 0.12296800111770397
mae: 0.054357255791229846
r2: 0.31819493523130005
pearson: 0.6175462824625639

=== Experiment 4738 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007310949940270172
rmse: 0.08550409311997977
mae: 0.03701227564450312
r2: 0.6703524849883644
pearson: 0.8204129647734917

=== Experiment 4584 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007064300429273497
rmse: 0.08404939279538845
mae: 0.03348460000924075
r2: 0.6814738028804528
pearson: 0.8316931234759273

=== Experiment 4846 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006122303084094777
rmse: 0.07824514735173535
mae: 0.032533091351491565
r2: 0.7239480485698233
pearson: 0.8582426263654108

=== Experiment 4145 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00651684592544184
rmse: 0.08072698387430215
mae: 0.033431819501831044
r2: 0.7061582855050685
pearson: 0.841314462181579

=== Experiment 4372 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007210542084832997
rmse: 0.08491491085099835
mae: 0.034225062935469446
r2: 0.67487983099715
pearson: 0.8229775401313427

=== Experiment 4977 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007308823395291314
rmse: 0.0854916568753426
mae: 0.04020191606939326
r2: 0.6704483699654972
pearson: 0.822371424796923

=== Experiment 4156 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005954325811033666
rmse: 0.07716427807628129
mae: 0.030846033182085994
r2: 0.7315220698796969
pearson: 0.856725299212747

=== Experiment 4326 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007099309263927949
rmse: 0.08425739886756503
mae: 0.034468494324519844
r2: 0.67989526993445
pearson: 0.8366881305092843

=== Experiment 4551 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00654001717317422
rmse: 0.0808703726538602
mae: 0.03258652437944495
r2: 0.7051135041432615
pearson: 0.8513660826725805

=== Experiment 4895 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0072483588172995665
rmse: 0.08513729392751197
mae: 0.03762148108331967
r2: 0.6731746911746491
pearson: 0.8275817973602746

=== Experiment 4547 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006925986701799956
rmse: 0.0832225131908425
mae: 0.037181503449017696
r2: 0.6877103079757079
pearson: 0.833536649148911

=== Experiment 4544 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0062946185550135005
rmse: 0.07933863217256458
mae: 0.03640823578098976
r2: 0.7161784198279366
pearson: 0.8472934014838791

=== Experiment 4193 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005694847649560898
rmse: 0.07546421436390163
mae: 0.029035478011786128
r2: 0.7432218259754313
pearson: 0.8625341595173657

=== Experiment 4162 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006642063385545155
rmse: 0.08149885511800245
mae: 0.033098236674176114
r2: 0.7005122853414287
pearson: 0.8436719934212877

=== Experiment 4805 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006429234629227834
rmse: 0.0801825082497912
mae: 0.04018397418224491
r2: 0.7101086402906777
pearson: 0.8475068029677162

=== Experiment 4637 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0076321633556062215
rmse: 0.08736225360878817
mae: 0.03849676724391723
r2: 0.6558691134676978
pearson: 0.8099749217218704

=== Experiment 4455 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005479303608576727
rmse: 0.0740223183139837
mae: 0.02760866818231526
r2: 0.7529406119152203
pearson: 0.8689459304572166

=== Experiment 4129 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005804090968581681
rmse: 0.07618458484878474
mae: 0.035484614207123615
r2: 0.738296092802446
pearson: 0.8617021730134727

=== Experiment 4532 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007059144807842626
rmse: 0.084018717009025
mae: 0.03988174091864859
r2: 0.6817062675816083
pearson: 0.8311470216746891

=== Experiment 4558 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01283606969710749
rmse: 0.11329637989409674
mae: 0.04015014137953288
r2: 0.4212272669437437
pearson: 0.7177787682809084

=== Experiment 4640 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008076468319932067
rmse: 0.08986917335734243
mae: 0.04056181863008988
r2: 0.6358355981798149
pearson: 0.8106807897665889

=== Experiment 4280 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006436453093700446
rmse: 0.08022750833536117
mae: 0.0315381098477065
r2: 0.7097831629046967
pearson: 0.8440835165045322

=== Experiment 4862 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006934877626126871
rmse: 0.08327591264061218
mae: 0.03662431330269852
r2: 0.6873094201110024
pearson: 0.8295532967830924

=== Experiment 4723 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006431224475406935
rmse: 0.08019491552091651
mae: 0.03148280671642626
r2: 0.7100189190022601
pearson: 0.8464805563779038

=== Experiment 4682 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006758732526883299
rmse: 0.08221151091473322
mae: 0.034501690102430344
r2: 0.6952517251085079
pearson: 0.8397561997290757

=== Experiment 4857 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005797811497175425
rmse: 0.07614336147803973
mae: 0.030351199277231514
r2: 0.7385792314043473
pearson: 0.8602936573469969

=== Experiment 4511 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0054265557680916775
rmse: 0.07366515979818192
mae: 0.027212994050510225
r2: 0.7553189888265911
pearson: 0.8694069537360221

=== Experiment 4328 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010058526756917395
rmse: 0.10029220686034082
mae: 0.042553895847208305
r2: 0.5464654556267782
pearson: 0.7499007221859252

=== Experiment 4830 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.012824979567789534
rmse: 0.11324742631861236
mae: 0.05148293205395146
r2: 0.42172731599355107
pearson: 0.6561579967355935

=== Experiment 4478 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006628323685617343
rmse: 0.08141451765881404
mae: 0.03672509825059586
r2: 0.7011318023638695
pearson: 0.8485346402657119

=== Experiment 4531 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0058050151780840616
rmse: 0.07619065020121604
mae: 0.030919501513606406
r2: 0.7382544206027594
pearson: 0.8619704405073694

=== Experiment 4675 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00805434379299901
rmse: 0.08974599597195972
mae: 0.04806457446012943
r2: 0.6368331833614773
pearson: 0.8162835145340037

=== Experiment 4819 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.009386934603155754
rmse: 0.09688619407921727
mae: 0.045958280276333405
r2: 0.576747250249579
pearson: 0.765290282055614

=== Experiment 4688 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.013146579148904282
rmse: 0.11465853282204636
mae: 0.056106991968106805
r2: 0.40722653242788764
pearson: 0.6733857050664073

=== Experiment 4559 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007567087710901541
rmse: 0.08698900913852015
mae: 0.04004732440959645
r2: 0.6588033456454503
pearson: 0.8139267230383789

=== Experiment 4863 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006992683475865186
rmse: 0.08362226662716807
mae: 0.04406068438115453
r2: 0.6847029797886018
pearson: 0.8364872315178238

=== Experiment 4911 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005837860480831907
rmse: 0.07640589297188997
mae: 0.031725459904957405
r2: 0.7367734403581825
pearson: 0.859948305246417

=== Experiment 4060 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006255560740971028
rmse: 0.07909210289890532
mae: 0.029078866152353024
r2: 0.7179395194724532
pearson: 0.8565441290624165

=== Experiment 4989 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006069565435458765
rmse: 0.07790741579245691
mae: 0.03216454259609423
r2: 0.7263259659352068
pearson: 0.8527360917883172

=== Experiment 4177 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005822810295841282
rmse: 0.07630734103506216
mae: 0.03188732810448623
r2: 0.7374520465753167
pearson: 0.8621395675563964

=== Experiment 4925 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007100010290613492
rmse: 0.08426155879529817
mae: 0.045594320032062964
r2: 0.6798636609497444
pearson: 0.8315796357263677

=== Experiment 4366 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0056420309818135846
rmse: 0.07511345406658906
mae: 0.027920926279073503
r2: 0.7456033062777645
pearson: 0.8641161435212535

=== Experiment 4002 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006042255377898015
rmse: 0.077731945671635
mae: 0.02913902027517755
r2: 0.7275573644105454
pearson: 0.8550207769006127

=== Experiment 4032 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006079542566283796
rmse: 0.077971421471484
mae: 0.02986437046776192
r2: 0.7258761014975772
pearson: 0.8531060873215417

=== Experiment 4601 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006491517088444707
rmse: 0.0805699515231622
mae: 0.03526437288211222
r2: 0.7073003516171956
pearson: 0.8434763945536241

=== Experiment 4249 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00805331869552656
rmse: 0.08974028468601244
mae: 0.04205609173321292
r2: 0.6368794045553303
pearson: 0.8011311657664761

=== Experiment 4286 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007157688661654237
rmse: 0.08460312442016688
mae: 0.0321536638092994
r2: 0.6772629685857097
pearson: 0.8528598963887477

=== Experiment 4486 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007313041244820401
rmse: 0.08551632151127878
mae: 0.04790018015023651
r2: 0.6702581889866481
pearson: 0.8365681397999127

=== Experiment 4700 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009645066914423467
rmse: 0.09820930156774085
mae: 0.043470459041301056
r2: 0.5651081779472362
pearson: 0.7696612848699156

=== Experiment 4552 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006663973075811795
rmse: 0.08163316161837537
mae: 0.03626933006423394
r2: 0.6995243870504982
pearson: 0.8391162102414012

=== Experiment 4994 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006396316016077581
rmse: 0.07997697178611841
mae: 0.035105030070037097
r2: 0.7115929260690328
pearson: 0.8444620365025836

=== Experiment 4814 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.014110624554447922
rmse: 0.11878814989066848
mae: 0.04772927805863263
r2: 0.36375814940075635
pearson: 0.6448318185963987

=== Experiment 4811 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006681459484550018
rmse: 0.08174019503616331
mae: 0.0319997004383192
r2: 0.6987359325768487
pearson: 0.8381076553646575

=== Experiment 4008 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006126542075104286
rmse: 0.07827223054892639
mae: 0.030063399928450845
r2: 0.723756914330927
pearson: 0.8513417419897408

=== Experiment 4330 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0065036961742945204
rmse: 0.08064549692508888
mae: 0.03147051626318785
r2: 0.706751201996653
pearson: 0.8459059120115786

=== Experiment 4869 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005953917697066903
rmse: 0.07716163358215598
mae: 0.029622561787097934
r2: 0.7315404715588335
pearson: 0.8560867591486556

=== Experiment 4535 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006663434419397317
rmse: 0.08162986230171725
mae: 0.03643128120691348
r2: 0.6995486748311461
pearson: 0.8459626776936703

=== Experiment 4728 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008123823579285021
rmse: 0.09013225604235713
mae: 0.05260157062702176
r2: 0.6337003703782337
pearson: 0.8224416067219755

=== Experiment 4620 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005835995371061392
rmse: 0.07639368672253874
mae: 0.03209874428883671
r2: 0.7368575373368373
pearson: 0.8665497906075925

=== Experiment 4864 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009012383793142493
rmse: 0.09493357568922858
mae: 0.041990704599986044
r2: 0.5936355814206584
pearson: 0.7765315177138576

=== Experiment 4670 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006190570543367651
rmse: 0.07868017884682044
mae: 0.034572131615548486
r2: 0.7208698988780159
pearson: 0.8508635303078501

=== Experiment 4828 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006845441278076993
rmse: 0.0827371819563429
mae: 0.040702291107239205
r2: 0.6913420656806242
pearson: 0.8378972424812081

=== Experiment 4138 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006233121423011248
rmse: 0.07895011984165222
mae: 0.029391035461445798
r2: 0.7189512984429605
pearson: 0.8501326624986586

=== Experiment 4908 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007201989634227243
rmse: 0.08486453696466648
mae: 0.0468885401608645
r2: 0.6752654572307419
pearson: 0.8308273708077618

=== Experiment 4867 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00768704505062721
rmse: 0.08767579512400905
mae: 0.04228075214687903
r2: 0.6533945219944826
pearson: 0.8103732247485583

=== Experiment 4522 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00605580100862294
rmse: 0.0778190272916781
mae: 0.030871179109279376
r2: 0.7269465978830465
pearson: 0.8569014002902801

=== Experiment 4875 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006149542614282519
rmse: 0.07841901946774468
mae: 0.039315015767271304
r2: 0.722719830142692
pearson: 0.8539167276116088

=== Experiment 4761 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009461250541853929
rmse: 0.09726895980657925
mae: 0.04708581818193232
r2: 0.5733963772826238
pearson: 0.7586473279690877

=== Experiment 4220 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006651999310888022
rmse: 0.08155978979183322
mae: 0.03241096371666718
r2: 0.700064278840854
pearson: 0.8371985571836609

=== Experiment 4332 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057876294569868935
rmse: 0.07607647111286704
mae: 0.02966208660199337
r2: 0.7390383351149896
pearson: 0.8597702577145488

=== Experiment 4702 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006275299218276516
rmse: 0.07921678621527457
mae: 0.03557991495640973
r2: 0.7170495202183169
pearson: 0.8475163098592691

=== Experiment 4592 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0059478433538624935
rmse: 0.07712226237515658
mae: 0.02971836433223206
r2: 0.7318143610203975
pearson: 0.8575201713976803

=== Experiment 4691 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006650735995385663
rmse: 0.08155204470389239
mae: 0.033480034075682265
r2: 0.7001212411808279
pearson: 0.8461018426362815

=== Experiment 4873 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006420811029004408
rmse: 0.08012996336579974
mae: 0.03978858800864501
r2: 0.710488456717242
pearson: 0.8462772081904869

=== Experiment 4445 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006601155863556969
rmse: 0.0812474975833531
mae: 0.03145622663282848
r2: 0.7023567875030988
pearson: 0.8528367212213551

=== Experiment 4765 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010612433791290609
rmse: 0.10301666754118291
mae: 0.03893396249073532
r2: 0.5214900312400197
pearson: 0.7608881259158512

=== Experiment 4194 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059112088458605685
rmse: 0.07688438622932857
mae: 0.031672188932300345
r2: 0.7334661948621242
pearson: 0.8574168795847539

=== Experiment 4741 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005885702109364264
rmse: 0.07671832968309636
mae: 0.028182809276021716
r2: 0.7346162823843694
pearson: 0.8614618105997697

=== Experiment 4578 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006163378546845218
rmse: 0.07850718786738714
mae: 0.035060895960403106
r2: 0.7220959740330996
pearson: 0.8512152515317242

=== Experiment 4439 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006537549227005969
rmse: 0.08085511255947869
mae: 0.030366554637071823
r2: 0.7052247827497617
pearson: 0.8461295237660091

=== Experiment 4374 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007268867685783247
rmse: 0.08525765470491929
mae: 0.03196311720810237
r2: 0.6722499553213632
pearson: 0.8380594476070944

=== Experiment 4692 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005864078623834592
rmse: 0.07657727224075425
mae: 0.030303724236179195
r2: 0.7355912758296785
pearson: 0.8607955672430128

=== Experiment 4798 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006846957585995356
rmse: 0.08274634484975972
mae: 0.03577917463437306
r2: 0.6912736960239634
pearson: 0.8327627485079948

=== Experiment 4810 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0137570485571589
rmse: 0.11729044529354853
mae: 0.05344044000614304
r2: 0.3797007354978209
pearson: 0.6280593416286022

=== Experiment 4888 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0119630108655235
rmse: 0.10937554966958338
mae: 0.05208883711113345
r2: 0.4605931053972879
pearson: 0.6981900406827203

=== Experiment 4685 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005679919196991342
rmse: 0.07536523865145882
mae: 0.03030210043319365
r2: 0.7438949433312784
pearson: 0.862933610719308

=== Experiment 4823 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006291524084490984
rmse: 0.07931912811227179
mae: 0.032977623282086414
r2: 0.7163179481418163
pearson: 0.8465245384529511

=== Experiment 4261 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006247445970382927
rmse: 0.07904078675204927
mae: 0.029705837126839534
r2: 0.7183054109066869
pearson: 0.8481921660499564

=== Experiment 4604 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005613356044446605
rmse: 0.07492233341565521
mae: 0.034968948370420944
r2: 0.7468962465828366
pearson: 0.8666514094443366

=== Experiment 4017 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008210365985116376
rmse: 0.09061106988175549
mae: 0.041573889305079416
r2: 0.6297982113895237
pearson: 0.8039979805577748

=== Experiment 4859 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0065498542037644505
rmse: 0.08093116954402951
mae: 0.029193519761595864
r2: 0.7046699567635561
pearson: 0.8460910943991469

=== Experiment 4931 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006793951551081919
rmse: 0.08242543024505192
mae: 0.03595024766201255
r2: 0.6936637148084701
pearson: 0.8365081325934516

=== Experiment 4844 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009157301009915622
rmse: 0.09569378772896191
mae: 0.04220965316974253
r2: 0.5871013278993029
pearson: 0.7688601006096581

=== Experiment 4662 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006585920086957526
rmse: 0.081153681906353
mae: 0.03916452306672221
r2: 0.7030437619641896
pearson: 0.8417025944829629

=== Experiment 4622 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006617936702407367
rmse: 0.08135070191711542
mae: 0.03343004604119952
r2: 0.7016001468651464
pearson: 0.8392367860783214

=== Experiment 4594 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005464634346020964
rmse: 0.07392316515153395
mae: 0.027340819883244435
r2: 0.7536020425074234
pearson: 0.8748428555126662

=== Experiment 4665 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006387301713389762
rmse: 0.07992059630276642
mae: 0.04337560744005576
r2: 0.7119993770097286
pearson: 0.8493206704337509

=== Experiment 4438 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006453260336615684
rmse: 0.08033218742581136
mae: 0.03113194223553199
r2: 0.709025331719082
pearson: 0.8436465184639528

=== Experiment 4500 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006710396316198092
rmse: 0.08191700871124441
mae: 0.043055312079699634
r2: 0.6974311835739107
pearson: 0.8416771828571441

=== Experiment 4758 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.011569812173619331
rmse: 0.10756306138084455
mae: 0.04181835775120752
r2: 0.478322261355268
pearson: 0.696036436849723

=== Experiment 4924 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00623929613596364
rmse: 0.07898921531426704
mae: 0.037107919555897595
r2: 0.7186728833536362
pearson: 0.8518223670399176

=== Experiment 4710 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006510748187570403
rmse: 0.08068920737973824
mae: 0.03260097198731424
r2: 0.7064332298218718
pearson: 0.8618561383135976

=== Experiment 4764 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005969515281759674
rmse: 0.07726263833030603
mae: 0.03300839346283236
r2: 0.7308371833300584
pearson: 0.8551450467357296

=== Experiment 4845 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0066629172952124855
rmse: 0.08162669474634193
mae: 0.04286476322220453
r2: 0.6995719917330365
pearson: 0.8436095793033043

=== Experiment 4735 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.015727316022318475
rmse: 0.12540859628557555
mae: 0.04893281812255916
r2: 0.290862242674804
pearson: 0.621889800880863

=== Experiment 4174 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005657597794532259
rmse: 0.07521700468997858
mae: 0.031909128219550754
r2: 0.7449014055437577
pearson: 0.8633601653924257

=== Experiment 4611 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007547166074777159
rmse: 0.08687442704718783
mae: 0.04090680331773771
r2: 0.6597016034501157
pearson: 0.8160299160576956

=== Experiment 4320 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009527229297351886
rmse: 0.09760752684784041
mae: 0.04450541623084707
r2: 0.5704214242366923
pearson: 0.7807907826028877

=== Experiment 4134 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006455385558742725
rmse: 0.08034541404923323
mae: 0.02987052905105537
r2: 0.7089295063887554
pearson: 0.8511307713191238

=== Experiment 4964 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007381869018305961
rmse: 0.08591780384941157
mae: 0.04839617825722977
r2: 0.6671547749736041
pearson: 0.8299461685582248

=== Experiment 4106 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006716588560264582
rmse: 0.08195479583443901
mae: 0.03598478470821775
r2: 0.6971519780143677
pearson: 0.8350772196977663

=== Experiment 4646 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005920620206594826
rmse: 0.07694556651682295
mae: 0.038216016901119726
r2: 0.7330418407488732
pearson: 0.8627900154555574

=== Experiment 4477 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006863468251421238
rmse: 0.082846051513764
mae: 0.03259813322986435
r2: 0.6905292373868099
pearson: 0.8503456444621481

=== Experiment 4499 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.012033873335069348
rmse: 0.10969901246168695
mae: 0.0690772290484001
r2: 0.45739794783442367
pearson: 0.7109709833936823

=== Experiment 4649 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007660269073317456
rmse: 0.08752296312007185
mae: 0.03865406397077262
r2: 0.6546018390263705
pearson: 0.8142538386637077

=== Experiment 4768 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008045254780026103
rmse: 0.08969534424944309
mae: 0.03927667996433347
r2: 0.6372430029560453
pearson: 0.7991925663840873

=== Experiment 4636 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007105568372139384
rmse: 0.08429453346534035
mae: 0.03213146665476279
r2: 0.6796130495000405
pearson: 0.8324603882168234

=== Experiment 4395 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00621806523734799
rmse: 0.07885470967131887
mae: 0.0300931306893574
r2: 0.7196301752277827
pearson: 0.8646587560559293

=== Experiment 4847 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006638346376523083
rmse: 0.08147604787005248
mae: 0.03537217523724337
r2: 0.7006798836422532
pearson: 0.8389087681409998

=== Experiment 4510 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005858093680017624
rmse: 0.07653818445728658
mae: 0.033184300641087364
r2: 0.7358611343122103
pearson: 0.8593510086351684

=== Experiment 4603 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059671264724785456
rmse: 0.07724717776384162
mae: 0.029660003364216415
r2: 0.7309448936891493
pearson: 0.8568113258022546

=== Experiment 4784 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006852236980365058
rmse: 0.08277823977571072
mae: 0.04692086253260743
r2: 0.6910356504554727
pearson: 0.8427151936902542

=== Experiment 4759 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006846942329352574
rmse: 0.08274625266048351
mae: 0.032584942723405105
r2: 0.6912743839392714
pearson: 0.852518526701253

=== Experiment 4616 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006206487430580984
rmse: 0.07878126319488019
mae: 0.036804946840090697
r2: 0.7201522134391245
pearson: 0.856428790527628

=== Experiment 4684 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005862552755979073
rmse: 0.07656730866354826
mae: 0.03176614778372124
r2: 0.735660076539698
pearson: 0.8596593152198279

=== Experiment 4540 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.01197468270401932
rmse: 0.10942889336925289
mae: 0.043557011540288154
r2: 0.46006682733668025
pearson: 0.7056900877969964

=== Experiment 4356 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005526791150131634
rmse: 0.07434239133987845
mae: 0.028002544664094322
r2: 0.750799419567375
pearson: 0.8675505487732553

=== Experiment 4876 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0061003176359806
rmse: 0.0781045301885915
mae: 0.03711977404374805
r2: 0.7249393627487557
pearson: 0.857751407431598

=== Experiment 4564 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006494395864267727
rmse: 0.08058781461404527
mae: 0.03376928839110764
r2: 0.7071705488823815
pearson: 0.8495066922230345

=== Experiment 4530 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005501077997762793
rmse: 0.07416925237430126
mae: 0.02674421652122296
r2: 0.7519588142904621
pearson: 0.8760442187668867

=== Experiment 4841 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006461440524326036
rmse: 0.08038308605873525
mae: 0.03620358718501528
r2: 0.7086564906555981
pearson: 0.8424987666085303

=== Experiment 4523 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007840029861042281
rmse: 0.08854394310760212
mae: 0.03628855328602734
r2: 0.6464965042266315
pearson: 0.8064281866833106

=== Experiment 4817 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008183146211927245
rmse: 0.09046074403810332
mae: 0.03831638827132165
r2: 0.6310255389822833
pearson: 0.8041456605558294

=== Experiment 4526 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006107536844033784
rmse: 0.07815073156429045
mae: 0.032861373943800835
r2: 0.7246138518350541
pearson: 0.8512885689870449

=== Experiment 4457 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005404062010037885
rmse: 0.07351232556543076
mae: 0.028345553299169643
r2: 0.7563332224769767
pearson: 0.8731140358135099

=== Experiment 4726 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006183274761328313
rmse: 0.0786338016461643
mae: 0.02969887694141923
r2: 0.7211988624790504
pearson: 0.8493650863432816

=== Experiment 4667 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0056574623702946416
rmse: 0.07521610446104372
mae: 0.028343396533584652
r2: 0.7449075117630273
pearson: 0.8700093640914868

=== Experiment 4914 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006089593655097707
rmse: 0.07803584852551874
mae: 0.03351312107206062
r2: 0.7254229023267804
pearson: 0.8705349587501711

=== Experiment 4434 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00618298151076229
rmse: 0.07863193696432952
mae: 0.03383333182954232
r2: 0.7212120850180029
pearson: 0.8585096826630446

=== Experiment 4786 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006538496470821681
rmse: 0.08086097000915633
mae: 0.03705447392809626
r2: 0.7051820719430251
pearson: 0.8430695439162179

=== Experiment 4631 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005839616887256289
rmse: 0.07641738602737135
mae: 0.031867566738515006
r2: 0.7366942447655604
pearson: 0.860525068080775

=== Experiment 4881 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0069401831003757745
rmse: 0.0833077613453619
mae: 0.03375515301603689
r2: 0.687070198612237
pearson: 0.848129046819722

=== Experiment 4766 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006056522720976893
rmse: 0.07782366427364425
mae: 0.03450198104628255
r2: 0.7269140561906566
pearson: 0.8554855775760646

=== Experiment 4626 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00632737036340324
rmse: 0.07954476955402687
mae: 0.03300150180269881
r2: 0.7147016552028165
pearson: 0.8476870630104744

=== Experiment 4606 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007207190368492602
rmse: 0.08489517282209043
mae: 0.03145881339407536
r2: 0.6750309584117351
pearson: 0.8554771190922197

=== Experiment 4569 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006884508335014362
rmse: 0.08297293736522988
mae: 0.03439699844745975
r2: 0.6895805492780449
pearson: 0.8327491235063621

=== Experiment 4644 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005530568200784672
rmse: 0.0743677900759776
mae: 0.031688363750078476
r2: 0.7506291140158367
pearson: 0.872003918518448

=== Experiment 4832 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005339831245919262
rmse: 0.07307414895788567
mae: 0.03099577886549054
r2: 0.7592293593609643
pearson: 0.8730577977575754

=== Experiment 4820 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0069087817553837244
rmse: 0.08311908177659162
mae: 0.037107317556808114
r2: 0.6884860714371397
pearson: 0.8360105548718855

=== Experiment 4872 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006702999518315212
rmse: 0.08187184814278478
mae: 0.0325708778681186
r2: 0.6977647019348701
pearson: 0.8593360384821807

=== Experiment 4460 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006942713099828597
rmse: 0.08332294461808583
mae: 0.04048172637268406
r2: 0.6869561220504472
pearson: 0.8394237349665573

=== Experiment 4725 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00664492057909603
rmse: 0.08151638227433815
mae: 0.030344314485142344
r2: 0.700383455741767
pearson: 0.8582524563851464

=== Experiment 4727 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00552898782639585
rmse: 0.0743571639211438
mae: 0.032318413567432515
r2: 0.7507003724014526
pearson: 0.8670036179963047

=== Experiment 4113 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007142289894290961
rmse: 0.08451206951844785
mae: 0.042544687806377386
r2: 0.6779572922286038
pearson: 0.8274853437133045

=== Experiment 4807 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006061766319728894
rmse: 0.07785734595867556
mae: 0.03019211727084176
r2: 0.7266776246308132
pearson: 0.8525730872558533

=== Experiment 4215 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006117439598546559
rmse: 0.07821406266488501
mae: 0.03753072650020747
r2: 0.7241673409926086
pearson: 0.8531293317815918

=== Experiment 4699 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006819769085169196
rmse: 0.08258189320407468
mae: 0.03889794801318977
r2: 0.6924996135596406
pearson: 0.8379878083229866

=== Experiment 4747 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006116777477308966
rmse: 0.07820982979976984
mae: 0.029339263455243476
r2: 0.7241971957477898
pearson: 0.8521442422934733

=== Experiment 4357 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005899561216546541
rmse: 0.07680860118858136
mae: 0.03345020385809357
r2: 0.7339913813413801
pearson: 0.8597645303741318

=== Experiment 4996 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0050656545226198425
rmse: 0.07117341162695408
mae: 0.026644658486009516
r2: 0.7715918671401305
pearson: 0.8794551885927154

=== Experiment 4672 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006633160766227988
rmse: 0.08144421874036234
mae: 0.029662949210504344
r2: 0.7009137005280974
pearson: 0.8389811053957907

=== Experiment 4664 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006316013473931497
rmse: 0.07947335071539073
mae: 0.029995884568901603
r2: 0.7152137323505481
pearson: 0.8526910722848717

=== Experiment 4775 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00717218946978204
rmse: 0.0846887800702197
mae: 0.03221985878176501
r2: 0.6766091335295208
pearson: 0.8360058161433873

=== Experiment 4318 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007738217864776759
rmse: 0.08796714082415524
mae: 0.038664137052177715
r2: 0.6510871623273574
pearson: 0.8128336981620726

=== Experiment 4056 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009814592232933956
rmse: 0.0990686238570717
mae: 0.045947101754600525
r2: 0.557464355949397
pearson: 0.7723158944974688

=== Experiment 4952 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006857522529067159
rmse: 0.08281015957638989
mae: 0.03825895610111847
r2: 0.690797327390843
pearson: 0.8323726554846201

=== Experiment 4225 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006086284801502707
rmse: 0.07801464478867226
mae: 0.029959108865448707
r2: 0.725572097079041
pearson: 0.8716635641159204

=== Experiment 4422 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010291119381523427
rmse: 0.10144515454926088
mae: 0.046642915542466704
r2: 0.535977956555135
pearson: 0.7453938847110825

=== Experiment 4658 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006229510140824254
rmse: 0.0789272458712722
mae: 0.030262911313210423
r2: 0.7191141295673252
pearson: 0.8496093051866589

=== Experiment 4831 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00870643360613238
rmse: 0.0933082719062591
mae: 0.03815303299399967
r2: 0.6074307406939692
pearson: 0.8248905397611352

=== Experiment 4030 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006702257039808449
rmse: 0.08186731362276674
mae: 0.0323501191002093
r2: 0.6977981799639501
pearson: 0.85681242197985

=== Experiment 4705 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006022044620672863
rmse: 0.0776018338744186
mae: 0.030636623027918475
r2: 0.728468658558393
pearson: 0.8555867506257838

=== Experiment 4135 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00643246662435644
rmse: 0.08020265970874307
mae: 0.03143198899871388
r2: 0.7099629110528385
pearson: 0.8430624068393463

=== Experiment 4178 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006770270196120707
rmse: 0.0822816516370491
mae: 0.032330401805751696
r2: 0.6947314966807099
pearson: 0.8337593124057695

=== Experiment 4507 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006326934576749795
rmse: 0.07954203025287823
mae: 0.03233398206178681
r2: 0.714721304631216
pearson: 0.8702673528798496

=== Experiment 4425 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009630734413638719
rmse: 0.09813630527811162
mae: 0.04032886303708169
r2: 0.5657544240994046
pearson: 0.7908519128365505

=== Experiment 4403 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005509924250752106
rmse: 0.07422886400014557
mae: 0.030680026584487197
r2: 0.7515599406367078
pearson: 0.8697194698250199

=== Experiment 4803 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00620842749375678
rmse: 0.07879357520608378
mae: 0.032216387029241296
r2: 0.7200647368444151
pearson: 0.8502376433540729

=== Experiment 4181 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006426285614372827
rmse: 0.08016411675040665
mae: 0.03050851168683577
r2: 0.7102416100725292
pearson: 0.8663983681443487

=== Experiment 4982 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005969457555207432
rmse: 0.07726226475587829
mae: 0.0315522006997892
r2: 0.7308397861948918
pearson: 0.8558603660154251

=== Experiment 4905 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0056962781147742135
rmse: 0.07547369154065682
mae: 0.03196039558228117
r2: 0.743157326928559
pearson: 0.8629244798606562

=== Experiment 4467 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007046867132190984
rmse: 0.08394562008938276
mae: 0.039923872992774875
r2: 0.6822598625729224
pearson: 0.826523433484894

=== Experiment 4459 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006221827459733882
rmse: 0.07887856147099719
mae: 0.028710484172321638
r2: 0.7194605382762835
pearson: 0.8500444217449284

=== Experiment 4848 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008415243626125418
rmse: 0.0917346370032902
mae: 0.03557078482283743
r2: 0.6205603687299717
pearson: 0.7882685720924285

=== Experiment 4167 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005620238196840323
rmse: 0.07496824792430674
mae: 0.0281958366000181
r2: 0.7465859333604707
pearson: 0.8674151134267192

=== Experiment 4331 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0071313760274265385
rmse: 0.084447474961816
mae: 0.037167459705545525
r2: 0.6784493936819608
pearson: 0.825896438137263

=== Experiment 4973 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006991945636764266
rmse: 0.083617854772556
mae: 0.03699669283685717
r2: 0.6847362486289145
pearson: 0.8340566120115241

=== Experiment 4829 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006092905037751015
rmse: 0.07805706270255763
mae: 0.02974002326228624
r2: 0.7252735935403938
pearson: 0.8602288708241805

=== Experiment 4336 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00685549887440501
rmse: 0.0827979400372075
mae: 0.034149105269698235
r2: 0.6908885730888805
pearson: 0.8378072575602058

=== Experiment 4512 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008818781408159957
rmse: 0.09390836708281088
mae: 0.05026095056408828
r2: 0.6023650277485949
pearson: 0.7821824082157263

=== Experiment 4470 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0062263794392953715
rmse: 0.07890741054739644
mae: 0.031064887479338648
r2: 0.7192552915213353
pearson: 0.8483290571216187

=== Experiment 4677 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005662658565267807
rmse: 0.07525063830472009
mae: 0.03231212037305181
r2: 0.7446732176187074
pearson: 0.8638037904183256

=== Experiment 4854 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006161445709613727
rmse: 0.07849487696412885
mae: 0.028075517413188672
r2: 0.7221831248131607
pearson: 0.8570406261954215

=== Experiment 4251 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005876720955359934
rmse: 0.07665977403671324
mae: 0.03147450959596851
r2: 0.7350212386655164
pearson: 0.8598418090302752

=== Experiment 4245 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006145105231714476
rmse: 0.07839072159200014
mae: 0.03047260759053894
r2: 0.7229199097696435
pearson: 0.8530547747247081

=== Experiment 4930 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007793384565830264
rmse: 0.08828014819782681
mae: 0.04095958889883316
r2: 0.6485997200575768
pearson: 0.8098133268739953

=== Experiment 4792 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006416147037865855
rmse: 0.08010085541282225
mae: 0.033814926693155514
r2: 0.7106987540248997
pearson: 0.8484372799557192

=== Experiment 4576 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005649402178777885
rmse: 0.07516250513905111
mae: 0.027642180121748787
r2: 0.745270942250956
pearson: 0.8639788492002201

=== Experiment 4029 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008937192571753839
rmse: 0.0945367260473613
mae: 0.04376954079677572
r2: 0.5970259204988855
pearson: 0.7728451590255265

=== Experiment 4303 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006152744177103864
rmse: 0.07843942998966695
mae: 0.02825106557051792
r2: 0.7225754730841931
pearson: 0.8728479928117037

=== Experiment 4411 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005732096965574506
rmse: 0.07571061329545882
mae: 0.030958975175023808
r2: 0.7415422707110555
pearson: 0.8612540312519376

=== Experiment 4566 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057847142780817215
rmse: 0.07605730916934757
mae: 0.030864240211471966
r2: 0.7391697792487556
pearson: 0.8683672808049309

=== Experiment 4166 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005991668377082516
rmse: 0.07740586784658199
mae: 0.028609548096275028
r2: 0.7298383100122745
pearson: 0.8815014061275263

=== Experiment 4271 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005888416587336677
rmse: 0.07673601883950376
mae: 0.029715382429745196
r2: 0.7344938877673264
pearson: 0.8578461358038802

=== Experiment 4879 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0050699558181945764
rmse: 0.07120362222664361
mae: 0.029143458524872143
r2: 0.7713979236158108
pearson: 0.8791085490876183

=== Experiment 4617 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006359692462221556
rmse: 0.07974767998018222
mae: 0.03137135722906527
r2: 0.7132442659931423
pearson: 0.8448235717161494

=== Experiment 4635 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008921755241283932
rmse: 0.09445504349310276
mae: 0.0433735264339563
r2: 0.5977219829352847
pearson: 0.7761241460139899

=== Experiment 4870 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006566357542353392
rmse: 0.0810330645005691
mae: 0.04561384235933662
r2: 0.7039258284902556
pearson: 0.8500349217064722

=== Experiment 4472 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0077958786835214365
rmse: 0.08829427322041582
mae: 0.03888220208498511
r2: 0.648487261388627
pearson: 0.8330970518809666

=== Experiment 4493 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006172581146175139
rmse: 0.07856577592167685
mae: 0.029213641483269315
r2: 0.721681032879687
pearson: 0.8601277310954513

=== Experiment 4849 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009227497356209674
rmse: 0.09605986339887057
mae: 0.0427793267850378
r2: 0.5839362055406787
pearson: 0.8019093456366345

=== Experiment 4774 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0061440570219962794
rmse: 0.07838403550466307
mae: 0.03150712732004529
r2: 0.7229671730844848
pearson: 0.8515618944779673

=== Experiment 4634 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005933608698732692
rmse: 0.07702992080180722
mae: 0.029506644412000887
r2: 0.7324561953550495
pearson: 0.8583154165277344

=== Experiment 4481 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006985916481329045
rmse: 0.08358179515498004
mae: 0.03627289938471915
r2: 0.6850081005938544
pearson: 0.8339539505713369

=== Experiment 4843 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006803936727024406
rmse: 0.08248597897233448
mae: 0.03527694735966059
r2: 0.6932134876200355
pearson: 0.8328086426258675

=== Experiment 4976 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006224908683021202
rmse: 0.0788980904903357
mae: 0.032907449860179606
r2: 0.7193216072744084
pearson: 0.8483262284505436

=== Experiment 4795 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00579023096745551
rmse: 0.07609356718839977
mae: 0.033482246339895286
r2: 0.7389210341529722
pearson: 0.8604763308602291

=== Experiment 4801 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0064195617963747195
rmse: 0.08012216794604798
mae: 0.029708171975688497
r2: 0.7105447840666852
pearson: 0.8531196549978415

=== Experiment 4707 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005230172517529538
rmse: 0.07231993167536553
mae: 0.027805454605625404
r2: 0.7641738231595594
pearson: 0.8743210325041932

=== Experiment 4734 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.010595852984435336
rmse: 0.10293615975173805
mae: 0.04609272968250921
r2: 0.5222376525233529
pearson: 0.7373729771026767

=== Experiment 4744 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006990091013806789
rmse: 0.08360676416299574
mae: 0.03995746589260709
r2: 0.6848198727617851
pearson: 0.8284472013937135

=== Experiment 4431 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005878478003051505
rmse: 0.07667123321723412
mae: 0.029678024573217283
r2: 0.7349420141584393
pearson: 0.8581273827661529

=== Experiment 4836 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005833839696243863
rmse: 0.0763795764340433
mae: 0.03005263748363248
r2: 0.7369547357655746
pearson: 0.8592163078946186

=== Experiment 4641 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006153474191574457
rmse: 0.07844408321584527
mae: 0.030079638399298628
r2: 0.722542557053018
pearson: 0.8516716980995379

=== Experiment 4596 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007841059244886331
rmse: 0.08854975575847926
mae: 0.0353887744823226
r2: 0.6464500897621697
pearson: 0.8347996936740621

=== Experiment 4490 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005421278559596084
rmse: 0.073629332195777
mae: 0.030816508583514427
r2: 0.7555569358349803
pearson: 0.8693517127826937

=== Experiment 4666 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0054025732283945645
rmse: 0.07350219879972683
mae: 0.0293434670733255
r2: 0.7564003509860111
pearson: 0.8697248693745824

=== Experiment 4731 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007930829723682248
rmse: 0.08905520604480262
mae: 0.04223048219326141
r2: 0.642402378384271
pearson: 0.815440557983147

=== Experiment 4690 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005898530535102412
rmse: 0.07680189148128067
mae: 0.03203728972639202
r2: 0.7340378543140582
pearson: 0.8568995650549251

=== Experiment 4796 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005862872424269881
rmse: 0.07656939613363736
mae: 0.030207123736843623
r2: 0.7356456628371622
pearson: 0.8607594336176967

=== Experiment 4625 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007169025209659314
rmse: 0.08467009631303908
mae: 0.03187700181896103
r2: 0.6767518086257571
pearson: 0.8471750630615072

=== Experiment 4985 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008007595821078633
rmse: 0.0894851709563022
mae: 0.0397242814799615
r2: 0.6389410288400114
pearson: 0.8094529423971198

=== Experiment 4348 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006094676374594947
rmse: 0.07806840830063687
mae: 0.029259712577613436
r2: 0.7251937247417258
pearson: 0.8564174057313102

=== Experiment 4698 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006167228225925207
rmse: 0.0785317020439848
mae: 0.031997310410200305
r2: 0.7219223936977561
pearson: 0.850296918677317

=== Experiment 4548 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005844689886511696
rmse: 0.07645057152508211
mae: 0.033794553980289486
r2: 0.7364655054619319
pearson: 0.8590664360335791

=== Experiment 4343 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00636179288435564
rmse: 0.07976084806692843
mae: 0.030594468638054265
r2: 0.713149558883583
pearson: 0.847689400981747

=== Experiment 4842 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01636618980069474
rmse: 0.12793040999189653
mae: 0.0546520195687528
r2: 0.26205570519767174
pearson: 0.6657728673407616

=== Experiment 4802 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006887296043951371
rmse: 0.08298973456971273
mae: 0.034270586024434016
r2: 0.6894548527090396
pearson: 0.8364678746300502

=== Experiment 4694 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005914902364456185
rmse: 0.07690840243078896
mae: 0.02785865033323718
r2: 0.7332996557342897
pearson: 0.8569526921396506

=== Experiment 4822 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0072473123535345026
rmse: 0.08513114796321322
mae: 0.03670561454281842
r2: 0.673221875765232
pearson: 0.8276614917422387

=== Experiment 4851 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005598464455721654
rmse: 0.07482288724529182
mae: 0.03431505901410205
r2: 0.7475677017641552
pearson: 0.8666525598630183

=== Experiment 4860 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005696889348998142
rmse: 0.07547774075181465
mae: 0.02949267710456987
r2: 0.7431297666464267
pearson: 0.862099596121745

=== Experiment 4821 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005733975808908099
rmse: 0.07572302033667239
mae: 0.030330039360817462
r2: 0.7414575544920861
pearson: 0.8617655992491317

=== Experiment 4198 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006226536962692264
rmse: 0.07890840869446211
mae: 0.030669045210221776
r2: 0.719248188860701
pearson: 0.8610005280467501

=== Experiment 4833 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.010994464361202641
rmse: 0.1048544913735346
mae: 0.0419654008959384
r2: 0.5042644409871988
pearson: 0.7110680864086263

=== Experiment 4612 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00945074540429321
rmse: 0.0972149443465006
mae: 0.04257149771465492
r2: 0.5738700493115723
pearson: 0.7858283250604587

=== Experiment 4850 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006554375913452258
rmse: 0.08095910025100488
mae: 0.03329759321387318
r2: 0.7044660748638898
pearson: 0.8397384625265719

=== Experiment 4754 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0064726810569571995
rmse: 0.08045297419584437
mae: 0.03337157403155398
r2: 0.7081496599865957
pearson: 0.8465874903157276

=== Experiment 4686 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007853578634469317
rmse: 0.08862041883487866
mae: 0.033034191651842446
r2: 0.6458855959960776
pearson: 0.8154247201401342

=== Experiment 4789 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007264383562931568
rmse: 0.08523135316848822
mae: 0.0348215677926496
r2: 0.6724521424471321
pearson: 0.822539820233412

=== Experiment 4292 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005873158367821207
rmse: 0.07663653415846261
mae: 0.030461020120126032
r2: 0.735181874169623
pearson: 0.8574433810615794

=== Experiment 4816 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006673347210654866
rmse: 0.08169055765910076
mae: 0.03779047156496334
r2: 0.6991017114362964
pearson: 0.838162421193563

=== Experiment 4887 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.010757860017482445
rmse: 0.10372010421071917
mae: 0.04411768186591369
r2: 0.5149328267080068
pearson: 0.7705299621938538

=== Experiment 4787 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00629236295447125
rmse: 0.07932441587853799
mae: 0.033243749235543865
r2: 0.7162801238636209
pearson: 0.8528821515608317

=== Experiment 4773 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0057551389082345
rmse: 0.07586263183039789
mae: 0.031040980206777036
r2: 0.7405033196580508
pearson: 0.8618232839536542

=== Experiment 4941 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.011054633721382826
rmse: 0.10514101826301106
mae: 0.05138852582093301
r2: 0.5015514310192348
pearson: 0.7096157570435077

=== Experiment 4778 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006530193999414047
rmse: 0.08080961576083658
mae: 0.03495425679521911
r2: 0.7055564267245982
pearson: 0.8480209035615331

=== Experiment 4556 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006210917862660559
rmse: 0.07880937674325664
mae: 0.034143650887350385
r2: 0.7199524472066388
pearson: 0.848630340892946

=== Experiment 4926 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006877399183646682
rmse: 0.08293008611864985
mae: 0.03353426994742739
r2: 0.6899010977842368
pearson: 0.8420644173864908

=== Experiment 4770 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.012647849882008661
rmse: 0.11246265994546217
mae: 0.04777173685427997
r2: 0.4297140155646739
pearson: 0.6572078969982208

=== Experiment 4967 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006530408514679014
rmse: 0.08081094303792657
mae: 0.034015531384179315
r2: 0.7055467543257163
pearson: 0.8405078038536835

=== Experiment 4436 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005290279154069865
rmse: 0.07273430520785817
mae: 0.02732394518980254
r2: 0.7614636413729099
pearson: 0.8726988109039945

=== Experiment 4295 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005616451086933657
rmse: 0.07494298557526019
mae: 0.03031842603908969
r2: 0.7467566924793276
pearson: 0.8641967817341726

=== Experiment 4953 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006864548548805604
rmse: 0.08285257116593066
mae: 0.03558358478376491
r2: 0.6904805272532242
pearson: 0.8355405658100642

=== Experiment 4582 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005711175775289206
rmse: 0.07557232148934692
mae: 0.028464737065453282
r2: 0.7424855979728998
pearson: 0.8653223908393498

=== Experiment 4443 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006624176459125419
rmse: 0.08138904385189336
mae: 0.0368890168875544
r2: 0.7013187989810563
pearson: 0.8380420580319371

=== Experiment 4891 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005485926869856767
rmse: 0.0740670430748843
mae: 0.03146715133930962
r2: 0.7526419719792236
pearson: 0.8682157285398172

=== Experiment 4633 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006364311226814903
rmse: 0.07977663333843377
mae: 0.030284222453371847
r2: 0.713036007930502
pearson: 0.8607144184185659

=== Experiment 4791 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007248686967696564
rmse: 0.08513922108932266
mae: 0.03535665979311623
r2: 0.6731598950176785
pearson: 0.8293481716125105

=== Experiment 4748 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005724044720483496
rmse: 0.07565741682401994
mae: 0.028122260728678936
r2: 0.7419053428981451
pearson: 0.866740315501967

=== Experiment 4656 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00686374735951274
rmse: 0.08284773599509367
mae: 0.030900037820562547
r2: 0.6905166525258088
pearson: 0.8430158821246225

=== Experiment 4724 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005565581037191091
rmse: 0.07460282191171519
mae: 0.03164374059668264
r2: 0.7490504006325991
pearson: 0.8708310277551791

=== Experiment 4968 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006244958682609554
rmse: 0.07902505098137902
mae: 0.03964773992442259
r2: 0.7184175616176494
pearson: 0.8515549759214396

=== Experiment 4169 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062361817759090344
rmse: 0.07896949902278116
mae: 0.03240524398950097
r2: 0.7188133084777637
pearson: 0.852259005225063

=== Experiment 4910 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006145648304377034
rmse: 0.07839418539902711
mae: 0.03158799178956806
r2: 0.7228954228623135
pearson: 0.8506350518445874

=== Experiment 4750 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0066939545957572615
rmse: 0.0818165912010349
mae: 0.032212424112959805
r2: 0.6981725335120332
pearson: 0.840067229606732

=== Experiment 4488 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006582450935233704
rmse: 0.08113230512708057
mae: 0.04252774719502714
r2: 0.7032001844885264
pearson: 0.8441531919163059

=== Experiment 4283 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005481637351996117
rmse: 0.07403808041809375
mae: 0.029042410374714403
r2: 0.7528353844515989
pearson: 0.8700691801596161

=== Experiment 4717 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005859388045683581
rmse: 0.0765466396759752
mae: 0.03428428993527382
r2: 0.7358027719340263
pearson: 0.8613324256344772

=== Experiment 4979 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005408804820764598
rmse: 0.073544577099638
mae: 0.031190196112984247
r2: 0.7561193712287797
pearson: 0.8721383574413234

=== Experiment 4835 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006516927412421543
rmse: 0.08072748857992265
mae: 0.032395861906634085
r2: 0.7061546112930188
pearson: 0.8609695195012869

=== Experiment 4680 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.01029003101168215
rmse: 0.10143979008102368
mae: 0.04742475691336509
r2: 0.5360270306722503
pearson: 0.7324565851221889

=== Experiment 4421 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006685236697805382
rmse: 0.08176329676453477
mae: 0.035907154276525625
r2: 0.6985656196936431
pearson: 0.8397760996047872

=== Experiment 4325 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.004691918097237407
rmse: 0.0684975773092553
mae: 0.024826075056255916
r2: 0.7884434780666443
pearson: 0.8883201622912243

=== Experiment 4516 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008151774840384398
rmse: 0.09028717982296489
mae: 0.03453218209480404
r2: 0.6324400603175537
pearson: 0.8234545927412745

=== Experiment 4839 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006554543615644283
rmse: 0.08096013596606841
mae: 0.03401052566340506
r2: 0.7044585132458627
pearson: 0.8441175866356263

=== Experiment 4648 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006346547789490104
rmse: 0.0796652232124539
mae: 0.03515728385686044
r2: 0.7138369535011906
pearson: 0.8455831630412212

=== Experiment 4545 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005187852332808484
rmse: 0.07202674734297311
mae: 0.029248753838481575
r2: 0.766082021662094
pearson: 0.8755954629046592

=== Experiment 4856 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005826327273957937
rmse: 0.07633038237790989
mae: 0.03191176013856567
r2: 0.7372934675799772
pearson: 0.859490763830047

=== Experiment 4661 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005554564778776425
rmse: 0.07452895262095413
mae: 0.0293046127459538
r2: 0.74954711887589
pearson: 0.8665816290018902

=== Experiment 4337 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005597648384801059
rmse: 0.07481743369563713
mae: 0.027002716900105868
r2: 0.7476044980427845
pearson: 0.8654117116510349

=== Experiment 4600 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005554060958583182
rmse: 0.07452557251429326
mae: 0.027310428483931986
r2: 0.7495698359066553
pearson: 0.8662965192574633

=== Experiment 4749 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008325068562460647
rmse: 0.09124181367366964
mae: 0.04170789155973722
r2: 0.6246263226615356
pearson: 0.791645043916833

=== Experiment 4185 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006356352893412585
rmse: 0.07972673888610135
mae: 0.03509052095635999
r2: 0.7133948456808823
pearson: 0.8504295259214133

=== Experiment 4769 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00597819040054331
rmse: 0.07731875840016644
mae: 0.031056670178007187
r2: 0.7304460260423162
pearson: 0.8593016741411837

=== Experiment 4339 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.020528602010706304
rmse: 0.1432780583714977
mae: 0.06528932117752652
r2: 0.07437437066596575
pearson: 0.28055388529796926

=== Experiment 4986 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005990582464846199
rmse: 0.07739885312358445
mae: 0.029671424037708997
r2: 0.7298872733170635
pearson: 0.8642563963398272

=== Experiment 4971 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006408627661420485
rmse: 0.08005390472313319
mae: 0.03423606605566267
r2: 0.7110377993992909
pearson: 0.844819688089322

=== Experiment 4797 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007768609299763071
rmse: 0.08813971465669192
mae: 0.03669357974012366
r2: 0.649716825900118
pearson: 0.8305356391699918

=== Experiment 4720 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006448203117361404
rmse: 0.08030070433913643
mae: 0.032751020854066115
r2: 0.7092533595094049
pearson: 0.8459348871460216

=== Experiment 4868 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005983874182297558
rmse: 0.07735550518416616
mae: 0.029426408001943127
r2: 0.7301897468246407
pearson: 0.8546128650742988

=== Experiment 4834 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0052949213691423445
rmse: 0.07276621035303642
mae: 0.027665209964897273
r2: 0.761254325938487
pearson: 0.8782710200994551

=== Experiment 4763 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00559557170660087
rmse: 0.07480355410407229
mae: 0.029300786329767035
r2: 0.747698134548815
pearson: 0.8654209016678965

=== Experiment 4737 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005909280512288586
rmse: 0.07687184473062024
mae: 0.028577033322192122
r2: 0.7335531425741604
pearson: 0.8687410599624338

=== Experiment 4273 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0055982834313652126
rmse: 0.0748216775497931
mae: 0.026975212497870688
r2: 0.7475758640727121
pearson: 0.8656924222406506

=== Experiment 4714 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0061685446474011255
rmse: 0.07854008306209719
mae: 0.03253256927139207
r2: 0.7218630368328083
pearson: 0.849908205347336

=== Experiment 4502 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006719091685465007
rmse: 0.08197006578907332
mae: 0.0347606905109589
r2: 0.6970391132007903
pearson: 0.8411070766222217

=== Experiment 4044 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008471347638580711
rmse: 0.0920399241556658
mae: 0.03252825313167672
r2: 0.618030663501627
pearson: 0.8304213749780852

=== Experiment 4980 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0062043962493780365
rmse: 0.07876799000468424
mae: 0.028670024481163577
r2: 0.7202465038791662
pearson: 0.8495760129252731

=== Experiment 4704 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005582928216250068
rmse: 0.07471899501632813
mae: 0.030342018599975693
r2: 0.7482682239639071
pearson: 0.8654123710639042

=== Experiment 4608 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006102784260096786
rmse: 0.07812031912439162
mae: 0.027980752744725636
r2: 0.7248281437530011
pearson: 0.8527962056064937

=== Experiment 4406 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005377159130872417
rmse: 0.0733291151649358
mae: 0.02688224284050601
r2: 0.757546261457685
pearson: 0.871876081811246

=== Experiment 4536 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.004998495493395813
rmse: 0.07070003885003044
mae: 0.026886352204296558
r2: 0.7746200382088928
pearson: 0.8818628098636331

=== Experiment 4650 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006447864797205989
rmse: 0.08029859772876478
mae: 0.037432540183659026
r2: 0.7092686142163066
pearson: 0.843208534826975

=== Experiment 4865 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008274053798568753
rmse: 0.09096182605119991
mae: 0.03948289737277888
r2: 0.6269265559121064
pearson: 0.8161258847526529

=== Experiment 4716 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006439832054528773
rmse: 0.08024856418982693
mae: 0.0287420862220225
r2: 0.7096308070481419
pearson: 0.859667869581148

=== Experiment 4130 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007761257061730148
rmse: 0.08809799692234863
mae: 0.04124641319381664
r2: 0.6500483350770552
pearson: 0.807285910635063

=== Experiment 4927 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005993174781789637
rmse: 0.07741559779391771
mae: 0.03063923932042774
r2: 0.7297703868870498
pearson: 0.8556899900419741

=== Experiment 4086 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007847717049666596
rmse: 0.08858734136244634
mae: 0.042753709300585134
r2: 0.6461498922749516
pearson: 0.804039326447647

=== Experiment 4473 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00587362449387652
rmse: 0.0766395752459297
mae: 0.02878646570220899
r2: 0.7351608567509467
pearson: 0.8653629317391284

=== Experiment 4587 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005196105905804361
rmse: 0.07208401976724356
mae: 0.02712029083978061
r2: 0.7657098716883852
pearson: 0.8752897357324441

=== Experiment 4614 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005408517758091698
rmse: 0.07354262545008641
mae: 0.026574278180360743
r2: 0.7561323147583549
pearson: 0.8717066009494264

=== Experiment 4659 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00648346893989495
rmse: 0.08051999093327662
mae: 0.031029372630765283
r2: 0.707663239093043
pearson: 0.8416086198860707

=== Experiment 4837 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005921265103264323
rmse: 0.07694975700588223
mae: 0.029865097735802648
r2: 0.7330127626418863
pearson: 0.856287985088288

=== Experiment 4451 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006898131173143264
rmse: 0.08305498885162325
mae: 0.038440909293291976
r2: 0.6889663015026823
pearson: 0.8307629901746553

=== Experiment 4827 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005428175043817819
rmse: 0.07367614976244224
mae: 0.026914954689184693
r2: 0.7552459763967971
pearson: 0.8695407295114828

=== Experiment 4237 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005761002762891931
rmse: 0.0759012698371505
mae: 0.027786724550057037
r2: 0.7402389210324272
pearson: 0.8614752619649094

=== Experiment 4385 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007285753439218672
rmse: 0.08535662504585495
mae: 0.03612281010637245
r2: 0.6714885841309484
pearson: 0.8216569562694032

=== Experiment 4838 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006957685789504709
rmse: 0.0834127435677829
mae: 0.03519490382245579
r2: 0.686281010062936
pearson: 0.8301806875526394

=== Experiment 4893 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006255199498457351
rmse: 0.07908981918336488
mae: 0.030084229164496813
r2: 0.7179558077383998
pearson: 0.8512104146192555

=== Experiment 4676 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005652690413753198
rmse: 0.07518437612797753
mae: 0.028220378664115538
r2: 0.7451226771831821
pearson: 0.8792562747344262

=== Experiment 4103 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005868452513316745
rmse: 0.07660582558341594
mae: 0.028151511246616353
r2: 0.7353940590780241
pearson: 0.8602082193256165

=== Experiment 4327 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007229323741509634
rmse: 0.08502542996956637
mae: 0.03841532861220741
r2: 0.6740329743640395
pearson: 0.8220396885076681

=== Experiment 4412 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00557265223289738
rmse: 0.07465019914841071
mae: 0.029451661824192066
r2: 0.7487315635304737
pearson: 0.8660322132205001

=== Experiment 4476 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005711822820033716
rmse: 0.07557660233189711
mae: 0.03137253757954569
r2: 0.7424564230101558
pearson: 0.8616649760414281

=== Experiment 4780 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00549442180138286
rmse: 0.07412436712298365
mae: 0.02833781026786688
r2: 0.7522589392556173
pearson: 0.8691914129255155

=== Experiment 4444 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005985339414502162
rmse: 0.07736497537324084
mae: 0.030043989936856915
r2: 0.7301236801494332
pearson: 0.8546925774974782

=== Experiment 4367 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0083582945538891
rmse: 0.09142370892656401
mae: 0.03900358979226686
r2: 0.6231281773319038
pearson: 0.7938222460869349

=== Experiment 4936 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005574525926733848
rmse: 0.07466274791844892
mae: 0.03265703158717197
r2: 0.748647079500063
pearson: 0.8653943175004816

=== Experiment 4736 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006663259165423166
rmse: 0.08162878882736878
mae: 0.037531219801431544
r2: 0.6995565769557054
pearson: 0.8375570190398847

=== Experiment 4119 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0063282800844994755
rmse: 0.07955048764463657
mae: 0.03592973850243629
r2: 0.7146606362789862
pearson: 0.8462400651782896

=== Experiment 4722 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006082127901396107
rmse: 0.07798799844460753
mae: 0.02955133859913557
r2: 0.7257595298752562
pearson: 0.8543999486779005

=== Experiment 4521 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00564967484275842
rmse: 0.07516431894694729
mae: 0.03148240763485787
r2: 0.7452586479520824
pearson: 0.8651694418837564

=== Experiment 5001 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007249780127773068
rmse: 0.0851456406856691
mae: 0.036480123420810785
r2: 0.6731106049109655
pearson: 0.8258472013654358

=== Experiment 5002 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007083552264692753
rmse: 0.08416384178905305
mae: 0.03743483645268205
r2: 0.6806057460947794
pearson: 0.8269930799733806

=== Experiment 5004 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006749744085182225
rmse: 0.08215682616302936
mae: 0.04066116265218578
r2: 0.6956570099886936
pearson: 0.8377753062730325

=== Experiment 5009 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006755845491755224
rmse: 0.082193950457167
mae: 0.03629381493369892
r2: 0.6953819002517515
pearson: 0.8349760771987291

=== Experiment 5005 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006823887359090977
rmse: 0.08260682392569622
mae: 0.03757347328058991
r2: 0.6923139224010983
pearson: 0.8405854040334754

=== Experiment 5010 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0075317420254672396
rmse: 0.08678560955289327
mae: 0.04495524356201184
r2: 0.6603970670448569
pearson: 0.8271529834476035

=== Experiment 5003 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0064093993831217935
rmse: 0.08005872459090135
mae: 0.034027744589093176
r2: 0.7110030028074396
pearson: 0.8434251030287654

=== Experiment 5016 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007550385875010404
rmse: 0.08689295641771204
mae: 0.037071275636719955
r2: 0.6595564240747414
pearson: 0.8184009021297344

=== Experiment 5008 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0074469869251954
rmse: 0.08629592646930329
mae: 0.04677313018309353
r2: 0.6642186372125418
pearson: 0.8231162761520051

=== Experiment 5019 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007109962979377331
rmse: 0.0843205964126045
mae: 0.04494399190744591
r2: 0.6794148985938963
pearson: 0.8324633237890952

=== Experiment 5012 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006456577545642987
rmse: 0.08035283159692001
mae: 0.04227493961709723
r2: 0.7088757602240563
pearson: 0.8489755044087582

=== Experiment 5048 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007397621537654696
rmse: 0.08600942702782466
mae: 0.03671665821276747
r2: 0.6664445008093836
pearson: 0.8347440616264216

=== Experiment 5032 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006890758575080217
rmse: 0.08301059314979153
mae: 0.034495079901857954
r2: 0.6892987287044166
pearson: 0.8378032972073383

=== Experiment 5050 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006428852305726227
rmse: 0.08018012413139697
mae: 0.03353976048951423
r2: 0.7101258790890916
pearson: 0.8451123496541776

=== Experiment 5056 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007694314050412545
rmse: 0.08771723918599209
mae: 0.039956665268853005
r2: 0.6530667659934933
pearson: 0.8139427349861026

=== Experiment 5013 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006039815856328364
rmse: 0.07771625220202248
mae: 0.03497604368259793
r2: 0.7276673613644054
pearson: 0.8537794496405124

=== Experiment 5057 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006851450530714005
rmse: 0.0827734892988933
mae: 0.03856633669865213
r2: 0.6910711111240957
pearson: 0.8325367283945276

=== Experiment 5027 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007254231433256898
rmse: 0.08517177603676525
mae: 0.04484040962999885
r2: 0.672909897505869
pearson: 0.8292187908337988

=== Experiment 5044 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006475559072450162
rmse: 0.08047085852934689
mae: 0.0320840591136726
r2: 0.7080198915347296
pearson: 0.8488852209194125

=== Experiment 5042 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007737216655226409
rmse: 0.08796144982449078
mae: 0.04960636026303319
r2: 0.6511323064253167
pearson: 0.8185629820052199

=== Experiment 5061 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006464047652084576
rmse: 0.08039930131589812
mae: 0.041203333207923964
r2: 0.7085389364124502
pearson: 0.8506697815131139

=== Experiment 5068 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00663974161104419
rmse: 0.08148460965755552
mae: 0.03716999820981863
r2: 0.7006169731317862
pearson: 0.8410695128286975

=== Experiment 5018 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.008908766814961049
rmse: 0.09438626391038607
mae: 0.039742495445346145
r2: 0.59830762536154
pearson: 0.7765856631114184

=== Experiment 5079 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006705022507308959
rmse: 0.08188420181762145
mae: 0.033613857079046926
r2: 0.6976734862515275
pearson: 0.8459627300079262

=== Experiment 5049 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006661701548767549
rmse: 0.08161924741608163
mae: 0.0388003092188063
r2: 0.6996268092051476
pearson: 0.8375587208349514

=== Experiment 5022 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008966354819564796
rmse: 0.09469083809727737
mae: 0.04588725981851402
r2: 0.5957110075803762
pearson: 0.7733509556296245

=== Experiment 5083 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0076503931998202325
rmse: 0.0874665261675587
mae: 0.03760448531717326
r2: 0.655047137815656
pearson: 0.8117623688820643

=== Experiment 5070 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0072502356596042955
rmse: 0.08514831565923248
mae: 0.03899405709050531
r2: 0.6730900651811904
pearson: 0.8246792176818684

=== Experiment 5053 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00811160852196362
rmse: 0.090064468698614
mae: 0.03886875226944346
r2: 0.6342511419365979
pearson: 0.8079980146691341

=== Experiment 5063 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007274659227999216
rmse: 0.0852916128819195
mae: 0.034191743152967
r2: 0.671988817231898
pearson: 0.8413035215039281

=== Experiment 5121 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007309031864308093
rmse: 0.08549287610267942
mae: 0.03987304426512373
r2: 0.6704389701892848
pearson: 0.821032767220895

=== Experiment 5097 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007407886959362509
rmse: 0.08606908248240194
mae: 0.037049830808658965
r2: 0.6659816374627359
pearson: 0.823163679186874

=== Experiment 5155 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006656371730328624
rmse: 0.08158659038303184
mae: 0.03552822659952885
r2: 0.6998671283727264
pearson: 0.838250070494737

=== Experiment 5109 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007212226882793942
rmse: 0.08492483077871832
mae: 0.03497496573309474
r2: 0.6748038641986271
pearson: 0.8262757597530205

=== Experiment 5025 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006258895812564193
rmse: 0.0791131835572567
mae: 0.03691264069060704
r2: 0.7177891425621945
pearson: 0.8488874138806768

=== Experiment 5124 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00696860633861446
rmse: 0.08347817881706847
mae: 0.03667701245639019
r2: 0.6857886073100785
pearson: 0.8368492172933357

=== Experiment 5078 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006683972896383667
rmse: 0.08175556798398301
mae: 0.03895658576414195
r2: 0.6986226039435067
pearson: 0.8386547703211131

=== Experiment 5145 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007665198249314464
rmse: 0.08755111792155748
mae: 0.036730219910284696
r2: 0.6543795846501619
pearson: 0.8093389264339966

=== Experiment 5084 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00703770787014393
rmse: 0.08389104761620235
mae: 0.03445739776678904
r2: 0.6826728496673249
pearson: 0.8272779460392048

=== Experiment 5033 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006599468906018849
rmse: 0.08123711532310124
mae: 0.03640454821864569
r2: 0.7024328516760059
pearson: 0.8389103398962193

=== Experiment 5133 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007138281293843293
rmse: 0.08448835004805866
mae: 0.038992138380605515
r2: 0.6781380382584692
pearson: 0.8238249414874126

=== Experiment 5093 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00777750565979792
rmse: 0.08819016759139263
mae: 0.04735884515387564
r2: 0.6493156929417342
pearson: 0.8144693550800397

=== Experiment 5092 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006824029966398329
rmse: 0.08260768709023592
mae: 0.03973478838669385
r2: 0.6923074923003762
pearson: 0.8340160205828158

=== Experiment 5120 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007032735744498227
rmse: 0.08386140795680828
mae: 0.03555678518844091
r2: 0.6828970406242896
pearson: 0.8266908449328523

=== Experiment 5034 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008006839421747413
rmse: 0.08948094446164173
mae: 0.03963606658920195
r2: 0.6389751345529393
pearson: 0.8304222616363364

=== Experiment 5111 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006502432367539294
rmse: 0.0806376609751256
mae: 0.034005013468125685
r2: 0.7068081864870023
pearson: 0.8413626941089778

=== Experiment 5028 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00727505065930742
rmse: 0.08529390751576235
mae: 0.045375854454088774
r2: 0.671971167766493
pearson: 0.8280234828875357

=== Experiment 5066 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006286816801296683
rmse: 0.07928944949548258
mae: 0.03366988609540104
r2: 0.7165301974692135
pearson: 0.84967661648341

=== Experiment 5135 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006985764795854313
rmse: 0.08358088774267902
mae: 0.03306629120691865
r2: 0.6850149400251483
pearson: 0.8322506771576157

=== Experiment 5165 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0076280302087311765
rmse: 0.08733859518409474
mae: 0.04111790344524318
r2: 0.6560554752411564
pearson: 0.8186797591884291

=== Experiment 5101 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007529212210120924
rmse: 0.08677103324336367
mae: 0.049454015264331824
r2: 0.6605111353053654
pearson: 0.8258738914210781

=== Experiment 5017 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006116257602025635
rmse: 0.07820650613616258
mae: 0.031855914907697146
r2: 0.7242206366955006
pearson: 0.8576277327824562

=== Experiment 5037 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006559822495940064
rmse: 0.08099273113026911
mae: 0.033978614501574404
r2: 0.7042204908567388
pearson: 0.8469445380396263

=== Experiment 5100 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007613492164474908
rmse: 0.08725532742746948
mae: 0.0401420246910282
r2: 0.656710989257989
pearson: 0.8226595440135183

=== Experiment 5082 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006167736616890401
rmse: 0.0785349388290995
mae: 0.036176395670790826
r2: 0.7218994705728956
pearson: 0.8505027685429446

=== Experiment 5062 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067775948052480215
rmse: 0.08232614897617901
mae: 0.04243401554326119
r2: 0.6944012332789662
pearson: 0.8386625171770319

=== Experiment 5157 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007051447831920158
rmse: 0.08397289938974453
mae: 0.04027723915906629
r2: 0.682053320838254
pearson: 0.8282414087117139

=== Experiment 5065 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007004712446338224
rmse: 0.08369416016866543
mae: 0.03994033615329118
r2: 0.6841605988043273
pearson: 0.8295854081462046

=== Experiment 5041 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008409705528654275
rmse: 0.09170444661331464
mae: 0.03991056109079534
r2: 0.6208100791074426
pearson: 0.8013762206553915

=== Experiment 5179 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007730326536656716
rmse: 0.08792227554298578
mae: 0.03531479362388029
r2: 0.651442978839051
pearson: 0.8312082467264519

=== Experiment 5173 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007998412672352434
rmse: 0.08943384522848402
mae: 0.038941070331365596
r2: 0.6393550929742627
pearson: 0.8044917936169885

=== Experiment 5085 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007673031039764226
rmse: 0.08759583916924495
mae: 0.04125649048594168
r2: 0.6540264075762567
pearson: 0.8239028092802698

=== Experiment 5180 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009447954855491493
rmse: 0.09720059081863389
mae: 0.03966652635006101
r2: 0.5739958739288273
pearson: 0.7849859006868072

=== Experiment 5193 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0070040424525561985
rmse: 0.08369015744133954
mae: 0.03862161421326284
r2: 0.684190808529072
pearson: 0.8284974187484355

=== Experiment 5166 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007329805566622646
rmse: 0.08561428366004499
mae: 0.03768819032368845
r2: 0.6695022930951273
pearson: 0.8200212270992371

=== Experiment 5177 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006507416942041416
rmse: 0.0806685622906558
mae: 0.03399868207979288
r2: 0.7065834342165805
pearson: 0.8423387213209774

=== Experiment 5125 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00677483730379106
rmse: 0.08230939985075253
mae: 0.035167130918965415
r2: 0.6945255678059914
pearson: 0.8336013473747171

=== Experiment 5146 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006484243737085285
rmse: 0.0805248019996652
mae: 0.03946891261728479
r2: 0.7076283038287451
pearson: 0.8436530303920116

=== Experiment 5218 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006766609206024091
rmse: 0.082259401930868
mae: 0.03849366973867746
r2: 0.6948965691128413
pearson: 0.8350252671978698

=== Experiment 5182 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007409366185757956
rmse: 0.08607767530409935
mae: 0.04136644276538527
r2: 0.6659149397956226
pearson: 0.8326116164759929

=== Experiment 5132 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006899371991549449
rmse: 0.08306245837650995
mae: 0.0391261906560125
r2: 0.6889103535468735
pearson: 0.8335199776519115

=== Experiment 5115 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007920219967092452
rmse: 0.08899561768476273
mae: 0.03934492566785562
r2: 0.642880767639194
pearson: 0.806631897462837

=== Experiment 5162 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00715347782396569
rmse: 0.08457823493054044
mae: 0.04564525894479907
r2: 0.6774528334037546
pearson: 0.8325458252606455

=== Experiment 5142 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0064681835681909255
rmse: 0.08042501829773448
mae: 0.034044654593660316
r2: 0.7083524497755709
pearson: 0.8422131536850044

=== Experiment 5196 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0076011247133215745
rmse: 0.08718442930547618
mae: 0.03332203207605551
r2: 0.6572686321871614
pearson: 0.813480415741228

=== Experiment 5220 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006516346086246752
rmse: 0.08072388795298918
mae: 0.033991225936120986
r2: 0.7061808230343788
pearson: 0.8475147044091564

=== Experiment 5214 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007322643735085783
rmse: 0.08557244728933364
mae: 0.036996444131023175
r2: 0.6698252169269613
pearson: 0.8210853051231447

=== Experiment 5222 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00720355430831545
rmse: 0.08487375512085847
mae: 0.038649584124100286
r2: 0.6751949067647705
pearson: 0.8264422265724056

=== Experiment 5026 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006487975132374691
rmse: 0.0805479678972393
mae: 0.031231086616513606
r2: 0.7074600568574583
pearson: 0.8453383895757368

=== Experiment 5103 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006808696121420988
rmse: 0.08251482364654843
mae: 0.04451047365409455
r2: 0.6929988886214633
pearson: 0.8388866414415469

=== Experiment 5116 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007244271280577978
rmse: 0.08511328498288606
mae: 0.035969413635254646
r2: 0.6733589964063631
pearson: 0.8207502556279479

=== Experiment 5122 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006437180324087365
rmse: 0.0802320405080624
mae: 0.03432000808739896
r2: 0.7097503724066291
pearson: 0.8439824127853844

=== Experiment 5099 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007949127611101649
rmse: 0.08915788025240197
mae: 0.03880243549216958
r2: 0.6415773346940463
pearson: 0.815280371704634

=== Experiment 5114 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006770623128600555
rmse: 0.08228379626998596
mae: 0.03161312346222434
r2: 0.6947155831105309
pearson: 0.8342772505371187

=== Experiment 5081 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006328554583207397
rmse: 0.07955221293721122
mae: 0.0373644507047925
r2: 0.7146482592530621
pearson: 0.8474061355456373

=== Experiment 5178 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00631556595240042
rmse: 0.0794705351208888
mae: 0.030380078508431198
r2: 0.715233910899415
pearson: 0.8461801967188906

=== Experiment 5169 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063716451953847045
rmse: 0.07982258574729777
mae: 0.03513424231026408
r2: 0.712705322515616
pearson: 0.8452019523735831

=== Experiment 5171 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007173177181570243
rmse: 0.0846946112900357
mae: 0.038235822495561296
r2: 0.6765645980397158
pearson: 0.8227553612795836

=== Experiment 5076 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066004773805714125
rmse: 0.0812433220675485
mae: 0.04166183727353862
r2: 0.7023873800022942
pearson: 0.8497734613327954

=== Experiment 5134 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007291096726119249
rmse: 0.08538791908765109
mae: 0.0428817441879588
r2: 0.6712476576763619
pearson: 0.822335146535331

=== Experiment 5130 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00711646608475215
rmse: 0.08435914938376364
mae: 0.0392177245706956
r2: 0.6791216764347812
pearson: 0.8357145222900192

=== Experiment 5104 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00911800316048892
rmse: 0.09548823571775175
mae: 0.04393058572073568
r2: 0.5888732506336465
pearson: 0.7761731471037473

=== Experiment 5249 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006616427259578324
rmse: 0.0813414240075641
mae: 0.035421439860260216
r2: 0.7016682069779514
pearson: 0.8397940238449599

=== Experiment 5054 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006790241350524389
rmse: 0.08240292076452381
mae: 0.03635072711514985
r2: 0.693831006118625
pearson: 0.852242218305616

=== Experiment 5241 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006994036323011092
rmse: 0.08363035527253901
mae: 0.036709412336596164
r2: 0.6846419805062265
pearson: 0.827466374105346

=== Experiment 5204 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008045166159561312
rmse: 0.08969485023991797
mae: 0.046197223716922205
r2: 0.6372469988137979
pearson: 0.8067119111716046

=== Experiment 5102 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009598442919575484
rmse: 0.09797164344633341
mae: 0.044936442935305854
r2: 0.5672104333541426
pearson: 0.7592356306084961

=== Experiment 5205 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007968563137076173
rmse: 0.08926680870892705
mae: 0.047309461707317306
r2: 0.6407009953820844
pearson: 0.8101452427426313

=== Experiment 5014 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.009452849392954134
rmse: 0.09722576506746622
mae: 0.034310903033925436
r2: 0.5737751813888875
pearson: 0.7613569271244399

=== Experiment 5119 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006995547045938019
rmse: 0.08363938692947252
mae: 0.03852640728151143
r2: 0.6845738626743713
pearson: 0.8274606092254969

=== Experiment 5236 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.008033101405296842
rmse: 0.08962757056451347
mae: 0.05619408135476563
r2: 0.6377909932734784
pearson: 0.8273972748320804

=== Experiment 5274 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006949679124643545
rmse: 0.08336473549795229
mae: 0.032414517020551675
r2: 0.6866420270575232
pearson: 0.8406310359848884

=== Experiment 5239 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007534497827339835
rmse: 0.08680148516782323
mae: 0.04417175787485557
r2: 0.6602728091513428
pearson: 0.8263031994980813

=== Experiment 5202 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006885440006696782
rmse: 0.08297855148589171
mae: 0.04066487749362186
r2: 0.6895385406119448
pearson: 0.8378382777441128

=== Experiment 5198 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007410325497013209
rmse: 0.08608324748180222
mae: 0.04738616774336289
r2: 0.6658716848733482
pearson: 0.8250009052739935

=== Experiment 5225 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00656182821238812
rmse: 0.08100511226082042
mae: 0.036454957164701575
r2: 0.7041300539848793
pearson: 0.8427795972786545

=== Experiment 5194 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006467791739555653
rmse: 0.08042258227361052
mae: 0.03925151421281077
r2: 0.7083701171562784
pearson: 0.8434839169138417

=== Experiment 5233 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007352985850794968
rmse: 0.08574955306469514
mae: 0.03717692103216517
r2: 0.6684571042842206
pearson: 0.8176867841845087

=== Experiment 5127 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008540947448503379
rmse: 0.09241724648843082
mae: 0.042750950342602324
r2: 0.6148924387054329
pearson: 0.7843267654854025

=== Experiment 5250 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007249243492793545
rmse: 0.08514248935046206
mae: 0.03490679102098369
r2: 0.673134801546001
pearson: 0.8329386498235017

=== Experiment 5229 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0070697637515152465
rmse: 0.08408188717860254
mae: 0.03611845011024089
r2: 0.6812274640851643
pearson: 0.8254397275273533

=== Experiment 5036 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006060825371579562
rmse: 0.0778513029536408
mae: 0.036174698116778745
r2: 0.7267200515687235
pearson: 0.8536976044526892

=== Experiment 5288 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008065877874706048
rmse: 0.08981023257238592
mae: 0.038711581335196185
r2: 0.6363131166938447
pearson: 0.8294350701321054

=== Experiment 5231 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0068327540650732705
rmse: 0.08266047462405034
mae: 0.03606661218070199
r2: 0.6919141265308926
pearson: 0.8322041336832104

=== Experiment 5024 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007253518800389597
rmse: 0.08516759243039336
mae: 0.03786672344738622
r2: 0.6729420298082018
pearson: 0.8358534161067788

=== Experiment 5136 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066794196066531
rmse: 0.08172771626965421
mae: 0.03432663004381499
r2: 0.698827909773402
pearson: 0.8361480259992867

=== Experiment 5210 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007279365628335116
rmse: 0.0853191984745234
mae: 0.03599170467958453
r2: 0.6717766077121974
pearson: 0.8292080286103659

=== Experiment 5277 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008594142147954977
rmse: 0.09270459615334602
mae: 0.04410887378718578
r2: 0.6124939131199378
pearson: 0.7940023789168859

=== Experiment 5255 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007737667890562119
rmse: 0.0879640147478622
mae: 0.038544083068211894
r2: 0.6511119604226334
pearson: 0.8316358961308954

=== Experiment 5296 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007161516707360866
rmse: 0.08462574494420044
mae: 0.0359494926331613
r2: 0.6770903636896495
pearson: 0.8248042913615343

=== Experiment 5300 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007522126968242049
rmse: 0.08673019640380188
mae: 0.03939527297093803
r2: 0.6608306057432305
pearson: 0.8175732154765956

=== Experiment 5112 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006270810135814017
rmse: 0.07918844698448137
mae: 0.03557986812963343
r2: 0.7172519309707548
pearson: 0.8521618732349423

=== Experiment 5262 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0067812002070470325
rmse: 0.0823480431282191
mae: 0.034342326867666785
r2: 0.6942386672987075
pearson: 0.8369901537994379

=== Experiment 5129 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006079994460534623
rmse: 0.07797431923739137
mae: 0.02989854252110947
r2: 0.7258557257847014
pearson: 0.855508893800884

=== Experiment 5216 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006077630044845615
rmse: 0.07795915626047793
mae: 0.03097680041547565
r2: 0.7259623362474596
pearson: 0.8545791876414348

=== Experiment 5064 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007008304659046751
rmse: 0.08371561777259218
mae: 0.033926304435358365
r2: 0.6839986275143535
pearson: 0.8353130859694062

=== Experiment 5047 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005724715917746135
rmse: 0.07566185246044492
mae: 0.02879774625259121
r2: 0.7418750789089892
pearson: 0.8614132111482319

=== Experiment 5299 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006546254403338773
rmse: 0.08090892659860699
mae: 0.033754942157640085
r2: 0.7048322701803567
pearson: 0.8423934531176016

=== Experiment 5045 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006002512454637453
rmse: 0.07747588305167907
mae: 0.03069234599678119
r2: 0.7293493553281659
pearson: 0.8603296588438488

=== Experiment 5043 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057963874604736575
rmse: 0.0761340098804316
mae: 0.033165482580194715
r2: 0.7386434405924636
pearson: 0.8594723944541752

=== Experiment 5094 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006021210820242055
rmse: 0.07759646138995034
mae: 0.03215209756599026
r2: 0.7285062542528344
pearson: 0.8590249804356238

=== Experiment 5247 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006385134008416965
rmse: 0.07990703353533382
mae: 0.03278100880891271
r2: 0.7120971178728721
pearson: 0.8458899912611407

=== Experiment 5321 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0069250003791478856
rmse: 0.08321658716354502
mae: 0.03587534669923593
r2: 0.687754780829976
pearson: 0.8331021827741794

=== Experiment 5201 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006356088561214379
rmse: 0.07972508113018374
mae: 0.035326930609900185
r2: 0.7134067643033577
pearson: 0.8457514454905808

=== Experiment 5252 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0070917603709213265
rmse: 0.08421259033494533
mae: 0.03870096353513349
r2: 0.6802356461975967
pearson: 0.8305804441064927

=== Experiment 5181 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007251889648299737
rmse: 0.0851580275035756
mae: 0.03710391371411678
r2: 0.673015487558889
pearson: 0.8281432781781184

=== Experiment 5305 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008222749633466654
rmse: 0.09067937821504211
mae: 0.037323089158743265
r2: 0.6292398381358701
pearson: 0.8260522072505021

=== Experiment 5088 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0059488202152298145
rmse: 0.07712859531477165
mae: 0.029868503120601422
r2: 0.7317703147712944
pearson: 0.8557616613845381

=== Experiment 5030 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0054205065496006856
rmse: 0.07362408946534202
mae: 0.033069872621221415
r2: 0.7555917454258853
pearson: 0.8714632923159197

=== Experiment 5301 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006840093974706465
rmse: 0.0827048606474037
mae: 0.03844364803988049
r2: 0.691583173236073
pearson: 0.8331093832273148

=== Experiment 5055 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007300007060565176
rmse: 0.08544007877199772
mae: 0.032277184831093556
r2: 0.670845894617933
pearson: 0.8271555726715252

=== Experiment 5324 ===
num_layers: 1
units: [128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007290183617571734
rmse: 0.08538257209508117
mae: 0.03816483307561042
r2: 0.6712888293388795
pearson: 0.8211515093351702

=== Experiment 5234 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007083470415030727
rmse: 0.08416335553571237
mae: 0.038178922762535795
r2: 0.6806094366600153
pearson: 0.8302119238350409

=== Experiment 5058 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006947031411101981
rmse: 0.0833488536879901
mae: 0.031078733946261143
r2: 0.6867614112957645
pearson: 0.8303227244792233

=== Experiment 5258 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00802698877627276
rmse: 0.08959346391491267
mae: 0.050981522259064826
r2: 0.6380666090258005
pearson: 0.8159002344468532

=== Experiment 5031 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006759886054077783
rmse: 0.08221852622175724
mae: 0.041765549684647706
r2: 0.6951997130750147
pearson: 0.8382621294839768

=== Experiment 5189 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.010345165764228958
rmse: 0.10171118799930004
mae: 0.045676273632742966
r2: 0.533541028946575
pearson: 0.7346684515327454

=== Experiment 5348 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006991125284325925
rmse: 0.0836129492622161
mae: 0.04069735892212128
r2: 0.6847732379592946
pearson: 0.8300741206599245

=== Experiment 5006 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007062320185003887
rmse: 0.08403761172834391
mae: 0.03719047151600601
r2: 0.6815630912229975
pearson: 0.8389159234527657

=== Experiment 5091 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006310759089093888
rmse: 0.07944028631049795
mae: 0.03553623978941785
r2: 0.7154506502502442
pearson: 0.8465612781194507

=== Experiment 5072 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006585070141578029
rmse: 0.08114844509648986
mae: 0.04128328698896002
r2: 0.7030820856272615
pearson: 0.8484109329039946

=== Experiment 5113 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00672013970833103
rmse: 0.08197645825681316
mae: 0.03536004524296044
r2: 0.6969918583110313
pearson: 0.8387841034958157

=== Experiment 5161 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007278989202369058
rmse: 0.08531699245970323
mae: 0.04133624763836311
r2: 0.6717935805933293
pearson: 0.830701701762922

=== Experiment 5154 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006394263954558241
rmse: 0.07996414167961938
mae: 0.03158123625050335
r2: 0.7116854526197585
pearson: 0.8441692934740207

=== Experiment 5011 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008019791905665364
rmse: 0.08955329087010351
mae: 0.03341465870738236
r2: 0.638391112753903
pearson: 0.8523612000729863

=== Experiment 5259 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006536296410213348
rmse: 0.08084736489344194
mae: 0.03434288409187489
r2: 0.7052812717075341
pearson: 0.8398796650060366

=== Experiment 5230 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005892065006450193
rmse: 0.07675978769153934
mae: 0.0296849457010325
r2: 0.73432938215529
pearson: 0.8586644760555793

=== Experiment 5303 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006990522259089059
rmse: 0.08360934313274479
mae: 0.036018553541161166
r2: 0.6848004281018131
pearson: 0.8370291885508379

=== Experiment 5175 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0066831740098857815
rmse: 0.08175068201480512
mae: 0.03784794964632543
r2: 0.6986586253840794
pearson: 0.8420876985064355

=== Experiment 5131 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006592767267870044
rmse: 0.08119585745510693
mae: 0.040143131378654794
r2: 0.7027350255905189
pearson: 0.8414500789188151

=== Experiment 5206 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007892914171832252
rmse: 0.08884207433323613
mae: 0.040771793780543256
r2: 0.6441119739292768
pearson: 0.8203942110913834

=== Experiment 5051 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006912553307901494
rmse: 0.08314176632656714
mae: 0.04299001980257178
r2: 0.6883160137941007
pearson: 0.8453996412462836

=== Experiment 5332 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0066082476197923835
rmse: 0.08129112878901598
mae: 0.04073919237778218
r2: 0.7020370233357647
pearson: 0.8436834275429262

=== Experiment 5333 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007026397287269547
rmse: 0.08382360817376897
mae: 0.04099050092066087
r2: 0.6831828388709624
pearson: 0.8306223225292894

=== Experiment 5309 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006917354557289906
rmse: 0.08317063518628379
mae: 0.039166994067718376
r2: 0.6880995275723687
pearson: 0.8319586868618718

=== Experiment 5424 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007405532787923207
rmse: 0.08605540533820759
mae: 0.03646009076451156
r2: 0.6660877860167839
pearson: 0.8216712405572427

=== Experiment 5304 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007869967004722466
rmse: 0.08871283449829831
mae: 0.0433671219052177
r2: 0.6451466515944364
pearson: 0.8054574371286504

=== Experiment 5273 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.011308112053994509
rmse: 0.10633960717434736
mae: 0.05150571676405565
r2: 0.4901222045661202
pearson: 0.7324392691059444

=== Experiment 5353 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006625633215482518
rmse: 0.08139799269934436
mae: 0.037578434924288126
r2: 0.7012531144780823
pearson: 0.838745131294978

=== Experiment 5242 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005988667798822406
rmse: 0.0773864833082781
mae: 0.029758797227771236
r2: 0.7299736047653673
pearson: 0.8640809748855444

=== Experiment 5358 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00668874475955445
rmse: 0.08178474649685263
mae: 0.038280470948031824
r2: 0.6984074427333937
pearson: 0.8370120287040727

=== Experiment 5361 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006411341568524918
rmse: 0.08007085342698002
mae: 0.031746538524966256
r2: 0.7109154305224338
pearson: 0.8440206140725727

=== Experiment 5046 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006245133522245378
rmse: 0.07902615720282354
mae: 0.03184710879225462
r2: 0.7184096781754227
pearson: 0.8481134863096237

=== Experiment 5266 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008021164665299651
rmse: 0.08956095502672831
mae: 0.04043537994276569
r2: 0.6383292156261906
pearson: 0.8049349379035496

=== Experiment 5038 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0071436179206367054
rmse: 0.08451992617505476
mae: 0.038577704109479284
r2: 0.6778974121051835
pearson: 0.8388323617591792

=== Experiment 5326 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0057856364904602985
rmse: 0.07606337154281487
mae: 0.03212909468909306
r2: 0.7391281970985022
pearson: 0.8600873697675667

=== Experiment 5172 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0067282627792522535
rmse: 0.08202598843813011
mae: 0.040554501702356784
r2: 0.6966255926184304
pearson: 0.8417555628902318

=== Experiment 5325 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006711621925376794
rmse: 0.08192448916762798
mae: 0.03756841063538104
r2: 0.6973759213954748
pearson: 0.8432139922862094

=== Experiment 5420 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007417136888715305
rmse: 0.08612280121265974
mae: 0.03677120831016246
r2: 0.6655645622194236
pearson: 0.8237765753267036

=== Experiment 5228 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005939002433718112
rmse: 0.07706492349777629
mae: 0.033217163326751536
r2: 0.732212994218521
pearson: 0.8557633152141964

=== Experiment 5035 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006608665054548646
rmse: 0.08129369627805495
mae: 0.03504172438517271
r2: 0.7020182013863319
pearson: 0.8480851977172357

=== Experiment 5400 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007091050354757288
rmse: 0.08420837461177652
mae: 0.04346182741289342
r2: 0.6802676605139312
pearson: 0.8322330711831376

=== Experiment 5148 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005555309065085997
rmse: 0.07453394572331454
mae: 0.02838353182257927
r2: 0.7495135593337771
pearson: 0.8732960628896341

=== Experiment 5007 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006764882989346606
rmse: 0.08224890874258822
mae: 0.031820160884522164
r2: 0.6949744034630629
pearson: 0.8352228442296518

=== Experiment 5429 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007257514947252714
rmse: 0.08519104968981608
mae: 0.04989502178832553
r2: 0.6727618453049826
pearson: 0.8323941777453441

=== Experiment 5090 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.018171111596832424
rmse: 0.13480026556662425
mae: 0.05394574017301632
r2: 0.1806725758166574
pearson: 0.4779784258718257

=== Experiment 5384 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007365177270724692
rmse: 0.08582061099016187
mae: 0.0341264397893852
r2: 0.6679073985254433
pearson: 0.8224457558863378

=== Experiment 5417 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006909916306550799
rmse: 0.0831259063502516
mae: 0.039869647715729865
r2: 0.6884349150243706
pearson: 0.8308761897916199

=== Experiment 5096 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006234206935501269
rmse: 0.07895699421521357
mae: 0.031087518807144283
r2: 0.718902353162556
pearson: 0.849899864138328

=== Experiment 5168 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007655825279541876
rmse: 0.08749757299229434
mae: 0.03613498833855404
r2: 0.6548022077318483
pearson: 0.8129358889467692

=== Experiment 5152 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006088114855045288
rmse: 0.07802637281743455
mae: 0.03434996165365306
r2: 0.7254895807702743
pearson: 0.8543984273942054

=== Experiment 5067 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006615665483460399
rmse: 0.08133674128867224
mae: 0.03062684123384703
r2: 0.7017025551278253
pearson: 0.842500641584198

=== Experiment 5465 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007418717613690979
rmse: 0.08613197788098784
mae: 0.035100969750578394
r2: 0.6654932880260007
pearson: 0.8252559878282179

=== Experiment 5448 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008124415878158894
rmse: 0.09013554170336413
mae: 0.03416345101997073
r2: 0.6336736638827014
pearson: 0.8121020852475158

=== Experiment 5158 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.01342715043228498
rmse: 0.11587558169124752
mae: 0.05353456931619447
r2: 0.3945756967491173
pearson: 0.6804267838898281

=== Experiment 5495 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006759822378731119
rmse: 0.0822181389884928
mae: 0.03821599679777512
r2: 0.6952025841683702
pearson: 0.8345436240999956

=== Experiment 5263 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007026110838640717
rmse: 0.08382189951701594
mae: 0.03573747029228307
r2: 0.6831957547135608
pearson: 0.853080189767682

=== Experiment 5362 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007944052658301268
rmse: 0.08912941522472403
mae: 0.04880622721215026
r2: 0.641806162082158
pearson: 0.8203904079490264

=== Experiment 5209 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005516664123796954
rmse: 0.07427424939908146
mae: 0.03132627503193822
r2: 0.7512560427275605
pearson: 0.8673430319650386

=== Experiment 5405 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007458212464660437
rmse: 0.08636094293522065
mae: 0.040880627151360316
r2: 0.6637124825788011
pearson: 0.8282153954255598

=== Experiment 5106 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008256652847217237
rmse: 0.0908661259613132
mae: 0.04185671435358655
r2: 0.6277111571498017
pearson: 0.7933703696490275

=== Experiment 5107 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007477609785760077
rmse: 0.08647317379257036
mae: 0.04259692450621719
r2: 0.6628378659078322
pearson: 0.8218420295299774

=== Experiment 5377 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006306118118173949
rmse: 0.07941107050137247
mae: 0.03589242862431087
r2: 0.7156599095863768
pearson: 0.8477560084577485

=== Experiment 5071 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006874710900922315
rmse: 0.0829138764075249
mae: 0.03105319737598539
r2: 0.6900223112690748
pearson: 0.8538623783000092

=== Experiment 5286 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007153496350550358
rmse: 0.08457834445382789
mae: 0.03804309211290019
r2: 0.6774519980482063
pearson: 0.8376996919593304

=== Experiment 5187 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007404194117623253
rmse: 0.08604762703075114
mae: 0.04353534219700234
r2: 0.6661481460714149
pearson: 0.8244635883021993

=== Experiment 5105 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01013586032780138
rmse: 0.10067700992680195
mae: 0.04940052704576863
r2: 0.5429785189527271
pearson: 0.737668978815997

=== Experiment 5310 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00645143041328995
rmse: 0.0803207968915271
mae: 0.03020402831976706
r2: 0.7091078421564283
pearson: 0.8466931266934299

=== Experiment 5140 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005738873235680962
rmse: 0.07575535120162115
mae: 0.030945410098195673
r2: 0.7412367316744201
pearson: 0.8615367168764375

=== Experiment 5385 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0063566943965053194
rmse: 0.07972888056724062
mae: 0.0367974824553965
r2: 0.7133794474567376
pearson: 0.8449976949610364

=== Experiment 5246 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0064803550370619405
rmse: 0.08050065240146778
mae: 0.034152733234799615
r2: 0.7078036436012499
pearson: 0.8442656414770281

=== Experiment 5536 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007212609346452645
rmse: 0.08492708252643938
mae: 0.044415273598555585
r2: 0.6747866190805913
pearson: 0.8293151244930782

=== Experiment 5098 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00831080968229723
rmse: 0.09116364232684666
mae: 0.03785739179689264
r2: 0.6252692492922911
pearson: 0.8163461426060862

=== Experiment 5289 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007386694729317117
rmse: 0.08594588256174415
mae: 0.039356931186870746
r2: 0.6669371857880688
pearson: 0.8216132223257746

=== Experiment 5149 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006131799808954894
rmse: 0.07830580954791856
mae: 0.0296176742765001
r2: 0.7235198454257088
pearson: 0.8549573513568238

=== Experiment 5314 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006271695215372355
rmse: 0.07919403522597113
mae: 0.03252326698134645
r2: 0.7172120231230226
pearson: 0.8547264210943762

=== Experiment 5457 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006883746911656638
rmse: 0.08296834885458815
mae: 0.040848027453788564
r2: 0.6896148815221168
pearson: 0.8408465542956817

=== Experiment 5144 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008915167678169548
rmse: 0.09442016563303386
mae: 0.04209018252563415
r2: 0.598019013256704
pearson: 0.7820951405085012

=== Experiment 5496 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007030744389012674
rmse: 0.08384953422060658
mae: 0.039225464529432645
r2: 0.6829868299666705
pearson: 0.8290467738061758

=== Experiment 5367 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006373824717080362
rmse: 0.07983623686697891
mae: 0.030828949875714346
r2: 0.7126070488416398
pearson: 0.8460538673375406

=== Experiment 5514 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0067072384748473445
rmse: 0.08189773180526641
mae: 0.03530947808429783
r2: 0.6975735692505436
pearson: 0.8421331171608025

=== Experiment 5460 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007779950448798271
rmse: 0.08820402739556892
mae: 0.03717676361727573
r2: 0.6492054584817423
pearson: 0.8078448792327395

=== Experiment 5143 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0069961910838722136
rmse: 0.08364323692847027
mae: 0.04725349029997935
r2: 0.6845448232873812
pearson: 0.837256618689681

=== Experiment 5489 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008393380163429777
rmse: 0.0916153926118847
mae: 0.039979088715886305
r2: 0.6215461826400722
pearson: 0.7888247083791543

=== Experiment 5184 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.009929880904421417
rmse: 0.09964878777196147
mae: 0.041731469562900254
r2: 0.5522660404944524
pearson: 0.7651442853521376

=== Experiment 5428 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0070955550856289275
rmse: 0.0842351178881405
mae: 0.043407846538663045
r2: 0.6800645441816127
pearson: 0.8289700845757069

=== Experiment 5529 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007411839095127406
rmse: 0.08609203851185895
mae: 0.04147692522765811
r2: 0.6658034374005646
pearson: 0.8211775481145056

=== Experiment 5473 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006031577155852724
rmse: 0.07766322911038868
mae: 0.03153594623940381
r2: 0.7280388407427227
pearson: 0.8546104146199397

=== Experiment 5538 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007040050096911926
rmse: 0.08390500638765203
mae: 0.03750371353946861
r2: 0.6825672396932485
pearson: 0.830605437917032

=== Experiment 5322 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005668134223629318
rmse: 0.07528701231706116
mae: 0.02807576328649232
r2: 0.7444263225932084
pearson: 0.8673907747907601

=== Experiment 5368 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.01280746680526265
rmse: 0.11317007910778648
mae: 0.043798750517077545
r2: 0.42251695874793216
pearson: 0.6865225316127371

=== Experiment 5170 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.011451360745375415
rmse: 0.10701103095183886
mae: 0.054245404708455666
r2: 0.4836631841203256
pearson: 0.7497217112580424

=== Experiment 5077 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.009412530477898556
rmse: 0.09701819663289231
mae: 0.04855816304421473
r2: 0.5755931435230319
pearson: 0.7602197969933412

=== Experiment 5335 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007523506872360859
rmse: 0.08673815119289123
mae: 0.03276331089242814
r2: 0.6607683864738555
pearson: 0.8252605673688781

=== Experiment 5480 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00777317065915326
rmse: 0.08816558659223711
mae: 0.05105929223089762
r2: 0.6495111562128222
pearson: 0.8258924932535989

=== Experiment 5533 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007053427737901159
rmse: 0.08398468752041148
mae: 0.04194387651324751
r2: 0.6819640477489941
pearson: 0.8290124787051819

=== Experiment 5185 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008184087157936192
rmse: 0.09046594474130136
mae: 0.05184449050034033
r2: 0.6309831121408819
pearson: 0.8193243404559897

=== Experiment 5576 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005878732169591162
rmse: 0.0766728907084581
mae: 0.028459956749652314
r2: 0.7349305539010335
pearson: 0.8579653743666502

=== Experiment 5147 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006054724289453588
rmse: 0.07781210888707225
mae: 0.03598627307858708
r2: 0.7269951466764922
pearson: 0.8619961205316324

=== Experiment 5391 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0076295800570971165
rmse: 0.087347467376548
mae: 0.0390897391966432
r2: 0.6559855932604766
pearson: 0.8157984777497299

=== Experiment 5015 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007048469421244484
rmse: 0.08395516316013259
mae: 0.036827502773206906
r2: 0.6821876160647216
pearson: 0.827570464270324

=== Experiment 5226 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005917983711099052
rmse: 0.07692843239725512
mae: 0.030511906673805343
r2: 0.7331607191703673
pearson: 0.8565589700654971

=== Experiment 5440 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006816472479130664
rmse: 0.08256193117369932
mae: 0.033714703556226876
r2: 0.6926482560749707
pearson: 0.8384609201219672

=== Experiment 5503 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007320386357383499
rmse: 0.08555925640971583
mae: 0.03669124174850796
r2: 0.6699270010940094
pearson: 0.8235620484846929

=== Experiment 5251 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005779854877809696
rmse: 0.0760253568081709
mae: 0.03009936358395225
r2: 0.7393888874682356
pearson: 0.8607897132382658

=== Experiment 5467 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006214053489274479
rmse: 0.07882926797373219
mae: 0.03474355046622971
r2: 0.719811063182712
pearson: 0.8486412006514894

=== Experiment 5275 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006332673705625017
rmse: 0.07957809815285244
mae: 0.030833120161290695
r2: 0.7144625298362162
pearson: 0.8506915983433672

=== Experiment 5327 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007200576104362234
rmse: 0.08485620840199162
mae: 0.03157397953388765
r2: 0.6753291926702131
pearson: 0.8337632421703559

=== Experiment 5192 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006170228804152883
rmse: 0.07855080396885115
mae: 0.03173898318166519
r2: 0.7217870989461257
pearson: 0.8626042598780337

=== Experiment 5334 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006609820199396529
rmse: 0.08130080073035277
mae: 0.03353100079298734
r2: 0.7019661164135589
pearson: 0.8420618242999746

=== Experiment 5513 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007355084981320234
rmse: 0.08576179208318956
mae: 0.04772139183283673
r2: 0.6683624554127343
pearson: 0.831561052808166

=== Experiment 5052 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006193903760940513
rmse: 0.07870135806287279
mae: 0.037497486414622805
r2: 0.7207196055647169
pearson: 0.8549262983554228

=== Experiment 5449 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005515869313135949
rmse: 0.0742688986934366
mae: 0.035711302595579295
r2: 0.7512918803904424
pearson: 0.8738625180652461

=== Experiment 5456 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006075034553068566
rmse: 0.07794250799832249
mae: 0.029754269691112487
r2: 0.7260793658293236
pearson: 0.8526616318111757

=== Experiment 5468 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005967322897971809
rmse: 0.07724844916224409
mae: 0.02914099965956078
r2: 0.7309360369501114
pearson: 0.855207342306947

=== Experiment 5549 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0076968943826638036
rmse: 0.08773194619215856
mae: 0.043225881061322045
r2: 0.6529504199479751
pearson: 0.8292757387987865

=== Experiment 5556 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007019582133854326
rmse: 0.08378294655748464
mae: 0.04062692308088379
r2: 0.6834901311388764
pearson: 0.832592772448204

=== Experiment 5553 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006933153830628279
rmse: 0.08326556209279007
mae: 0.04228224719427947
r2: 0.6873871452913335
pearson: 0.8325426405152182

=== Experiment 5311 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005952313128522012
rmse: 0.0771512354309509
mae: 0.029163145684118263
r2: 0.7316128208482982
pearson: 0.8554315711091203

=== Experiment 5631 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00885317007782944
rmse: 0.09409128587616092
mae: 0.04309696552281363
r2: 0.600814457768809
pearson: 0.7775911611266695

=== Experiment 5284 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006816234733630231
rmse: 0.08256049136015502
mae: 0.0356089863400019
r2: 0.6926589759149454
pearson: 0.8444774334295047

=== Experiment 5504 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007528044721972729
rmse: 0.0867643055753501
mae: 0.039089796996873066
r2: 0.6605637768321166
pearson: 0.8200738401484232

=== Experiment 5278 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007068993631727222
rmse: 0.08407730747191672
mae: 0.030930424406313287
r2: 0.6812621884474507
pearson: 0.8302991419695872

=== Experiment 5441 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006310325371337453
rmse: 0.07943755642853985
mae: 0.034028379607954046
r2: 0.7154702063929881
pearson: 0.8469322253050493

=== Experiment 5219 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007970140238003675
rmse: 0.08927564190754203
mae: 0.04066133259030862
r2: 0.6406298845953984
pearson: 0.8410158106828585

=== Experiment 5629 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006771857156161911
rmse: 0.08229129453448834
mae: 0.03376532574135082
r2: 0.6946599413509265
pearson: 0.8387351192311945

=== Experiment 5481 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007248303351917951
rmse: 0.08513696818608207
mae: 0.042468840458579765
r2: 0.6731771920842942
pearson: 0.8308322197625447

=== Experiment 5512 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0073501076234284935
rmse: 0.08573276866769493
mae: 0.04896751510227043
r2: 0.6685868822893701
pearson: 0.8295136483478951

=== Experiment 5491 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00745779280967522
rmse: 0.08635851324377475
mae: 0.04012278542480634
r2: 0.6637314046373802
pearson: 0.8158208459275579

=== Experiment 5224 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006681812701052068
rmse: 0.08174235561232664
mae: 0.03304105000860394
r2: 0.6987200062002338
pearson: 0.8432006875088733

=== Experiment 5650 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00753371713735376
rmse: 0.08679698806614064
mae: 0.03915130807425382
r2: 0.6603080101192071
pearson: 0.8209900138286635

=== Experiment 5128 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005998646575745984
rmse: 0.07745093011543491
mae: 0.031342995286528744
r2: 0.7295236661058788
pearson: 0.8548641163549564

=== Experiment 5620 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0068682102291908985
rmse: 0.08287466578629984
mae: 0.03948310731827874
r2: 0.69031542369628
pearson: 0.8319317453388168

=== Experiment 5139 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.010559665461978621
rmse: 0.10276023288207663
mae: 0.0375281356008495
r2: 0.5238693319835794
pearson: 0.756768467521551

=== Experiment 5618 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007123653004311588
rmse: 0.08440173578968378
mae: 0.03463824749249526
r2: 0.6787976213950513
pearson: 0.8265299509672394

=== Experiment 5163 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005850941555391922
rmse: 0.07649144759639422
mae: 0.030626458065611484
r2: 0.7361836204636925
pearson: 0.8581078399380712

=== Experiment 5588 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0071021157662262855
rmse: 0.08427405155933994
mae: 0.03794068343830403
r2: 0.6797687259810541
pearson: 0.8411969449706296

=== Experiment 5153 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.020028699894394115
rmse: 0.1415227893111004
mae: 0.060461552640993926
r2: 0.09691473706673526
pearson: 0.32228246840431485

=== Experiment 5635 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006548306895702398
rmse: 0.08092160957187146
mae: 0.03555310498659119
r2: 0.7047397242030518
pearson: 0.8405729298873217

=== Experiment 5478 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007923535263697391
rmse: 0.08901424191497331
mae: 0.03920216968303518
r2: 0.6427312823744509
pearson: 0.8023690834869288

=== Experiment 5212 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007897782214600985
rmse: 0.08886946727983118
mae: 0.03967120613035091
r2: 0.6438924760234241
pearson: 0.8229714747408511

=== Experiment 5282 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007146428673143043
rmse: 0.08453655229037343
mae: 0.03190085390209015
r2: 0.6777706765117795
pearson: 0.8364518230222678

=== Experiment 5592 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007820185795237515
rmse: 0.08843181438395073
mae: 0.0480824888219139
r2: 0.6473912644197269
pearson: 0.8121472105665175

=== Experiment 5540 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006902724210149761
rmse: 0.08308263482912516
mae: 0.042602849713582476
r2: 0.6887592034856096
pearson: 0.8335598435638466

=== Experiment 5159 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.009999141681530604
rmse: 0.09999570831556025
mae: 0.04025900836857534
r2: 0.5491431025385982
pearson: 0.7880370731850797

=== Experiment 5479 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00625083945179738
rmse: 0.07906225048528141
mae: 0.03315333858222015
r2: 0.7181524003232946
pearson: 0.8582367216116189

=== Experiment 5638 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006926747240777671
rmse: 0.08322708237573676
mae: 0.037480287419102004
r2: 0.6876760156079416
pearson: 0.8295347510209597

=== Experiment 5471 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0067683129806556525
rmse: 0.08226975738784971
mae: 0.03633376493041777
r2: 0.6948197466645927
pearson: 0.8436701194377626

=== Experiment 5431 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006951664835067208
rmse: 0.08337664442196752
mae: 0.034979365545102614
r2: 0.6865524922484982
pearson: 0.8344782468429033

=== Experiment 5647 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006757704763724029
rmse: 0.08220525995168454
mae: 0.03763018961349546
r2: 0.6952980664970063
pearson: 0.8347602095057857

=== Experiment 5566 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0060271606631017135
rmse: 0.07763479028825745
mae: 0.03573171064631746
r2: 0.7282379784570185
pearson: 0.8568452421392203

=== Experiment 5633 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007405159243559569
rmse: 0.08605323493953942
mae: 0.0398952425479448
r2: 0.6661046289677309
pearson: 0.8166641615690063

=== Experiment 5519 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006704590394202833
rmse: 0.08188156321298973
mae: 0.03712127482826651
r2: 0.6976929700412953
pearson: 0.8354694142722234

=== Experiment 5118 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006366732128359589
rmse: 0.07979180489473583
mae: 0.03191669705593549
r2: 0.7129268505453723
pearson: 0.8535654394826597

=== Experiment 5559 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0062638556435562895
rmse: 0.07914452377490365
mae: 0.03188326235669016
r2: 0.7175655059657495
pearson: 0.8486631179353932

=== Experiment 5714 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007057587009742172
rmse: 0.08400944595545297
mae: 0.041336658101509556
r2: 0.6817765080123184
pearson: 0.8292991414548977

=== Experiment 5710 ===
num_layers: 1
units: [256]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007087928595460951
rmse: 0.08418983665182485
mae: 0.043012982956637934
r2: 0.6804084192666118
pearson: 0.8292189848130146

=== Experiment 5524 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0067302166272346165
rmse: 0.08203789750618075
mae: 0.03160621485084268
r2: 0.6965374944728598
pearson: 0.8412637591152466

=== Experiment 5331 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006858707850842774
rmse: 0.0828173161291935
mae: 0.031672429839313225
r2: 0.6907438817536804
pearson: 0.8358050180250685

=== Experiment 5644 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007444233918026491
rmse: 0.0862799740265752
mae: 0.030945405339269837
r2: 0.6643427690940968
pearson: 0.8170308826003295

=== Experiment 5402 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006036424303461896
rmse: 0.0776944290374921
mae: 0.030146924456899364
r2: 0.7278202849904178
pearson: 0.85554521521888

=== Experiment 5343 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006218906720508035
rmse: 0.07886004514650011
mae: 0.030890115155155027
r2: 0.7195922331224613
pearson: 0.8489136380949364

=== Experiment 5450 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005981961342033847
rmse: 0.07734314023902732
mae: 0.028761549378826693
r2: 0.7302759959502261
pearson: 0.8554229777960508

=== Experiment 5674 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00718481298707094
rmse: 0.08476327616999557
mae: 0.04033829098334527
r2: 0.676039944690999
pearson: 0.8301166045981426

=== Experiment 5735 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007083624857695151
rmse: 0.0841642730479813
mae: 0.03797137287355057
r2: 0.6806024729082518
pearson: 0.825723869474822

=== Experiment 5732 ===
num_layers: 2
units: [128, 256]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006853165811539319
rmse: 0.0827838499439312
mae: 0.03413337225832654
r2: 0.6909937698666351
pearson: 0.8323185875487652

=== Experiment 5621 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0070160780731842575
rmse: 0.08376203240839049
mae: 0.046904994191877034
r2: 0.6836481276922346
pearson: 0.8391714260283526

=== Experiment 5745 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006306596057095278
rmse: 0.07941407971572345
mae: 0.032985159072943936
r2: 0.7156383595307712
pearson: 0.8470078948289604

=== Experiment 5786 ===
num_layers: 1
units: [256]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006731594037126782
rmse: 0.08204629203764655
mae: 0.03557124071517442
r2: 0.6964753876670666
pearson: 0.8348720358101667

=== Experiment 5269 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00650199985428021
rmse: 0.08063497909890106
mae: 0.0340333222532637
r2: 0.7068276883194906
pearson: 0.8423049507831826

=== Experiment 5459 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00595210491758422
rmse: 0.07714988605036445
mae: 0.0300715965773897
r2: 0.7316222089878424
pearson: 0.860016229812981

=== Experiment 5439 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006597649637792873
rmse: 0.0812259172788641
mae: 0.036825808123783506
r2: 0.7025148816795939
pearson: 0.8406506850831401

=== Experiment 5156 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006643787976021115
rmse: 0.08150943488959493
mae: 0.03870697345170324
r2: 0.7004345243159182
pearson: 0.8411760647748984

=== Experiment 5821 ===
num_layers: 1
units: [256]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007336958848775484
rmse: 0.0856560496916329
mae: 0.03940157769180494
r2: 0.6691797547512559
pearson: 0.819752192077584

=== Experiment 5746 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006805592688479569
rmse: 0.08249601619762963
mae: 0.036175597170149676
r2: 0.6931388210468665
pearson: 0.8371927044461991

=== Experiment 5645 ===
num_layers: 2
units: [256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006610168470735669
rmse: 0.08130294257119891
mae: 0.03654447168714888
r2: 0.701950413012163
pearson: 0.839789874743808

=== Experiment 5126 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0068181055360442945
rmse: 0.0825718204719037
mae: 0.037249510620732194
r2: 0.6925746222574983
pearson: 0.8398030048131483

=== Experiment 5854 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007731969793458084
rmse: 0.08793161998654457
mae: 0.04083471182778333
r2: 0.6513688851131298
pearson: 0.8150281333330067

=== Experiment 5240 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005797061908241032
rmse: 0.07613843909774505
mae: 0.027969261057478385
r2: 0.7386130300394799
pearson: 0.8597079898655482

=== Experiment 5292 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007053659716393741
rmse: 0.08398606858517513
mae: 0.04166835755323787
r2: 0.6819535879408651
pearson: 0.8317030054259318

=== Experiment 5652 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006956806649915256
rmse: 0.08340747358549626
mae: 0.04283339453436118
r2: 0.6863206500800855
pearson: 0.8358404479078917

=== Experiment 5313 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007580650462063079
rmse: 0.08706693093283512
mae: 0.038903433660399826
r2: 0.6581918071649997
pearson: 0.8225412285005319

=== Experiment 5279 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007837640940861888
rmse: 0.08853045205386612
mae: 0.04456580350755869
r2: 0.646604219586121
pearson: 0.8182737184319583

=== Experiment 5833 ===
num_layers: 2
units: [256, 128]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007161612535373626
rmse: 0.08462631112942136
mae: 0.04260930000128942
r2: 0.6770860428467314
pearson: 0.8285121035833148

=== Experiment 5963 ===
num_layers: 1
units: [128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.0070747162380378655
rmse: 0.08411133239961109
mae: 0.037089200928908356
r2: 0.6810041586476155
pearson: 0.8271940301918408

=== Experiment 5021 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007157293465526588
rmse: 0.0846007887996713
mae: 0.03772508588152051
r2: 0.6772807878051663
pearson: 0.8232837457301936

=== Experiment 5397 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.00713276959763202
rmse: 0.08445572566517927
mae: 0.04528824743257331
r2: 0.6783865582147526
pearson: 0.8351692695849371

=== Experiment 5716 ===
num_layers: 2
units: [256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0073411908674398925
rmse: 0.08568074968999684
mae: 0.040961573410084565
r2: 0.6689889348923317
pearson: 0.823694160322846

=== Experiment 5080 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005681763520597708
rmse: 0.07537747356205107
mae: 0.02863235045853384
r2: 0.7438117835916163
pearson: 0.8637793128289313

=== Experiment 5123 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006973531277126155
rmse: 0.08350767196567124
mae: 0.03706072237594647
r2: 0.6855665440002174
pearson: 0.830984185623411

=== Experiment 5211 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006147505239920041
rmse: 0.07840602808407043
mae: 0.028284512730169
r2: 0.7228116944559791
pearson: 0.8623736561534542

=== Experiment 5883 ===
num_layers: 2
units: [128, 256]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007730895067422815
rmse: 0.08792550862760372
mae: 0.04860990174916769
r2: 0.6514173440370371
pearson: 0.8218968730925141

=== Experiment 5539 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.016102674300838046
rmse: 0.1268963131885164
mae: 0.04976461844348768
r2: 0.2739375031041691
pearson: 0.6734471773382773

=== Experiment 5825 ===
num_layers: 2
units: [128, 256]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007699132608867887
rmse: 0.08774470131505313
mae: 0.04368545793931088
r2: 0.6528494993135516
pearson: 0.8159662553552405

=== Experiment 5151 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006624644665663612
rmse: 0.08139192014975204
mae: 0.031129487514070592
r2: 0.7012976877543227
pearson: 0.8515717987439235

=== Experiment 5167 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006463052888069921
rmse: 0.08039311468073569
mae: 0.034525607224638606
r2: 0.7085837898840419
pearson: 0.8475083178089418

=== Experiment 5285 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.009211381137525074
rmse: 0.09597594040969369
mae: 0.038230179094541
r2: 0.5846628787478771
pearson: 0.8116443261419625

=== Experiment 5959 ===
num_layers: 1
units: [128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00731857255330474
rmse: 0.08554865605785249
mae: 0.039373492228560715
r2: 0.6700087847216043
pearson: 0.8262113732742163

=== Experiment 5268 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00559472346146476
rmse: 0.07479788407077274
mae: 0.03443360534433754
r2: 0.7477363815486611
pearson: 0.8660092321571254

=== Experiment 5466 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0063591957288711524
rmse: 0.0797445655130878
mae: 0.032327973494162256
r2: 0.713266663481283
pearson: 0.8451812431961994

=== Experiment 5023 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006380064654121356
rmse: 0.07987530691096816
mae: 0.033080128368081545
r2: 0.7123256928268678
pearson: 0.847704627836516

=== Experiment 5039 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006556629129930973
rmse: 0.08097301482550204
mae: 0.030489626890494382
r2: 0.7043644783245807
pearson: 0.8444944859685579

=== Experiment 5554 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005923368293400171
rmse: 0.07696342178853648
mae: 0.030734360753065997
r2: 0.7329179307243456
pearson: 0.8561715899967702

=== Experiment 5856 ===
num_layers: 3
units: [128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006209160670935716
rmse: 0.0787982275875271
mae: 0.03655323144511731
r2: 0.7200316782081126
pearson: 0.8496687214147488

=== Experiment 5597 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00680008207915799
rmse: 0.08246261018884855
mae: 0.03697904821466335
r2: 0.693387291995772
pearson: 0.8361565785167971

=== Experiment 5720 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008961536351722973
rmse: 0.09466539152046524
mae: 0.03981534680392688
r2: 0.5959282701745942
pearson: 0.7800893028946557

=== Experiment 5427 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007950620578109337
rmse: 0.08916625246195635
mae: 0.04125690722822615
r2: 0.6415100174687749
pearson: 0.8288887090686495

=== Experiment 5087 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009591810541140517
rmse: 0.09793778913749543
mae: 0.03879884720686471
r2: 0.5675094843786421
pearson: 0.8269608459501014

=== Experiment 5197 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005864163725223352
rmse: 0.07657782789569936
mae: 0.02997660045979514
r2: 0.7355874386455152
pearson: 0.8605022048546246

=== Experiment 5359 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005721117480729524
rmse: 0.07563806899127928
mae: 0.0277701554145463
r2: 0.742037330850275
pearson: 0.8643436108634013

=== Experiment 5336 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005409305380411447
rmse: 0.07354798012461965
mae: 0.027768311335180994
r2: 0.7560968012146165
pearson: 0.8696590847972007

=== Experiment 5642 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005900126678513511
rmse: 0.0768122820811458
mae: 0.028780316178171474
r2: 0.7339658849101682
pearson: 0.8571165753505244

=== Experiment 5608 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006114892787645015
rmse: 0.07819777994064163
mae: 0.03431883878913099
r2: 0.7242821755752221
pearson: 0.8523537127475558

=== Experiment 5307 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007257405676649456
rmse: 0.08519040836062154
mae: 0.04000880887275008
r2: 0.6727667722683899
pearson: 0.8228241635249696

=== Experiment 5393 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006161618484203711
rmse: 0.07849597750333269
mae: 0.03102502929860131
r2: 0.7221753344829422
pearson: 0.852393268913227

=== Experiment 5584 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007201166275625948
rmse: 0.08485968580913994
mae: 0.04295826282504966
r2: 0.6753025821076866
pearson: 0.8288557969673724

=== Experiment 5360 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005803833261692126
rmse: 0.0761828934977671
mae: 0.032846068292293
r2: 0.7383077126926706
pearson: 0.8601700408678655

=== Experiment 5518 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005339062051388367
rmse: 0.07306888565859183
mae: 0.028139577307005822
r2: 0.7592640420038138
pearson: 0.8723646042959778

=== Experiment 5164 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005437647434826091
rmse: 0.07374040571373398
mae: 0.02881514442588196
r2: 0.7548188704553522
pearson: 0.8710835951591874

=== Experiment 5075 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0061916579672739586
rmse: 0.07868708894903889
mae: 0.0320622258398857
r2: 0.7208208674126922
pearson: 0.849743729286224

=== Experiment 5520 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005495611313875857
rmse: 0.07413239045030084
mae: 0.030746269268924827
r2: 0.7522053046608526
pearson: 0.8679055738811607

=== Experiment 5709 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.008590598101271062
rmse: 0.0926854794521292
mae: 0.04183244561987247
r2: 0.6126537126250614
pearson: 0.8023906828645445

=== Experiment 5627 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0073657453420469836
rmse: 0.0858239205702407
mae: 0.034828696624045655
r2: 0.6678817844395486
pearson: 0.8236346863536386

=== Experiment 5599 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00690378844274186
rmse: 0.08308903924550975
mae: 0.03162913002467807
r2: 0.6887112177064365
pearson: 0.8383570575520933

=== Experiment 5117 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007754183057531856
rmse: 0.08805783927358117
mae: 0.03804104219704366
r2: 0.6503672988128487
pearson: 0.8101616078915503

=== Experiment 5186 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005763889683998499
rmse: 0.0759202850626794
mae: 0.02863959199794252
r2: 0.7401087510303632
pearson: 0.8624473460004652

=== Experiment 5264 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0074829710116727155
rmse: 0.08650416759713207
mae: 0.03519058338413766
r2: 0.6625961305910875
pearson: 0.8218122108328649

=== Experiment 5318 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006228174001463956
rmse: 0.07891878104395655
mae: 0.028839417625174033
r2: 0.7191743755030011
pearson: 0.8482225545369613

=== Experiment 5992 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007310049995012465
rmse: 0.08549883037218968
mae: 0.03876707943127933
r2: 0.6703930631239383
pearson: 0.8243446174902869

=== Experiment 5781 ===
num_layers: 5
units: [128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00624729775820902
rmse: 0.07903984917880992
mae: 0.033346201932331294
r2: 0.7183120937283753
pearson: 0.853448990901608

=== Experiment 5996 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00759363128676191
rmse: 0.08714144413975425
mae: 0.04909861814567238
r2: 0.6576065074926272
pearson: 0.8222037952986231

=== Experiment 5663 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00658928693548414
rmse: 0.08117442291438937
mae: 0.031183708093733668
r2: 0.7028919522459879
pearson: 0.8430639205044617

=== Experiment 5464 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.0066657786218095065
rmse: 0.0816442197697394
mae: 0.0343149950643527
r2: 0.699442975776143
pearson: 0.8486013705609633

=== Experiment 5059 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006526205286545991
rmse: 0.08078493229895035
mae: 0.02942279162991905
r2: 0.7057362760322521
pearson: 0.8537262082899737

=== Experiment 5748 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00587967402363193
rmse: 0.07667903249019206
mae: 0.030837047446503094
r2: 0.7348880861168767
pearson: 0.8575363483754591

=== Experiment 5215 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.011967100622043538
rmse: 0.10939424400782492
mae: 0.047763470178108836
r2: 0.460408700075839
pearson: 0.680366520450723

=== Experiment 5634 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006977814987141553
rmse: 0.08353331662960327
mae: 0.03567300722237374
r2: 0.6853733934010282
pearson: 0.8292090924102715

=== Experiment 5404 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006204455799154078
rmse: 0.07876836801123963
mae: 0.030429192753723522
r2: 0.7202438188059743
pearson: 0.849903699960743

=== Experiment 5923 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006560919156810577
rmse: 0.08099950096642927
mae: 0.04040857876227246
r2: 0.7041710429007647
pearson: 0.841671139001688

=== Experiment 5983 ===
num_layers: 4
units: [128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006536249837744115
rmse: 0.08084707686579716
mae: 0.03751223899602076
r2: 0.7052833716396737
pearson: 0.8415337802343472

=== Experiment 5563 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006063210092856696
rmse: 0.07786661731998312
mae: 0.029996928987701635
r2: 0.7266125255359339
pearson: 0.8541108939304688

=== Experiment 5494 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.011869125173267872
rmse: 0.10894551470009159
mae: 0.04954586149856438
r2: 0.46482636993884074
pearson: 0.6906533821571437

=== Experiment 5673 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007777710371579158
rmse: 0.08819132821076661
mae: 0.03939890825694187
r2: 0.6493064625776186
pearson: 0.8116740889679489

=== Experiment 5435 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006690484815376634
rmse: 0.08179538382681895
mae: 0.037799087476541905
r2: 0.6983289843822286
pearson: 0.8365800518839117

=== Experiment 5020 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005577369997849981
rmse: 0.07468179160846358
mae: 0.02952226836076897
r2: 0.748518841585208
pearson: 0.868308906500665

=== Experiment 5993 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006386114019317603
rmse: 0.07991316549428888
mae: 0.0367156480781997
r2: 0.7120529296126981
pearson: 0.8478117980030999

=== Experiment 5568 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007453367986886831
rmse: 0.08633289052781003
mae: 0.03678770965535147
r2: 0.6639309179493953
pearson: 0.8281732320556374

=== Experiment 5290 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00606355317199516
rmse: 0.07786882028125994
mae: 0.031912685289625195
r2: 0.7265970562485812
pearson: 0.861068092422313

=== Experiment 5920 ===
num_layers: 3
units: [512, 256, 128]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007204459324710569
rmse: 0.08487908649785629
mae: 0.038413291913010426
r2: 0.6751540999738428
pearson: 0.8244700767677964

=== Experiment 5298 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007786120116445958
rmse: 0.08823899430776598
mae: 0.03817652412433482
r2: 0.6489272708829863
pearson: 0.8294305012074114

=== Experiment 5461 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00608805406263976
rmse: 0.07802598325327122
mae: 0.03732868549727687
r2: 0.7254923218730827
pearson: 0.8526462463179023

=== Experiment 5694 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006299677392313368
rmse: 0.07937050706851612
mae: 0.031909203799392785
r2: 0.7159503190806502
pearson: 0.8476638960971894

=== Experiment 5630 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007015005665232483
rmse: 0.08375563064793007
mae: 0.033587790205984853
r2: 0.6836964820947774
pearson: 0.8405713562034054

=== Experiment 5863 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006708262144321391
rmse: 0.08190398124829702
mae: 0.03602314801787387
r2: 0.6975274124445103
pearson: 0.8369265975331012

=== Experiment 5354 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00666436021757639
rmse: 0.08163553281247321
mae: 0.03446218002236394
r2: 0.6995069309987267
pearson: 0.8535330835689364

=== Experiment 5751 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005996574514895878
rmse: 0.07743755235604931
mae: 0.02980574186796879
r2: 0.7296170944176263
pearson: 0.8588372975290103

=== Experiment 5419 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007974284047769528
rmse: 0.08929884684456753
mae: 0.03961171485749285
r2: 0.6404430420368863
pearson: 0.8128848700917217

=== Experiment 5366 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005307493452795192
rmse: 0.07285254595959699
mae: 0.028155861855619767
r2: 0.7606874562199774
pearson: 0.8723130410249259

=== Experiment 5901 ===
num_layers: 6
units: [128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005662466299512711
rmse: 0.07524936079138952
mae: 0.028678194135242002
r2: 0.744681886796982
pearson: 0.8634593187256817

=== Experiment 5667 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007865033058557454
rmse: 0.08868502161333364
mae: 0.04007089975020773
r2: 0.6453691210554211
pearson: 0.81027530350895

=== Experiment 5770 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0070507989479538045
rmse: 0.08396903564977869
mae: 0.03297305830690235
r2: 0.6820825787307013
pearson: 0.8460162321333539

=== Experiment 5356 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006678051512896804
rmse: 0.08171934601363869
mae: 0.03484671415065142
r2: 0.6988895965187292
pearson: 0.8479267767931672

=== Experiment 5742 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007550121133204247
rmse: 0.08689143302538085
mae: 0.05082782622175894
r2: 0.6595683611662593
pearson: 0.8252757935949463

=== Experiment 5557 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005948869536044504
rmse: 0.07712891504516645
mae: 0.02934603930614721
r2: 0.731768090917468
pearson: 0.8817145215318396

=== Experiment 5625 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006523962928810984
rmse: 0.08077105254242378
mae: 0.03426079959509646
r2: 0.7058373829555866
pearson: 0.8402170362686175

=== Experiment 5610 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005993188942113573
rmse: 0.07741568925039402
mae: 0.030423119358236553
r2: 0.729769748404276
pearson: 0.8564099409565679

=== Experiment 5774 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.012184097368935973
rmse: 0.11038159886926795
mae: 0.04850695210584317
r2: 0.4506244122660311
pearson: 0.7148824433766606

=== Experiment 5547 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.013460170031162469
rmse: 0.11601797287990542
mae: 0.049186438640339454
r2: 0.39308685756876816
pearson: 0.6272542733343893

=== Experiment 5543 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006839948637335569
rmse: 0.08270398199201517
mae: 0.03395534500769709
r2: 0.6915897264341591
pearson: 0.8341234495769897

=== Experiment 5291 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007242488729775959
rmse: 0.08510281270190756
mae: 0.036456921861795825
r2: 0.6734393708374655
pearson: 0.848310010631978

=== Experiment 5340 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006073219940943298
rmse: 0.0779308664198166
mae: 0.030502163635100853
r2: 0.72616118589138
pearson: 0.8651991915882314

=== Experiment 5453 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005683589714624037
rmse: 0.07538958624786342
mae: 0.030053248736799855
r2: 0.7437294413067383
pearson: 0.8629890948081494

=== Experiment 5265 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00788691828345108
rmse: 0.08880832327800744
mae: 0.03903271012554531
r2: 0.6443823258973915
pearson: 0.8336892257934317

=== Experiment 5271 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.009548832605526214
rmse: 0.09771812833618036
mae: 0.04411965665967283
r2: 0.5694473405794549
pearson: 0.7834145950552013

=== Experiment 5525 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.010928280800414187
rmse: 0.1045384178205036
mae: 0.050902587027675056
r2: 0.5072486286135376
pearson: 0.7131799718500128

=== Experiment 5545 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007643219648594199
rmse: 0.087425509141178
mae: 0.03983136663819356
r2: 0.6553705900831165
pearson: 0.8184812700668667

=== Experiment 5578 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006309266720551606
rmse: 0.07943089273419761
mae: 0.03305993599684424
r2: 0.7155179404909742
pearson: 0.8543369019432481

=== Experiment 5346 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007425841573291388
rmse: 0.08617332286323529
mae: 0.05053025678981433
r2: 0.6651720718231127
pearson: 0.8325706895123608

=== Experiment 5260 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008063211794718172
rmse: 0.08979538849360902
mae: 0.03463107939320909
r2: 0.6364333290670665
pearson: 0.8250497234201501

=== Experiment 5254 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006855202865098866
rmse: 0.08279615247762946
mae: 0.037741995669593145
r2: 0.6909019200182109
pearson: 0.8418387798453514

=== Experiment 5328 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005685630856185421
rmse: 0.07540312232384956
mae: 0.030270970163756503
r2: 0.7436374071321107
pearson: 0.8624207391887414

=== Experiment 5662 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006703430650994115
rmse: 0.08187448107312874
mae: 0.036385242132388584
r2: 0.6977452623521343
pearson: 0.8377584528240228

=== Experiment 5297 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0066345516026015666
rmse: 0.08145275687539107
mae: 0.034802535278599285
r2: 0.7008509883281668
pearson: 0.8389369179822712

=== Experiment 5458 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006184310782836869
rmse: 0.0786403890048674
mae: 0.03160345116977392
r2: 0.7211521487252175
pearson: 0.8674356148453254

=== Experiment 5463 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006855983043736963
rmse: 0.08280086378617656
mae: 0.03367333858275891
r2: 0.690866742106802
pearson: 0.8329737802719231

=== Experiment 5641 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007203695684993122
rmse: 0.08487458798128637
mae: 0.0327450291603868
r2: 0.6751885321525989
pearson: 0.8329801709266893

=== Experiment 5492 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006242012382327746
rmse: 0.07900640722326098
mae: 0.03277270697145993
r2: 0.7185504090005901
pearson: 0.8502878394753627

=== Experiment 5555 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006132598478763956
rmse: 0.07831090906613175
mae: 0.034273358192390316
r2: 0.7234838337555397
pearson: 0.8541826821036693

=== Experiment 5141 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00624464402602318
rmse: 0.07902306009022417
mae: 0.03492793091486799
r2: 0.7184317493446378
pearson: 0.8578442503186776

=== Experiment 5571 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006311797897248987
rmse: 0.07944682433709346
mae: 0.034265213477515925
r2: 0.7154038108477453
pearson: 0.8656162236874576

=== Experiment 5270 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00642583671749988
rmse: 0.08016131683985661
mae: 0.033629489228781194
r2: 0.7102618506349558
pearson: 0.847021640660175

=== Experiment 5561 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.01070397404530059
rmse: 0.10346001181761284
mae: 0.050690098252497
r2: 0.5173625214766566
pearson: 0.720470374270244

=== Experiment 5550 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.010892137007327812
rmse: 0.1043654013901533
mae: 0.052464746921263405
r2: 0.5088783363357021
pearson: 0.7208576891217728

=== Experiment 5227 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005858599712831651
rmse: 0.07654149013986893
mae: 0.027661465889250045
r2: 0.735838317515348
pearson: 0.8582616323134333

=== Experiment 5497 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006220933756803419
rmse: 0.07887289621158475
mae: 0.03139211531535922
r2: 0.7195008349480693
pearson: 0.8484788089185383

=== Experiment 5388 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005843565894709257
rmse: 0.07644322007025382
mae: 0.029478747027531522
r2: 0.7365161857575973
pearson: 0.8649742731387351

=== Experiment 5073 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059644411101800335
rmse: 0.0772297941870884
mae: 0.028240112186883656
r2: 0.7310659754932708
pearson: 0.8664264900564924

=== Experiment 5613 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006386257736710238
rmse: 0.07991406469896421
mae: 0.03054122753770084
r2: 0.7120464494587191
pearson: 0.8468364381080635

=== Experiment 5195 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.017293029724286584
rmse: 0.1315029646977078
mae: 0.06281495255140866
r2: 0.2202649009763622
pearson: 0.4876756909348268

=== Experiment 5287 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005073639784783754
rmse: 0.07122948676484868
mae: 0.02722800419137707
r2: 0.7712318151837425
pearson: 0.8811354994612485

=== Experiment 5704 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006075024609139141
rmse: 0.0779424442081408
mae: 0.031025421013822524
r2: 0.7260798141967246
pearson: 0.8522271546129745

=== Experiment 5806 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00845702545015698
rmse: 0.0919620870258879
mae: 0.039752980540245966
r2: 0.6186764446739756
pearson: 0.789276399754737

=== Experiment 5628 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006307314846342164
rmse: 0.07941860516492445
mae: 0.034916091974527795
r2: 0.7156059496399922
pearson: 0.8480275180847799

=== Experiment 5508 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007986861928329167
rmse: 0.08936924486829441
mae: 0.03842216402422495
r2: 0.6398759109384007
pearson: 0.8236517687149029

=== Experiment 5190 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005713082237972286
rmse: 0.07558493393509243
mae: 0.034908653972105685
r2: 0.742399636409618
pearson: 0.8637246753073803

=== Experiment 5511 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006253842820965122
rmse: 0.07908124190328021
mae: 0.03732817812237909
r2: 0.7180169797294049
pearson: 0.847658573744692

=== Experiment 5591 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006001329257424935
rmse: 0.0774682467687564
mae: 0.029590340364112216
r2: 0.7294027051697134
pearson: 0.8542537439478024

=== Experiment 5203 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005605840569654192
rmse: 0.07487216151316985
mae: 0.02703625400042683
r2: 0.747235116033413
pearson: 0.864887724596903

=== Experiment 5700 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008299424919072542
rmse: 0.09110117957014904
mae: 0.042234466444478445
r2: 0.6257825832552746
pearson: 0.7934720456369856

=== Experiment 5418 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005930368340307889
rmse: 0.07700888481407771
mae: 0.03418568384835693
r2: 0.732602301690233
pearson: 0.8582372288344623

=== Experiment 5469 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006038073051087623
rmse: 0.07770503877540776
mae: 0.030915698893550857
r2: 0.727745943685643
pearson: 0.8572820075119498

=== Experiment 5985 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.007154159125257947
rmse: 0.0845822624742206
mae: 0.04464172229433882
r2: 0.6774221138283507
pearson: 0.8449927139464599

=== Experiment 5671 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.017847712931039324
rmse: 0.13359533274422172
mae: 0.049700509976106436
r2: 0.1952544793186305
pearson: 0.5501156667714154

=== Experiment 5422 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008179670378097718
rmse: 0.09044153016229722
mae: 0.042041367053366266
r2: 0.6311822627998251
pearson: 0.8035631037881065

=== Experiment 5199 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006043470429516341
rmse: 0.07773976093040383
mae: 0.02952185429491229
r2: 0.7275025782678601
pearson: 0.8579995562142945

=== Experiment 5658 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008351292018407152
rmse: 0.09138540374921562
mae: 0.03876865489702134
r2: 0.6234439185747332
pearson: 0.8022311980940641

=== Experiment 5411 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063383184802637835
rmse: 0.07961355713861669
mae: 0.04073414884699874
r2: 0.7142080094322094
pearson: 0.8568146094662018

=== Experiment 5315 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.008435546379317848
rmse: 0.09184523057468934
mae: 0.04415988671595156
r2: 0.6196449265244501
pearson: 0.7874245110294934

=== Experiment 5946 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008976983171553208
rmse: 0.09474694280847909
mae: 0.03428290671952424
r2: 0.5952317798671141
pearson: 0.8377947950983345

=== Experiment 5995 ===
num_layers: 4
units: [512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0066838635415674115
rmse: 0.08175489919000213
mae: 0.031362692436177675
r2: 0.6986275347040409
pearson: 0.8402535090430632

=== Experiment 5029 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005430381470678964
rmse: 0.07369112206147335
mae: 0.031001044047399808
r2: 0.7551464895807509
pearson: 0.870273795415796

=== Experiment 5546 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006629416862919471
rmse: 0.0814212310329405
mae: 0.03260482515379065
r2: 0.7010825114804606
pearson: 0.8398631298666666

=== Experiment 5887 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.008738911803807503
rmse: 0.09348214697902217
mae: 0.04099591935903773
r2: 0.6059663130555458
pearson: 0.7885154429521138

=== Experiment 5953 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006196946213367798
rmse: 0.07872068478721332
mae: 0.0354975302944366
r2: 0.7205824227238562
pearson: 0.850341751415706

=== Experiment 5060 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063171516234578635
rmse: 0.07948051096626055
mae: 0.02843509770386348
r2: 0.7151624136893422
pearson: 0.8474278054001217

=== Experiment 5875 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006818422950346833
rmse: 0.08257374249933712
mae: 0.037086825849995744
r2: 0.6925603101863018
pearson: 0.8335583903607006

=== Experiment 5476 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006037193493966974
rmse: 0.0776993789805747
mae: 0.03137743371437954
r2: 0.7277856025290905
pearson: 0.8546058353275242

=== Experiment 5528 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00896497423208991
rmse: 0.0946835478427478
mae: 0.04241157666085603
r2: 0.5957732576619759
pearson: 0.77382198981816

=== Experiment 5632 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006170951834243041
rmse: 0.07855540614269041
mae: 0.03268719231610752
r2: 0.7217544978375903
pearson: 0.8505135482657904

=== Experiment 5283 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.00563515618836245
rmse: 0.07506767738755776
mae: 0.03072160538135491
r2: 0.7459132876886482
pearson: 0.8651424233842087

=== Experiment 5312 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006119688577435436
rmse: 0.07822843841874537
mae: 0.028524970445842547
r2: 0.7240659355243605
pearson: 0.8520654075682779

=== Experiment 5436 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006473276472875327
rmse: 0.08045667450793208
mae: 0.030943728482502486
r2: 0.70812281294491
pearson: 0.85340816577444

=== Experiment 5626 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006897095950664195
rmse: 0.08304875646669367
mae: 0.03682619780997583
r2: 0.6890129792286274
pearson: 0.8319426970574719

=== Experiment 5765 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0063313144343172725
rmse: 0.0795695572082519
mae: 0.0393954124100588
r2: 0.7145238187812211
pearson: 0.8484010685751954

=== Experiment 5137 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00807564754648839
rmse: 0.08986460675086932
mae: 0.04124132280881569
r2: 0.6358726064931404
pearson: 0.8121215411205353

=== Experiment 5452 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007017773373556581
rmse: 0.08377215153949778
mae: 0.03877217996786271
r2: 0.6835716873446053
pearson: 0.8270966457350538

=== Experiment 5619 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006020006530039064
rmse: 0.0775887010462159
mae: 0.0372276529945493
r2: 0.7285605551680387
pearson: 0.8613029080021083

=== Experiment 5572 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006094049622014652
rmse: 0.07806439407319224
mae: 0.03937407307356339
r2: 0.7252219847397163
pearson: 0.8570744370406044

=== Experiment 5693 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006646944261698278
rmse: 0.08152879406503127
mae: 0.042810084753597155
r2: 0.7002922087839223
pearson: 0.8388295982146502

=== Experiment 5850 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.008736419312686683
rmse: 0.09346881465326648
mae: 0.040704185689483946
r2: 0.6060786983831543
pearson: 0.7897202106020748

=== Experiment 5401 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0055538551846290255
rmse: 0.07452419194214067
mae: 0.0287002427397414
r2: 0.7495791141636801
pearson: 0.8661313840898757

=== Experiment 5432 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009517182116063023
rmse: 0.09755604602515942
mae: 0.04349278661929848
r2: 0.5708744472189047
pearson: 0.7677787922447538

=== Experiment 5564 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006203396852777435
rmse: 0.07876164582318881
mae: 0.029323656357922317
r2: 0.7202915662320193
pearson: 0.8589168802875758

=== Experiment 5541 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.00570518872756505
rmse: 0.07553269972379545
mae: 0.03272366359022898
r2: 0.7427555513196806
pearson: 0.8628847496771924

=== Experiment 5379 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006687394051124909
rmse: 0.08177648837609078
mae: 0.031031522775531017
r2: 0.6984683455819893
pearson: 0.8548595902654339

=== Experiment 5138 ===
num_layers: 4
units: [64, 128, 256, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006020281156417193
rmse: 0.0775904707835775
mae: 0.035858868417225055
r2: 0.7285481723855212
pearson: 0.8544440766880045

=== Experiment 5378 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005550144163488262
rmse: 0.07449928968445445
mae: 0.02545844839939298
r2: 0.7497464424735718
pearson: 0.8671615748396311

=== Experiment 5987 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005600361343775916
rmse: 0.07483556202619124
mae: 0.02879905199302659
r2: 0.7474821719166729
pearson: 0.86483756360765

=== Experiment 5293 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008856287497588643
rmse: 0.09410785035048162
mae: 0.0436011758673573
r2: 0.6006738946839478
pearson: 0.7771437114023515

=== Experiment 5447 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006628831177878912
rmse: 0.08141763431762748
mae: 0.03092109120285487
r2: 0.7011089197611597
pearson: 0.861651396006152

=== Experiment 5582 ===
num_layers: 3
units: [128, 256, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006711156430213359
rmse: 0.08192164811704754
mae: 0.03998585160910922
r2: 0.697396910367514
pearson: 0.8390626592588017

=== Experiment 5956 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.0059314142219632545
rmse: 0.07701567517046938
mae: 0.030814342839405307
r2: 0.7325551433467203
pearson: 0.8597446691404916

=== Experiment 5922 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006321153805346582
rmse: 0.07950568410715414
mae: 0.04337935301037791
r2: 0.7149819570694748
pearson: 0.8529331404250661

=== Experiment 5317 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.007133994561861376
rmse: 0.08446297746268111
mae: 0.03324983534401565
r2: 0.6783313251167989
pearson: 0.8297920301875115

=== Experiment 5724 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006560798657200055
rmse: 0.08099875713367492
mae: 0.03638557768506826
r2: 0.7041764761751671
pearson: 0.8425338293248944

=== Experiment 5727 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005965910623904536
rmse: 0.07723930750534042
mae: 0.028897673270168425
r2: 0.7309997157662155
pearson: 0.8551252991703258

=== Experiment 5678 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006114194972742289
rmse: 0.078193317954556
mae: 0.031066382789116227
r2: 0.7243136397420542
pearson: 0.853859355049189

=== Experiment 5574 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006963192751695114
rmse: 0.08344574735536325
mae: 0.029807904180572605
r2: 0.6860327035615701
pearson: 0.8527658620283406

=== Experiment 5455 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005941002837467783
rmse: 0.07707790109666832
mae: 0.029074986988431733
r2: 0.7321227968938935
pearson: 0.8674051000097144

=== Experiment 5912 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006518490800258066
rmse: 0.08073717111874842
mae: 0.030732379389991033
r2: 0.7060841188235696
pearson: 0.8407736422071531

=== Experiment 5544 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005644037574728699
rmse: 0.07512680995975204
mae: 0.028357886526720354
r2: 0.7455128298863911
pearson: 0.8639374770865811

=== Experiment 5320 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006898897746707404
rmse: 0.08305960357904078
mae: 0.03266151989489269
r2: 0.6889317370380701
pearson: 0.832184044186265

=== Experiment 5738 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005781180265222617
rmse: 0.07603407305427361
mae: 0.03329829666950611
r2: 0.7393291263331345
pearson: 0.8611852049429841

=== Experiment 5617 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006761859739776458
rmse: 0.08223052802807762
mae: 0.03667184645142586
r2: 0.695110720455547
pearson: 0.8343671942598768

=== Experiment 5968 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.019729140494009707
rmse: 0.14046045882742128
mae: 0.05969847537679795
r2: 0.11042173858389304
pearson: 0.3451965371219716

=== Experiment 5696 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005712106794308701
rmse: 0.07557848102673605
mae: 0.028615278089131904
r2: 0.7424436187350835
pearson: 0.8628753748363251

=== Experiment 5454 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.011097603501419334
rmse: 0.10534516363563794
mae: 0.04095043063878583
r2: 0.49961394254983527
pearson: 0.7559520574604265

=== Experiment 5713 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005733574711388596
rmse: 0.07572037183868419
mae: 0.028747668882013393
r2: 0.741475639802704
pearson: 0.8611224595006441

=== Experiment 5238 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0073682312990693884
rmse: 0.08583840223972827
mae: 0.0337611678228667
r2: 0.6677696937315618
pearson: 0.8286126107249139

=== Experiment 5988 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.007048714133857785
rmse: 0.08395662054810082
mae: 0.03642709507522976
r2: 0.6821765820806941
pearson: 0.8304028152661247

=== Experiment 5383 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007025517226132017
rmse: 0.08381835852682881
mae: 0.03329507404035928
r2: 0.6832225204403091
pearson: 0.8447210908867497

=== Experiment 5906 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0072701416637588265
rmse: 0.08526512571830776
mae: 0.04123063614942099
r2: 0.672192512215165
pearson: 0.8332641969337572

=== Experiment 5596 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007056205773728997
rmse: 0.0840012248346951
mae: 0.03796638101085498
r2: 0.6818387873362239
pearson: 0.8274740060260286

=== Experiment 5425 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007124906052153626
rmse: 0.08440915857982252
mae: 0.0398271053221313
r2: 0.6787411220193623
pearson: 0.8244512939014327

=== Experiment 5472 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006480547855153684
rmse: 0.08050185001075244
mae: 0.036331722134962725
r2: 0.7077949495183589
pearson: 0.8426734285594493

=== Experiment 5847 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007068941228653762
rmse: 0.08407699583509012
mae: 0.03051987813367245
r2: 0.6812645512789686
pearson: 0.8409857793711059

=== Experiment 5295 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.00574739704639755
rmse: 0.07581158912987876
mae: 0.028573509510799136
r2: 0.7408523968008249
pearson: 0.8642924032154051

=== Experiment 5976 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.0059665245919473964
rmse: 0.07724328185640092
mae: 0.029310556150709082
r2: 0.7309720322173909
pearson: 0.8565185995491907

=== Experiment 5345 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006056728163761065
rmse: 0.07782498418734864
mae: 0.030558015126225066
r2: 0.7269047928659406
pearson: 0.8640653682454963

=== Experiment 5581 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.011548682376681942
rmse: 0.10746479598771842
mae: 0.04483301643487219
r2: 0.47927499459924017
pearson: 0.7001327441869357

=== Experiment 5891 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006153900464896604
rmse: 0.0784468002208924
mae: 0.036913273931286794
r2: 0.7225233365765396
pearson: 0.8647588144890881

=== Experiment 5609 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005987661537909592
rmse: 0.07737998150626292
mae: 0.02978791639727823
r2: 0.7300189766270369
pearson: 0.8626725629457823

=== Experiment 5208 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005929180669137497
rmse: 0.07700117316728036
mae: 0.03111771729757274
r2: 0.7326558532605718
pearson: 0.8613777066844632

=== Experiment 5958 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.007736896413033832
rmse: 0.08795962945029857
mae: 0.0451060685384047
r2: 0.6511467460048306
pearson: 0.8319933556376256

=== Experiment 5531 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0069302470216144755
rmse: 0.08324810521335892
mae: 0.03442693928446361
r2: 0.6875182120303813
pearson: 0.8297933001326108

=== Experiment 5294 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.005517150668883224
rmse: 0.0742775246550612
mae: 0.026943522180097563
r2: 0.7512341046237655
pearson: 0.8673169651478453

=== Experiment 5200 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.00566383109691161
rmse: 0.07525842874330828
mae: 0.028377250974308616
r2: 0.744620348682959
pearson: 0.8634669740807949

=== Experiment 5982 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.008368240898060899
rmse: 0.09147808971584889
mae: 0.04259837478836187
r2: 0.6226797010509183
pearson: 0.7936885108534772

=== Experiment 5174 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0060139333832506
rmse: 0.07754955437170867
mae: 0.029563617697487447
r2: 0.7288343906837342
pearson: 0.8573045348489946

=== Experiment 5074 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006927924857133481
rmse: 0.0832341567935513
mae: 0.03244991065976956
r2: 0.6876229174047667
pearson: 0.8337742046663624

=== Experiment 5110 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005849408673532079
rmse: 0.07648142698415138
mae: 0.030307203389871223
r2: 0.73625273743208
pearson: 0.8587493393902808

=== Experiment 5443 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.013463728702292664
rmse: 0.11603330858978668
mae: 0.05495205475006071
r2: 0.3929263986537985
pearson: 0.6284710524388644

=== Experiment 5586 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.008559393447774124
rmse: 0.09251699004925594
mae: 0.03621843437209247
r2: 0.6140607167170201
pearson: 0.7887933627879311

=== Experiment 5733 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.009092317506662423
rmse: 0.09535364443303897
mae: 0.043221202540798684
r2: 0.5900314054595626
pearson: 0.7711824545600465

=== Experiment 5648 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.009040518085225059
rmse: 0.09508163905415734
mae: 0.035500057859494685
r2: 0.5923670185735044
pearson: 0.8505439588157848

=== Experiment 5706 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006512399954958237
rmse: 0.08069944209818453
mae: 0.032080026599893015
r2: 0.7063587523573527
pearson: 0.8427181655048056

=== Experiment 5990 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006523287314654836
rmse: 0.08076687015512508
mae: 0.035916320427140436
r2: 0.7058678461005268
pearson: 0.8409941179111953

=== Experiment 5616 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.007884332822373392
rmse: 0.08879376567289729
mae: 0.04014541526577514
r2: 0.644498903199436
pearson: 0.8183415096828586

=== Experiment 5612 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.005640741342083052
rmse: 0.07510486896388976
mae: 0.02700468983234232
r2: 0.7456614555656005
pearson: 0.864396999743967

=== Experiment 5160 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.007789840477864682
rmse: 0.08826007295410922
mae: 0.03512218271103805
r2: 0.6487595214240724
pearson: 0.8231642135046369

=== Experiment 5445 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006278336416831525
rmse: 0.07923595406651911
mae: 0.03645773033189617
r2: 0.7169125742722449
pearson: 0.8481450184086686

=== Experiment 5176 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.005902903862968656
rmse: 0.07683035769127107
mae: 0.030989718852855223
r2: 0.733840662885418
pearson: 0.8582487519049764

=== Experiment 5423 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005456720685952155
rmse: 0.07386961950593868
mae: 0.027050712881049745
r2: 0.7539588659568579
pearson: 0.869384938111308

=== Experiment 5841 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005727684406470176
rmse: 0.07568146673043656
mae: 0.02872229587151966
r2: 0.7417412310589541
pearson: 0.862328608018661

=== Experiment 5999 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.0064034397343341555
rmse: 0.08002149545174818
mae: 0.034270361822307596
r2: 0.7112717207482321
pearson: 0.8495626557122506

=== Experiment 5970 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006452268752097382
rmse: 0.08032601541279999
mae: 0.031046842771726587
r2: 0.7090700418285825
pearson: 0.8523640911632764

=== Experiment 5866 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005507027725176392
rmse: 0.07420935065863596
mae: 0.030229776372522596
r2: 0.7516905437000583
pearson: 0.8672222183277303

=== Experiment 5579 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006134608741576892
rmse: 0.07832374315350928
mae: 0.03251171318397775
r2: 0.7233931918900887
pearson: 0.8520335393213946

=== Experiment 5350 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005956465873621871
rmse: 0.07717814375600045
mae: 0.027541448913274254
r2: 0.7314255753995083
pearson: 0.8693294880934588

=== Experiment 5329 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005668422283682885
rmse: 0.07528892537208168
mae: 0.027865332507878187
r2: 0.7444133340921779
pearson: 0.866367262997393

=== Experiment 5903 ===
num_layers: 7
units: [512, 512, 256, 256, 128, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.0072319928866531056
rmse: 0.08504112467890523
mae: 0.0316652438629109
r2: 0.6739126237843011
pearson: 0.8317372894377184

=== Experiment 5392 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.007056821290311523
rmse: 0.0840048884905606
mae: 0.03659013732577543
r2: 0.6818110339644274
pearson: 0.8279457491380365

=== Experiment 5565 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005944161874228905
rmse: 0.07709839086666403
mae: 0.02952905870891001
r2: 0.7319803573167328
pearson: 0.8556661043530163

=== Experiment 5967 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007984575032733483
rmse: 0.08935644930688262
mae: 0.035085153466048034
r2: 0.6399790260542704
pearson: 0.8084033943714896

=== Experiment 5522 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.0062878287661503485
rmse: 0.07929583069840651
mae: 0.02872662703195471
r2: 0.7164845684193615
pearson: 0.849279686294373

=== Experiment 5237 ===
num_layers: 6
units: [512, 512, 256, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.005675480921962384
rmse: 0.07533578779014914
mae: 0.032973211367015066
r2: 0.7440950631988998
pearson: 0.8627097724471992

=== Experiment 5639 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.008073491821817907
rmse: 0.08985261165830355
mae: 0.04087330108980262
r2: 0.6359698071697254
pearson: 0.7987388908964044

=== Experiment 5873 ===
num_layers: 5
units: [512, 512, 256, 128, 64]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005959313015987117
rmse: 0.07719658681565603
mae: 0.030402410130417966
r2: 0.7312971990033824
pearson: 0.856468932203663

=== Experiment 5974 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00840992069602301
rmse: 0.09170561976249335
mae: 0.03981783829506324
r2: 0.6208003773054886
pearson: 0.8029535096465226

=== Experiment 5150 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006037232272199198
rmse: 0.07769962852034235
mae: 0.03082156672086896
r2: 0.7277838540356671
pearson: 0.8544359909358383

=== Experiment 5089 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006412275212133846
rmse: 0.08007668332376065
mae: 0.02932416711060236
r2: 0.7108733329430352
pearson: 0.84729936348748

=== Experiment 5881 ===
num_layers: 10
units: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006472549508516017
rmse: 0.08045215664303858
mae: 0.027602625627348918
r2: 0.7081555914479097
pearson: 0.8445768919198359

=== Experiment 5896 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.005906095970270957
rmse: 0.07685112862067126
mae: 0.032735151455714255
r2: 0.7336967321721111
pearson: 0.8568842026535849

=== Experiment 5095 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006142127434210383
rmse: 0.07837172598718484
mae: 0.028315449796117702
r2: 0.7230541773484742
pearson: 0.8522015100097803

=== Experiment 5966 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.00604076427633538
rmse: 0.07772235377505869
mae: 0.029001313876614105
r2: 0.7276245975237206
pearson: 0.8551441754476307

=== Experiment 5594 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006748044860577469
rmse: 0.08214648416443317
mae: 0.03503891311021464
r2: 0.6957336272782363
pearson: 0.839310738887006

=== Experiment 5548 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.0054817103112940815
rmse: 0.07403857313113267
mae: 0.02937163853696504
r2: 0.7528320947489656
pearson: 0.8678832050437403

=== Experiment 5949 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.007057016157499248
rmse: 0.08400604833879075
mae: 0.03762728742763242
r2: 0.6818022474887006
pearson: 0.8315180536562493

=== Experiment 5605 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.005778513936913474
rmse: 0.0760165372594245
mae: 0.03064252979155301
r2: 0.7394493499030529
pearson: 0.8600732390429929

=== Experiment 5660 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.006197413571525406
rmse: 0.07872365318965709
mae: 0.03451300251391305
r2: 0.7205613497502299
pearson: 0.8520226273709103

=== Experiment 5406 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.00571001288497423
rmse: 0.07556462720727357
mae: 0.027391005089188573
r2: 0.7425380321853747
pearson: 0.8633585937670895

=== Experiment 5398 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.0
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006400682143163762
rmse: 0.08000426328117623
mae: 0.03579093377377563
r2: 0.7113960593203965
pearson: 0.844048723470908

=== Experiment 5207 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.006430241962251217
rmse: 0.080188789504838
mae: 0.029157059118814083
r2: 0.7100632200880144
pearson: 0.8447435268569167

=== Experiment 5606 ===
num_layers: 8
units: [512, 512, 256, 256, 128, 128, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005947831692179518
rmse: 0.07712218676995303
mae: 0.028746798364451567
r2: 0.7318148868405502
pearson: 0.8643572842130938

=== Experiment 5183 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006356089598732555
rmse: 0.07972508763703277
mae: 0.0347335120525119
r2: 0.7134067175221199
pearson: 0.8454282495530735

=== Experiment 5502 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.0
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 3
mse: 0.005969881731660166
rmse: 0.07726500974995193
mae: 0.029722126268966424
r2: 0.7308206602653322
pearson: 0.8721784645429048

=== Experiment 5433 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006933501947802651
rmse: 0.08326765246962743
mae: 0.03701812712098208
r2: 0.687371448841159
pearson: 0.8374337397268578

=== Experiment 5860 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.01197670191429602
rmse: 0.10943811910982398
mae: 0.04315013468400086
r2: 0.45997578203402634
pearson: 0.6792491175985194

=== Experiment 5535 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 3
mse: 0.006164655616870963
rmse: 0.07851532090535555
mae: 0.029815987953998785
r2: 0.7220383915077202
pearson: 0.8501614288464888

=== Experiment 5960 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005328592641602041
rmse: 0.07299720982066397
mae: 0.03134819162144823
r2: 0.7597361030831397
pearson: 0.8720725167826018

=== Experiment 5604 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 256
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.007719070259996338
rmse: 0.08785823956804699
mae: 0.0349872710127201
r2: 0.6519505193994011
pearson: 0.8353071420140599

=== Experiment 5374 ===
num_layers: 10
units: [512, 512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005217545413478604
rmse: 0.07223257861573684
mae: 0.02781213696448714
r2: 0.7647431737236026
pearson: 0.8746000591779846

=== Experiment 5661 ===
num_layers: 10
units: [32, 64, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 1
mse: 0.00784910532508616
rmse: 0.08859517664684777
mae: 0.03874501875569092
r2: 0.64608729554731
pearson: 0.8053778220805115

=== Experiment 5040 ===
num_layers: 9
units: [512, 512, 256, 256, 128, 128, 64, 64, 32]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.005731485776412207
rmse: 0.07570657683723526
mae: 0.027487992713448886
r2: 0.7415698289613788
pearson: 0.8617579700200008

=== Experiment 5980 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 256
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006264798973673822
rmse: 0.07915048309185373
mae: 0.030229340167567946
r2: 0.7175229716259414
pearson: 0.8472354630637128

=== Experiment 5736 ===
num_layers: 6
units: [64, 128, 256, 256, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.006842764493418129
rmse: 0.08272100394348542
mae: 0.03538332565360181
r2: 0.6914627607227495
pearson: 0.8321322628398395

=== Experiment 5069 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.4
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 1
mse: 0.006070982779466992
rmse: 0.07791651159713833
mae: 0.031339737509158146
r2: 0.7262620585176965
pearson: 0.8540996669528256

=== Experiment 5962 ===
num_layers: 7
units: [64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.4
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 2
mse: 0.0061782954555416126
rmse: 0.0786021339121376
mae: 0.030091923134658808
r2: 0.7214233771854127
pearson: 0.8497347199142242

=== Experiment 5938 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.2
num_dense_layers: 1
mse: 0.006257784203160504
rmse: 0.07910615780810305
mae: 0.030618525382558014
r2: 0.7178392645409502
pearson: 0.8488596574542462

=== Experiment 5281 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 128
dense_units: 256
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.00561130594370505
rmse: 0.0749086506600209
mae: 0.028514470261065197
r2: 0.746988684722955
pearson: 0.8654430652907142

=== Experiment 5223 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.0
recurrent_dropout: 0.2
batch_size: 128
dense_units: 128
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006050796081822201
rmse: 0.07778686316996078
mae: 0.02873981117756816
r2: 0.727172267829655
pearson: 0.8528842750810968

=== Experiment 5086 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.2
recurrent_dropout: 0.2
batch_size: 64
dense_units: 128
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.005632892567395178
rmse: 0.07505259867183267
mae: 0.02812348594411674
r2: 0.7460153533617666
pearson: 0.8660634364400973

=== Experiment 5991 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 128
dense_units: 64
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 2
mse: 0.006725411946499508
rmse: 0.08200860897795735
mae: 0.03170967041336959
r2: 0.6967541354125041
pearson: 0.8373149055342344

=== Experiment 5232 ===
num_layers: 9
units: [32, 64, 128, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 256
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006443716174657955
rmse: 0.0802727611002509
mae: 0.03539904810083426
r2: 0.7094556737810489
pearson: 0.8455465027816862

=== Experiment 5108 ===
num_layers: 5
units: [64, 128, 256, 512, 512]
dropout: 0.4
recurrent_dropout: 0.2
batch_size: 64
dense_units: 512
dense_activation: relu
dense_dropout: 0.0
num_dense_layers: 3
mse: 0.006536258517148091
rmse: 0.08084713054368776
mae: 0.03263776499081785
r2: 0.7052829802891687
pearson: 0.8517308865436753

=== Experiment 5773 ===
num_layers: 8
units: [32, 64, 128, 256, 256, 512, 512, 512]
dropout: 0.4
recurrent_dropout: 0.4
batch_size: 64
dense_units: 64
dense_activation: relu
dense_dropout: 0.3
num_dense_layers: 2
mse: 0.006271896236354156
rmse: 0.07919530438324078
mae: 0.03269848243483166
r2: 0.7172029591754292
pearson: 0.8485203449923009

